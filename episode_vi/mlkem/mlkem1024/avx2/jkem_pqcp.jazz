/* -------------------------------------------------------------------- */
/* After typing */

param int MLKEM_K = 4;

param int MLKEM_POLYCOMPRESSEDBYTES = 160;

param int MLKEM_POLYVECCOMPRESSEDBYTES = (MLKEM_K * 352);

param int MLKEM_Q = 3329;

param int MLKEM_N = 256;

param int MLKEM_VECN = (MLKEM_K * MLKEM_N);

param int MLKEM_SYMBYTES = 32;

param int MLKEM_SSBYTES = 32;

param int MLKEM_ETA1 = 2;

param int MLKEM_ETA2 = 2;

param int MLKEM_POLYBYTES = 384;

param int MLKEM_POLYVECBYTES = (MLKEM_K * MLKEM_POLYBYTES);

param int MLKEM_INDCPA_MSGBYTES = MLKEM_SYMBYTES;

param int MLKEM_INDCPA_PUBLICKEYBYTES = (MLKEM_POLYVECBYTES + MLKEM_SYMBYTES);

param int MLKEM_INDCPA_SECRETKEYBYTES = MLKEM_POLYVECBYTES;

param int MLKEM_INDCPA_CIPHERTEXTBYTES = (MLKEM_POLYVECCOMPRESSEDBYTES +
                                         MLKEM_POLYCOMPRESSEDBYTES);

param int MLKEM_PUBLICKEYBYTES = MLKEM_INDCPA_PUBLICKEYBYTES;

param int MLKEM_SECRETKEYBYTES = ((MLKEM_INDCPA_SECRETKEYBYTES +
                                  MLKEM_INDCPA_PUBLICKEYBYTES) +
                                 (2 * MLKEM_SYMBYTES));

param int MLKEM_CIPHERTEXTBYTES = MLKEM_INDCPA_CIPHERTEXTBYTES;

inline
fn __shuffle8 (reg u256 a, reg u256 b) -> (reg u256, reg u256) {
  reg u256 r0;
  reg u256 r1;
  
  r0 = #VPERM2I128(a, b, ((8u) 32)); /*  */
  r1 = #VPERM2I128(a, b, ((8u) 49)); /*  */
  return (r0, r1);
}

inline
fn __shuffle4 (reg u256 a, reg u256 b) -> (reg u256, reg u256) {
  reg u256 r0;
  reg u256 r1;
  
  r0 = #VPUNPCKL_4u64(a, b); /*  */
  r1 = #VPUNPCKH_4u64(a, b); /*  */
  return (r0, r1);
}

inline
fn __shuffle2 (reg u256 a, reg u256 b) -> (reg u256, reg u256) {
  reg u256 t0;
  reg u256 t1;
  
  t0 = #VMOVSLDUP_256(b); /*  */
  t0 = #VPBLEND_8u32(a, t0, ((8u) 170)); /*  */
  a = #VPSRL_4u64(a, ((128u) 32)); /*  */
  t1 = #VPBLEND_8u32(a, b, ((8u) 170)); /*  */
  return (t0, t1);
}

inline
fn __shuffle1 (reg u256 a, reg u256 b) -> (reg u256, reg u256) {
  reg u256 r0;
  reg u256 r1;
  reg u256 t0;
  reg u256 t1;
  
  t0 = #VPSLL_8u32(b, ((128u) 16)); /*  */
  r0 = #VPBLEND_16u16(a, t0, ((8u) 170)); /*  */
  t1 = #VPSRL_8u32(a, ((128u) 16)); /*  */
  r1 = #VPBLEND_16u16(t1, b, ((8u) 170)); /*  */
  return (r0, r1);
}

inline
fn __nttunpack128 (reg u256 r0, reg u256 r1, reg u256 r2, reg u256 r3,
                  reg u256 r4, reg u256 r5, reg u256 r6, reg u256 r7) -> 
(reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256,
reg u256) {
  
  #[inline]
  (r0, r4) = __shuffle8(r0, r4);
  #[inline]
  (r1, r5) = __shuffle8(r1, r5);
  #[inline]
  (r2, r6) = __shuffle8(r2, r6);
  #[inline]
  (r3, r7) = __shuffle8(r3, r7);
  #[inline]
  (r0, r2) = __shuffle4(r0, r2);
  #[inline]
  (r4, r6) = __shuffle4(r4, r6);
  #[inline]
  (r1, r3) = __shuffle4(r1, r3);
  #[inline]
  (r5, r7) = __shuffle4(r5, r7);
  #[inline]
  (r0, r1) = __shuffle2(r0, r1);
  #[inline]
  (r2, r3) = __shuffle2(r2, r3);
  #[inline]
  (r4, r5) = __shuffle2(r4, r5);
  #[inline]
  (r6, r7) = __shuffle2(r6, r7);
  #[inline]
  (r0, r4) = __shuffle1(r0, r4);
  #[inline]
  (r1, r5) = __shuffle1(r1, r5);
  #[inline]
  (r2, r6) = __shuffle1(r2, r6);
  #[inline]
  (r3, r7) = __shuffle1(r3, r7);
  return (r0, r4, r1, r5, r2, r6, r3, r7);
}

fn _nttunpack (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N]) {
  reg u256 r0;
  reg u256 r1;
  reg u256 r2;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r6;
  reg u256 r7;
  
  r0 = rp.[#unaligned :u256 (32 * 0)]; /* u256 */
  r1 = rp.[#unaligned :u256 (32 * 1)]; /* u256 */
  r2 = rp.[#unaligned :u256 (32 * 2)]; /* u256 */
  r3 = rp.[#unaligned :u256 (32 * 3)]; /* u256 */
  r4 = rp.[#unaligned :u256 (32 * 4)]; /* u256 */
  r5 = rp.[#unaligned :u256 (32 * 5)]; /* u256 */
  r6 = rp.[#unaligned :u256 (32 * 6)]; /* u256 */
  r7 = rp.[#unaligned :u256 (32 * 7)]; /* u256 */
  #[inline]
  (r0, r1, r2, r3, r4, r5, r6, r7) =
    __nttunpack128(r0, r1, r2, r3, r4, r5, r6, r7);
  rp.[#unaligned :u256 (32 * 0)] = r0; /* u256 */
  rp.[#unaligned :u256 (32 * 1)] = r1; /* u256 */
  rp.[#unaligned :u256 (32 * 2)] = r2; /* u256 */
  rp.[#unaligned :u256 (32 * 3)] = r3; /* u256 */
  rp.[#unaligned :u256 (32 * 4)] = r4; /* u256 */
  rp.[#unaligned :u256 (32 * 5)] = r5; /* u256 */
  rp.[#unaligned :u256 (32 * 6)] = r6; /* u256 */
  rp.[#unaligned :u256 (32 * 7)] = r7; /* u256 */
  r0 = rp.[#unaligned :u256 (32 * 8)]; /* u256 */
  r1 = rp.[#unaligned :u256 (32 * 9)]; /* u256 */
  r2 = rp.[#unaligned :u256 (32 * 10)]; /* u256 */
  r3 = rp.[#unaligned :u256 (32 * 11)]; /* u256 */
  r4 = rp.[#unaligned :u256 (32 * 12)]; /* u256 */
  r5 = rp.[#unaligned :u256 (32 * 13)]; /* u256 */
  r6 = rp.[#unaligned :u256 (32 * 14)]; /* u256 */
  r7 = rp.[#unaligned :u256 (32 * 15)]; /* u256 */
  #[inline]
  (r0, r1, r2, r3, r4, r5, r6, r7) =
    __nttunpack128(r0, r1, r2, r3, r4, r5, r6, r7);
  rp.[#unaligned :u256 (32 * 8)] = r0; /* u256 */
  rp.[#unaligned :u256 (32 * 9)] = r1; /* u256 */
  rp.[#unaligned :u256 (32 * 10)] = r2; /* u256 */
  rp.[#unaligned :u256 (32 * 11)] = r3; /* u256 */
  rp.[#unaligned :u256 (32 * 12)] = r4; /* u256 */
  rp.[#unaligned :u256 (32 * 13)] = r5; /* u256 */
  rp.[#unaligned :u256 (32 * 14)] = r6; /* u256 */
  rp.[#unaligned :u256 (32 * 15)] = r7; /* u256 */
  return (rp);
}

u16[128] jzetas = {((16u) 2285), ((16u) 2571), ((16u) 2970), ((16u) 1812),
                   ((16u) 1493), ((16u) 1422), ((16u) 287), ((16u) 202),
                   ((16u) 3158), ((16u) 622), ((16u) 1577), ((16u) 182),
                   ((16u) 962), ((16u) 2127), ((16u) 1855), ((16u) 1468),
                   ((16u) 573), ((16u) 2004), ((16u) 264), ((16u) 383),
                   ((16u) 2500), ((16u) 1458), ((16u) 1727), ((16u) 3199),
                   ((16u) 2648), ((16u) 1017), ((16u) 732), ((16u) 608),
                   ((16u) 1787), ((16u) 411), ((16u) 3124), ((16u) 1758),
                   ((16u) 1223), ((16u) 652), ((16u) 2777), ((16u) 1015),
                   ((16u) 2036), ((16u) 1491), ((16u) 3047), ((16u) 1785),
                   ((16u) 516), ((16u) 3321), ((16u) 3009), ((16u) 2663),
                   ((16u) 1711), ((16u) 2167), ((16u) 126), ((16u) 1469),
                   ((16u) 2476), ((16u) 3239), ((16u) 3058), ((16u) 830),
                   ((16u) 107), ((16u) 1908), ((16u) 3082), ((16u) 2378),
                   ((16u) 2931), ((16u) 961), ((16u) 1821), ((16u) 2604),
                   ((16u) 448), ((16u) 2264), ((16u) 677), ((16u) 2054),
                   ((16u) 2226), ((16u) 430), ((16u) 555), ((16u) 843),
                   ((16u) 2078), ((16u) 871), ((16u) 1550), ((16u) 105),
                   ((16u) 422), ((16u) 587), ((16u) 177), ((16u) 3094),
                   ((16u) 3038), ((16u) 2869), ((16u) 1574), ((16u) 1653),
                   ((16u) 3083), ((16u) 778), ((16u) 1159), ((16u) 3182),
                   ((16u) 2552), ((16u) 1483), ((16u) 2727), ((16u) 1119),
                   ((16u) 1739), ((16u) 644), ((16u) 2457), ((16u) 349),
                   ((16u) 418), ((16u) 329), ((16u) 3173), ((16u) 3254),
                   ((16u) 817), ((16u) 1097), ((16u) 603), ((16u) 610),
                   ((16u) 1322), ((16u) 2044), ((16u) 1864), ((16u) 384),
                   ((16u) 2114), ((16u) 3193), ((16u) 1218), ((16u) 1994),
                   ((16u) 2455), ((16u) 220), ((16u) 2142), ((16u) 1670),
                   ((16u) 2144), ((16u) 1799), ((16u) 2051), ((16u) 794),
                   ((16u) 1819), ((16u) 2475), ((16u) 2459), ((16u) 478),
                   ((16u) 3221), ((16u) 3021), ((16u) 996), ((16u) 991),
                   ((16u) 958), ((16u) 1869), ((16u) 1522), ((16u) 1628)};

u16[128] jzetas_inv = {((16u) 1701), ((16u) 1807), ((16u) 1460),
                       ((16u) 2371), ((16u) 2338), ((16u) 2333), ((16u) 308),
                       ((16u) 108), ((16u) 2851), ((16u) 870), ((16u) 854),
                       ((16u) 1510), ((16u) 2535), ((16u) 1278),
                       ((16u) 1530), ((16u) 1185), ((16u) 1659),
                       ((16u) 1187), ((16u) 3109), ((16u) 874), ((16u) 1335),
                       ((16u) 2111), ((16u) 136), ((16u) 1215), ((16u) 2945),
                       ((16u) 1465), ((16u) 1285), ((16u) 2007),
                       ((16u) 2719), ((16u) 2726), ((16u) 2232),
                       ((16u) 2512), ((16u) 75), ((16u) 156), ((16u) 3000),
                       ((16u) 2911), ((16u) 2980), ((16u) 872), ((16u) 2685),
                       ((16u) 1590), ((16u) 2210), ((16u) 602), ((16u) 1846),
                       ((16u) 777), ((16u) 147), ((16u) 2170), ((16u) 2551),
                       ((16u) 246), ((16u) 1676), ((16u) 1755), ((16u) 460),
                       ((16u) 291), ((16u) 235), ((16u) 3152), ((16u) 2742),
                       ((16u) 2907), ((16u) 3224), ((16u) 1779),
                       ((16u) 2458), ((16u) 1251), ((16u) 2486),
                       ((16u) 2774), ((16u) 2899), ((16u) 1103),
                       ((16u) 1275), ((16u) 2652), ((16u) 1065),
                       ((16u) 2881), ((16u) 725), ((16u) 1508), ((16u) 2368),
                       ((16u) 398), ((16u) 951), ((16u) 247), ((16u) 1421),
                       ((16u) 3222), ((16u) 2499), ((16u) 271), ((16u) 90),
                       ((16u) 853), ((16u) 1860), ((16u) 3203), ((16u) 1162),
                       ((16u) 1618), ((16u) 666), ((16u) 320), ((16u) 8),
                       ((16u) 2813), ((16u) 1544), ((16u) 282), ((16u) 1838),
                       ((16u) 1293), ((16u) 2314), ((16u) 552), ((16u) 2677),
                       ((16u) 2106), ((16u) 1571), ((16u) 205), ((16u) 2918),
                       ((16u) 1542), ((16u) 2721), ((16u) 2597),
                       ((16u) 2312), ((16u) 681), ((16u) 130), ((16u) 1602),
                       ((16u) 1871), ((16u) 829), ((16u) 2946), ((16u) 3065),
                       ((16u) 1325), ((16u) 2756), ((16u) 1861),
                       ((16u) 1474), ((16u) 1202), ((16u) 2367),
                       ((16u) 3147), ((16u) 1752), ((16u) 2707), ((16u) 171),
                       ((16u) 3127), ((16u) 3042), ((16u) 1907),
                       ((16u) 1836), ((16u) 1517), ((16u) 359), ((16u) 758),
                       ((16u) 1441)};

u16[400] jzetas_exp = {((16u) 31499), ((16u) 31499), ((16u) 2571),
                       ((16u) 2571), ((16u) 14746), ((16u) 14746),
                       ((16u) 2970), ((16u) 2970), ((16u) 13525),
                       ((16u) 13525), ((16u) 13525), ((16u) 13525),
                       ((16u) 13525), ((16u) 13525), ((16u) 13525),
                       ((16u) 13525), ((16u) 53134), ((16u) 53134),
                       ((16u) 53134), ((16u) 53134), ((16u) 53134),
                       ((16u) 53134), ((16u) 53134), ((16u) 53134),
                       ((16u) 1493), ((16u) 1493), ((16u) 1493),
                       ((16u) 1493), ((16u) 1493), ((16u) 1493),
                       ((16u) 1493), ((16u) 1493), ((16u) 1422),
                       ((16u) 1422), ((16u) 1422), ((16u) 1422),
                       ((16u) 1422), ((16u) 1422), ((16u) 1422),
                       ((16u) 1422), ((16u) 44630), ((16u) 44630),
                       ((16u) 44630), ((16u) 44630), ((16u) 27758),
                       ((16u) 27758), ((16u) 27758), ((16u) 27758),
                       ((16u) 61737), ((16u) 61737), ((16u) 61737),
                       ((16u) 61737), ((16u) 49846), ((16u) 49846),
                       ((16u) 49846), ((16u) 49846), ((16u) 3158),
                       ((16u) 3158), ((16u) 3158), ((16u) 3158), ((16u) 622),
                       ((16u) 622), ((16u) 622), ((16u) 622), ((16u) 1577),
                       ((16u) 1577), ((16u) 1577), ((16u) 1577), ((16u) 182),
                       ((16u) 182), ((16u) 182), ((16u) 182), ((16u) 59709),
                       ((16u) 59709), ((16u) 17364), ((16u) 17364),
                       ((16u) 39176), ((16u) 39176), ((16u) 36479),
                       ((16u) 36479), ((16u) 5572), ((16u) 5572),
                       ((16u) 64434), ((16u) 64434), ((16u) 21439),
                       ((16u) 21439), ((16u) 39295), ((16u) 39295),
                       ((16u) 573), ((16u) 573), ((16u) 2004), ((16u) 2004),
                       ((16u) 264), ((16u) 264), ((16u) 383), ((16u) 383),
                       ((16u) 2500), ((16u) 2500), ((16u) 1458),
                       ((16u) 1458), ((16u) 1727), ((16u) 1727),
                       ((16u) 3199), ((16u) 3199), ((16u) 59847),
                       ((16u) 59020), ((16u) 1497), ((16u) 30967),
                       ((16u) 41972), ((16u) 20179), ((16u) 20711),
                       ((16u) 25081), ((16u) 52740), ((16u) 26617),
                       ((16u) 16065), ((16u) 53095), ((16u) 9135),
                       ((16u) 64887), ((16u) 39550), ((16u) 27837),
                       ((16u) 1223), ((16u) 652), ((16u) 2777), ((16u) 1015),
                       ((16u) 2036), ((16u) 1491), ((16u) 3047),
                       ((16u) 1785), ((16u) 516), ((16u) 3321), ((16u) 3009),
                       ((16u) 2663), ((16u) 1711), ((16u) 2167), ((16u) 126),
                       ((16u) 1469), ((16u) 65202), ((16u) 54059),
                       ((16u) 33310), ((16u) 20494), ((16u) 37798),
                       ((16u) 945), ((16u) 50654), ((16u) 6182),
                       ((16u) 32011), ((16u) 10631), ((16u) 29176),
                       ((16u) 36775), ((16u) 47051), ((16u) 17561),
                       ((16u) 51106), ((16u) 60261), ((16u) 2226),
                       ((16u) 555), ((16u) 2078), ((16u) 1550), ((16u) 422),
                       ((16u) 177), ((16u) 3038), ((16u) 1574), ((16u) 3083),
                       ((16u) 1159), ((16u) 2552), ((16u) 2727),
                       ((16u) 1739), ((16u) 2457), ((16u) 418), ((16u) 3173),
                       ((16u) 11182), ((16u) 13387), ((16u) 51303),
                       ((16u) 43881), ((16u) 13131), ((16u) 60950),
                       ((16u) 23093), ((16u) 5493), ((16u) 33034),
                       ((16u) 30318), ((16u) 46795), ((16u) 12639),
                       ((16u) 20100), ((16u) 18525), ((16u) 19529),
                       ((16u) 52918), ((16u) 430), ((16u) 843), ((16u) 871),
                       ((16u) 105), ((16u) 587), ((16u) 3094), ((16u) 2869),
                       ((16u) 1653), ((16u) 778), ((16u) 3182), ((16u) 1483),
                       ((16u) 1119), ((16u) 644), ((16u) 349), ((16u) 329),
                       ((16u) 3254), ((16u) 788), ((16u) 788), ((16u) 1812),
                       ((16u) 1812), ((16u) 28191), ((16u) 28191),
                       ((16u) 28191), ((16u) 28191), ((16u) 28191),
                       ((16u) 28191), ((16u) 28191), ((16u) 28191),
                       ((16u) 48842), ((16u) 48842), ((16u) 48842),
                       ((16u) 48842), ((16u) 48842), ((16u) 48842),
                       ((16u) 48842), ((16u) 48842), ((16u) 287),
                       ((16u) 287), ((16u) 287), ((16u) 287), ((16u) 287),
                       ((16u) 287), ((16u) 287), ((16u) 287), ((16u) 202),
                       ((16u) 202), ((16u) 202), ((16u) 202), ((16u) 202),
                       ((16u) 202), ((16u) 202), ((16u) 202), ((16u) 10690),
                       ((16u) 10690), ((16u) 10690), ((16u) 10690),
                       ((16u) 1359), ((16u) 1359), ((16u) 1359),
                       ((16u) 1359), ((16u) 54335), ((16u) 54335),
                       ((16u) 54335), ((16u) 54335), ((16u) 31164),
                       ((16u) 31164), ((16u) 31164), ((16u) 31164),
                       ((16u) 962), ((16u) 962), ((16u) 962), ((16u) 962),
                       ((16u) 2127), ((16u) 2127), ((16u) 2127),
                       ((16u) 2127), ((16u) 1855), ((16u) 1855),
                       ((16u) 1855), ((16u) 1855), ((16u) 1468),
                       ((16u) 1468), ((16u) 1468), ((16u) 1468),
                       ((16u) 37464), ((16u) 37464), ((16u) 24313),
                       ((16u) 24313), ((16u) 55004), ((16u) 55004),
                       ((16u) 8800), ((16u) 8800), ((16u) 18427),
                       ((16u) 18427), ((16u) 8859), ((16u) 8859),
                       ((16u) 26676), ((16u) 26676), ((16u) 49374),
                       ((16u) 49374), ((16u) 2648), ((16u) 2648),
                       ((16u) 1017), ((16u) 1017), ((16u) 732), ((16u) 732),
                       ((16u) 608), ((16u) 608), ((16u) 1787), ((16u) 1787),
                       ((16u) 411), ((16u) 411), ((16u) 3124), ((16u) 3124),
                       ((16u) 1758), ((16u) 1758), ((16u) 19884),
                       ((16u) 37287), ((16u) 49650), ((16u) 56638),
                       ((16u) 37227), ((16u) 9076), ((16u) 35338),
                       ((16u) 18250), ((16u) 13427), ((16u) 14017),
                       ((16u) 36381), ((16u) 52780), ((16u) 16832),
                       ((16u) 4312), ((16u) 41381), ((16u) 47622),
                       ((16u) 2476), ((16u) 3239), ((16u) 3058), ((16u) 830),
                       ((16u) 107), ((16u) 1908), ((16u) 3082), ((16u) 2378),
                       ((16u) 2931), ((16u) 961), ((16u) 1821), ((16u) 2604),
                       ((16u) 448), ((16u) 2264), ((16u) 677), ((16u) 2054),
                       ((16u) 34353), ((16u) 25435), ((16u) 58154),
                       ((16u) 24392), ((16u) 44610), ((16u) 10946),
                       ((16u) 24215), ((16u) 16990), ((16u) 10336),
                       ((16u) 57603), ((16u) 43035), ((16u) 10907),
                       ((16u) 31637), ((16u) 28644), ((16u) 23998),
                       ((16u) 48114), ((16u) 817), ((16u) 603), ((16u) 1322),
                       ((16u) 1864), ((16u) 2114), ((16u) 1218),
                       ((16u) 2455), ((16u) 2142), ((16u) 2144),
                       ((16u) 2051), ((16u) 1819), ((16u) 2459),
                       ((16u) 3221), ((16u) 996), ((16u) 958), ((16u) 1522),
                       ((16u) 20297), ((16u) 2146), ((16u) 15356),
                       ((16u) 33152), ((16u) 59257), ((16u) 50634),
                       ((16u) 54492), ((16u) 14470), ((16u) 44039),
                       ((16u) 45338), ((16u) 23211), ((16u) 48094),
                       ((16u) 41677), ((16u) 45279), ((16u) 7757),
                       ((16u) 23132), ((16u) 1097), ((16u) 610),
                       ((16u) 2044), ((16u) 384), ((16u) 3193), ((16u) 1994),
                       ((16u) 220), ((16u) 1670), ((16u) 1799), ((16u) 794),
                       ((16u) 2475), ((16u) 478), ((16u) 3021), ((16u) 991),
                       ((16u) 1869), ((16u) 1628), ((16u) 0), ((16u) 0),
                       ((16u) 0), ((16u) 0)};

u16[400] jzetas_inv_exp = {((16u) 42405), ((16u) 57780), ((16u) 20258),
                           ((16u) 23860), ((16u) 17443), ((16u) 42326),
                           ((16u) 20199), ((16u) 21498), ((16u) 51067),
                           ((16u) 11045), ((16u) 14903), ((16u) 6280),
                           ((16u) 32385), ((16u) 50181), ((16u) 63391),
                           ((16u) 45240), ((16u) 1701), ((16u) 1460),
                           ((16u) 2338), ((16u) 308), ((16u) 2851),
                           ((16u) 854), ((16u) 2535), ((16u) 1530),
                           ((16u) 1659), ((16u) 3109), ((16u) 1335),
                           ((16u) 136), ((16u) 2945), ((16u) 1285),
                           ((16u) 2719), ((16u) 2232), ((16u) 17423),
                           ((16u) 41539), ((16u) 36893), ((16u) 33900),
                           ((16u) 54630), ((16u) 22502), ((16u) 7934),
                           ((16u) 55201), ((16u) 48547), ((16u) 41322),
                           ((16u) 54591), ((16u) 20927), ((16u) 41145),
                           ((16u) 7383), ((16u) 40102), ((16u) 31184),
                           ((16u) 1807), ((16u) 2371), ((16u) 2333),
                           ((16u) 108), ((16u) 870), ((16u) 1510),
                           ((16u) 1278), ((16u) 1185), ((16u) 1187),
                           ((16u) 874), ((16u) 2111), ((16u) 1215),
                           ((16u) 1465), ((16u) 2007), ((16u) 2726),
                           ((16u) 2512), ((16u) 17915), ((16u) 24156),
                           ((16u) 61225), ((16u) 48705), ((16u) 12757),
                           ((16u) 29156), ((16u) 51520), ((16u) 52110),
                           ((16u) 47287), ((16u) 30199), ((16u) 56461),
                           ((16u) 28310), ((16u) 8899), ((16u) 15887),
                           ((16u) 28250), ((16u) 45653), ((16u) 1275),
                           ((16u) 2652), ((16u) 1065), ((16u) 2881),
                           ((16u) 725), ((16u) 1508), ((16u) 2368),
                           ((16u) 398), ((16u) 951), ((16u) 247),
                           ((16u) 1421), ((16u) 3222), ((16u) 2499),
                           ((16u) 271), ((16u) 90), ((16u) 853),
                           ((16u) 16163), ((16u) 16163), ((16u) 38861),
                           ((16u) 38861), ((16u) 56678), ((16u) 56678),
                           ((16u) 47110), ((16u) 47110), ((16u) 56737),
                           ((16u) 56737), ((16u) 10533), ((16u) 10533),
                           ((16u) 41224), ((16u) 41224), ((16u) 28073),
                           ((16u) 28073), ((16u) 1571), ((16u) 1571),
                           ((16u) 205), ((16u) 205), ((16u) 2918),
                           ((16u) 2918), ((16u) 1542), ((16u) 1542),
                           ((16u) 2721), ((16u) 2721), ((16u) 2597),
                           ((16u) 2597), ((16u) 2312), ((16u) 2312),
                           ((16u) 681), ((16u) 681), ((16u) 34373),
                           ((16u) 34373), ((16u) 34373), ((16u) 34373),
                           ((16u) 11202), ((16u) 11202), ((16u) 11202),
                           ((16u) 11202), ((16u) 64178), ((16u) 64178),
                           ((16u) 64178), ((16u) 64178), ((16u) 54847),
                           ((16u) 54847), ((16u) 54847), ((16u) 54847),
                           ((16u) 1861), ((16u) 1861), ((16u) 1861),
                           ((16u) 1861), ((16u) 1474), ((16u) 1474),
                           ((16u) 1474), ((16u) 1474), ((16u) 1202),
                           ((16u) 1202), ((16u) 1202), ((16u) 1202),
                           ((16u) 2367), ((16u) 2367), ((16u) 2367),
                           ((16u) 2367), ((16u) 16695), ((16u) 16695),
                           ((16u) 16695), ((16u) 16695), ((16u) 16695),
                           ((16u) 16695), ((16u) 16695), ((16u) 16695),
                           ((16u) 37346), ((16u) 37346), ((16u) 37346),
                           ((16u) 37346), ((16u) 37346), ((16u) 37346),
                           ((16u) 37346), ((16u) 37346), ((16u) 3127),
                           ((16u) 3127), ((16u) 3127), ((16u) 3127),
                           ((16u) 3127), ((16u) 3127), ((16u) 3127),
                           ((16u) 3127), ((16u) 3042), ((16u) 3042),
                           ((16u) 3042), ((16u) 3042), ((16u) 3042),
                           ((16u) 3042), ((16u) 3042), ((16u) 3042),
                           ((16u) 64749), ((16u) 64749), ((16u) 1517),
                           ((16u) 1517), ((16u) 12619), ((16u) 46008),
                           ((16u) 47012), ((16u) 45437), ((16u) 52898),
                           ((16u) 18742), ((16u) 35219), ((16u) 32503),
                           ((16u) 60044), ((16u) 42444), ((16u) 4587),
                           ((16u) 52406), ((16u) 21656), ((16u) 14234),
                           ((16u) 52150), ((16u) 54355), ((16u) 75),
                           ((16u) 3000), ((16u) 2980), ((16u) 2685),
                           ((16u) 2210), ((16u) 1846), ((16u) 147),
                           ((16u) 2551), ((16u) 1676), ((16u) 460),
                           ((16u) 235), ((16u) 2742), ((16u) 3224),
                           ((16u) 2458), ((16u) 2486), ((16u) 2899),
                           ((16u) 5276), ((16u) 14431), ((16u) 47976),
                           ((16u) 18486), ((16u) 28762), ((16u) 36361),
                           ((16u) 54906), ((16u) 33526), ((16u) 59355),
                           ((16u) 14883), ((16u) 64592), ((16u) 27739),
                           ((16u) 45043), ((16u) 32227), ((16u) 11478),
                           ((16u) 335), ((16u) 156), ((16u) 2911),
                           ((16u) 872), ((16u) 1590), ((16u) 602),
                           ((16u) 777), ((16u) 2170), ((16u) 246),
                           ((16u) 1755), ((16u) 291), ((16u) 3152),
                           ((16u) 2907), ((16u) 1779), ((16u) 1251),
                           ((16u) 2774), ((16u) 1103), ((16u) 37700),
                           ((16u) 25987), ((16u) 650), ((16u) 56402),
                           ((16u) 12442), ((16u) 49472), ((16u) 38920),
                           ((16u) 12797), ((16u) 40456), ((16u) 44826),
                           ((16u) 45358), ((16u) 23565), ((16u) 34570),
                           ((16u) 64040), ((16u) 6517), ((16u) 5690),
                           ((16u) 1860), ((16u) 3203), ((16u) 1162),
                           ((16u) 1618), ((16u) 666), ((16u) 320), ((16u) 8),
                           ((16u) 2813), ((16u) 1544), ((16u) 282),
                           ((16u) 1838), ((16u) 1293), ((16u) 2314),
                           ((16u) 552), ((16u) 2677), ((16u) 2106),
                           ((16u) 26242), ((16u) 26242), ((16u) 44098),
                           ((16u) 44098), ((16u) 1103), ((16u) 1103),
                           ((16u) 59965), ((16u) 59965), ((16u) 29058),
                           ((16u) 29058), ((16u) 26361), ((16u) 26361),
                           ((16u) 48173), ((16u) 48173), ((16u) 5828),
                           ((16u) 5828), ((16u) 130), ((16u) 130),
                           ((16u) 1602), ((16u) 1602), ((16u) 1871),
                           ((16u) 1871), ((16u) 829), ((16u) 829),
                           ((16u) 2946), ((16u) 2946), ((16u) 3065),
                           ((16u) 3065), ((16u) 1325), ((16u) 1325),
                           ((16u) 2756), ((16u) 2756), ((16u) 15691),
                           ((16u) 15691), ((16u) 15691), ((16u) 15691),
                           ((16u) 3800), ((16u) 3800), ((16u) 3800),
                           ((16u) 3800), ((16u) 37779), ((16u) 37779),
                           ((16u) 37779), ((16u) 37779), ((16u) 20907),
                           ((16u) 20907), ((16u) 20907), ((16u) 20907),
                           ((16u) 3147), ((16u) 3147), ((16u) 3147),
                           ((16u) 3147), ((16u) 1752), ((16u) 1752),
                           ((16u) 1752), ((16u) 1752), ((16u) 2707),
                           ((16u) 2707), ((16u) 2707), ((16u) 2707),
                           ((16u) 171), ((16u) 171), ((16u) 171),
                           ((16u) 171), ((16u) 12403), ((16u) 12403),
                           ((16u) 12403), ((16u) 12403), ((16u) 12403),
                           ((16u) 12403), ((16u) 12403), ((16u) 12403),
                           ((16u) 52012), ((16u) 52012), ((16u) 52012),
                           ((16u) 52012), ((16u) 52012), ((16u) 52012),
                           ((16u) 52012), ((16u) 52012), ((16u) 1907),
                           ((16u) 1907), ((16u) 1907), ((16u) 1907),
                           ((16u) 1907), ((16u) 1907), ((16u) 1907),
                           ((16u) 1907), ((16u) 1836), ((16u) 1836),
                           ((16u) 1836), ((16u) 1836), ((16u) 1836),
                           ((16u) 1836), ((16u) 1836), ((16u) 1836),
                           ((16u) 50791), ((16u) 50791), ((16u) 359),
                           ((16u) 359), ((16u) 60300), ((16u) 60300),
                           ((16u) 1932), ((16u) 1932), ((16u) 0), ((16u) 0),
                           ((16u) 0), ((16u) 0)};

u16[16] jqx16 = {((16u) MLKEM_Q), ((16u) MLKEM_Q), ((16u) MLKEM_Q),
                 ((16u) MLKEM_Q), ((16u) MLKEM_Q), ((16u) MLKEM_Q),
                 ((16u) MLKEM_Q), ((16u) MLKEM_Q), ((16u) MLKEM_Q),
                 ((16u) MLKEM_Q), ((16u) MLKEM_Q), ((16u) MLKEM_Q),
                 ((16u) MLKEM_Q), ((16u) MLKEM_Q), ((16u) MLKEM_Q),
                 ((16u) MLKEM_Q)};

u16[16] jqinvx16 = {((16u) 62209), ((16u) 62209), ((16u) 62209),
                    ((16u) 62209), ((16u) 62209), ((16u) 62209),
                    ((16u) 62209), ((16u) 62209), ((16u) 62209),
                    ((16u) 62209), ((16u) 62209), ((16u) 62209),
                    ((16u) 62209), ((16u) 62209), ((16u) 62209),
                    ((16u) 62209)};

u16[16] jvx16 = {((16u) 20159), ((16u) 20159), ((16u) 20159), ((16u) 20159),
                 ((16u) 20159), ((16u) 20159), ((16u) 20159), ((16u) 20159),
                 ((16u) 20159), ((16u) 20159), ((16u) 20159), ((16u) 20159),
                 ((16u) 20159), ((16u) 20159), ((16u) 20159), ((16u) 20159)};

u16[16] jfhix16 = {((16u) 1441), ((16u) 1441), ((16u) 1441), ((16u) 1441),
                   ((16u) 1441), ((16u) 1441), ((16u) 1441), ((16u) 1441),
                   ((16u) 1441), ((16u) 1441), ((16u) 1441), ((16u) 1441),
                   ((16u) 1441), ((16u) 1441), ((16u) 1441), ((16u) 1441)};

u16[16] jflox16 = {((16u) 55457), ((16u) 55457), ((16u) 55457),
                   ((16u) 55457), ((16u) 55457), ((16u) 55457),
                   ((16u) 55457), ((16u) 55457), ((16u) 55457),
                   ((16u) 55457), ((16u) 55457), ((16u) 55457),
                   ((16u) 55457), ((16u) 55457), ((16u) 55457), ((16u) 55457)};

u16[16] maskx16 = {((16u) 4095), ((16u) 4095), ((16u) 4095), ((16u) 4095),
                   ((16u) 4095), ((16u) 4095), ((16u) 4095), ((16u) 4095),
                   ((16u) 4095), ((16u) 4095), ((16u) 4095), ((16u) 4095),
                   ((16u) 4095), ((16u) 4095), ((16u) 4095), ((16u) 4095)};

u16[16] hqx16_p1 = {((16u) 1665), ((16u) 1665), ((16u) 1665), ((16u) 1665),
                    ((16u) 1665), ((16u) 1665), ((16u) 1665), ((16u) 1665),
                    ((16u) 1665), ((16u) 1665), ((16u) 1665), ((16u) 1665),
                    ((16u) 1665), ((16u) 1665), ((16u) 1665), ((16u) 1665)};

u16[16] hqx16_m1 = {((16u) 1664), ((16u) 1664), ((16u) 1664), ((16u) 1664),
                    ((16u) 1664), ((16u) 1664), ((16u) 1664), ((16u) 1664),
                    ((16u) 1664), ((16u) 1664), ((16u) 1664), ((16u) 1664),
                    ((16u) 1664), ((16u) 1664), ((16u) 1664), ((16u) 1664)};

u16[16] hhqx16 = {((16u) 832), ((16u) 832), ((16u) 832), ((16u) 832),
                  ((16u) 832), ((16u) 832), ((16u) 832), ((16u) 832),
                  ((16u) 832), ((16u) 832), ((16u) 832), ((16u) 832),
                  ((16u) 832), ((16u) 832), ((16u) 832), ((16u) 832)};

u16[16] mqinvx16 = {((16u) 80635), ((16u) 80635), ((16u) 80635),
                    ((16u) 80635), ((16u) 80635), ((16u) 80635),
                    ((16u) 80635), ((16u) 80635), ((16u) 80635),
                    ((16u) 80635), ((16u) 80635), ((16u) 80635),
                    ((16u) 80635), ((16u) 80635), ((16u) 80635),
                    ((16u) 80635)};

u16[16] jdmontx16 = {((16u) 1353), ((16u) 1353), ((16u) 1353), ((16u) 1353),
                     ((16u) 1353), ((16u) 1353), ((16u) 1353), ((16u) 1353),
                     ((16u) 1353), ((16u) 1353), ((16u) 1353), ((16u) 1353),
                     ((16u) 1353), ((16u) 1353), ((16u) 1353), ((16u) 1353)};

param int QINV = 62209;

param int MONT = 2285;

param int BARR = 20159;

inline
fn __csubq (reg u256 r, reg u256 qx16) -> (reg u256) {
  reg u256 t;
  
  r = #VPSUB_16u16(r, qx16); /*  */
  t = #VPSRA_16u16(r, ((128u) 15)); /*  */
  t = #VPAND_256(t, qx16); /*  */
  r = #VPADD_16u16(t, r); /*  */
  return (r);
}

inline
fn __red16x (reg u256 r, reg u256 qx16, reg u256 vx16) -> (reg u256) {
  reg u256 x;
  
  x = #VPMULH_16u16(r, vx16); /*  */
  x = #VPSRA_16u16(x, ((128u) 10)); /*  */
  x = #VPMULL_16u16(x, qx16); /*  */
  r = #VPSUB_16u16(r, x); /*  */
  return (r);
}

inline
fn __fqmulprecomp16x (reg u256 b, reg u256 al, reg u256 ah, reg u256 qx16) -> 
(reg u256) {
  reg u256 x;
  
  x = #VPMULL_16u16(al, b); /*  */
  b = #VPMULH_16u16(ah, b); /*  */
  x = #VPMULH_16u16(x, qx16); /*  */
  b = #VPSUB_16u16(b, x); /*  */
  return (b);
}

inline
fn __fqmulx16 (reg u256 a, reg u256 b, reg u256 qx16, reg u256 qinvx16) -> 
(reg u256) {
  reg u256 rd;
  reg u256 rhi;
  reg u256 rlo;
  
  rhi = #VPMULH_16u16(a, b); /*  */
  rlo = #VPMULL_16u16(a, b); /*  */
  rlo = #VPMULL_16u16(rlo, qinvx16); /*  */
  rlo = #VPMULH_16u16(rlo, qx16); /*  */
  rd = #VPSUB_16u16(rhi, rlo); /*  */
  return (rd);
}

param int KECCAK_ROUNDS = 24;

u64[24] KECCAK1600_RC = {((64u) 1), ((64u) 32898),
                         ((64u) 9223372036854808714),
                         ((64u) 9223372039002292224), ((64u) 32907),
                         ((64u) 2147483649), ((64u) 9223372039002292353),
                         ((64u) 9223372036854808585), ((64u) 138),
                         ((64u) 136), ((64u) 2147516425), ((64u) 2147483658),
                         ((64u) 2147516555), ((64u) 9223372036854775947),
                         ((64u) 9223372036854808713),
                         ((64u) 9223372036854808579),
                         ((64u) 9223372036854808578),
                         ((64u) 9223372036854775936), ((64u) 32778),
                         ((64u) 9223372039002259466),
                         ((64u) 9223372039002292353),
                         ((64u) 9223372036854808704), ((64u) 2147483649),
                         ((64u) 9223372039002292232)};

inline
fn keccakf1600_index (inline int x, inline int y) -> (inline int) {
  inline int r;
  
  r = ((x % 5) + (5 * (y % 5))); /* int:i */
  return (r);
}

inline
fn keccakf1600_rho_offsets (inline int i) -> (inline int) {
  inline int r;
  inline int x;
  inline int y;
  inline int t;
  inline int z;
  
  r = 0; /* int:i */
  x = 1; /* int:i */
  y = 0; /* int:i */
  for t = 0 to 24 {
    if (i == (x + (5 * y))) {
      r = ((((t + 1) * (t + 2)) / 2) % 64); /* int:i */
    }
    z = (((2 * x) + (3 * y)) % 5); /* int:i */
    x = y; /* int:i */
    y = z; /* int:i */
  }
  return (r);
}

inline
fn keccakf1600_rhotates (inline int x, inline int y) -> (inline int) {
  inline int r;
  inline int i;
  
  #[inline]
  i = keccakf1600_index(x, y);
  #[inline]
  r = keccakf1600_rho_offsets(i);
  return (r);
}

u256[6] KECCAK_RHOTATES_LEFT = {(4u64)[41, 36, 18, 3], (4u64)[27, 28, 62, 1],
                                (4u64)[39, 56, 6, 45], (4u64)[8, 55, 61, 10],
                                (4u64)[20, 25, 15, 2], (4u64)[14, 21, 43, 44]};

u256[6] KECCAK_RHOTATES_RIGHT = {(4u64)[(64 - 41), (64 - 36), (64 - 18),
                                 (64 - 3)],
                                 (4u64)[(64 - 27), (64 - 28), (64 - 62),
                                 (64 - 1)],
                                 (4u64)[(64 - 39), (64 - 56), (64 - 6),
                                 (64 - 45)],
                                 (4u64)[(64 - 8), (64 - 55), (64 - 61),
                                 (64 - 10)],
                                 (4u64)[(64 - 20), (64 - 25), (64 - 15),
                                 (64 - 2)],
                                 (4u64)[(64 - 14), (64 - 21), (64 - 43),
                                 (64 - 44)]};

inline
fn __keccakf1600_pround_avx2 (reg u256[7] state) -> (reg u256[7]) {
  reg u256 c00;
  reg u256 c14;
  reg u256 t2;
  reg u256 t4;
  reg u256 t0;
  reg u256 t1;
  reg u256 d14;
  reg u256 d00;
  reg u256 t3;
  reg u256 t5;
  reg u256 t6;
  reg u256 t7;
  reg u256 t8;
  
  c00 = #VPSHUFD_256(state[2], (4u2)[1, 0, 3, 2]); /*  */
  c14 = (state[5] ^256u state[3]); /* u256 */
  t2 = (state[4] ^256u state[6]); /* u256 */
  c14 = (c14 ^256u state[1]); /* u256 */
  c14 = (c14 ^256u t2); /* u256 */
  t4 = #VPERMQ(c14, (4u2)[2, 1, 0, 3]); /*  */
  c00 = (c00 ^256u state[2]); /* u256 */
  t0 = #VPERMQ(c00, (4u2)[1, 0, 3, 2]); /*  */
  t1 = (c14 >>4u64 ((128u) 63)); /* u256 */
  t2 = (c14 +4u64 c14); /* u256 */
  t1 = (t1 |256u t2); /* u256 */
  d14 = #VPERMQ(t1, (4u2)[0, 3, 2, 1]); /*  */
  d00 = (t1 ^256u t4); /* u256 */
  d00 = #VPERMQ(d00, (4u2)[0, 0, 0, 0]); /*  */
  c00 = (c00 ^256u state[0]); /* u256 */
  c00 = (c00 ^256u t0); /* u256 */
  t0 = (c00 >>4u64 ((128u) 63)); /* u256 */
  t1 = (c00 +4u64 c00); /* u256 */
  t1 = (t1 |256u t0); /* u256 */
  state[2] = (state[2] ^256u d00); /* u256 */
  state[0] = (state[0] ^256u d00); /* u256 */
  d14 = #VPBLEND_8u32(d14, t1, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  t4 = #VPBLEND_8u32(t4, c00, (8u1)[0, 0, 0, 0, 0, 0, 1, 1]); /*  */
  d14 = (d14 ^256u t4); /* u256 */
  t3 = #VPSLLV_4u64(state[2], /* global: */ KECCAK_RHOTATES_LEFT[0]); /*  */
  state[2] =
    #VPSRLV_4u64(state[2], /* global: */ KECCAK_RHOTATES_RIGHT[0]); /*  */
  state[2] = (state[2] |256u t3); /* u256 */
  state[3] = (state[3] ^256u d14); /* u256 */
  t4 = #VPSLLV_4u64(state[3], /* global: */ KECCAK_RHOTATES_LEFT[2]); /*  */
  state[3] =
    #VPSRLV_4u64(state[3], /* global: */ KECCAK_RHOTATES_RIGHT[2]); /*  */
  state[3] = (state[3] |256u t4); /* u256 */
  state[4] = (state[4] ^256u d14); /* u256 */
  t5 = #VPSLLV_4u64(state[4], /* global: */ KECCAK_RHOTATES_LEFT[3]); /*  */
  state[4] =
    #VPSRLV_4u64(state[4], /* global: */ KECCAK_RHOTATES_RIGHT[3]); /*  */
  state[4] = (state[4] |256u t5); /* u256 */
  state[5] = (state[5] ^256u d14); /* u256 */
  t6 = #VPSLLV_4u64(state[5], /* global: */ KECCAK_RHOTATES_LEFT[4]); /*  */
  state[5] =
    #VPSRLV_4u64(state[5], /* global: */ KECCAK_RHOTATES_RIGHT[4]); /*  */
  state[5] = (state[5] |256u t6); /* u256 */
  state[6] = (state[6] ^256u d14); /* u256 */
  t3 = #VPERMQ(state[2], (4u2)[2, 0, 3, 1]); /*  */
  t4 = #VPERMQ(state[3], (4u2)[2, 0, 3, 1]); /*  */
  t7 = #VPSLLV_4u64(state[6], /* global: */ KECCAK_RHOTATES_LEFT[5]); /*  */
  t1 = #VPSRLV_4u64(state[6], /* global: */ KECCAK_RHOTATES_RIGHT[5]); /*  */
  t1 = (t1 |256u t7); /* u256 */
  state[1] = (state[1] ^256u d14); /* u256 */
  t5 = #VPERMQ(state[4], (4u2)[0, 1, 2, 3]); /*  */
  t6 = #VPERMQ(state[5], (4u2)[1, 3, 0, 2]); /*  */
  t8 = #VPSLLV_4u64(state[1], /* global: */ KECCAK_RHOTATES_LEFT[1]); /*  */
  t2 = #VPSRLV_4u64(state[1], /* global: */ KECCAK_RHOTATES_RIGHT[1]); /*  */
  t2 = (t2 |256u t8); /* u256 */
  t7 = #VPSRLDQ_256(t1, ((8u) 8)); /*  */
  t0 = ((!256u t1) &256u t7); /* u256 */
  state[3] = #VPBLEND_8u32(t2, t6, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */
  t8 = #VPBLEND_8u32(t4, t2, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */
  state[5] = #VPBLEND_8u32(t3, t4, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */
  t7 = #VPBLEND_8u32(t2, t3, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */
  state[3] =
    #VPBLEND_8u32(state[3], t4, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */
  t8 = #VPBLEND_8u32(t8, t5, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */
  state[5] =
    #VPBLEND_8u32(state[5], t2, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */
  t7 = #VPBLEND_8u32(t7, t6, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */
  state[3] =
    #VPBLEND_8u32(state[3], t5, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  t8 = #VPBLEND_8u32(t8, t6, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  state[5] =
    #VPBLEND_8u32(state[5], t6, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  t7 = #VPBLEND_8u32(t7, t4, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  state[3] = ((!256u state[3]) &256u t8); /* u256 */
  state[5] = ((!256u state[5]) &256u t7); /* u256 */
  state[6] = #VPBLEND_8u32(t5, t2, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */
  t8 = #VPBLEND_8u32(t3, t5, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */
  state[3] = (state[3] ^256u t3); /* u256 */
  state[6] =
    #VPBLEND_8u32(state[6], t3, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */
  t8 = #VPBLEND_8u32(t8, t4, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */
  state[5] = (state[5] ^256u t5); /* u256 */
  state[6] =
    #VPBLEND_8u32(state[6], t4, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  t8 = #VPBLEND_8u32(t8, t2, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  state[6] = ((!256u state[6]) &256u t8); /* u256 */
  state[6] = (state[6] ^256u t6); /* u256 */
  state[4] = #VPERMQ(t1, (4u2)[0, 1, 3, 2]); /*  */
  t8 =
    #VPBLEND_8u32(state[4], state[0], (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */
  state[1] = #VPERMQ(t1, (4u2)[0, 3, 2, 1]); /*  */
  state[1] =
    #VPBLEND_8u32(state[1], state[0], (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  state[1] = ((!256u state[1]) &256u t8); /* u256 */
  state[2] = #VPBLEND_8u32(t4, t5, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */
  t7 = #VPBLEND_8u32(t6, t4, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */
  state[2] =
    #VPBLEND_8u32(state[2], t6, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */
  t7 = #VPBLEND_8u32(t7, t3, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */
  state[2] =
    #VPBLEND_8u32(state[2], t3, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  t7 = #VPBLEND_8u32(t7, t5, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  state[2] = ((!256u state[2]) &256u t7); /* u256 */
  state[2] = (state[2] ^256u t2); /* u256 */
  t0 = #VPERMQ(t0, (4u2)[0, 0, 0, 0]); /*  */
  state[3] = #VPERMQ(state[3], (4u2)[0, 1, 2, 3]); /*  */
  state[5] = #VPERMQ(state[5], (4u2)[2, 0, 3, 1]); /*  */
  state[6] = #VPERMQ(state[6], (4u2)[1, 3, 0, 2]); /*  */
  state[4] = #VPBLEND_8u32(t6, t3, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */
  t7 = #VPBLEND_8u32(t5, t6, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */
  state[4] =
    #VPBLEND_8u32(state[4], t5, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */
  t7 = #VPBLEND_8u32(t7, t2, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */
  state[4] =
    #VPBLEND_8u32(state[4], t2, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  t7 = #VPBLEND_8u32(t7, t3, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */
  state[4] = ((!256u state[4]) &256u t7); /* u256 */
  state[0] = (state[0] ^256u t0); /* u256 */
  state[1] = (state[1] ^256u t1); /* u256 */
  state[4] = (state[4] ^256u t4); /* u256 */
  return (state);
}

fn _keccakf1600_avx2 (reg u256[7] state) -> (reg u256[7]) {
  reg mut ptr u64[24] round_constants;
  reg u64 r;
  reg u256 rc;
  
  round_constants = /* global: */ KECCAK1600_RC; /* u64[24] */
  r = ((64u) 0); /* u64 */
  while {
    #[inline]
    state = __keccakf1600_pround_avx2(state);
    rc = #VPBROADCAST_4u64(round_constants[r]); /*  */
    state[0] = (state[0] ^256u rc); /* u256 */
    r = (r +64u ((64u) 1)); /* u64 */
  } ((r <u ((64u) KECCAK_ROUNDS)))
  return (state);
}

inline
fn __stavx2_pack (reg const ptr u64[25] st) -> (reg u256[7]) {
  reg u256[7] state;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 r;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  
  state[0] = #VPBROADCAST_4u64(st.[#unaligned (8 * 0)]); /*  */
  state[1] = st.[#unaligned :u256 (1 * 8)]; /* u256 */
  t128_0 = #VMOV_64(st[5]); /*  */
  state[3] = st.[#unaligned :u256 (6 * 8)]; /* u256 */
  t128_1 = #VMOV_64(st[10]); /*  */
  state[4] = st.[#unaligned :u256 (11 * 8)]; /* u256 */
  r = st[15]; /* u64 */
  t128_0 = #VPINSR_2u64(t128_0, r, ((8u) 1)); /*  */
  state[5] = st.[#unaligned :u256 (16 * 8)]; /* u256 */
  r = st[20]; /* u64 */
  t128_1 = #VPINSR_2u64(t128_1, r, ((8u) 1)); /*  */
  state[2] =
    (2u128)[((uint /* of u128 */) t128_0), ((uint /* of u128 */) t128_1)]; /* u256 */
  state[6] = st.[#unaligned :u256 (21 * 8)]; /* u256 */
  t256_0 =
    #VPBLEND_8u32(state[3], state[5], (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  t256_1 =
    #VPBLEND_8u32(state[6], state[4], (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  t256_2 =
    #VPBLEND_8u32(state[4], state[3], (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  state[3] =
    #VPBLEND_8u32(t256_0, t256_1, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  state[4] =
    #VPBLEND_8u32(t256_1, t256_0, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  t256_0 =
    #VPBLEND_8u32(state[5], state[6], (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  state[5] =
    #VPBLEND_8u32(t256_0, t256_2, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  state[6] =
    #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  return (state);
}

inline
fn __stavx2_unpack (reg mut ptr u64[25] st, reg u256[7] state) -> (reg mut ptr u64[25]) {
  reg u128 t128_0;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u128 t128_1;
  reg u256 t256_4;
  
  t128_0 = state[0]; /* u128 */
  st[0] = #VMOVLPD(t128_0); /*  */
  st.[#unaligned :u256 (1 * 8)] = state[1]; /* u256 */
  t256_0 =
    #VPBLEND_8u32(state[3], state[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  t256_1 =
    #VPBLEND_8u32(state[4], state[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  t256_2 =
    #VPBLEND_8u32(state[5], state[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  t256_3 =
    #VPBLEND_8u32(state[6], state[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  t128_1 = #VEXTRACTI128(state[2], ((8u) 1)); /*  */
  st[5] = #VMOVLPD(t128_1); /*  */
  t256_4 =
    #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  st.[#unaligned :u256 (6 * 8)] = t256_4; /* u256 */
  t128_0 = state[2]; /* u128 */
  st[10] = #VMOVLPD(t128_0); /*  */
  t256_4 =
    #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  st.[#unaligned :u256 (11 * 8)] = t256_4; /* u256 */
  st[15] = #VMOVHPD(t128_1); /*  */
  t256_4 =
    #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  st.[#unaligned :u256 (16 * 8)] = t256_4; /* u256 */
  st[20] = #VMOVHPD(t128_0); /*  */
  t256_4 =
    #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  st.[#unaligned :u256 (21 * 8)] = t256_4; /* u256 */
  return (st);
}

param int R72 = 72;

param int R104 = 104;

param int R136 = 136;

param int R144 = 144;

param int R168 = 168;

param int UNFINISHED = 0;

param int SHA3 = 6;

param int RAWSHAKE = 7;

param int SHAKE = 31;

inline
fn __mread_subu64 (reg u64 buf, inline int LEN, inline int TRAIL) -> 
(reg u64, inline int, inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = [:u64 buf]; /* u64 */
      buf = (buf +64u ((64u) 8)); /* u64 */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w = ((64u) [:u32 buf]); /* u64 */
        buf = (buf +64u ((64u) 4)); /* u64 */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 = ((64u) [:u16 buf]); /* u64 */
        buf = (buf +64u ((64u) 2)); /* u64 */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 = ((64u) [:u8 buf]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          buf = (buf +64u ((64u) 1)); /* u64 */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (buf, LEN, TRAIL, w);
}

inline
fn __mread_bcast_4subu64 (reg u64 buf, inline int LEN, inline int TRAIL) -> 
(reg u64, inline int, inline int, reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w = #VPBROADCAST_4u64([:u64 buf]); /*  */
      buf = (buf +64u ((64u) 8)); /* u64 */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (buf, LEN, TRAIL, t64) = __mread_subu64(buf, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (buf, LEN, TRAIL, w);
}

inline
fn __mread_subu128 (reg u64 buf, inline int LEN, inline int TRAIL) -> 
(reg u64, inline int, inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = [:u128 buf]; /* u128 */
      buf = (buf +64u ((64u) 16)); /* u64 */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w = #VMOV_64([:u64 buf]); /*  */
        buf = (buf +64u ((64u) 8)); /* u64 */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (buf, LEN, TRAIL, t64) = __mread_subu64(buf, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (buf, LEN, TRAIL, t64) = __mread_subu64(buf, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (buf, LEN, TRAIL, w);
}

inline
fn __mread_subu256 (reg u64 buf, inline int LEN, inline int TRAIL) -> 
(reg u64, inline int, inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = [:u256 buf]; /* u256 */
      buf = (buf +64u ((64u) 32)); /* u64 */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 = [:u128 buf]; /* u128 */
        buf = (buf +64u ((64u) 16)); /* u64 */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (buf, LEN, TRAIL, t128_1) = __mread_subu128(buf, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (buf, LEN, TRAIL, t128_0) = __mread_subu128(buf, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (buf, LEN, TRAIL, w);
}

inline
fn __mwrite_subu64 (reg u64 buf, inline int LEN, reg u64 w) -> (reg u64,
                                                               inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      [:u64 buf] = w; /* u64 */
      buf = (buf +64u ((64u) 8)); /* u64 */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        [:u32 buf] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        buf = (buf +64u ((64u) 4)); /* u64 */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        [:u16 buf] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        buf = (buf +64u ((64u) 2)); /* u64 */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        [:u8 buf] = w; /* u8 */
        buf = (buf +64u ((64u) 1)); /* u64 */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, LEN);
}

inline
fn __mwrite_subu128 (reg u64 buf, inline int LEN, reg u128 w) -> (reg u64,
                                                                 inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      [:u128 buf] = w; /* u128 */
      buf = (buf +64u ((64u) 16)); /* u64 */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        [:u64 buf] = #MOVV_64(w); /*  */
        buf = (buf +64u ((64u) 8)); /* u64 */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, LEN) = __mwrite_subu64(buf, LEN, t64);
    }
  }
  return (buf, LEN);
}

inline
fn __mwrite_subu256 (reg u64 buf, inline int LEN, reg u256 w) -> (reg u64,
                                                                 inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      [:u256 buf] = w; /* u256 */
      buf = (buf +64u ((64u) 32)); /* u64 */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        [:u128 buf] = t128; /* u128 */
        buf = (buf +64u ((64u) 16)); /* u64 */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, LEN) = __mwrite_subu128(buf, LEN, t128);
    }
  }
  return (buf, LEN);
}

inline
fn __u64_to_u256 (reg u64 x, inline int L) -> (reg u256) {
  reg u256 t256;
  reg u128 t128;
  
  if ((L % 2) == 0) {
    t128 = ((128u) x); /* u128 */
  } else {
    t128 = #set0_128(); /*  */
    t128 = #VPINSR_2u64(t128, x, ((8u) 1)); /*  */
  }
  t256 = #set0_256(); /*  */
  if ((L / 2) == 0) {
    t256 = #VINSERTI128(t256, t128, ((8u) 0)); /*  */
  } else {
    t256 = #VINSERTI128(t256, t128, ((8u) 1)); /*  */
  }
  return (t256);
}

inline
fn __state_init_avx2 () -> (reg u256[7]) {
  reg u256[7] st;
  inline int i;
  
  for i = 0 to 7 {
    st[i] = #set0_256(); /*  */
  }
  return (st);
}

inline
fn __pstate_init_avx2 (reg mut ptr u64[25] pst) -> (reg mut ptr u64[25],
                                                   reg u256[7]) {
  reg u256[7] st;
  reg u256 z256;
  inline int i;
  reg u64 z64;
  
  z256 = #set0_256(); /*  */
  for i = 0 to (25 / 4) {
    pst[:u256 i] = z256; /* u256 */
  }
  z64 = ((64u) 0); /* u64 */
  pst[24] = z64; /* u64 */
  #[inline]
  st = __state_init_avx2();
  return (pst, st);
}

inline
fn __perm_reg3456_avx2 (reg u256 r3, reg u256 r4, reg u256 r5, reg u256 r6) -> 
(reg u256, reg u256, reg u256, reg u256) {
  reg u256 st3;
  reg u256 st4;
  reg u256 st5;
  reg u256 st6;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  
  t256_0 = #VPBLEND_8u32(r3, r5, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  t256_1 = #VPBLEND_8u32(r6, r4, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  t256_2 = #VPBLEND_8u32(r4, r3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  st3 = #VPBLEND_8u32(t256_0, t256_1, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  st4 = #VPBLEND_8u32(t256_1, t256_0, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  t256_0 = #VPBLEND_8u32(r5, r6, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  st5 = #VPBLEND_8u32(t256_0, t256_2, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  st6 = #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  return (st3, st4, st5, st6);
}

inline
fn __unperm_reg3456_avx2 (reg u256 st3, reg u256 st4, reg u256 st5,
                         reg u256 st6) -> (reg u256, reg u256, reg u256,
                                          reg u256) {
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r6;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  t256_0 = #VPBLEND_8u32(st3, st4, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  t256_1 = #VPBLEND_8u32(st4, st3, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  t256_2 = #VPBLEND_8u32(st5, st6, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  t256_3 = #VPBLEND_8u32(st6, st5, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
  r3 = #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  r4 = #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  r5 = #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  r6 = #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
  return (r3, r4, r5, r6);
}

inline
fn __state_from_pstate_avx2 (reg const ptr u64[25] pst) -> (reg u256[7]) {
  reg u256[7] st;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  
  st[0] = #VPBROADCAST_4u64(pst.[#unaligned 0]); /*  */
  st[1] = pst.[#unaligned :u256 8]; /* u256 */
  t128_0 = #VMOV_64(pst.[#unaligned (5 * 8)]); /*  */
  st[3] = pst.[#unaligned :u256 (6 * 8)]; /* u256 */
  t128_1 = #VMOV_64(pst.[#unaligned (10 * 8)]); /*  */
  st[4] = pst.[#unaligned :u256 (11 * 8)]; /* u256 */
  t = pst.[#unaligned (15 * 8)]; /* u64 */
  t128_0 = #VPINSR_2u64(t128_0, t, ((8u) 1)); /*  */
  st[5] = pst.[#unaligned :u256 (16 * 8)]; /* u256 */
  t = pst.[#unaligned (20 * 8)]; /* u64 */
  t128_1 = #VPINSR_2u64(t128_1, t, ((8u) 1)); /*  */
  st[2] =
    (2u128)[((uint /* of u128 */) t128_0), ((uint /* of u128 */) t128_1)]; /* u256 */
  st[6] = pst.[#unaligned :u256 (21 * 8)]; /* u256 */
  #[inline]
  (st[3], st[4], st[5], st[6]) =
    __perm_reg3456_avx2(st[3], st[4], st[5], st[6]);
  return (st);
}

inline
fn __addstate_r3456_avx2 (reg u256[7] st, reg u256 r3, reg u256 r4,
                         reg u256 r5, reg u256 r6) -> (reg u256[7]) {
  
  #[inline]
  (r3, r4, r5, r6) = __perm_reg3456_avx2(r3, r4, r5, r6);
  st[3] = (st[3] ^256u r3); /* u256 */
  st[4] = (st[4] ^256u r4); /* u256 */
  st[5] = (st[5] ^256u r5); /* u256 */
  st[6] = (st[6] ^256u r6); /* u256 */
  return (st);
}

inline
fn __addpst01_avx2 (reg u256[7] st, reg const ptr u64[25] pst) -> (reg u256[7]) {
  reg u256 t256;
  
  t256 = #VPBROADCAST_4u64(pst.[#unaligned 0]); /*  */
  st[0] = (st[0] ^256u t256); /* u256 */
  t256 = pst.[#unaligned :u256 (8 * 1)]; /* u256 */
  st[1] = (st[1] ^256u t256); /* u256 */
  return (st);
}

inline
fn __addpst23456_avx2 (reg u256[7] st, reg const ptr u64[25] pst) -> 
(reg u256[7]) {
  reg u128 t128_0;
  reg u256 r3;
  reg u128 t128_1;
  reg u256 r4;
  reg u64 t;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  t128_0 = #VMOV_64(pst.[#unaligned (5 * 8)]); /*  */
  r3 = pst.[#unaligned :u256 (6 * 8)]; /* u256 */
  t128_1 = #VMOV_64(pst.[#unaligned (10 * 8)]); /*  */
  r4 = pst.[#unaligned :u256 (11 * 8)]; /* u256 */
  t = pst.[#unaligned (15 * 8)]; /* u64 */
  t128_0 = #VPINSR_2u64(t128_0, t, ((8u) 1)); /*  */
  r5 = pst.[#unaligned :u256 (16 * 8)]; /* u256 */
  t = pst.[#unaligned (20 * 8)]; /* u64 */
  t128_1 = #VPINSR_2u64(t128_1, t, ((8u) 1)); /*  */
  r2 =
    (2u128)[((uint /* of u128 */) t128_0), ((uint /* of u128 */) t128_1)]; /* u256 */
  st[2] = (st[2] ^256u r2); /* u256 */
  r6 = pst.[#unaligned :u256 (21 * 8)]; /* u256 */
  #[inline]
  st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  return (st);
}

fn _addpstate_avx2 (reg u256[7] st, reg const ptr u64[25] pst) -> (reg u256[7]) {
  
  #[inline]
  st = __addpst01_avx2(st, pst);
  #[inline]
  st = __addpst23456_avx2(st, pst);
  return (st);
}

inline
fn __stavx2_pos_avx2 (inline int POS) -> (inline int, inline int) {
  inline int R;
  inline int L;
  
  R = 0; /* int:i */
  L = 0; /* int:i */
  if (0 < POS) {
    if (POS <= 4) {
      R = 1; /* int:i */
      L = (POS - 1); /* int:i */
    } else {
      if (POS == 10) {
        R = 2; /* int:i */
        L = 0; /* int:i */
      } else {
        if (POS == 20) {
          R = 2; /* int:i */
          L = 1; /* int:i */
        } else {
          if (POS == 5) {
            R = 2; /* int:i */
            L = 2; /* int:i */
          } else {
            if (POS == 15) {
              R = 2; /* int:i */
              L = 3; /* int:i */
            } else {
              if (POS == 16) {
                R = 3; /* int:i */
                L = 0; /* int:i */
              } else {
                if (POS == 7) {
                  R = 3; /* int:i */
                  L = 1; /* int:i */
                } else {
                  if (POS == 23) {
                    R = 3; /* int:i */
                    L = 2; /* int:i */
                  } else {
                    if (POS == 14) {
                      R = 3; /* int:i */
                      L = 3; /* int:i */
                    } else {
                      if (POS == 11) {
                        R = 4; /* int:i */
                        L = 0; /* int:i */
                      } else {
                        if (POS == 22) {
                          R = 4; /* int:i */
                          L = 1; /* int:i */
                        } else {
                          if (POS == 8) {
                            R = 4; /* int:i */
                            L = 2; /* int:i */
                          } else {
                            if (POS == 19) {
                              R = 4; /* int:i */
                              L = 3; /* int:i */
                            } else {
                              if (POS == 21) {
                                R = 5; /* int:i */
                                L = 0; /* int:i */
                              } else {
                                if (POS == 17) {
                                  R = 5; /* int:i */
                                  L = 1; /* int:i */
                                } else {
                                  if (POS == 13) {
                                    R = 5; /* int:i */
                                    L = 2; /* int:i */
                                  } else {
                                    if (POS == 9) {
                                      R = 5; /* int:i */
                                      L = 3; /* int:i */
                                    } else {
                                      if (POS == 6) {
                                        R = 6; /* int:i */
                                        L = 0; /* int:i */
                                      } else {
                                        if (POS == 12) {
                                          R = 6; /* int:i */
                                          L = 1; /* int:i */
                                        } else {
                                          if (POS == 18) {
                                            R = 6; /* int:i */
                                            L = 2; /* int:i */
                                          } else {
                                            if (POS == 24) {
                                              R = 6; /* int:i */
                                              L = 3; /* int:i */
                                            }
                                          }
                                        }
                                      }
                                    }
                                  }
                                }
                              }
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
  return (R, L);
}

inline
fn __addratebit_avx2 (reg u256[7] st, inline int RATE8) -> (reg u256[7]) {
  reg u64 t64;
  inline int R;
  inline int L;
  reg u256 t256;
  
  t64 = ((64u) 1); /* u64 */
  t64 = (t64 <<64u ((8u) (((8 * RATE8) - 1) % 64))); /* u64 */
  #[inline]
  (R, L) = __stavx2_pos_avx2(((RATE8 - 1) / 8));
  if (R == 0) {
    t256 = #VPBROADCAST_4u64(t64); /*  */
  } else {
    #[inline]
    t256 = __u64_to_u256(t64, L);
  }
  st[R] = (st[R] ^256u t256); /* u256 */
  return (st);
}

inline
fn __addstate_imem_avx2 (reg u256[7] st, reg u64 buf, inline int LEN,
                        inline int TRAILB) -> (reg u256[7], reg u64) {
  reg u256 r0;
  reg u256 r1;
  reg u64 t64;
  reg u128 t128_1;
  reg u256 r3;
  reg u128 t128_0;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  #[inline]
  (buf, LEN, TRAILB, r0) = __mread_bcast_4subu64(buf, LEN, TRAILB);
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (buf, LEN, TRAILB, r1) = __mread_subu256(buf, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (buf, LEN, TRAILB, t64) = __mread_subu64(buf, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (buf, LEN, TRAILB, r3) = __mread_subu256(buf, LEN, TRAILB);
    #[inline]
    (buf, LEN, TRAILB, t64) = __mread_subu64(buf, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (buf, LEN, TRAILB, r4) = __mread_subu256(buf, LEN, TRAILB);
    #[inline]
    (buf, LEN, TRAILB, t64) = __mread_subu64(buf, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (buf, LEN, TRAILB, r5) = __mread_subu256(buf, LEN, TRAILB);
    #[inline]
    (buf, LEN, TRAILB, t64) = __mread_subu64(buf, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (buf, LEN, TRAILB, r6) = __mread_subu256(buf, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  return (st, buf);
}

inline
fn __absorb_imem_avx2 (reg u256[7] st, reg u64 buf, inline int LEN,
                      inline int RATE8, inline int TRAILB) -> (reg u256[7],
                                                              reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, buf) = __addstate_imem_avx2(st, buf, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, buf) = __addstate_imem_avx2(st, buf, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, buf);
}

inline
fn __pstate_imem_avx2 (reg mut ptr u64[25] pst, inline int AT, reg u64 buf,
                      inline int LEN, inline int TRAILB) -> (reg mut ptr u64[25],
                                                            inline int,
                                                            reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (buf, _ /* int */, TRAILB, t64) = __mread_subu64(buf, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = [:u64 buf]; /* u64 */
        buf = (buf +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (buf, _ /* int */, _ /* int */, t64) =
          __mread_subu64(buf, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = [:u256 buf]; /* u256 */
      buf = (buf +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = [:u128 buf]; /* u128 */
    buf = (buf +64u ((64u) 16)); /* u64 */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = [:u64 buf]; /* u64 */
    buf = (buf +64u ((64u) 8)); /* u64 */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LEN) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (buf, _ /* int */, TRAILB, t64) = __mread_subu64(buf, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  return (pst, ALL, buf);
}

inline
fn __pabsorb_imem_avx2 (reg mut ptr u64[25] pst, inline int AT,
                       reg u256[7] st, reg u64 buf, inline int LEN,
                       inline int RATE8, inline int TRAILB) -> (reg mut ptr u64[25],
                                                               inline int,
                                                               reg u256[7],
                                                               reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, buf) = __pstate_imem_avx2(pst, AT, buf, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, buf) =
        __pstate_imem_avx2(pst, AT, buf, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, buf) = __addstate_imem_avx2(st, buf, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, buf) = __addstate_imem_avx2(st, buf, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
      AT = 0; /* int:i */
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, buf) = __pstate_imem_avx2(pst, 0, buf, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, buf);
}

inline
fn __dumpstate_imem_avx2 (reg u64 buf, inline int LEN, reg u256[7] st) -> 
(reg u64) {
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  if (8 <= LEN) {
    #[inline]
    (buf, _ /* int */) = __mwrite_subu256(buf, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, LEN) = __mwrite_subu256(buf, LEN, st[0]);
  }
  #[inline]
  (buf, LEN) = __mwrite_subu256(buf, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, LEN) = __mwrite_subu64(buf, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, LEN) = __mwrite_subu256(buf, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, LEN) = __mwrite_subu64(buf, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, LEN) = __mwrite_subu256(buf, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, LEN) = __mwrite_subu64(buf, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, LEN) = __mwrite_subu256(buf, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, LEN) = __mwrite_subu64(buf, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, LEN) = __mwrite_subu256(buf, LEN, t256_4);
      }
    }
  }
  return (buf);
}

inline
fn __squeeze_imem_avx2 (reg u64 buf, inline int LEN, reg u256[7] st,
                       inline int RATE8) -> (reg u64, reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        buf = __dumpstate_imem_avx2(buf, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      buf = __dumpstate_imem_avx2(buf, LO, st);
    }
  }
  return (buf, st);
}

u256 ROL56 = ((256u)
             10910488462195273559651782724632284871561478246514020268633800075540923875841);

u256 ROL8 = ((256u)
            13620818001941277694121380808605999856886653716761013959207994299728839901191);

inline
fn keccakf1600_4x_theta_sum (reg const ptr u256[25] a) -> (reg u256[5]) {
  reg u256[5] c;
  inline int x;
  inline int y;
  
  for x = 0 to 5 {
    c[x] = a[(x + 0)]; /* u256 */
  }
  for y = 1 to 5 {
    for x = 0 to 5 {
      c[x] = (c[x] ^256u a[(x + (y * 5))]); /* u256 */
    }
  }
  return (c);
}

inline
fn keccakf1600_4x_rol (reg u256[5] a, inline int x, inline int r,
                      reg u256 r8, reg u256 r56) -> (reg u256[5]) {
  reg u256 t;
  
  if (r == 8) {
    a[x] = #VPSHUFB_256(a[x], r8); /*  */
  } else {
    if (r == 56) {
      a[x] = #VPSHUFB_256(a[x], r56); /*  */
    } else {
      t = #VPSLL_4u64(a[x], ((128u) r)); /*  */
      a[x] = #VPSRL_4u64(a[x], ((128u) (64 - r))); /*  */
      a[x] = (a[x] |256u t); /* u256 */
    }
  }
  return (a);
}

inline
fn keccakf1600_4x_theta_rol (reg u256[5] c, reg u256 r8, reg u256 r56) -> 
(reg u256[5]) {
  reg u256[5] d;
  inline int x;
  
  for x = 0 to 5 {
    d[x] = c[((x + 1) % 5)]; /* u256 */
    #[inline]
    d = keccakf1600_4x_rol(d, x, 1, r8, r56);
    d[x] = (d[x] ^256u c[(((x - 1) + 5) % 5)]); /* u256 */
  }
  return (d);
}

inline
fn keccakf1600_4x_rol_sum (reg const ptr u256[25] a, reg u256[5] d,
                          inline int y, reg u256 r8, reg u256 r56) -> 
(reg u256[5]) {
  reg u256[5] b;
  inline int x;
  inline int x_;
  inline int y_;
  inline int r;
  
  for x = 0 to 5 {
    x_ = ((x + (3 * y)) % 5); /* int:i */
    y_ = x; /* int:i */
    #[inline]
    r = keccakf1600_rhotates(x_, y_);
    b[x] = a[(x_ + (y_ * 5))]; /* u256 */
    b[x] = (b[x] ^256u d[x_]); /* u256 */
    if (r != 0) {
      #[inline]
      b = keccakf1600_4x_rol(b, x, r, r8, r56);
    }
  }
  return (b);
}

inline
fn keccakf1600_4x_set_row (reg mut ptr u256[25] e, reg u256[5] b,
                          inline int y, reg u256 rc) -> (reg mut ptr u256[25]) {
  inline int x;
  inline int x1;
  inline int x2;
  reg u256 t;
  
  for x = 0 to 5 {
    x1 = ((x + 1) % 5); /* int:i */
    x2 = ((x + 2) % 5); /* int:i */
    t = #VPANDN_256(b[x1], b[x2]); /*  */
    t = (t ^256u b[x]); /* u256 */
    if ((x == 0) && (y == 0)) {
      t = (t ^256u rc); /* u256 */
    }
    e[(x + (y * 5))] = t; /* u256 */
  }
  return (e);
}

fn _keccakf1600_4x_round (reg mut ptr u256[25] e, reg const ptr u256[25] a,
                         reg u256 rc, reg u256 r8, reg u256 r56) -> (reg mut ptr u256[25]) {
  reg u256[5] c;
  reg u256[5] d;
  inline int y;
  reg u256[5] b;
  
  #[inline]
  c = keccakf1600_4x_theta_sum(a);
  #[inline]
  d = keccakf1600_4x_theta_rol(c, r8, r56);
  for y = 0 to 5 {
    #[inline]
    b = keccakf1600_4x_rol_sum(a, d, y, r8, r56);
    #[inline]
    e = keccakf1600_4x_set_row(e, b, y, rc);
  }
  return (e);
}

inline
fn __keccakf1600_avx2x4 (reg mut ptr u256[25] a) -> (reg mut ptr u256[25]) {
  reg mut ptr u64[24] RC;
  reg mut ptr u256[25] e;
  stack u256[25] s_e;
  reg u256 r8;
  reg u256 r56;
  reg u64 c;
  reg u256 rc;
  
  RC = /* global: */ KECCAK1600_RC; /* u64[24] */
  e = s_e; /* u256[25] */
  r8 = /* global: */ ROL8; /* u256 */
  r56 = /* global: */ ROL56; /* u256 */
  c = ((64u) 0); /* u64 */
  while ((c <u ((64u) KECCAK_ROUNDS))) {
    rc = #VPBROADCAST_4u64(RC[c]); /*  */
    e = _keccakf1600_4x_round(e, a, rc, r8, r56);
    (a, e) = #swap(e, a); /*  */
    rc = #VPBROADCAST_4u64(RC[(((uint /* of u64 */) c) + 1)]); /*  */
    a = _keccakf1600_4x_round(a, e, rc, r8, r56);
    (a, e) = #swap(e, a); /*  */
    c = (c +64u ((64u) 2)); /* u64 */
  }
  return (a);
}

fn _keccakf1600_avx2x4 (reg mut ptr u256[25] a) -> (reg mut ptr u256[25]) {
  
  #[inline]
  a = __keccakf1600_avx2x4(a);
  return (a);
}

inline
fn _keccakf1600_avx2x4_ (reg mut ptr u256[25] a) -> (reg mut ptr u256[25]) {
  
  a = a; /* u256[25] */
  a = _keccakf1600_avx2x4(a);
  a = a; /* u256[25] */
  return (a);
}

inline
fn __state_init_avx2x4 (reg mut ptr u256[25] st) -> (reg mut ptr u256[25]) {
  reg u256 z256;
  reg u64 i;
  
  z256 = #set0_256(); /*  */
  i = ((64u) 0); /* u64 */
  while ((i <u ((64u) (32 * 25)))) {
    st.[#unaligned i] = z256; /* u256 */
    i = (i +64u ((64u) 32)); /* u64 */
  }
  return (st);
}

inline
fn __addratebit_avx2x4 (reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u256[25]) {
  reg u64 t64;
  reg u128 t128;
  reg u256 t256;
  
  t64 = ((64u) 1); /* u64 */
  t64 = (t64 <<64u ((8u) (((8 * RATE8) - 1) % 64))); /* u64 */
  t128 = ((128u) t64); /* u128 */
  t256 = #VPBROADCAST_4u64(t128); /*  */
  t256 = (t256 ^256u st[((RATE8 - 1) / 8)]); /* u256 */
  st[((RATE8 - 1) / 8)] = t256; /* u256 */
  return (st);
}

inline
fn __u256x4_4u64x4 (reg u256 x0, reg u256 x1, reg u256 x2, reg u256 x3) -> 
(reg u256, reg u256, reg u256, reg u256) {
  reg u256 y0;
  reg u256 y1;
  reg u256 y2;
  reg u256 y3;
  
  y0 = #VPUNPCKL_4u64(x0, x1); /*  */
  y1 = #VPUNPCKH_4u64(x0, x1); /*  */
  y2 = #VPUNPCKL_4u64(x2, x3); /*  */
  y3 = #VPUNPCKH_4u64(x2, x3); /*  */
  x0 = #VPERM2I128(y0, y2, ((8u) 32)); /*  */
  x1 = #VPERM2I128(y1, y3, ((8u) 32)); /*  */
  x2 = #VPERM2I128(y0, y2, ((8u) 49)); /*  */
  x3 = #VPERM2I128(y1, y3, ((8u) 49)); /*  */
  return (x0, x1, x2, x3);
}

inline
fn __4u64x4_u256x4 (reg u256 y0, reg u256 y1, reg u256 y2, reg u256 y3) -> 
(reg u256, reg u256, reg u256, reg u256) {
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  
  x0 = #VPERM2I128(y0, y2, ((8u) 32)); /*  */
  x1 = #VPERM2I128(y1, y3, ((8u) 32)); /*  */
  x2 = #VPERM2I128(y0, y2, ((8u) 49)); /*  */
  x3 = #VPERM2I128(y1, y3, ((8u) 49)); /*  */
  y0 = #VPUNPCKL_4u64(x0, x1); /*  */
  y1 = #VPUNPCKH_4u64(x0, x1); /*  */
  y2 = #VPUNPCKL_4u64(x2, x3); /*  */
  y3 = #VPUNPCKH_4u64(x2, x3); /*  */
  return (y0, y1, y2, y3);
}

inline
fn __addstate_bcast_imem_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg u64 buf, inline int LEN,
                                inline int TRAILB) -> (reg mut ptr u256[25],
                                                      inline int, reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  reg u256 t256;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (32 * (AT / 8))); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (buf, _ /* int */, TRAILB, t256) =
        __mread_bcast_4subu64(buf, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t256 = #VPBROADCAST_4u64([:u64 buf]); /*  */
        buf = (buf +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (buf, _ /* int */, _ /* int */, t256) =
          __mread_bcast_4subu64(buf, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64([:u64 buf]); /*  */
      buf = (buf +64u ((64u) 8)); /* u64 */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (buf, _ /* int */, TRAILB, t256) =
      __mread_bcast_4subu64(buf, LO, TRAILB);
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
    st.[#unaligned at] = t256; /* u256 */
  }
  return (st, ALL, buf);
}

inline
fn __absorb_bcast_imem_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                              reg u64 buf, inline int LEN, inline int RATE8,
                              inline int TRAILB) -> (reg mut ptr u256[25],
                                                    inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, buf) = __addstate_bcast_imem_avx2x4(st, AT, buf, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, buf) =
        __addstate_bcast_imem_avx2x4(st, AT, buf, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, buf) =
        __addstate_bcast_imem_avx2x4(st, 0, buf, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, buf) = __addstate_bcast_imem_avx2x4(st, 0, buf, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, buf);
}

inline
fn __addstate_imem_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                          reg u64 buf0, reg u64 buf1, reg u64 buf2,
                          reg u64 buf3, inline int LEN, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64, reg u64, reg u64, reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (4 * (AT / 8))); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (buf0, _ /* int */, _ /* int */, t0) =
        __mread_subu64(buf0, LEN, TRAILB);
      #[inline]
      (buf1, _ /* int */, _ /* int */, t1) =
        __mread_subu64(buf1, LEN, TRAILB);
      #[inline]
      (buf2, _ /* int */, _ /* int */, t2) =
        __mread_subu64(buf2, LEN, TRAILB);
      #[inline]
      (buf3, _ /* int */, _ /* int */, t3) =
        __mread_subu64(buf3, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
      TRAILB = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t0 = [:u64 buf0]; /* u64 */
        buf0 = (buf0 +64u ((64u) (8 - LO))); /* u64 */
        t1 = [:u64 buf1]; /* u64 */
        buf1 = (buf1 +64u ((64u) (8 - LO))); /* u64 */
        t2 = [:u64 buf2]; /* u64 */
        buf2 = (buf2 +64u ((64u) (8 - LO))); /* u64 */
        t3 = [:u64 buf3]; /* u64 */
        buf3 = (buf3 +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (buf0, _ /* int */, _ /* int */, t0) =
          __mread_subu64(buf0, (8 - LO), TRAILB);
        #[inline]
        (buf1, _ /* int */, _ /* int */, t1) =
          __mread_subu64(buf1, (8 - LO), TRAILB);
        #[inline]
        (buf2, _ /* int */, _ /* int */, t2) =
          __mread_subu64(buf2, (8 - LO), TRAILB);
        #[inline]
        (buf3, _ /* int */, _ /* int */, t3) =
          __mread_subu64(buf3, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (32 * (LEN / 32)))))) {
      t256_0 = [:u256 buf0]; /* u256 */
      buf0 = (buf0 +64u ((64u) 32)); /* u64 */
      t256_1 = [:u256 buf1]; /* u256 */
      buf1 = (buf1 +64u ((64u) 32)); /* u64 */
      t256_2 = [:u256 buf2]; /* u256 */
      buf2 = (buf2 +64u ((64u) 32)); /* u64 */
      t256_3 = [:u256 buf3]; /* u256 */
      buf3 = (buf3 +64u ((64u) 32)); /* u64 */
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = [:u64 buf0]; /* u64 */
      buf0 = (buf0 +64u ((64u) 8)); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = [:u64 buf1]; /* u64 */
      buf1 = (buf1 +64u ((64u) 8)); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = [:u64 buf2]; /* u64 */
      buf2 = (buf2 +64u ((64u) 8)); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = [:u64 buf3]; /* u64 */
      buf3 = (buf3 +64u ((64u) 8)); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (buf0, _ /* int */, _ /* int */, t0) = __mread_subu64(buf0, LO, TRAILB);
    #[inline]
    (buf1, _ /* int */, _ /* int */, t1) = __mread_subu64(buf1, LO, TRAILB);
    #[inline]
    (buf2, _ /* int */, _ /* int */, t2) = __mread_subu64(buf2, LO, TRAILB);
    #[inline]
    (buf3, _ /* int */, _ /* int */, t3) = __mread_subu64(buf3, LO, TRAILB);
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
      TRAILB = 0; /* int:i */
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
  }
  return (st, ALL, buf0, buf1, buf2, buf3);
}

inline
fn __absorb_imem_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                        reg u64 buf0, reg u64 buf1, reg u64 buf2,
                        reg u64 buf3, inline int LEN, inline int RATE8,
                        inline int TRAILB) -> (reg mut ptr u256[25],
                                              inline int, reg u64, reg u64,
                                              reg u64, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, buf0, buf1, buf2, buf3) =
      __addstate_imem_avx2x4(st, AT, buf0, buf1, buf2, buf3, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, buf0, buf1, buf2, buf3) =
        __addstate_imem_avx2x4(st, AT, buf0, buf1, buf2, buf3, (RATE8 - AT),
                               0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, buf0, buf1, buf2, buf3) =
        __addstate_imem_avx2x4(st, 0, buf0, buf1, buf2, buf3, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, buf0, buf1, buf2, buf3) =
      __addstate_imem_avx2x4(st, 0, buf0, buf1, buf2, buf3, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, buf0, buf1, buf2, buf3);
}

inline
fn __dumpstate_imem_avx2x4 (reg u64 buf0, reg u64 buf1, reg u64 buf2,
                           reg u64 buf3, inline int LEN,
                           reg const ptr u256[25] st) -> (reg u64, reg u64,
                                                         reg u64, reg u64) {
  reg int i;
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  
  i = 0; /* int */
  while ((i < (32 * (LEN / 32)))) {
    x0 = st.[#unaligned ((4 * i) + (0 * 32))]; /* u256 */
    x1 = st.[#unaligned ((4 * i) + (1 * 32))]; /* u256 */
    x2 = st.[#unaligned ((4 * i) + (2 * 32))]; /* u256 */
    x3 = st.[#unaligned ((4 * i) + (3 * 32))]; /* u256 */
    i = (i + 32); /* int */
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    [:u256 buf0] = x0; /* u256 */
    buf0 = (buf0 +64u ((64u) 32)); /* u64 */
    [:u256 buf1] = x1; /* u256 */
    buf1 = (buf1 +64u ((64u) 32)); /* u64 */
    [:u256 buf2] = x2; /* u256 */
    buf2 = (buf2 +64u ((64u) 32)); /* u64 */
    [:u256 buf3] = x3; /* u256 */
    buf3 = (buf3 +64u ((64u) 32)); /* u64 */
  }
  while ((i < (8 * (LEN / 8)))) {
    t0 = st.[#unaligned :u64 ((4 * i) + (0 * 8))]; /* u64 */
    t1 = st.[#unaligned :u64 ((4 * i) + (1 * 8))]; /* u64 */
    t2 = st.[#unaligned :u64 ((4 * i) + (2 * 8))]; /* u64 */
    t3 = st.[#unaligned :u64 ((4 * i) + (3 * 8))]; /* u64 */
    i = (i + 8); /* int */
    [:u64 buf0] = t0; /* u64 */
    buf0 = (buf0 +64u ((64u) 8)); /* u64 */
    [:u64 buf1] = t1; /* u64 */
    buf1 = (buf1 +64u ((64u) 8)); /* u64 */
    [:u64 buf2] = t2; /* u64 */
    buf2 = (buf2 +64u ((64u) 8)); /* u64 */
    [:u64 buf3] = t3; /* u64 */
    buf3 = (buf3 +64u ((64u) 8)); /* u64 */
  }
  if (0 < (LEN % 8)) {
    t0 = st.[#unaligned :u64 ((4 * i) + (0 * 8))]; /* u64 */
    t1 = st.[#unaligned :u64 ((4 * i) + (1 * 8))]; /* u64 */
    t2 = st.[#unaligned :u64 ((4 * i) + (2 * 8))]; /* u64 */
    t3 = st.[#unaligned :u64 ((4 * i) + (3 * 8))]; /* u64 */
    #[inline]
    (buf0, _ /* int */) = __mwrite_subu64(buf0, (LEN % 8), t0);
    #[inline]
    (buf1, _ /* int */) = __mwrite_subu64(buf1, (LEN % 8), t1);
    #[inline]
    (buf2, _ /* int */) = __mwrite_subu64(buf2, (LEN % 8), t2);
    #[inline]
    (buf3, _ /* int */) = __mwrite_subu64(buf3, (LEN % 8), t3);
  }
  return (buf0, buf1, buf2, buf3);
}

inline
fn __squeeze_imem_avx2x4 (reg u64 buf0, reg u64 buf1, reg u64 buf2,
                         reg u64 buf3, inline int LEN,
                         reg mut ptr u256[25] st, inline int RATE8) -> 
(reg u64, reg u64, reg u64, reg u64, reg mut ptr u256[25]) {
  inline int ITERS;
  inline int LO;
  reg int i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = 0; /* int */
      while ((i < ITERS)) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3) =
          __dumpstate_imem_avx2x4(buf0, buf1, buf2, buf3, RATE8, st);
        i = (i + 1); /* int */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3) =
        __dumpstate_imem_avx2x4(buf0, buf1, buf2, buf3, LO, st);
    }
  }
  return (buf0, buf1, buf2, buf3, st);
}

param int A1::ASIZE = 1;

inline
fn A1::__aread_subu64 (reg const ptr u8[A1::ASIZE] buf, reg u64 offset,
                      inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          DELTA = (DELTA + 1); /* int:i */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1::__aread_bcast_4subu64 (reg const ptr u8[A1::ASIZE] buf,
                             reg u64 offset, inline int DELTA,
                             inline int LEN, inline int TRAIL) -> (inline int,
                                                                  inline int,
                                                                  inline int,
                                                                  reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A1::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1::__aread_subu128 (reg const ptr u8[A1::ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1::__aread_subu256 (reg const ptr u8[A1::ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A1::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A1::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1::__awrite_subu64 (reg mut ptr u8[A1::ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1::ASIZE], inline int, inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */
        DELTA = (DELTA + 1); /* int:i */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1::__awrite_subu128 (reg mut ptr u8[A1::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A1::ASIZE], inline int, inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, DELTA, LEN) = A1::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1::__awrite_subu256 (reg mut ptr u8[A1::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A1::ASIZE], inline int, inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = t128; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1::__addstate_array_avx2 (reg u256[7] st,
                             reg const ptr u8[A1::ASIZE] buf, reg u64 offset,
                             inline int LEN, inline int TRAILB) -> (reg u256[7],
                                                                   reg u64) {
  inline int DELTA;
  reg u64 t64;
  reg u128 t128_0;
  reg u256 r0;
  reg u256 r1;
  reg u128 t128_1;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  DELTA = 0; /* int:i */
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = ((128u) t64); /* u128 */
  r0 = #VPBROADCAST_4u64(t128_0); /*  */
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A1::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A1::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A1::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A1::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A1::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (st, offset);
}

inline
fn A1::__absorb_array_avx2 (reg u256[7] st, reg const ptr u8[A1::ASIZE] buf,
                           reg u64 offset, inline int LEN, inline int RATE8,
                           inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, offset) = A1::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A1::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                           reg const ptr u8[A1::ASIZE] buf, reg u64 offset,
                           inline int LEN, inline int TRAILB) -> (reg mut ptr u64[25],
                                                                 inline int,
                                                                 reg u64) {
  inline int ALL;
  inline int DELTA;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  DELTA = 0; /* int:i */
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A1::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = buf.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
    DELTA = (DELTA + 16); /* int:i */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
    DELTA = (DELTA + 8); /* int:i */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A1::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (pst, ALL, offset);
}

inline
fn A1::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                            reg u256[7] st, reg const ptr u8[A1::ASIZE] buf,
                            reg u64 offset, inline int LEN, inline int RATE8,
                            inline int TRAILB) -> (reg mut ptr u64[25],
                                                  inline int, reg u256[7],
                                                  reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A1::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A1::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, offset) = A1::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A1::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A1::__dumpstate_array_avx2 (reg mut ptr u8[A1::ASIZE] buf, reg u64 offset,
                              inline int LEN, reg u256[7] st) -> (reg mut ptr u8[A1::ASIZE],
                                                                 reg u64) {
  inline int DELTA;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, DELTA, LEN) = A1::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A1::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, DELTA, LEN) = A1::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
      #[inline]
      (buf, DELTA, LEN) =
        A1::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A1::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A1::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A1::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (buf, offset);
}

inline
fn A1::__squeeze_array_avx2 (reg mut ptr u8[A1::ASIZE] buf, reg u64 offset,
                            inline int LEN, reg u256[7] st, inline int RATE8) -> 
(reg mut ptr u8[A1::ASIZE], reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A1::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A1::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A1::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                     reg const ptr u8[A1::ASIZE] buf,
                                     reg u64 offset, inline int LEN,
                                     inline int TRAILB) -> (reg mut ptr u256[25],
                                                           inline int,
                                                           reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u256 t256;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (32 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A1::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A1::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A1::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
    st.[#unaligned at] = t256; /* u256 */
  }
  return (st, ALL, offset);
}

inline
fn A1::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                   reg const ptr u8[A1::ASIZE] buf,
                                   reg u64 offset, inline int LEN,
                                   inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1::__addstate_bcast_array_avx2x4(st, AT, buf, offset, (RATE8 - AT),
                                          0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A1::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                               reg const ptr u8[A1::ASIZE] buf0,
                               reg const ptr u8[A1::ASIZE] buf1,
                               reg const ptr u8[A1::ASIZE] buf2,
                               reg const ptr u8[A1::ASIZE] buf3,
                               reg u64 offset, inline int LEN,
                               inline int TRAILB) -> (reg mut ptr u256[25],
                                                     inline int, reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (4 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A1::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A1::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A1::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A1::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
      TRAILB = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A1::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A1::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A1::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A1::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 = (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */
      at = (at +64u ((64u) 16)); /* u64 */
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A1::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A1::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A1::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A1::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
      TRAILB = 0; /* int:i */
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
  }
  return (st, ALL, offset);
}

inline
fn A1::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                             reg const ptr u8[A1::ASIZE] buf0,
                             reg const ptr u8[A1::ASIZE] buf1,
                             reg const ptr u8[A1::ASIZE] buf2,
                             reg const ptr u8[A1::ASIZE] buf3,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                  LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                    (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                    RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A1::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset, LEN,
                                  TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1::__dumpstate_array_avx2x4 (reg mut ptr u8[A1::ASIZE] buf0,
                                reg mut ptr u8[A1::ASIZE] buf1,
                                reg mut ptr u8[A1::ASIZE] buf2,
                                reg mut ptr u8[A1::ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                reg const ptr u256[25] st) -> (reg mut ptr u8[A1::ASIZE],
                                                              reg mut ptr u8[A1::ASIZE],
                                                              reg mut ptr u8[A1::ASIZE],
                                                              reg mut ptr u8[A1::ASIZE],
                                                              reg u64) {
  reg u64 i;
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  
  i = ((64u) 0); /* u64 */
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */
    i = (i +64u ((64u) 32)); /* u64 */
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */
    buf1.[#unaligned :u256 offset] = x1; /* u256 */
    buf2.[#unaligned :u256 offset] = x2; /* u256 */
    buf3.[#unaligned :u256 offset] = x3; /* u256 */
    offset = (offset +64u ((64u) 32)); /* u64 */
  }
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    buf0.[#unaligned :u64 offset] = t0; /* u64 */
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    buf1.[#unaligned :u64 offset] = t1; /* u64 */
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    buf2.[#unaligned :u64 offset] = t2; /* u64 */
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    buf3.[#unaligned :u64 offset] = t3; /* u64 */
    i = (i +64u ((64u) 8)); /* u64 */
    offset = (offset +64u ((64u) 8)); /* u64 */
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1::__squeeze_array_avx2x4 (reg mut ptr u8[A1::ASIZE] buf0,
                              reg mut ptr u8[A1::ASIZE] buf1,
                              reg mut ptr u8[A1::ASIZE] buf2,
                              reg mut ptr u8[A1::ASIZE] buf3, reg u64 offset,
                              inline int LEN, reg mut ptr u256[25] st,
                              inline int RATE8) -> (reg mut ptr u8[A1::ASIZE],
                                                   reg mut ptr u8[A1::ASIZE],
                                                   reg mut ptr u8[A1::ASIZE],
                                                   reg mut ptr u8[A1::ASIZE],
                                                   reg u64,
                                                   reg mut ptr u256[25]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A1::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, RATE8,
                                       st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A2::ASIZE = 2;

inline
fn A2::__aread_subu64 (reg const ptr u8[A2::ASIZE] buf, reg u64 offset,
                      inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          DELTA = (DELTA + 1); /* int:i */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A2::__aread_bcast_4subu64 (reg const ptr u8[A2::ASIZE] buf,
                             reg u64 offset, inline int DELTA,
                             inline int LEN, inline int TRAIL) -> (inline int,
                                                                  inline int,
                                                                  inline int,
                                                                  reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A2::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A2::__aread_subu128 (reg const ptr u8[A2::ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A2::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A2::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A2::__aread_subu256 (reg const ptr u8[A2::ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A2::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A2::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A2::__awrite_subu64 (reg mut ptr u8[A2::ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A2::ASIZE], inline int, inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */
        DELTA = (DELTA + 1); /* int:i */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A2::__awrite_subu128 (reg mut ptr u8[A2::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A2::ASIZE], inline int, inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, DELTA, LEN) = A2::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A2::__awrite_subu256 (reg mut ptr u8[A2::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A2::ASIZE], inline int, inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = t128; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, DELTA, LEN) =
        A2::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A2::__addstate_array_avx2 (reg u256[7] st,
                             reg const ptr u8[A2::ASIZE] buf, reg u64 offset,
                             inline int LEN, inline int TRAILB) -> (reg u256[7],
                                                                   reg u64) {
  inline int DELTA;
  reg u64 t64;
  reg u128 t128_0;
  reg u256 r0;
  reg u256 r1;
  reg u128 t128_1;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  DELTA = 0; /* int:i */
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = ((128u) t64); /* u128 */
  r0 = #VPBROADCAST_4u64(t128_0); /*  */
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A2::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A2::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A2::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A2::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A2::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (st, offset);
}

inline
fn A2::__absorb_array_avx2 (reg u256[7] st, reg const ptr u8[A2::ASIZE] buf,
                           reg u64 offset, inline int LEN, inline int RATE8,
                           inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A2::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, offset) = A2::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A2::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                           reg const ptr u8[A2::ASIZE] buf, reg u64 offset,
                           inline int LEN, inline int TRAILB) -> (reg mut ptr u64[25],
                                                                 inline int,
                                                                 reg u64) {
  inline int ALL;
  inline int DELTA;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  DELTA = 0; /* int:i */
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A2::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = buf.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
    DELTA = (DELTA + 16); /* int:i */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
    DELTA = (DELTA + 8); /* int:i */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A2::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (pst, ALL, offset);
}

inline
fn A2::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                            reg u256[7] st, reg const ptr u8[A2::ASIZE] buf,
                            reg u64 offset, inline int LEN, inline int RATE8,
                            inline int TRAILB) -> (reg mut ptr u64[25],
                                                  inline int, reg u256[7],
                                                  reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A2::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A2::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A2::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, offset) = A2::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A2::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A2::__dumpstate_array_avx2 (reg mut ptr u8[A2::ASIZE] buf, reg u64 offset,
                              inline int LEN, reg u256[7] st) -> (reg mut ptr u8[A2::ASIZE],
                                                                 reg u64) {
  inline int DELTA;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A2::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, DELTA, LEN) = A2::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A2::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, DELTA, LEN) = A2::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
      #[inline]
      (buf, DELTA, LEN) =
        A2::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A2::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A2::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A2::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A2::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A2::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A2::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (buf, offset);
}

inline
fn A2::__squeeze_array_avx2 (reg mut ptr u8[A2::ASIZE] buf, reg u64 offset,
                            inline int LEN, reg u256[7] st, inline int RATE8) -> 
(reg mut ptr u8[A2::ASIZE], reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A2::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A2::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A2::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                     reg const ptr u8[A2::ASIZE] buf,
                                     reg u64 offset, inline int LEN,
                                     inline int TRAILB) -> (reg mut ptr u256[25],
                                                           inline int,
                                                           reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u256 t256;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (32 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A2::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A2::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A2::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
    st.[#unaligned at] = t256; /* u256 */
  }
  return (st, ALL, offset);
}

inline
fn A2::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                   reg const ptr u8[A2::ASIZE] buf,
                                   reg u64 offset, inline int LEN,
                                   inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A2::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A2::__addstate_bcast_array_avx2x4(st, AT, buf, offset, (RATE8 - AT),
                                          0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A2::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A2::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A2::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                               reg const ptr u8[A2::ASIZE] buf0,
                               reg const ptr u8[A2::ASIZE] buf1,
                               reg const ptr u8[A2::ASIZE] buf2,
                               reg const ptr u8[A2::ASIZE] buf3,
                               reg u64 offset, inline int LEN,
                               inline int TRAILB) -> (reg mut ptr u256[25],
                                                     inline int, reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (4 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A2::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A2::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A2::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A2::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
      TRAILB = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A2::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A2::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A2::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A2::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 = (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */
      at = (at +64u ((64u) 16)); /* u64 */
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A2::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A2::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A2::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A2::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
      TRAILB = 0; /* int:i */
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
  }
  return (st, ALL, offset);
}

inline
fn A2::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                             reg const ptr u8[A2::ASIZE] buf0,
                             reg const ptr u8[A2::ASIZE] buf1,
                             reg const ptr u8[A2::ASIZE] buf2,
                             reg const ptr u8[A2::ASIZE] buf3,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A2::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                  LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A2::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                    (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A2::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                    RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A2::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset, LEN,
                                  TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A2::__dumpstate_array_avx2x4 (reg mut ptr u8[A2::ASIZE] buf0,
                                reg mut ptr u8[A2::ASIZE] buf1,
                                reg mut ptr u8[A2::ASIZE] buf2,
                                reg mut ptr u8[A2::ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                reg const ptr u256[25] st) -> (reg mut ptr u8[A2::ASIZE],
                                                              reg mut ptr u8[A2::ASIZE],
                                                              reg mut ptr u8[A2::ASIZE],
                                                              reg mut ptr u8[A2::ASIZE],
                                                              reg u64) {
  reg u64 i;
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  
  i = ((64u) 0); /* u64 */
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */
    i = (i +64u ((64u) 32)); /* u64 */
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */
    buf1.[#unaligned :u256 offset] = x1; /* u256 */
    buf2.[#unaligned :u256 offset] = x2; /* u256 */
    buf3.[#unaligned :u256 offset] = x3; /* u256 */
    offset = (offset +64u ((64u) 32)); /* u64 */
  }
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    buf0.[#unaligned :u64 offset] = t0; /* u64 */
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    buf1.[#unaligned :u64 offset] = t1; /* u64 */
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    buf2.[#unaligned :u64 offset] = t2; /* u64 */
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    buf3.[#unaligned :u64 offset] = t3; /* u64 */
    i = (i +64u ((64u) 8)); /* u64 */
    offset = (offset +64u ((64u) 8)); /* u64 */
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A2::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A2::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A2::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A2::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A2::__squeeze_array_avx2x4 (reg mut ptr u8[A2::ASIZE] buf0,
                              reg mut ptr u8[A2::ASIZE] buf1,
                              reg mut ptr u8[A2::ASIZE] buf2,
                              reg mut ptr u8[A2::ASIZE] buf3, reg u64 offset,
                              inline int LEN, reg mut ptr u256[25] st,
                              inline int RATE8) -> (reg mut ptr u8[A2::ASIZE],
                                                   reg mut ptr u8[A2::ASIZE],
                                                   reg mut ptr u8[A2::ASIZE],
                                                   reg mut ptr u8[A2::ASIZE],
                                                   reg u64,
                                                   reg mut ptr u256[25]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A2::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, RATE8,
                                       st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A2::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A32::ASIZE = 32;

inline
fn A32::__aread_subu64 (reg const ptr u8[A32::ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          DELTA = (DELTA + 1); /* int:i */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A32::__aread_bcast_4subu64 (reg const ptr u8[A32::ASIZE] buf,
                              reg u64 offset, inline int DELTA,
                              inline int LEN, inline int TRAIL) -> (inline int,
                                                                   inline int,
                                                                   inline int,
                                                                   reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A32::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A32::__aread_subu128 (reg const ptr u8[A32::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A32::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A32::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A32::__aread_subu256 (reg const ptr u8[A32::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A32::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A32::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A32::__awrite_subu64 (reg mut ptr u8[A32::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A32::ASIZE], inline int, inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */
        DELTA = (DELTA + 1); /* int:i */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A32::__awrite_subu128 (reg mut ptr u8[A32::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A32::ASIZE], inline int, inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, DELTA, LEN) = A32::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A32::__awrite_subu256 (reg mut ptr u8[A32::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A32::ASIZE], inline int, inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = t128; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, DELTA, LEN) =
        A32::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A32::__addstate_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A32::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int DELTA;
  reg u64 t64;
  reg u128 t128_0;
  reg u256 r0;
  reg u256 r1;
  reg u128 t128_1;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  DELTA = 0; /* int:i */
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = ((128u) t64); /* u128 */
  r0 = #VPBROADCAST_4u64(t128_0); /*  */
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A32::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A32::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A32::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A32::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A32::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (st, offset);
}

inline
fn A32::__absorb_array_avx2 (reg u256[7] st,
                            reg const ptr u8[A32::ASIZE] buf, reg u64 offset,
                            inline int LEN, inline int RATE8,
                            inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A32::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, offset) = A32::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A32::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                            reg const ptr u8[A32::ASIZE] buf, reg u64 offset,
                            inline int LEN, inline int TRAILB) -> (reg mut ptr u64[25],
                                                                  inline int,
                                                                  reg u64) {
  inline int ALL;
  inline int DELTA;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  DELTA = 0; /* int:i */
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A32::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = buf.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
    DELTA = (DELTA + 16); /* int:i */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
    DELTA = (DELTA + 8); /* int:i */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A32::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (pst, ALL, offset);
}

inline
fn A32::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                             reg u256[7] st,
                             reg const ptr u8[A32::ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A32::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A32::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A32::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A32::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A32::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A32::__dumpstate_array_avx2 (reg mut ptr u8[A32::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st) -> (reg mut ptr u8[A32::ASIZE],
                                                  reg u64) {
  inline int DELTA;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A32::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A32::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A32::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, DELTA, LEN) = A32::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
      #[inline]
      (buf, DELTA, LEN) =
        A32::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A32::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A32::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A32::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A32::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A32::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A32::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (buf, offset);
}

inline
fn A32::__squeeze_array_avx2 (reg mut ptr u8[A32::ASIZE] buf, reg u64 offset,
                             inline int LEN, reg u256[7] st,
                             inline int RATE8) -> (reg mut ptr u8[A32::ASIZE],
                                                  reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A32::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A32::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A32::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A32::ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int TRAILB) -> (reg mut ptr u256[25],
                                                            inline int,
                                                            reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u256 t256;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (32 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A32::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A32::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A32::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
    st.[#unaligned at] = t256; /* u256 */
  }
  return (st, ALL, offset);
}

inline
fn A32::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                    reg const ptr u8[A32::ASIZE] buf,
                                    reg u64 offset, inline int LEN,
                                    inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A32::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A32::__addstate_bcast_array_avx2x4(st, AT, buf, offset, (RATE8 - AT),
                                           0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A32::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A32::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A32::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A32::ASIZE] buf0,
                                reg const ptr u8[A32::ASIZE] buf1,
                                reg const ptr u8[A32::ASIZE] buf2,
                                reg const ptr u8[A32::ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg mut ptr u256[25],
                                                      inline int, reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (4 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A32::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A32::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A32::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A32::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
      TRAILB = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A32::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A32::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A32::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A32::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 = (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */
      at = (at +64u ((64u) 16)); /* u64 */
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A32::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A32::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A32::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A32::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
      TRAILB = 0; /* int:i */
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
  }
  return (st, ALL, offset);
}

inline
fn A32::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                              reg const ptr u8[A32::ASIZE] buf0,
                              reg const ptr u8[A32::ASIZE] buf1,
                              reg const ptr u8[A32::ASIZE] buf2,
                              reg const ptr u8[A32::ASIZE] buf3,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A32::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                   LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A32::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A32::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A32::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                   LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A32::__dumpstate_array_avx2x4 (reg mut ptr u8[A32::ASIZE] buf0,
                                 reg mut ptr u8[A32::ASIZE] buf1,
                                 reg mut ptr u8[A32::ASIZE] buf2,
                                 reg mut ptr u8[A32::ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg const ptr u256[25] st) -> (reg mut ptr u8[A32::ASIZE],
                                                               reg mut ptr u8[A32::ASIZE],
                                                               reg mut ptr u8[A32::ASIZE],
                                                               reg mut ptr u8[A32::ASIZE],
                                                               reg u64) {
  reg u64 i;
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  
  i = ((64u) 0); /* u64 */
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */
    i = (i +64u ((64u) 32)); /* u64 */
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */
    buf1.[#unaligned :u256 offset] = x1; /* u256 */
    buf2.[#unaligned :u256 offset] = x2; /* u256 */
    buf3.[#unaligned :u256 offset] = x3; /* u256 */
    offset = (offset +64u ((64u) 32)); /* u64 */
  }
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    buf0.[#unaligned :u64 offset] = t0; /* u64 */
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    buf1.[#unaligned :u64 offset] = t1; /* u64 */
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    buf2.[#unaligned :u64 offset] = t2; /* u64 */
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    buf3.[#unaligned :u64 offset] = t3; /* u64 */
    i = (i +64u ((64u) 8)); /* u64 */
    offset = (offset +64u ((64u) 8)); /* u64 */
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A32::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A32::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A32::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A32::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A32::__squeeze_array_avx2x4 (reg mut ptr u8[A32::ASIZE] buf0,
                               reg mut ptr u8[A32::ASIZE] buf1,
                               reg mut ptr u8[A32::ASIZE] buf2,
                               reg mut ptr u8[A32::ASIZE] buf3,
                               reg u64 offset, inline int LEN,
                               reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A32::ASIZE], reg mut ptr u8[A32::ASIZE],
reg mut ptr u8[A32::ASIZE], reg mut ptr u8[A32::ASIZE], reg u64,
reg mut ptr u256[25]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A32::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                        RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A32::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A33::ASIZE = 33;

inline
fn A33::__aread_subu64 (reg const ptr u8[A33::ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          DELTA = (DELTA + 1); /* int:i */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A33::__aread_bcast_4subu64 (reg const ptr u8[A33::ASIZE] buf,
                              reg u64 offset, inline int DELTA,
                              inline int LEN, inline int TRAIL) -> (inline int,
                                                                   inline int,
                                                                   inline int,
                                                                   reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A33::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A33::__aread_subu128 (reg const ptr u8[A33::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A33::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A33::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A33::__aread_subu256 (reg const ptr u8[A33::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A33::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A33::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A33::__awrite_subu64 (reg mut ptr u8[A33::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A33::ASIZE], inline int, inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */
        DELTA = (DELTA + 1); /* int:i */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A33::__awrite_subu128 (reg mut ptr u8[A33::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A33::ASIZE], inline int, inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, DELTA, LEN) = A33::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A33::__awrite_subu256 (reg mut ptr u8[A33::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A33::ASIZE], inline int, inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = t128; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, DELTA, LEN) =
        A33::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A33::__addstate_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A33::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int DELTA;
  reg u64 t64;
  reg u128 t128_0;
  reg u256 r0;
  reg u256 r1;
  reg u128 t128_1;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  DELTA = 0; /* int:i */
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = ((128u) t64); /* u128 */
  r0 = #VPBROADCAST_4u64(t128_0); /*  */
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A33::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A33::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A33::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A33::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A33::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (st, offset);
}

inline
fn A33::__absorb_array_avx2 (reg u256[7] st,
                            reg const ptr u8[A33::ASIZE] buf, reg u64 offset,
                            inline int LEN, inline int RATE8,
                            inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A33::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, offset) = A33::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A33::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                            reg const ptr u8[A33::ASIZE] buf, reg u64 offset,
                            inline int LEN, inline int TRAILB) -> (reg mut ptr u64[25],
                                                                  inline int,
                                                                  reg u64) {
  inline int ALL;
  inline int DELTA;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  DELTA = 0; /* int:i */
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A33::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = buf.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
    DELTA = (DELTA + 16); /* int:i */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
    DELTA = (DELTA + 8); /* int:i */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A33::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (pst, ALL, offset);
}

inline
fn A33::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                             reg u256[7] st,
                             reg const ptr u8[A33::ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A33::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A33::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A33::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A33::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A33::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A33::__dumpstate_array_avx2 (reg mut ptr u8[A33::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st) -> (reg mut ptr u8[A33::ASIZE],
                                                  reg u64) {
  inline int DELTA;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A33::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A33::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A33::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, DELTA, LEN) = A33::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
      #[inline]
      (buf, DELTA, LEN) =
        A33::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A33::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A33::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A33::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A33::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A33::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A33::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (buf, offset);
}

inline
fn A33::__squeeze_array_avx2 (reg mut ptr u8[A33::ASIZE] buf, reg u64 offset,
                             inline int LEN, reg u256[7] st,
                             inline int RATE8) -> (reg mut ptr u8[A33::ASIZE],
                                                  reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A33::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A33::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A33::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A33::ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int TRAILB) -> (reg mut ptr u256[25],
                                                            inline int,
                                                            reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u256 t256;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (32 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A33::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A33::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A33::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
    st.[#unaligned at] = t256; /* u256 */
  }
  return (st, ALL, offset);
}

inline
fn A33::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                    reg const ptr u8[A33::ASIZE] buf,
                                    reg u64 offset, inline int LEN,
                                    inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A33::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A33::__addstate_bcast_array_avx2x4(st, AT, buf, offset, (RATE8 - AT),
                                           0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A33::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A33::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A33::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A33::ASIZE] buf0,
                                reg const ptr u8[A33::ASIZE] buf1,
                                reg const ptr u8[A33::ASIZE] buf2,
                                reg const ptr u8[A33::ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg mut ptr u256[25],
                                                      inline int, reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (4 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A33::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A33::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A33::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A33::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
      TRAILB = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A33::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A33::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A33::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A33::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 = (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */
      at = (at +64u ((64u) 16)); /* u64 */
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A33::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A33::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A33::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A33::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
      TRAILB = 0; /* int:i */
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
  }
  return (st, ALL, offset);
}

inline
fn A33::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                              reg const ptr u8[A33::ASIZE] buf0,
                              reg const ptr u8[A33::ASIZE] buf1,
                              reg const ptr u8[A33::ASIZE] buf2,
                              reg const ptr u8[A33::ASIZE] buf3,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A33::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                   LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A33::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A33::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A33::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                   LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A33::__dumpstate_array_avx2x4 (reg mut ptr u8[A33::ASIZE] buf0,
                                 reg mut ptr u8[A33::ASIZE] buf1,
                                 reg mut ptr u8[A33::ASIZE] buf2,
                                 reg mut ptr u8[A33::ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg const ptr u256[25] st) -> (reg mut ptr u8[A33::ASIZE],
                                                               reg mut ptr u8[A33::ASIZE],
                                                               reg mut ptr u8[A33::ASIZE],
                                                               reg mut ptr u8[A33::ASIZE],
                                                               reg u64) {
  reg u64 i;
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  
  i = ((64u) 0); /* u64 */
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */
    i = (i +64u ((64u) 32)); /* u64 */
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */
    buf1.[#unaligned :u256 offset] = x1; /* u256 */
    buf2.[#unaligned :u256 offset] = x2; /* u256 */
    buf3.[#unaligned :u256 offset] = x3; /* u256 */
    offset = (offset +64u ((64u) 32)); /* u64 */
  }
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    buf0.[#unaligned :u64 offset] = t0; /* u64 */
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    buf1.[#unaligned :u64 offset] = t1; /* u64 */
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    buf2.[#unaligned :u64 offset] = t2; /* u64 */
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    buf3.[#unaligned :u64 offset] = t3; /* u64 */
    i = (i +64u ((64u) 8)); /* u64 */
    offset = (offset +64u ((64u) 8)); /* u64 */
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A33::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A33::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A33::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A33::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A33::__squeeze_array_avx2x4 (reg mut ptr u8[A33::ASIZE] buf0,
                               reg mut ptr u8[A33::ASIZE] buf1,
                               reg mut ptr u8[A33::ASIZE] buf2,
                               reg mut ptr u8[A33::ASIZE] buf3,
                               reg u64 offset, inline int LEN,
                               reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A33::ASIZE], reg mut ptr u8[A33::ASIZE],
reg mut ptr u8[A33::ASIZE], reg mut ptr u8[A33::ASIZE], reg u64,
reg mut ptr u256[25]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A33::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                        RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A33::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A64::ASIZE = 64;

inline
fn A64::__aread_subu64 (reg const ptr u8[A64::ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          DELTA = (DELTA + 1); /* int:i */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A64::__aread_bcast_4subu64 (reg const ptr u8[A64::ASIZE] buf,
                              reg u64 offset, inline int DELTA,
                              inline int LEN, inline int TRAIL) -> (inline int,
                                                                   inline int,
                                                                   inline int,
                                                                   reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A64::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A64::__aread_subu128 (reg const ptr u8[A64::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A64::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A64::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A64::__aread_subu256 (reg const ptr u8[A64::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A64::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A64::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A64::__awrite_subu64 (reg mut ptr u8[A64::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A64::ASIZE], inline int, inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */
        DELTA = (DELTA + 1); /* int:i */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A64::__awrite_subu128 (reg mut ptr u8[A64::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A64::ASIZE], inline int, inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, DELTA, LEN) = A64::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A64::__awrite_subu256 (reg mut ptr u8[A64::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A64::ASIZE], inline int, inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = t128; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, DELTA, LEN) =
        A64::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A64::__addstate_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A64::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int DELTA;
  reg u64 t64;
  reg u128 t128_0;
  reg u256 r0;
  reg u256 r1;
  reg u128 t128_1;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  DELTA = 0; /* int:i */
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = ((128u) t64); /* u128 */
  r0 = #VPBROADCAST_4u64(t128_0); /*  */
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A64::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A64::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A64::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A64::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A64::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (st, offset);
}

inline
fn A64::__absorb_array_avx2 (reg u256[7] st,
                            reg const ptr u8[A64::ASIZE] buf, reg u64 offset,
                            inline int LEN, inline int RATE8,
                            inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A64::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, offset) = A64::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A64::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                            reg const ptr u8[A64::ASIZE] buf, reg u64 offset,
                            inline int LEN, inline int TRAILB) -> (reg mut ptr u64[25],
                                                                  inline int,
                                                                  reg u64) {
  inline int ALL;
  inline int DELTA;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  DELTA = 0; /* int:i */
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A64::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = buf.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
    DELTA = (DELTA + 16); /* int:i */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
    DELTA = (DELTA + 8); /* int:i */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A64::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (pst, ALL, offset);
}

inline
fn A64::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                             reg u256[7] st,
                             reg const ptr u8[A64::ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A64::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A64::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A64::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A64::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A64::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A64::__dumpstate_array_avx2 (reg mut ptr u8[A64::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st) -> (reg mut ptr u8[A64::ASIZE],
                                                  reg u64) {
  inline int DELTA;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A64::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A64::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A64::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, DELTA, LEN) = A64::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
      #[inline]
      (buf, DELTA, LEN) =
        A64::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A64::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A64::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A64::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A64::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) = A64::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A64::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (buf, offset);
}

inline
fn A64::__squeeze_array_avx2 (reg mut ptr u8[A64::ASIZE] buf, reg u64 offset,
                             inline int LEN, reg u256[7] st,
                             inline int RATE8) -> (reg mut ptr u8[A64::ASIZE],
                                                  reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A64::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A64::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

param int A128::ASIZE = 128;

inline
fn A128::__aread_subu64 (reg const ptr u8[A128::ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          DELTA = (DELTA + 1); /* int:i */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A128::__aread_bcast_4subu64 (reg const ptr u8[A128::ASIZE] buf,
                               reg u64 offset, inline int DELTA,
                               inline int LEN, inline int TRAIL) -> (inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A128::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A128::__aread_subu128 (reg const ptr u8[A128::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A128::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A128::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A128::__aread_subu256 (reg const ptr u8[A128::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A128::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A128::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A128::__awrite_subu64 (reg mut ptr u8[A128::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A128::ASIZE], inline int, inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */
        DELTA = (DELTA + 1); /* int:i */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A128::__awrite_subu128 (reg mut ptr u8[A128::ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A128::ASIZE], inline int, inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, DELTA, LEN) =
        A128::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A128::__awrite_subu256 (reg mut ptr u8[A128::ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A128::ASIZE], inline int, inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = t128; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, DELTA, LEN) =
        A128::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A128::__addstate_array_avx2 (reg u256[7] st,
                               reg const ptr u8[A128::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int DELTA;
  reg u64 t64;
  reg u128 t128_0;
  reg u256 r0;
  reg u256 r1;
  reg u128 t128_1;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  DELTA = 0; /* int:i */
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = ((128u) t64); /* u128 */
  r0 = #VPBROADCAST_4u64(t128_0); /*  */
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A128::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A128::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A128::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A128::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A128::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (st, offset);
}

inline
fn A128::__absorb_array_avx2 (reg u256[7] st,
                             reg const ptr u8[A128::ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A128::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, offset) = A128::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A128::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                             reg const ptr u8[A128::ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int TRAILB) -> (reg mut ptr u64[25],
                                                   inline int, reg u64) {
  inline int ALL;
  inline int DELTA;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  DELTA = 0; /* int:i */
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A128::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = buf.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
    DELTA = (DELTA + 16); /* int:i */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
    DELTA = (DELTA + 8); /* int:i */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A128::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (pst, ALL, offset);
}

inline
fn A128::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                              reg u256[7] st,
                              reg const ptr u8[A128::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A128::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A128::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A128::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A128::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A128::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A128::__dumpstate_array_avx2 (reg mut ptr u8[A128::ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                reg u256[7] st) -> (reg mut ptr u8[A128::ASIZE],
                                                   reg u64) {
  inline int DELTA;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A128::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A128::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A128::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, DELTA, LEN) = A128::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
      #[inline]
      (buf, DELTA, LEN) =
        A128::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (buf, offset);
}

inline
fn A128::__squeeze_array_avx2 (reg mut ptr u8[A128::ASIZE] buf,
                              reg u64 offset, inline int LEN, reg u256[7] st,
                              inline int RATE8) -> (reg mut ptr u8[A128::ASIZE],
                                                   reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A128::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A128::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A128::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                       inline int AT,
                                       reg const ptr u8[A128::ASIZE] buf,
                                       reg u64 offset, inline int LEN,
                                       inline int TRAILB) -> (reg mut ptr u256[25],
                                                             inline int,
                                                             reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u256 t256;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (32 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A128::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A128::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A128::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
    st.[#unaligned at] = t256; /* u256 */
  }
  return (st, ALL, offset);
}

inline
fn A128::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                     reg const ptr u8[A128::ASIZE] buf,
                                     reg u64 offset, inline int LEN,
                                     inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A128::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A128::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                            (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A128::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A128::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A128::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                 reg const ptr u8[A128::ASIZE] buf0,
                                 reg const ptr u8[A128::ASIZE] buf1,
                                 reg const ptr u8[A128::ASIZE] buf2,
                                 reg const ptr u8[A128::ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 inline int TRAILB) -> (reg mut ptr u256[25],
                                                       inline int, reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (4 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A128::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A128::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A128::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A128::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
      TRAILB = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A128::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A128::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A128::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A128::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 = (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */
      at = (at +64u ((64u) 16)); /* u64 */
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A128::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A128::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A128::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A128::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
      TRAILB = 0; /* int:i */
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
  }
  return (st, ALL, offset);
}

inline
fn A128::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                               reg const ptr u8[A128::ASIZE] buf0,
                               reg const ptr u8[A128::ASIZE] buf1,
                               reg const ptr u8[A128::ASIZE] buf2,
                               reg const ptr u8[A128::ASIZE] buf3,
                               reg u64 offset, inline int LEN,
                               inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A128::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                    LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A128::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                      (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A128::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                      RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A128::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                    LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A128::__dumpstate_array_avx2x4 (reg mut ptr u8[A128::ASIZE] buf0,
                                  reg mut ptr u8[A128::ASIZE] buf1,
                                  reg mut ptr u8[A128::ASIZE] buf2,
                                  reg mut ptr u8[A128::ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  reg const ptr u256[25] st) -> (reg mut ptr u8[A128::ASIZE],
                                                                reg mut ptr u8[A128::ASIZE],
                                                                reg mut ptr u8[A128::ASIZE],
                                                                reg mut ptr u8[A128::ASIZE],
                                                                reg u64) {
  reg u64 i;
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  
  i = ((64u) 0); /* u64 */
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */
    i = (i +64u ((64u) 32)); /* u64 */
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */
    buf1.[#unaligned :u256 offset] = x1; /* u256 */
    buf2.[#unaligned :u256 offset] = x2; /* u256 */
    buf3.[#unaligned :u256 offset] = x3; /* u256 */
    offset = (offset +64u ((64u) 32)); /* u64 */
  }
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    buf0.[#unaligned :u64 offset] = t0; /* u64 */
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    buf1.[#unaligned :u64 offset] = t1; /* u64 */
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    buf2.[#unaligned :u64 offset] = t2; /* u64 */
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    buf3.[#unaligned :u64 offset] = t3; /* u64 */
    i = (i +64u ((64u) 8)); /* u64 */
    offset = (offset +64u ((64u) 8)); /* u64 */
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A128::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A128::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A128::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A128::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A128::__squeeze_array_avx2x4 (reg mut ptr u8[A128::ASIZE] buf0,
                                reg mut ptr u8[A128::ASIZE] buf1,
                                reg mut ptr u8[A128::ASIZE] buf2,
                                reg mut ptr u8[A128::ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A128::ASIZE], reg mut ptr u8[A128::ASIZE],
reg mut ptr u8[A128::ASIZE], reg mut ptr u8[A128::ASIZE], reg u64,
reg mut ptr u256[25]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A128::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                         RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A128::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A1184::ASIZE = 1184;

inline
fn A1184::__aread_subu64 (reg const ptr u8[A1184::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          DELTA = (DELTA + 1); /* int:i */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1184::__aread_bcast_4subu64 (reg const ptr u8[A1184::ASIZE] buf,
                                reg u64 offset, inline int DELTA,
                                inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1184::__aread_subu128 (reg const ptr u8[A1184::ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1184::__aread_subu256 (reg const ptr u8[A1184::ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A1184::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A1184::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1184::__awrite_subu64 (reg mut ptr u8[A1184::ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1184::ASIZE], inline int, inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */
        DELTA = (DELTA + 1); /* int:i */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1184::__awrite_subu128 (reg mut ptr u8[A1184::ASIZE] buf, reg u64 offset,
                           inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A1184::ASIZE], inline int, inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, DELTA, LEN) =
        A1184::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1184::__awrite_subu256 (reg mut ptr u8[A1184::ASIZE] buf, reg u64 offset,
                           inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A1184::ASIZE], inline int, inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = t128; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1184::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1184::__addstate_array_avx2 (reg u256[7] st,
                                reg const ptr u8[A1184::ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int DELTA;
  reg u64 t64;
  reg u128 t128_0;
  reg u256 r0;
  reg u256 r1;
  reg u128 t128_1;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  DELTA = 0; /* int:i */
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = ((128u) t64); /* u128 */
  r0 = #VPBROADCAST_4u64(t128_0); /*  */
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A1184::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A1184::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A1184::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A1184::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A1184::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (st, offset);
}

inline
fn A1184::__absorb_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A1184::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1184::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, offset) = A1184::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A1184::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                              reg const ptr u8[A1184::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg mut ptr u64[25],
                                                    inline int, reg u64) {
  inline int ALL;
  inline int DELTA;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  DELTA = 0; /* int:i */
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A1184::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = buf.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
    DELTA = (DELTA + 16); /* int:i */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
    DELTA = (DELTA + 8); /* int:i */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A1184::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (pst, ALL, offset);
}

inline
fn A1184::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                               reg u256[7] st,
                               reg const ptr u8[A1184::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A1184::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A1184::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1184::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A1184::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A1184::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A1184::__dumpstate_array_avx2 (reg mut ptr u8[A1184::ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 reg u256[7] st) -> (reg mut ptr u8[A1184::ASIZE],
                                                    reg u64) {
  inline int DELTA;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1184::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A1184::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) =
    A1184::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, DELTA, LEN) = A1184::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
      #[inline]
      (buf, DELTA, LEN) =
        A1184::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (buf, offset);
}

inline
fn A1184::__squeeze_array_avx2 (reg mut ptr u8[A1184::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st, inline int RATE8) -> (reg mut ptr u8[A1184::ASIZE],
                                                                    reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) =
          A1184::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A1184::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A1184::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                        inline int AT,
                                        reg const ptr u8[A1184::ASIZE] buf,
                                        reg u64 offset, inline int LEN,
                                        inline int TRAILB) -> (reg mut ptr u256[25],
                                                              inline int,
                                                              reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u256 t256;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (32 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A1184::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A1184::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A1184::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
    st.[#unaligned at] = t256; /* u256 */
  }
  return (st, ALL, offset);
}

inline
fn A1184::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A1184::ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1184::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1184::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                             (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1184::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A1184::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1184::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1184::ASIZE] buf0,
                                  reg const ptr u8[A1184::ASIZE] buf1,
                                  reg const ptr u8[A1184::ASIZE] buf2,
                                  reg const ptr u8[A1184::ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  inline int TRAILB) -> (reg mut ptr u256[25],
                                                        inline int, reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (4 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A1184::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A1184::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A1184::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A1184::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
      TRAILB = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A1184::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A1184::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A1184::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A1184::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 = (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */
      at = (at +64u ((64u) 16)); /* u64 */
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A1184::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A1184::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A1184::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A1184::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
      TRAILB = 0; /* int:i */
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
  }
  return (st, ALL, offset);
}

inline
fn A1184::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1184::ASIZE] buf0,
                                reg const ptr u8[A1184::ASIZE] buf1,
                                reg const ptr u8[A1184::ASIZE] buf2,
                                reg const ptr u8[A1184::ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1184::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1184::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                       offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1184::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                       RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A1184::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1184::__dumpstate_array_avx2x4 (reg mut ptr u8[A1184::ASIZE] buf0,
                                   reg mut ptr u8[A1184::ASIZE] buf1,
                                   reg mut ptr u8[A1184::ASIZE] buf2,
                                   reg mut ptr u8[A1184::ASIZE] buf3,
                                   reg u64 offset, inline int LEN,
                                   reg const ptr u256[25] st) -> (reg mut ptr u8[A1184::ASIZE],
                                                                 reg mut ptr u8[A1184::ASIZE],
                                                                 reg mut ptr u8[A1184::ASIZE],
                                                                 reg mut ptr u8[A1184::ASIZE],
                                                                 reg u64) {
  reg u64 i;
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  
  i = ((64u) 0); /* u64 */
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */
    i = (i +64u ((64u) 32)); /* u64 */
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */
    buf1.[#unaligned :u256 offset] = x1; /* u256 */
    buf2.[#unaligned :u256 offset] = x2; /* u256 */
    buf3.[#unaligned :u256 offset] = x3; /* u256 */
    offset = (offset +64u ((64u) 32)); /* u64 */
  }
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    buf0.[#unaligned :u64 offset] = t0; /* u64 */
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    buf1.[#unaligned :u64 offset] = t1; /* u64 */
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    buf2.[#unaligned :u64 offset] = t2; /* u64 */
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    buf3.[#unaligned :u64 offset] = t3; /* u64 */
    i = (i +64u ((64u) 8)); /* u64 */
    offset = (offset +64u ((64u) 8)); /* u64 */
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1184::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1184::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1184::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1184::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1184::__squeeze_array_avx2x4 (reg mut ptr u8[A1184::ASIZE] buf0,
                                 reg mut ptr u8[A1184::ASIZE] buf1,
                                 reg mut ptr u8[A1184::ASIZE] buf2,
                                 reg mut ptr u8[A1184::ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A1184::ASIZE], reg mut ptr u8[A1184::ASIZE],
reg mut ptr u8[A1184::ASIZE], reg mut ptr u8[A1184::ASIZE], reg u64,
reg mut ptr u256[25]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A1184::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                          RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1184::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO,
                                        st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A1568::ASIZE = 1568;

inline
fn A1568::__aread_subu64 (reg const ptr u8[A1568::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          DELTA = (DELTA + 1); /* int:i */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1568::__aread_bcast_4subu64 (reg const ptr u8[A1568::ASIZE] buf,
                                reg u64 offset, inline int DELTA,
                                inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1568::__aread_subu128 (reg const ptr u8[A1568::ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1568::__aread_subu256 (reg const ptr u8[A1568::ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A1568::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A1568::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1568::__awrite_subu64 (reg mut ptr u8[A1568::ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1568::ASIZE], inline int, inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */
        DELTA = (DELTA + 1); /* int:i */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1568::__awrite_subu128 (reg mut ptr u8[A1568::ASIZE] buf, reg u64 offset,
                           inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A1568::ASIZE], inline int, inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, DELTA, LEN) =
        A1568::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1568::__awrite_subu256 (reg mut ptr u8[A1568::ASIZE] buf, reg u64 offset,
                           inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A1568::ASIZE], inline int, inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = t128; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1568::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1568::__addstate_array_avx2 (reg u256[7] st,
                                reg const ptr u8[A1568::ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int DELTA;
  reg u64 t64;
  reg u128 t128_0;
  reg u256 r0;
  reg u256 r1;
  reg u128 t128_1;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  DELTA = 0; /* int:i */
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = ((128u) t64); /* u128 */
  r0 = #VPBROADCAST_4u64(t128_0); /*  */
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A1568::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A1568::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A1568::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A1568::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A1568::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (st, offset);
}

inline
fn A1568::__absorb_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A1568::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1568::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, offset) = A1568::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A1568::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                              reg const ptr u8[A1568::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg mut ptr u64[25],
                                                    inline int, reg u64) {
  inline int ALL;
  inline int DELTA;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  DELTA = 0; /* int:i */
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A1568::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = buf.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
    DELTA = (DELTA + 16); /* int:i */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
    DELTA = (DELTA + 8); /* int:i */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A1568::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (pst, ALL, offset);
}

inline
fn A1568::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                               reg u256[7] st,
                               reg const ptr u8[A1568::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A1568::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A1568::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1568::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A1568::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A1568::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A1568::__dumpstate_array_avx2 (reg mut ptr u8[A1568::ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 reg u256[7] st) -> (reg mut ptr u8[A1568::ASIZE],
                                                    reg u64) {
  inline int DELTA;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1568::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A1568::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) =
    A1568::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, DELTA, LEN) = A1568::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
      #[inline]
      (buf, DELTA, LEN) =
        A1568::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (buf, offset);
}

inline
fn A1568::__squeeze_array_avx2 (reg mut ptr u8[A1568::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st, inline int RATE8) -> (reg mut ptr u8[A1568::ASIZE],
                                                                    reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) =
          A1568::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A1568::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A1568::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                        inline int AT,
                                        reg const ptr u8[A1568::ASIZE] buf,
                                        reg u64 offset, inline int LEN,
                                        inline int TRAILB) -> (reg mut ptr u256[25],
                                                              inline int,
                                                              reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u256 t256;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (32 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A1568::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A1568::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A1568::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
    st.[#unaligned at] = t256; /* u256 */
  }
  return (st, ALL, offset);
}

inline
fn A1568::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A1568::ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1568::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1568::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                             (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1568::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A1568::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1568::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1568::ASIZE] buf0,
                                  reg const ptr u8[A1568::ASIZE] buf1,
                                  reg const ptr u8[A1568::ASIZE] buf2,
                                  reg const ptr u8[A1568::ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  inline int TRAILB) -> (reg mut ptr u256[25],
                                                        inline int, reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (4 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A1568::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A1568::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A1568::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A1568::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
      TRAILB = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A1568::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A1568::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A1568::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A1568::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 = (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */
      at = (at +64u ((64u) 16)); /* u64 */
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A1568::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A1568::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A1568::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A1568::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
      TRAILB = 0; /* int:i */
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
  }
  return (st, ALL, offset);
}

inline
fn A1568::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1568::ASIZE] buf0,
                                reg const ptr u8[A1568::ASIZE] buf1,
                                reg const ptr u8[A1568::ASIZE] buf2,
                                reg const ptr u8[A1568::ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1568::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1568::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                       offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1568::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                       RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A1568::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1568::__dumpstate_array_avx2x4 (reg mut ptr u8[A1568::ASIZE] buf0,
                                   reg mut ptr u8[A1568::ASIZE] buf1,
                                   reg mut ptr u8[A1568::ASIZE] buf2,
                                   reg mut ptr u8[A1568::ASIZE] buf3,
                                   reg u64 offset, inline int LEN,
                                   reg const ptr u256[25] st) -> (reg mut ptr u8[A1568::ASIZE],
                                                                 reg mut ptr u8[A1568::ASIZE],
                                                                 reg mut ptr u8[A1568::ASIZE],
                                                                 reg mut ptr u8[A1568::ASIZE],
                                                                 reg u64) {
  reg u64 i;
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  
  i = ((64u) 0); /* u64 */
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */
    i = (i +64u ((64u) 32)); /* u64 */
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */
    buf1.[#unaligned :u256 offset] = x1; /* u256 */
    buf2.[#unaligned :u256 offset] = x2; /* u256 */
    buf3.[#unaligned :u256 offset] = x3; /* u256 */
    offset = (offset +64u ((64u) 32)); /* u64 */
  }
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    buf0.[#unaligned :u64 offset] = t0; /* u64 */
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    buf1.[#unaligned :u64 offset] = t1; /* u64 */
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    buf2.[#unaligned :u64 offset] = t2; /* u64 */
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    buf3.[#unaligned :u64 offset] = t3; /* u64 */
    i = (i +64u ((64u) 8)); /* u64 */
    offset = (offset +64u ((64u) 8)); /* u64 */
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1568::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1568::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1568::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1568::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1568::__squeeze_array_avx2x4 (reg mut ptr u8[A1568::ASIZE] buf0,
                                 reg mut ptr u8[A1568::ASIZE] buf1,
                                 reg mut ptr u8[A1568::ASIZE] buf2,
                                 reg mut ptr u8[A1568::ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A1568::ASIZE], reg mut ptr u8[A1568::ASIZE],
reg mut ptr u8[A1568::ASIZE], reg mut ptr u8[A1568::ASIZE], reg u64,
reg mut ptr u256[25]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A1568::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                          RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1568::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO,
                                        st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A1120::ASIZE = 1120;

inline
fn A1120::__aread_subu64 (reg const ptr u8[A1120::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          DELTA = (DELTA + 1); /* int:i */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1120::__aread_bcast_4subu64 (reg const ptr u8[A1120::ASIZE] buf,
                                reg u64 offset, inline int DELTA,
                                inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1120::__aread_subu128 (reg const ptr u8[A1120::ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1120::__aread_subu256 (reg const ptr u8[A1120::ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A1120::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A1120::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1120::__awrite_subu64 (reg mut ptr u8[A1120::ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1120::ASIZE], inline int, inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */
        DELTA = (DELTA + 1); /* int:i */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1120::__awrite_subu128 (reg mut ptr u8[A1120::ASIZE] buf, reg u64 offset,
                           inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A1120::ASIZE], inline int, inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, DELTA, LEN) =
        A1120::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1120::__awrite_subu256 (reg mut ptr u8[A1120::ASIZE] buf, reg u64 offset,
                           inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A1120::ASIZE], inline int, inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = t128; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1120::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1120::__addstate_array_avx2 (reg u256[7] st,
                                reg const ptr u8[A1120::ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int DELTA;
  reg u64 t64;
  reg u128 t128_0;
  reg u256 r0;
  reg u256 r1;
  reg u128 t128_1;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  DELTA = 0; /* int:i */
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = ((128u) t64); /* u128 */
  r0 = #VPBROADCAST_4u64(t128_0); /*  */
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A1120::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A1120::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A1120::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A1120::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A1120::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (st, offset);
}

inline
fn A1120::__absorb_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A1120::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1120::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, offset) = A1120::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A1120::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                              reg const ptr u8[A1120::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg mut ptr u64[25],
                                                    inline int, reg u64) {
  inline int ALL;
  inline int DELTA;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  DELTA = 0; /* int:i */
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A1120::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = buf.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
    DELTA = (DELTA + 16); /* int:i */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
    DELTA = (DELTA + 8); /* int:i */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A1120::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (pst, ALL, offset);
}

inline
fn A1120::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                               reg u256[7] st,
                               reg const ptr u8[A1120::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A1120::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A1120::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1120::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A1120::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A1120::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A1120::__dumpstate_array_avx2 (reg mut ptr u8[A1120::ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 reg u256[7] st) -> (reg mut ptr u8[A1120::ASIZE],
                                                    reg u64) {
  inline int DELTA;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1120::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A1120::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) =
    A1120::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, DELTA, LEN) = A1120::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
      #[inline]
      (buf, DELTA, LEN) =
        A1120::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (buf, offset);
}

inline
fn A1120::__squeeze_array_avx2 (reg mut ptr u8[A1120::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st, inline int RATE8) -> (reg mut ptr u8[A1120::ASIZE],
                                                                    reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) =
          A1120::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A1120::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A1120::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                        inline int AT,
                                        reg const ptr u8[A1120::ASIZE] buf,
                                        reg u64 offset, inline int LEN,
                                        inline int TRAILB) -> (reg mut ptr u256[25],
                                                              inline int,
                                                              reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u256 t256;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (32 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A1120::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A1120::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A1120::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
    st.[#unaligned at] = t256; /* u256 */
  }
  return (st, ALL, offset);
}

inline
fn A1120::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A1120::ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1120::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1120::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                             (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1120::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A1120::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1120::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1120::ASIZE] buf0,
                                  reg const ptr u8[A1120::ASIZE] buf1,
                                  reg const ptr u8[A1120::ASIZE] buf2,
                                  reg const ptr u8[A1120::ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  inline int TRAILB) -> (reg mut ptr u256[25],
                                                        inline int, reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (4 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A1120::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A1120::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A1120::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A1120::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
      TRAILB = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A1120::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A1120::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A1120::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A1120::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 = (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */
      at = (at +64u ((64u) 16)); /* u64 */
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A1120::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A1120::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A1120::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A1120::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
      TRAILB = 0; /* int:i */
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
  }
  return (st, ALL, offset);
}

inline
fn A1120::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1120::ASIZE] buf0,
                                reg const ptr u8[A1120::ASIZE] buf1,
                                reg const ptr u8[A1120::ASIZE] buf2,
                                reg const ptr u8[A1120::ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1120::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1120::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                       offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1120::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                       RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A1120::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1120::__dumpstate_array_avx2x4 (reg mut ptr u8[A1120::ASIZE] buf0,
                                   reg mut ptr u8[A1120::ASIZE] buf1,
                                   reg mut ptr u8[A1120::ASIZE] buf2,
                                   reg mut ptr u8[A1120::ASIZE] buf3,
                                   reg u64 offset, inline int LEN,
                                   reg const ptr u256[25] st) -> (reg mut ptr u8[A1120::ASIZE],
                                                                 reg mut ptr u8[A1120::ASIZE],
                                                                 reg mut ptr u8[A1120::ASIZE],
                                                                 reg mut ptr u8[A1120::ASIZE],
                                                                 reg u64) {
  reg u64 i;
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  
  i = ((64u) 0); /* u64 */
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */
    i = (i +64u ((64u) 32)); /* u64 */
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */
    buf1.[#unaligned :u256 offset] = x1; /* u256 */
    buf2.[#unaligned :u256 offset] = x2; /* u256 */
    buf3.[#unaligned :u256 offset] = x3; /* u256 */
    offset = (offset +64u ((64u) 32)); /* u64 */
  }
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    buf0.[#unaligned :u64 offset] = t0; /* u64 */
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    buf1.[#unaligned :u64 offset] = t1; /* u64 */
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    buf2.[#unaligned :u64 offset] = t2; /* u64 */
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    buf3.[#unaligned :u64 offset] = t3; /* u64 */
    i = (i +64u ((64u) 8)); /* u64 */
    offset = (offset +64u ((64u) 8)); /* u64 */
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1120::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1120::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1120::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1120::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1120::__squeeze_array_avx2x4 (reg mut ptr u8[A1120::ASIZE] buf0,
                                 reg mut ptr u8[A1120::ASIZE] buf1,
                                 reg mut ptr u8[A1120::ASIZE] buf2,
                                 reg mut ptr u8[A1120::ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A1120::ASIZE], reg mut ptr u8[A1120::ASIZE],
reg mut ptr u8[A1120::ASIZE], reg mut ptr u8[A1120::ASIZE], reg u64,
reg mut ptr u256[25]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A1120::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                          RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1120::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO,
                                        st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A1600::ASIZE = 1600;

inline
fn A1600::__aread_subu64 (reg const ptr u8[A1600::ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          DELTA = (DELTA + 1); /* int:i */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1600::__aread_bcast_4subu64 (reg const ptr u8[A1600::ASIZE] buf,
                                reg u64 offset, inline int DELTA,
                                inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1600::__aread_subu128 (reg const ptr u8[A1600::ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1600::__aread_subu256 (reg const ptr u8[A1600::ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A1600::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A1600::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1600::__awrite_subu64 (reg mut ptr u8[A1600::ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1600::ASIZE], inline int, inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */
        DELTA = (DELTA + 1); /* int:i */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1600::__awrite_subu128 (reg mut ptr u8[A1600::ASIZE] buf, reg u64 offset,
                           inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A1600::ASIZE], inline int, inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, DELTA, LEN) =
        A1600::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1600::__awrite_subu256 (reg mut ptr u8[A1600::ASIZE] buf, reg u64 offset,
                           inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A1600::ASIZE], inline int, inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = t128; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1600::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1600::__addstate_array_avx2 (reg u256[7] st,
                                reg const ptr u8[A1600::ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg u256[7], reg u64) {
  inline int DELTA;
  reg u64 t64;
  reg u128 t128_0;
  reg u256 r0;
  reg u256 r1;
  reg u128 t128_1;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  DELTA = 0; /* int:i */
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = ((128u) t64); /* u128 */
  r0 = #VPBROADCAST_4u64(t128_0); /*  */
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A1600::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A1600::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A1600::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A1600::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A1600::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (st, offset);
}

inline
fn A1600::__absorb_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A1600::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1600::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, offset) = A1600::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A1600::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                              reg const ptr u8[A1600::ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg mut ptr u64[25],
                                                    inline int, reg u64) {
  inline int ALL;
  inline int DELTA;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  DELTA = 0; /* int:i */
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A1600::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = buf.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
    DELTA = (DELTA + 16); /* int:i */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
    DELTA = (DELTA + 8); /* int:i */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A1600::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (pst, ALL, offset);
}

inline
fn A1600::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                               reg u256[7] st,
                               reg const ptr u8[A1600::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A1600::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A1600::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1600::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A1600::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A1600::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A1600::__dumpstate_array_avx2 (reg mut ptr u8[A1600::ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 reg u256[7] st) -> (reg mut ptr u8[A1600::ASIZE],
                                                    reg u64) {
  inline int DELTA;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1600::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A1600::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) =
    A1600::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, DELTA, LEN) = A1600::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
      #[inline]
      (buf, DELTA, LEN) =
        A1600::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (buf, offset);
}

inline
fn A1600::__squeeze_array_avx2 (reg mut ptr u8[A1600::ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st, inline int RATE8) -> (reg mut ptr u8[A1600::ASIZE],
                                                                    reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) =
          A1600::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A1600::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A1600::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                        inline int AT,
                                        reg const ptr u8[A1600::ASIZE] buf,
                                        reg u64 offset, inline int LEN,
                                        inline int TRAILB) -> (reg mut ptr u256[25],
                                                              inline int,
                                                              reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u256 t256;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (32 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A1600::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A1600::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A1600::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
    st.[#unaligned at] = t256; /* u256 */
  }
  return (st, ALL, offset);
}

inline
fn A1600::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A1600::ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1600::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1600::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                             (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1600::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A1600::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1600::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1600::ASIZE] buf0,
                                  reg const ptr u8[A1600::ASIZE] buf1,
                                  reg const ptr u8[A1600::ASIZE] buf2,
                                  reg const ptr u8[A1600::ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  inline int TRAILB) -> (reg mut ptr u256[25],
                                                        inline int, reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (4 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A1600::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A1600::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A1600::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A1600::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
      TRAILB = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A1600::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A1600::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A1600::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A1600::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 = (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */
      at = (at +64u ((64u) 16)); /* u64 */
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A1600::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A1600::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A1600::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A1600::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
      TRAILB = 0; /* int:i */
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
  }
  return (st, ALL, offset);
}

inline
fn A1600::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1600::ASIZE] buf0,
                                reg const ptr u8[A1600::ASIZE] buf1,
                                reg const ptr u8[A1600::ASIZE] buf2,
                                reg const ptr u8[A1600::ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1600::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1600::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                       offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1600::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                       RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      A1600::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1600::__dumpstate_array_avx2x4 (reg mut ptr u8[A1600::ASIZE] buf0,
                                   reg mut ptr u8[A1600::ASIZE] buf1,
                                   reg mut ptr u8[A1600::ASIZE] buf2,
                                   reg mut ptr u8[A1600::ASIZE] buf3,
                                   reg u64 offset, inline int LEN,
                                   reg const ptr u256[25] st) -> (reg mut ptr u8[A1600::ASIZE],
                                                                 reg mut ptr u8[A1600::ASIZE],
                                                                 reg mut ptr u8[A1600::ASIZE],
                                                                 reg mut ptr u8[A1600::ASIZE],
                                                                 reg u64) {
  reg u64 i;
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  
  i = ((64u) 0); /* u64 */
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */
    i = (i +64u ((64u) 32)); /* u64 */
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */
    buf1.[#unaligned :u256 offset] = x1; /* u256 */
    buf2.[#unaligned :u256 offset] = x2; /* u256 */
    buf3.[#unaligned :u256 offset] = x3; /* u256 */
    offset = (offset +64u ((64u) 32)); /* u64 */
  }
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    buf0.[#unaligned :u64 offset] = t0; /* u64 */
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    buf1.[#unaligned :u64 offset] = t1; /* u64 */
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    buf2.[#unaligned :u64 offset] = t2; /* u64 */
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    buf3.[#unaligned :u64 offset] = t3; /* u64 */
    i = (i +64u ((64u) 8)); /* u64 */
    offset = (offset +64u ((64u) 8)); /* u64 */
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1600::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1600::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1600::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1600::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1600::__squeeze_array_avx2x4 (reg mut ptr u8[A1600::ASIZE] buf0,
                                 reg mut ptr u8[A1600::ASIZE] buf1,
                                 reg mut ptr u8[A1600::ASIZE] buf2,
                                 reg mut ptr u8[A1600::ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A1600::ASIZE], reg mut ptr u8[A1600::ASIZE],
reg mut ptr u8[A1600::ASIZE], reg mut ptr u8[A1600::ASIZE], reg u64,
reg mut ptr u256[25]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A1600::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                          RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1600::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO,
                                        st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int ABUFLEN::ASIZE = 536;

inline
fn ABUFLEN::__aread_subu64 (reg const ptr u8[ABUFLEN::ASIZE] buf,
                           reg u64 offset, inline int DELTA, inline int LEN,
                           inline int TRAIL) -> (inline int, inline int,
                                                inline int, reg u64) {
  reg u64 w;
  inline int ILEN;
  reg u64 t16;
  reg u64 t8;
  
  ILEN = LEN; /* int:i */
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */
    TRAIL = 0; /* int:i */
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      } else {
        w = ((64u) 0); /* u64 */
      }
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      } else {
        t16 = ((64u) 0); /* u64 */
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */
          }
          DELTA = (DELTA + 1); /* int:i */
          LEN = (LEN - 1); /* int:i */
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */
        }
        TRAIL = 0; /* int:i */
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */
        t16 = (t16 |64u t8); /* u64 */
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */
      w = (w |64u t16); /* u64 */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn ABUFLEN::__aread_bcast_4subu64 (reg const ptr u8[ABUFLEN::ASIZE] buf,
                                  reg u64 offset, inline int DELTA,
                                  inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256) {
  reg u256 w;
  reg u64 t64;
  reg u128 t128;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      t128 = ((128u) t64); /* u128 */
      w = #VPBROADCAST_4u64(t128); /*  */
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn ABUFLEN::__aread_subu128 (reg const ptr u8[ABUFLEN::ASIZE] buf,
                            reg u64 offset, inline int DELTA, inline int LEN,
                            inline int TRAIL) -> (inline int, inline int,
                                                 inline int, reg u128) {
  reg u128 w;
  reg u64 t64;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn ABUFLEN::__aread_subu256 (reg const ptr u8[ABUFLEN::ASIZE] buf,
                            reg u64 offset, inline int DELTA, inline int LEN,
                            inline int TRAIL) -> (inline int, inline int,
                                                 inline int, reg u256) {
  reg u256 w;
  reg u128 t128_0;
  reg u128 t128_1;
  
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          ABUFLEN::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      } else {
        t128_1 = #set0_128(); /*  */
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          ABUFLEN::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn ABUFLEN::__awrite_subu64 (reg mut ptr u8[ABUFLEN::ASIZE] buf,
                            reg u64 offset, inline int DELTA, inline int LEN,
                            reg u64 w) -> (reg mut ptr u8[ABUFLEN::ASIZE],
                                          inline int, inline int) {
  
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */
      DELTA = (DELTA + 8); /* int:i */
      LEN = (LEN - 8); /* int:i */
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */
        w = (w >>64u ((8u) 32)); /* u64 */
        DELTA = (DELTA + 4); /* int:i */
        LEN = (LEN - 4); /* int:i */
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */
        w = (w >>64u ((8u) 16)); /* u64 */
        DELTA = (DELTA + 2); /* int:i */
        LEN = (LEN - 2); /* int:i */
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */
        DELTA = (DELTA + 1); /* int:i */
        LEN = (LEN - 1); /* int:i */
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn ABUFLEN::__awrite_subu128 (reg mut ptr u8[ABUFLEN::ASIZE] buf,
                             reg u64 offset, inline int DELTA,
                             inline int LEN, reg u128 w) -> (reg mut ptr u8[ABUFLEN::ASIZE],
                                                            inline int,
                                                            inline int) {
  reg u64 t64;
  
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */
      DELTA = (DELTA + 16); /* int:i */
      LEN = (LEN - 16); /* int:i */
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */
        DELTA = (DELTA + 8); /* int:i */
        LEN = (LEN - 8); /* int:i */
        w = #VPUNPCKH_2u64(w, w); /*  */
      }
      t64 = w; /* u64 */
      #[inline]
      (buf, DELTA, LEN) =
        ABUFLEN::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn ABUFLEN::__awrite_subu256 (reg mut ptr u8[ABUFLEN::ASIZE] buf,
                             reg u64 offset, inline int DELTA,
                             inline int LEN, reg u256 w) -> (reg mut ptr u8[ABUFLEN::ASIZE],
                                                            inline int,
                                                            inline int) {
  reg u128 t128;
  
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */
      DELTA = (DELTA + 32); /* int:i */
      LEN = (LEN - 32); /* int:i */
    } else {
      t128 = w; /* u128 */
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = t128; /* u128 */
        DELTA = (DELTA + 16); /* int:i */
        LEN = (LEN - 16); /* int:i */
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */
      }
      #[inline]
      (buf, DELTA, LEN) =
        ABUFLEN::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn ABUFLEN::__addstate_array_avx2 (reg u256[7] st,
                                  reg const ptr u8[ABUFLEN::ASIZE] buf,
                                  reg u64 offset, inline int LEN,
                                  inline int TRAILB) -> (reg u256[7],
                                                        reg u64) {
  inline int DELTA;
  reg u64 t64;
  reg u128 t128_0;
  reg u256 r0;
  reg u256 r1;
  reg u128 t128_1;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r2;
  reg u256 r6;
  
  DELTA = 0; /* int:i */
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = ((128u) t64); /* u128 */
  r0 = #VPBROADCAST_4u64(t128_0); /*  */
  st[0] = (st[0] ^256u r0); /* u256 */
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    ABUFLEN::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      ABUFLEN::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      ABUFLEN::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      ABUFLEN::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */
    st[2] = (st[2] ^256u r2); /* u256 */
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      ABUFLEN::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (st, offset);
}

inline
fn ABUFLEN::__absorb_array_avx2 (reg u256[7] st,
                                reg const ptr u8[ABUFLEN::ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */
  ITERS = (LEN / RATE8); /* int:i */
  if (0 < ITERS) {
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) =
        ABUFLEN::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
  }
  LEN = (LEN % RATE8); /* int:i */
  #[inline]
  (st, offset) =
    ABUFLEN::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn ABUFLEN::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                                reg const ptr u8[ABUFLEN::ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg mut ptr u64[25],
                                                      inline int, reg u64) {
  inline int ALL;
  inline int DELTA;
  inline int LO;
  reg u64 at;
  reg u64 t64;
  reg u256 t256;
  reg u128 t128;
  
  DELTA = 0; /* int:i */
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (AT / 8)); /* u64 */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          ABUFLEN::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */
      pst[at] = (pst[at] ^64u t64); /* u64 */
      at = (at +64u ((64u) 1)); /* u64 */
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      t256 = buf.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = (LEN % 32); /* int:i */
  }
  if (16 <= LEN) {
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */
    DELTA = (DELTA + 16); /* int:i */
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */
    at = (at +64u ((64u) 2)); /* u64 */
    LEN = (LEN - 16); /* int:i */
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
    DELTA = (DELTA + 8); /* int:i */
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */
    at = (at +64u ((64u) 1)); /* u64 */
    LEN = (LEN - 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      ABUFLEN::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (pst, ALL, offset);
}

inline
fn ABUFLEN::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                                 reg u256[7] st,
                                 reg const ptr u8[ABUFLEN::ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64) {
  inline int ALL;
  reg u64 i;
  inline int ITERS;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      ABUFLEN::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */
          i = (i +64u ((64u) 1)); /* u64 */
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        ABUFLEN::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) =
        ABUFLEN::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        ABUFLEN::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          ABUFLEN::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn ABUFLEN::__dumpstate_array_avx2 (reg mut ptr u8[ABUFLEN::ASIZE] buf,
                                   reg u64 offset, inline int LEN,
                                   reg u256[7] st) -> (reg mut ptr u8[ABUFLEN::ASIZE],
                                                      reg u64) {
  inline int DELTA;
  reg u128 t128_0;
  reg u128 t128_1;
  reg u64 t;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  reg u256 t256_4;
  
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      ABUFLEN::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) =
    ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    t128_0 = st[2]; /* u128 */
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */
    t = t128_1; /* u64 */
    #[inline]
    (buf, DELTA, LEN) = ABUFLEN::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */
    if (0 < LEN) {
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
      #[inline]
      (buf, DELTA, LEN) =
        ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  return (buf, offset);
}

inline
fn ABUFLEN::__squeeze_array_avx2 (reg mut ptr u8[ABUFLEN::ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 reg u256[7] st, inline int RATE8) -> 
(reg mut ptr u8[ABUFLEN::ASIZE], reg u256[7]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) =
          ABUFLEN::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = ABUFLEN::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn ABUFLEN::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                          inline int AT,
                                          reg const ptr u8[ABUFLEN::ASIZE] buf,
                                          reg u64 offset, inline int LEN,
                                          inline int TRAILB) -> (reg mut ptr u256[25],
                                                                inline int,
                                                                reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u256 t256;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (32 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        ABUFLEN::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */
        DELTA = (DELTA + (8 - LO)); /* int:i */
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          ABUFLEN::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    DELTA = 0; /* int:i */
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
      st.[#unaligned at] = t256; /* u256 */
      at = (at +64u ((64u) 32)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      ABUFLEN::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */
    st.[#unaligned at] = t256; /* u256 */
  }
  return (st, ALL, offset);
}

inline
fn ABUFLEN::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                        inline int AT,
                                        reg const ptr u8[ABUFLEN::ASIZE] buf,
                                        reg u64 offset, inline int LEN,
                                        inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      ABUFLEN::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        ABUFLEN::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                               (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        ABUFLEN::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      ABUFLEN::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn ABUFLEN::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                    reg const ptr u8[ABUFLEN::ASIZE] buf0,
                                    reg const ptr u8[ABUFLEN::ASIZE] buf1,
                                    reg const ptr u8[ABUFLEN::ASIZE] buf2,
                                    reg const ptr u8[ABUFLEN::ASIZE] buf3,
                                    reg u64 offset, inline int LEN,
                                    inline int TRAILB) -> (reg mut ptr u256[25],
                                                          inline int,
                                                          reg u64) {
  inline int ALL;
  inline int LO;
  reg u64 at;
  inline int DELTA;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  reg u256 t256_0;
  reg u256 t256_1;
  reg u256 t256_2;
  reg u256 t256_3;
  
  ALL = (AT + LEN); /* int:i */
  LO = (AT % 8); /* int:i */
  at = ((64u) (4 * (AT / 8))); /* u64 */
  DELTA = 0; /* int:i */
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        ABUFLEN::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        ABUFLEN::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        ABUFLEN::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        ABUFLEN::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      LO = 0; /* int:i */
      AT = 0; /* int:i */
      LEN = 0; /* int:i */
      TRAILB = 0; /* int:i */
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          ABUFLEN::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          ABUFLEN::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          ABUFLEN::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          ABUFLEN::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */
      AT = (AT + (8 - LO)); /* int:i */
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */
  DELTA = 0; /* int:i */
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */
      offset = (offset +64u ((64u) 32)); /* u64 */
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 = (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */
      at = (at +64u ((64u) 16)); /* u64 */
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */
      offset = (offset +64u ((64u) 8)); /* u64 */
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
      at = (at +64u ((64u) 4)); /* u64 */
    }
    LEN = ((AT + LEN) % 8); /* int:i */
  }
  LO = ((AT + LEN) % 8); /* int:i */
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      ABUFLEN::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      ABUFLEN::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      ABUFLEN::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      ABUFLEN::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */
      TRAILB = 0; /* int:i */
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */
  }
  return (st, ALL, offset);
}

inline
fn ABUFLEN::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[ABUFLEN::ASIZE] buf0,
                                  reg const ptr u8[ABUFLEN::ASIZE] buf1,
                                  reg const ptr u8[ABUFLEN::ASIZE] buf2,
                                  reg const ptr u8[ABUFLEN::ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64) {
  inline int ALL;
  inline int ITERS;
  reg u64 i;
  
  ALL = (AT + LEN); /* int:i */
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      ABUFLEN::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                       offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        ABUFLEN::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                         offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */
    }
    ITERS = (LEN / RATE8); /* int:i */
    i = ((64u) 0); /* u64 */
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        ABUFLEN::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3,
                                         offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */
    }
    LEN = (ALL % RATE8); /* int:i */
    #[inline]
    (st, AT, offset) =
      ABUFLEN::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                       LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn ABUFLEN::__dumpstate_array_avx2x4 (reg mut ptr u8[ABUFLEN::ASIZE] buf0,
                                     reg mut ptr u8[ABUFLEN::ASIZE] buf1,
                                     reg mut ptr u8[ABUFLEN::ASIZE] buf2,
                                     reg mut ptr u8[ABUFLEN::ASIZE] buf3,
                                     reg u64 offset, inline int LEN,
                                     reg const ptr u256[25] st) -> (reg mut ptr u8[ABUFLEN::ASIZE],
                                                                   reg mut ptr u8[ABUFLEN::ASIZE],
                                                                   reg mut ptr u8[ABUFLEN::ASIZE],
                                                                   reg mut ptr u8[ABUFLEN::ASIZE],
                                                                   reg u64) {
  reg u64 i;
  reg u256 x0;
  reg u256 x1;
  reg u256 x2;
  reg u256 x3;
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  
  i = ((64u) 0); /* u64 */
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */
    i = (i +64u ((64u) 32)); /* u64 */
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */
    buf1.[#unaligned :u256 offset] = x1; /* u256 */
    buf2.[#unaligned :u256 offset] = x2; /* u256 */
    buf3.[#unaligned :u256 offset] = x3; /* u256 */
    offset = (offset +64u ((64u) 32)); /* u64 */
  }
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    buf0.[#unaligned :u64 offset] = t0; /* u64 */
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    buf1.[#unaligned :u64 offset] = t1; /* u64 */
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    buf2.[#unaligned :u64 offset] = t2; /* u64 */
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    buf3.[#unaligned :u64 offset] = t3; /* u64 */
    i = (i +64u ((64u) 8)); /* u64 */
    offset = (offset +64u ((64u) 8)); /* u64 */
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      ABUFLEN::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      ABUFLEN::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      ABUFLEN::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      ABUFLEN::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn ABUFLEN::__squeeze_array_avx2x4 (reg mut ptr u8[ABUFLEN::ASIZE] buf0,
                                   reg mut ptr u8[ABUFLEN::ASIZE] buf1,
                                   reg mut ptr u8[ABUFLEN::ASIZE] buf2,
                                   reg mut ptr u8[ABUFLEN::ASIZE] buf3,
                                   reg u64 offset, inline int LEN,
                                   reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[ABUFLEN::ASIZE], reg mut ptr u8[ABUFLEN::ASIZE],
reg mut ptr u8[ABUFLEN::ASIZE], reg mut ptr u8[ABUFLEN::ASIZE], reg u64,
reg mut ptr u256[25]) {
  inline int ITERS;
  inline int LO;
  reg u64 i;
  
  ITERS = (LEN / RATE8); /* int:i */
  LO = (LEN % RATE8); /* int:i */
  if (0 < LEN) {
    if (0 < ITERS) {
      i = ((64u) 0); /* u64 */
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          ABUFLEN::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                            RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        ABUFLEN::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO,
                                          st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

fn _sha3_256A_M1184 (#[spill_to_mmx] reg mut ptr u8[32] out, #[spill_to_mmx]
                    reg u64 in) -> (reg mut ptr u8[32]) {
  reg u256[7] st;
  reg u64 offset;
  
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* u64 */) = __absorb_imem_avx2(st, in, 1184, R136, SHA3);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (out, _ /* u256[?] */) =
    A32::__squeeze_array_avx2(out, offset, 32, st, R136);
  return (out);
}

fn _sha3_256A_M1568 (#[spill_to_mmx] reg mut ptr u8[32] out, #[spill_to_mmx]
                    reg u64 in) -> (reg mut ptr u8[32]) {
  reg u256[7] st;
  reg u64 offset;
  
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* u64 */) = __absorb_imem_avx2(st, in, 1568, R136, SHA3);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (out, _ /* u256[?] */) =
    A32::__squeeze_array_avx2(out, offset, 32, st, R136);
  return (out);
}

fn _sha3_512A_A33 (#[spill_to_mmx] reg mut ptr u8[64] out,
                  reg const ptr u8[33] in) -> (reg mut ptr u8[64]) {
  reg u256[7] st;
  reg u64 offset;
  
  #[inline]
  st = __state_init_avx2();
  offset = ((64u) 0); /* u64 */
  #[inline]
  (st, _ /* u64 */) =
    A33::__absorb_array_avx2(st, in, offset, 33, R72, SHA3);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (out, _ /* u256[?] */) =
    A64::__squeeze_array_avx2(out, offset, 64, st, R72);
  return (out);
}

fn _sha3_512A_A64 (reg mut ptr u8[64] out, reg const ptr u8[64] in) -> 
(reg mut ptr u8[64]) {
  reg u256[7] st;
  reg u64 offset;
  
  #[inline]
  st = __state_init_avx2();
  offset = ((64u) 0); /* u64 */
  #[inline]
  (st, _ /* u64 */) =
    A64::__absorb_array_avx2(st, in, offset, 64, R72, SHA3);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (out, _ /* u256[?] */) =
    A64::__squeeze_array_avx2(out, offset, 64, st, R72);
  return (out);
}

fn _shake256_M32__M32_M1088 (reg u64 out, reg u64 in0, reg u64 in1) -> 
() {
  reg mut ptr u64[25] pst;
  stack u64[25] pst_s;
  reg u256[7] st;
  
  pst = pst_s; /* u64[25] */
  #[inline]
  (pst, st) = __pstate_init_avx2(pst);
  #[inline]
  (pst, _ /* int */, st, _ /* u64 */) =
    __pabsorb_imem_avx2(pst, 0, st, in0, 32, R136, UNFINISHED);
  #[inline]
  (pst, _ /* int */, st, _ /* u64 */) =
    __pabsorb_imem_avx2(pst, 32, st, in1, 1088, R136, SHAKE);
  #[inline]
  (_ /* u64 */, _ /* u256[?] */) = __squeeze_imem_avx2(out, 32, st, R136);
  return ();
}

fn _shake256_M32__M32_M1600 (reg u64 out, reg u64 in0, reg u64 in1) -> 
() {
  reg mut ptr u64[25] pst;
  stack u64[25] pst_s;
  reg u256[7] st;
  
  pst = pst_s; /* u64[25] */
  #[inline]
  (pst, st) = __pstate_init_avx2(pst);
  #[inline]
  (pst, _ /* int */, st, _ /* u64 */) =
    __pabsorb_imem_avx2(pst, 0, st, in0, 32, R136, UNFINISHED);
  #[inline]
  (pst, _ /* int */, st, _ /* u64 */) =
    __pabsorb_imem_avx2(pst, 32, st, in1, 1568, R136, SHAKE);
  #[inline]
  (_ /* u64 */, _ /* u256[?] */) = __squeeze_imem_avx2(out, 32, st, R136);
  return ();
}

fn _shake256_A128__A32_A1 (reg mut ptr u8[128] out,
                          reg const ptr u8[32] seed,
                          reg const ptr u8[1] nonce) -> (reg mut ptr u8[128]) {
  reg mut ptr u64[25] pst;
  stack u64[25] pst_s;
  reg u256[7] st;
  reg u64 offset;
  
  pst = pst_s; /* u64[25] */
  #[inline]
  (pst, st) = __pstate_init_avx2(pst);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (pst, _ /* int */, st, _ /* u64 */) =
    A32::__pabsorb_array_avx2(pst, 0, st, seed, offset, 32, R136, UNFINISHED);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (pst, _ /* int */, st, _ /* u64 */) =
    A1::__pabsorb_array_avx2(pst, 32, st, nonce, offset, 1, R136, SHAKE);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (out, _ /* u256[?] */) =
    A128::__squeeze_array_avx2(out, offset, 128, st, R136);
  return (out);
}

fn _shake256x4_A128__A32_A1 (reg mut ptr u8[128] out0,
                            reg mut ptr u8[128] out1,
                            reg mut ptr u8[128] out2,
                            reg mut ptr u8[128] out3,
                            reg const ptr u8[32] seed,
                            reg const ptr u8[4] nonces) -> (reg mut ptr u8[128],
                                                           reg mut ptr u8[128],
                                                           reg mut ptr u8[128],
                                                           reg mut ptr u8[128]) {
  reg mut ptr u256[25] st;
  stack u256[25] st_s;
  reg u64 offset;
  
  st = st_s; /* u256[25] */
  #[inline]
  st = __state_init_avx2x4(st);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (st, _ /* int */, _ /* u64 */) =
    A32::__absorb_bcast_array_avx2x4(st, 0, seed, offset, 32, R136,
                                     UNFINISHED);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (st, _ /* int */, _ /* u64 */) =
    A1::__absorb_array_avx2x4(st, 32, nonces[0 : 1], nonces[1 : 1],
                              nonces[2 : 1], nonces[3 : 1], offset, 1, R136,
                              SHAKE);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (out0, out1, out2, out3, _ /* u64 */, st) =
    A128::__squeeze_array_avx2x4(out0, out1, out2, out3, offset, 128, st,
                                 R136);
  return (out0, out1, out2, out3);
}

fn _shake128_absorb_A32_A2 (reg const ptr u8[32] seed,
                           reg const ptr u8[2] pos) -> (reg u256[7]) {
  reg u256[7] st;
  reg mut ptr u64[25] pst;
  stack u64[25] pst_s;
  reg u64 offset;
  
  pst = pst_s; /* u64[25] */
  #[inline]
  (pst, st) = __pstate_init_avx2(pst);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (pst, _ /* int */, st, _ /* u64 */) =
    A32::__pabsorb_array_avx2(pst, 0, st, seed, offset, 32, R168, UNFINISHED);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (pst, _ /* int */, st, _ /* u64 */) =
    A2::__pabsorb_array_avx2(pst, 32, st, pos, offset, 2, R168, SHAKE);
  return (st);
}

fn _shake128x4_absorb_A32_A2 (reg mut ptr u256[25] st,
                             reg const ptr u8[32] seed,
                             reg const ptr u8[(4 * 2)] pos) -> (reg mut ptr u256[25]) {
  reg u64 offset;
  inline int AT;
  
  #[inline]
  st = __state_init_avx2x4(st);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (st, AT, _ /* u64 */) =
    A32::__absorb_bcast_array_avx2x4(st, 0, seed, offset, 32, R168,
                                     UNFINISHED);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (st, _ /* int */, _ /* u64 */) =
    A2::__absorb_array_avx2x4(st, AT, pos[0 : 2], pos[2 : 2], pos[4 : 2],
                              pos[6 : 2], offset, 2, R168, SHAKE);
  return (st);
}

fn _shake128_squeeze3blocks (reg mut ptr u8[ABUFLEN::ASIZE] buf,
                            reg u256[7] st) -> (reg mut ptr u8[ABUFLEN::ASIZE]) {
  reg u64 offset;
  
  st = _keccakf1600_avx2(st);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (buf, offset) = ABUFLEN::__dumpstate_array_avx2(buf, offset, R168, st);
  st = _keccakf1600_avx2(st);
  #[inline]
  (buf, offset) = ABUFLEN::__dumpstate_array_avx2(buf, offset, R168, st);
  st = _keccakf1600_avx2(st);
  #[inline]
  (buf, offset) = ABUFLEN::__dumpstate_array_avx2(buf, offset, 200, st);
  return (buf);
}

fn _shake128_next_state (reg mut ptr u8[ABUFLEN::ASIZE] buf) -> (reg mut ptr u8[ABUFLEN::ASIZE]) {
  reg mut ptr u64[25] pst;
  reg u256[7] st;
  reg u64 offset;
  
  pst = buf[:u64 (2 * (168 / 8)) : 25]; /* u64[25] */
  #[inline]
  st = __state_from_pstate_avx2(pst);
  st = _keccakf1600_avx2(st);
  offset = ((64u) (2 * 168)); /* u64 */
  #[inline]
  (buf, _ /* u64 */) = ABUFLEN::__dumpstate_array_avx2(buf, offset, 200, st);
  return (buf);
}

fn _shake128x4_squeeze3blocks (reg mut ptr u256[25] st,
                              reg mut ptr u8[(4 * ABUFLEN::ASIZE)] buf) -> 
(reg mut ptr u256[25], reg mut ptr u8[(4 * ABUFLEN::ASIZE)]) {
  reg mut ptr u8[ABUFLEN::ASIZE] buf0;
  reg mut ptr u8[ABUFLEN::ASIZE] buf1;
  reg mut ptr u8[ABUFLEN::ASIZE] buf2;
  reg mut ptr u8[ABUFLEN::ASIZE] buf3;
  reg u64 offset;
  
  buf0 = buf[(0 * ABUFLEN::ASIZE) : ABUFLEN::ASIZE]; /* u8[ABUFLEN::ASIZE] */
  buf1 = buf[(1 * ABUFLEN::ASIZE) : ABUFLEN::ASIZE]; /* u8[ABUFLEN::ASIZE] */
  buf2 = buf[(2 * ABUFLEN::ASIZE) : ABUFLEN::ASIZE]; /* u8[ABUFLEN::ASIZE] */
  buf3 = buf[(3 * ABUFLEN::ASIZE) : ABUFLEN::ASIZE]; /* u8[ABUFLEN::ASIZE] */
  offset = ((64u) 0); /* u64 */
  st = _keccakf1600_avx2x4(st);
  #[inline]
  (buf0, buf1, buf2, buf3, offset) =
    ABUFLEN::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, R168,
                                      st);
  st = _keccakf1600_avx2x4(st);
  #[inline]
  (buf0, buf1, buf2, buf3, offset) =
    ABUFLEN::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, R168,
                                      st);
  st = _keccakf1600_avx2x4(st);
  #[inline]
  (buf0, buf1, buf2, buf3, offset) =
    ABUFLEN::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, 200, st);
  buf[(0 * ABUFLEN::ASIZE) : ABUFLEN::ASIZE] = buf0; /* u8[ABUFLEN::ASIZE] */
  buf[(1 * ABUFLEN::ASIZE) : ABUFLEN::ASIZE] = buf1; /* u8[ABUFLEN::ASIZE] */
  buf[(2 * ABUFLEN::ASIZE) : ABUFLEN::ASIZE] = buf2; /* u8[ABUFLEN::ASIZE] */
  buf[(3 * ABUFLEN::ASIZE) : ABUFLEN::ASIZE] = buf3; /* u8[ABUFLEN::ASIZE] */
  return (st, buf);
}

fn _sha3_256A_A1184 (#[spill_to_mmx] reg mut ptr u8[32] out, #[spill_to_mmx]
                    reg const ptr u8[1184] in) -> (reg mut ptr u8[32]) {
  reg u256[7] st;
  reg u64 offset;
  
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* u64 */) =
    A1184::__absorb_array_avx2(st, in, ((64u) 0), 1184, R136, SHA3);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (out, _ /* u256[?] */) =
    A32::__squeeze_array_avx2(out, offset, 32, st, R136);
  return (out);
}

fn _sha3_256A_A1568 (#[spill_to_mmx] reg mut ptr u8[32] out, #[spill_to_mmx]
                    reg const ptr u8[1568] in) -> (reg mut ptr u8[32]) {
  reg u256[7] st;
  reg u64 offset;
  
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* u64 */) =
    A1568::__absorb_array_avx2(st, in, ((64u) 0), 1568, R136, SHA3);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (out, _ /* u256[?] */) =
    A32::__squeeze_array_avx2(out, offset, 32, st, R136);
  return (out);
}

fn _shake256_A32__A1120 (reg mut ptr u8[32] out, reg const ptr u8[1120] in) -> 
(reg mut ptr u8[32]) {
  reg u256[7] st;
  reg u64 offset;
  
  #[inline]
  st = __state_init_avx2();
  offset = ((64u) 0); /* u64 */
  #[inline]
  (st, _ /* u64 */) =
    A1120::__absorb_array_avx2(st, in, offset, 1120, R136, SHAKE);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (out, _ /* u256[?] */) =
    A32::__squeeze_array_avx2(out, offset, 32, st, R136);
  return (out);
}

fn _shake256_A32__A1600 (reg mut ptr u8[32] out, reg const ptr u8[1600] in) -> 
(reg mut ptr u8[32]) {
  reg u256[7] st;
  reg u64 offset;
  
  #[inline]
  st = __state_init_avx2();
  offset = ((64u) 0); /* u64 */
  #[inline]
  (st, _ /* u64 */) =
    A1600::__absorb_array_avx2(st, in, offset, 1600, R136, SHAKE);
  offset = ((64u) 0); /* u64 */
  #[inline]
  (out, _ /* u256[?] */) =
    A32::__squeeze_array_avx2(out, offset, 32, st, R136);
  return (out);
}

fn _poly_add2 (reg mut ptr u16[MLKEM_N] rp, reg const ptr u16[MLKEM_N] bp) -> 
(reg mut ptr u16[MLKEM_N]) {
  inline int i;
  reg u256 a;
  reg u256 b;
  reg u256 r;
  
  for i = 0 to 16 {
    a = rp.[#unaligned :u256 (32 * i)]; /* u256 */
    b = bp.[#unaligned :u256 (32 * i)]; /* u256 */
    r = #VPADD_16u16(a, b); /*  */
    rp.[#unaligned :u256 (32 * i)] = r; /* u256 */
  }
  return (rp);
}

fn _poly_csubq (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N]) {
  reg u256 qx16;
  inline int i;
  reg u256 r;
  
  qx16 = /* global: */ jqx16[:u256 0]; /* u256 */
  for i = 0 to 16 {
    r = rp.[#unaligned :u256 (32 * i)]; /* u256 */
    #[inline]
    r = __csubq(r, qx16);
    rp.[#unaligned :u256 (32 * i)] = r; /* u256 */
  }
  return (rp);
}

inline
fn __w256_interleave_u16 (reg u256 al, reg u256 ah) -> (reg u256, reg u256) {
  reg u256 a0;
  reg u256 a1;
  
  a0 = #VPUNPCKL_16u16(al, ah); /*  */
  a1 = #VPUNPCKH_16u16(al, ah); /*  */
  return (a0, a1);
}

inline
fn __w256_deinterleave_u16 (reg u256 _zero, reg u256 a0, reg u256 a1) -> 
(reg u256, reg u256) {
  reg u256 al;
  reg u256 ah;
  
  al = #VPBLEND_16u16(a0, _zero, ((8u) 170)); /*  */
  ah = #VPBLEND_16u16(a1, _zero, ((8u) 170)); /*  */
  al = #VPACKUS_8u32(al, ah); /*  */
  a0 = #VPSRL_8u32(a0, ((128u) 16)); /*  */
  a1 = #VPSRL_8u32(a1, ((128u) 16)); /*  */
  ah = #VPACKUS_8u32(a0, a1); /*  */
  return (al, ah);
}

inline
fn __mont_red (reg u256 lo, reg u256 hi, reg u256 qx16, reg u256 qinvx16) -> 
(reg u256) {
  reg u256 m;
  
  m = #VPMULL_16u16(lo, qinvx16); /*  */
  m = #VPMULH_16u16(m, qx16); /*  */
  lo = #VPSUB_16u16(hi, m); /*  */
  return (lo);
}

inline
fn __wmul_16u16 (reg u256 x, reg u256 y) -> (reg u256, reg u256) {
  reg u256 xy0;
  reg u256 xy1;
  reg u256 xyL;
  reg u256 xyH;
  
  xyL = #VPMULL_16u16(x, y); /*  */
  xyH = #VPMULH_16u16(x, y); /*  */
  #[inline]
  (xy0, xy1) = __w256_interleave_u16(xyL, xyH);
  return (xy0, xy1);
}

inline
fn __schoolbook16x (reg u256 are, reg u256 aim, reg u256 bre, reg u256 bim,
                   reg u256 zeta, reg u256 zetaqinv, reg u256 qx16,
                   reg u256 qinvx16, inline int sign) -> (reg u256, reg u256) {
  reg u256 x0;
  reg u256 y0;
  reg u256 zaim;
  reg u256 ac0;
  reg u256 ac1;
  reg u256 ad0;
  reg u256 ad1;
  reg u256 bc0;
  reg u256 bc1;
  reg u256 zbd0;
  reg u256 zbd1;
  reg u256 x1;
  reg u256 y1;
  reg u256 _zero;
  
  #[inline]
  zaim = __fqmulprecomp16x(aim, zetaqinv, zeta, qx16);
  #[inline]
  (ac0, ac1) = __wmul_16u16(are, bre);
  #[inline]
  (ad0, ad1) = __wmul_16u16(are, bim);
  #[inline]
  (bc0, bc1) = __wmul_16u16(aim, bre);
  #[inline]
  (zbd0, zbd1) = __wmul_16u16(zaim, bim);
  if (sign == 0) {
    x0 = #VPADD_8u32(ac0, zbd0); /*  */
    x1 = #VPADD_8u32(ac1, zbd1); /*  */
  } else {
    x0 = #VPSUB_8u32(ac0, zbd0); /*  */
    x1 = #VPSUB_8u32(ac1, zbd1); /*  */
  }
  y0 = #VPADD_8u32(bc0, ad0); /*  */
  y1 = #VPADD_8u32(bc1, ad1); /*  */
  _zero = #set0_256(); /*  */
  #[inline]
  (x0, x1) = __w256_deinterleave_u16(_zero, x0, x1);
  #[inline]
  (y0, y1) = __w256_deinterleave_u16(_zero, y0, y1);
  #[inline]
  x0 = __mont_red(x0, x1, qx16, qinvx16);
  #[inline]
  y0 = __mont_red(y0, y1, qx16, qinvx16);
  return (x0, y0);
}

fn _poly_basemul (reg mut ptr u16[MLKEM_N] rp, reg const ptr u16[MLKEM_N] ap,
                 reg const ptr u16[MLKEM_N] bp) -> (reg mut ptr u16[MLKEM_N]) {
  reg u256 qx16;
  reg u256 qinvx16;
  reg u256 zetaqinv;
  reg u256 zeta;
  reg u256 are;
  reg u256 aim;
  reg u256 bre;
  reg u256 bim;
  
  qx16 = /* global: */ jqx16.[#unaligned :u256 0]; /* u256 */
  qinvx16 = /* global: */ jqinvx16.[#unaligned :u256 0]; /* u256 */
  zetaqinv = /* global: */ jzetas_exp.[#unaligned :u256 272]; /* u256 */
  zeta = /* global: */ jzetas_exp.[#unaligned :u256 304]; /* u256 */
  are = ap.[#unaligned :u256 (32 * 0)]; /* u256 */
  aim = ap.[#unaligned :u256 (32 * 1)]; /* u256 */
  bre = bp.[#unaligned :u256 (32 * 0)]; /* u256 */
  bim = bp.[#unaligned :u256 (32 * 1)]; /* u256 */
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[#unaligned :u256 (32 * 0)] = are; /* u256 */
  rp.[#unaligned :u256 (32 * 1)] = aim; /* u256 */
  are = ap.[#unaligned :u256 (32 * 2)]; /* u256 */
  aim = ap.[#unaligned :u256 (32 * 3)]; /* u256 */
  bre = bp.[#unaligned :u256 (32 * 2)]; /* u256 */
  bim = bp.[#unaligned :u256 (32 * 3)]; /* u256 */
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[#unaligned :u256 (32 * 2)] = are; /* u256 */
  rp.[#unaligned :u256 (32 * 3)] = aim; /* u256 */
  zetaqinv = /* global: */ jzetas_exp.[#unaligned :u256 336]; /* u256 */
  zeta = /* global: */ jzetas_exp.[#unaligned :u256 368]; /* u256 */
  are = ap.[#unaligned :u256 (32 * 4)]; /* u256 */
  aim = ap.[#unaligned :u256 (32 * 5)]; /* u256 */
  bre = bp.[#unaligned :u256 (32 * 4)]; /* u256 */
  bim = bp.[#unaligned :u256 (32 * 5)]; /* u256 */
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[#unaligned :u256 (32 * 4)] = are; /* u256 */
  rp.[#unaligned :u256 (32 * 5)] = aim; /* u256 */
  are = ap.[#unaligned :u256 (32 * 6)]; /* u256 */
  aim = ap.[#unaligned :u256 (32 * 7)]; /* u256 */
  bre = bp.[#unaligned :u256 (32 * 6)]; /* u256 */
  bim = bp.[#unaligned :u256 (32 * 7)]; /* u256 */
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[#unaligned :u256 (32 * 6)] = are; /* u256 */
  rp.[#unaligned :u256 (32 * 7)] = aim; /* u256 */
  zetaqinv = /* global: */ jzetas_exp.[#unaligned :u256 664]; /* u256 */
  zeta = /* global: */ jzetas_exp.[#unaligned :u256 696]; /* u256 */
  are = ap.[#unaligned :u256 (32 * 8)]; /* u256 */
  aim = ap.[#unaligned :u256 (32 * 9)]; /* u256 */
  bre = bp.[#unaligned :u256 (32 * 8)]; /* u256 */
  bim = bp.[#unaligned :u256 (32 * 9)]; /* u256 */
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[#unaligned :u256 (32 * 8)] = are; /* u256 */
  rp.[#unaligned :u256 (32 * 9)] = aim; /* u256 */
  are = ap.[#unaligned :u256 (32 * 10)]; /* u256 */
  aim = ap.[#unaligned :u256 (32 * 11)]; /* u256 */
  bre = bp.[#unaligned :u256 (32 * 10)]; /* u256 */
  bim = bp.[#unaligned :u256 (32 * 11)]; /* u256 */
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[#unaligned :u256 (32 * 10)] = are; /* u256 */
  rp.[#unaligned :u256 (32 * 11)] = aim; /* u256 */
  zetaqinv = /* global: */ jzetas_exp.[#unaligned :u256 728]; /* u256 */
  zeta = /* global: */ jzetas_exp.[#unaligned :u256 760]; /* u256 */
  are = ap.[#unaligned :u256 (32 * 12)]; /* u256 */
  aim = ap.[#unaligned :u256 (32 * 13)]; /* u256 */
  bre = bp.[#unaligned :u256 (32 * 12)]; /* u256 */
  bim = bp.[#unaligned :u256 (32 * 13)]; /* u256 */
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[#unaligned :u256 (32 * 12)] = are; /* u256 */
  rp.[#unaligned :u256 (32 * 13)] = aim; /* u256 */
  are = ap.[#unaligned :u256 (32 * 14)]; /* u256 */
  aim = ap.[#unaligned :u256 (32 * 15)]; /* u256 */
  bre = bp.[#unaligned :u256 (32 * 14)]; /* u256 */
  bim = bp.[#unaligned :u256 (32 * 15)]; /* u256 */
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[#unaligned :u256 (32 * 14)] = are; /* u256 */
  rp.[#unaligned :u256 (32 * 15)] = aim; /* u256 */
  return (rp);
}

fn _i_poly_frombytes (reg mut ptr u16[MLKEM_N] rp,
                     reg const ptr u8[MLKEM_POLYBYTES] ap) -> (reg mut ptr u16[MLKEM_N]) {
  reg u256 mask;
  inline int i;
  reg u256 t0;
  reg u256 t1;
  reg u256 t2;
  reg u256 t3;
  reg u256 t4;
  reg u256 t5;
  reg u256 tt;
  reg u256 t6;
  reg u256 t7;
  reg u256 t8;
  reg u256 t9;
  reg u256 t10;
  reg u256 t11;
  
  mask = /* global: */ maskx16[:u256 0]; /* u256 */
  for i = 0 to 2 {
    t0 = ap.[#unaligned :u256 (192 * i)]; /* u256 */
    t1 = ap.[#unaligned :u256 ((192 * i) + 32)]; /* u256 */
    t2 = ap.[#unaligned :u256 ((192 * i) + 64)]; /* u256 */
    t3 = ap.[#unaligned :u256 ((192 * i) + 96)]; /* u256 */
    t4 = ap.[#unaligned :u256 ((192 * i) + 128)]; /* u256 */
    t5 = ap.[#unaligned :u256 ((192 * i) + 160)]; /* u256 */
    #[inline]
    (tt, t3) = __shuffle8(t0, t3);
    #[inline]
    (t0, t4) = __shuffle8(t1, t4);
    #[inline]
    (t1, t5) = __shuffle8(t2, t5);
    #[inline]
    (t2, t4) = __shuffle4(tt, t4);
    #[inline]
    (tt, t1) = __shuffle4(t3, t1);
    #[inline]
    (t3, t5) = __shuffle4(t0, t5);
    #[inline]
    (t0, t1) = __shuffle2(t2, t1);
    #[inline]
    (t2, t3) = __shuffle2(t4, t3);
    #[inline]
    (t4, t5) = __shuffle2(tt, t5);
    #[inline]
    (t6, t3) = __shuffle1(t0, t3);
    #[inline]
    (t0, t4) = __shuffle1(t1, t4);
    #[inline]
    (t1, t5) = __shuffle1(t2, t5);
    t7 = #VPSRL_16u16(t6, ((128u) 12)); /*  */
    t8 = #VPSLL_16u16(t3, ((128u) 4)); /*  */
    t7 = #VPOR_256(t7, t8); /*  */
    t6 = #VPAND_256(mask, t6); /*  */
    t7 = #VPAND_256(mask, t7); /*  */
    t8 = #VPSRL_16u16(t3, ((128u) 8)); /*  */
    t9 = #VPSLL_16u16(t0, ((128u) 8)); /*  */
    t8 = #VPOR_256(t8, t9); /*  */
    t8 = #VPAND_256(mask, t8); /*  */
    t9 = #VPSRL_16u16(t0, ((128u) 4)); /*  */
    t9 = #VPAND_256(mask, t9); /*  */
    t10 = #VPSRL_16u16(t4, ((128u) 12)); /*  */
    t11 = #VPSLL_16u16(t1, ((128u) 4)); /*  */
    t10 = #VPOR_256(t10, t11); /*  */
    t4 = #VPAND_256(mask, t4); /*  */
    t10 = #VPAND_256(mask, t10); /*  */
    t11 = #VPSRL_16u16(t1, ((128u) 8)); /*  */
    tt = #VPSLL_16u16(t5, ((128u) 8)); /*  */
    t11 = #VPOR_256(t11, tt); /*  */
    t11 = #VPAND_256(mask, t11); /*  */
    tt = #VPSRL_16u16(t5, ((128u) 4)); /*  */
    tt = #VPAND_256(mask, tt); /*  */
    rp[:u256 (8 * i)] = t6; /* u256 */
    rp[:u256 ((8 * i) + 1)] = t7; /* u256 */
    rp[:u256 ((8 * i) + 2)] = t8; /* u256 */
    rp[:u256 ((8 * i) + 3)] = t9; /* u256 */
    rp[:u256 ((8 * i) + 4)] = t4; /* u256 */
    rp[:u256 ((8 * i) + 5)] = t10; /* u256 */
    rp[:u256 ((8 * i) + 6)] = t11; /* u256 */
    rp[:u256 ((8 * i) + 7)] = tt; /* u256 */
  }
  return (rp);
}

param int DMONT = 1353;

fn _poly_frommont (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N]) {
  reg u256 qx16;
  reg u256 qinvx16;
  reg u256 dmontx16;
  inline int i;
  reg u256 t;
  
  qx16 = /* global: */ jqx16[:u256 0]; /* u256 */
  qinvx16 = /* global: */ jqinvx16[:u256 0]; /* u256 */
  dmontx16 = /* global: */ jdmontx16[:u256 0]; /* u256 */
  for i = 0 to (MLKEM_N / 16) {
    t = rp[:u256 i]; /* u256 */
    #[inline]
    t = __fqmulx16(t, dmontx16, qx16, qinvx16);
    rp[:u256 i] = t; /* u256 */
  }
  return (rp);
}

u32[4] pfm_shift_s = {((32u) 3), ((32u) 2), ((32u) 1), ((32u) 0)};

u8[16] pfm_idx_s = {((8u) 0), ((8u) 1), ((8u) 4), ((8u) 5), ((8u) 8),
                    ((8u) 9), ((8u) 12), ((8u) 13), ((8u) 2), ((8u) 3),
                    ((8u) 6), ((8u) 7), ((8u) 10), ((8u) 11), ((8u) 14),
                    ((8u) 15)};

fn _i_poly_frommsg (reg mut ptr u16[MLKEM_N] rp, reg const ptr u8[32] ap) -> 
(reg mut ptr u16[MLKEM_N]) {
  reg u256 hqs;
  reg u256 shift;
  reg u256 idx;
  reg u256 f;
  inline int i;
  reg u256 g3;
  reg u256 g0;
  reg u256 g1;
  reg u256 g2;
  reg u256 h0;
  reg u256 h2;
  reg u256 h1;
  reg u256 h3;
  
  hqs = /* global: */ hqx16_p1[:u256 0]; /* u256 */
  shift = #VPBROADCAST_2u128(/* global: */ pfm_shift_s[:u128 0]); /*  */
  idx = #VPBROADCAST_2u128(/* global: */ pfm_idx_s[:u128 0]); /*  */
  f = ap[:u256 0]; /* u256 */
  for i = 0 to 4 {
    g3 = #VPSHUFD_256(f, ((8u) (85 * i))); /*  */
    g3 = #VPSLLV_8u32(g3, shift); /*  */
    g3 = #VPSHUFB_256(g3, idx); /*  */
    g0 = #VPSLL_16u16(g3, ((128u) 12)); /*  */
    g1 = #VPSLL_16u16(g3, ((128u) 8)); /*  */
    g2 = #VPSLL_16u16(g3, ((128u) 4)); /*  */
    g0 = #VPSRA_16u16(g0, ((128u) 15)); /*  */
    g1 = #VPSRA_16u16(g1, ((128u) 15)); /*  */
    g2 = #VPSRA_16u16(g2, ((128u) 15)); /*  */
    g3 = #VPSRA_16u16(g3, ((128u) 15)); /*  */
    g0 = #VPAND_256(g0, hqs); /*  */
    g1 = #VPAND_256(g1, hqs); /*  */
    g2 = #VPAND_256(g2, hqs); /*  */
    g3 = #VPAND_256(g3, hqs); /*  */
    h0 = #VPUNPCKL_4u64(g0, g1); /*  */
    h2 = #VPUNPCKH_4u64(g0, g1); /*  */
    h1 = #VPUNPCKL_4u64(g2, g3); /*  */
    h3 = #VPUNPCKH_4u64(g2, g3); /*  */
    g0 = #VPERM2I128(h0, h1, ((8u) 32)); /*  */
    g2 = #VPERM2I128(h0, h1, ((8u) 49)); /*  */
    g1 = #VPERM2I128(h2, h3, ((8u) 32)); /*  */
    g3 = #VPERM2I128(h2, h3, ((8u) 49)); /*  */
    rp[:u256 (2 * i)] = g0; /* u256 */
    rp[:u256 ((2 * i) + 1)] = g1; /* u256 */
    rp[:u256 ((2 * i) + 8)] = g2; /* u256 */
    rp[:u256 (((2 * i) + 8) + 1)] = g3; /* u256 */
  }
  return (rp);
}

inline
fn __cbd2 (reg mut ptr u16[MLKEM_N] rp,
          reg const ptr u8[((MLKEM_ETA2 * MLKEM_N) / 4)] buf) -> (reg mut ptr u16[MLKEM_N]) {
  stack u32 mask55_s;
  stack u32 mask33_s;
  stack u32 mask03_s;
  stack u32 mask0F_s;
  reg u256 mask55;
  reg u256 mask33;
  reg u256 mask03;
  reg u256 mask0F;
  inline int i;
  reg u256 f0;
  reg u256 f1;
  reg u256 f2;
  reg u256 f3;
  reg u128 t;
  
  mask55_s = ((32u) 1431655765); /* u32 */
  mask33_s = ((32u) 858993459); /* u32 */
  mask03_s = ((32u) 50529027); /* u32 */
  mask0F_s = ((32u) 252645135); /* u32 */
  mask55 = #VPBROADCAST_8u32(mask55_s); /*  */
  mask33 = #VPBROADCAST_8u32(mask33_s); /*  */
  mask03 = #VPBROADCAST_8u32(mask03_s); /*  */
  mask0F = #VPBROADCAST_8u32(mask0F_s); /*  */
  for i = 0 to (MLKEM_N / 64) {
    f0 = buf[:u256 i]; /* u256 */
    f1 = #VPSRL_16u16(f0, ((128u) 1)); /*  */
    f0 = #VPAND_256(mask55, f0); /*  */
    f1 = #VPAND_256(mask55, f1); /*  */
    f0 = #VPADD_32u8(f0, f1); /*  */
    f1 = #VPSRL_16u16(f0, ((128u) 2)); /*  */
    f0 = #VPAND_256(mask33, f0); /*  */
    f1 = #VPAND_256(mask33, f1); /*  */
    f0 = #VPADD_32u8(f0, mask33); /*  */
    f0 = #VPSUB_32u8(f0, f1); /*  */
    f1 = #VPSRL_16u16(f0, ((128u) 4)); /*  */
    f0 = #VPAND_256(mask0F, f0); /*  */
    f1 = #VPAND_256(mask0F, f1); /*  */
    f0 = #VPSUB_32u8(f0, mask03); /*  */
    f1 = #VPSUB_32u8(f1, mask03); /*  */
    f2 = #VPUNPCKL_32u8(f0, f1); /*  */
    f3 = #VPUNPCKH_32u8(f0, f1); /*  */
    t = f2; /* u128 */
    f0 = #VPMOVSX_16u8_16u16(t); /*  */
    t = #VEXTRACTI128(f2, ((8u) 1)); /*  */
    f1 = #VPMOVSX_16u8_16u16(t); /*  */
    t = f3; /* u128 */
    f2 = #VPMOVSX_16u8_16u16(t); /*  */
    t = #VEXTRACTI128(f3, ((8u) 1)); /*  */
    f3 = #VPMOVSX_16u8_16u16(t); /*  */
    rp[:u256 (4 * i)] = f0; /* u256 */
    rp[:u256 ((4 * i) + 1)] = f2; /* u256 */
    rp[:u256 ((4 * i) + 2)] = f1; /* u256 */
    rp[:u256 ((4 * i) + 3)] = f3; /* u256 */
  }
  return (rp);
}

inline
fn __poly_cbd_eta1 (reg mut ptr u16[MLKEM_N] rp,
                   reg const ptr u8[(((MLKEM_ETA1 * MLKEM_N) / 4) +
                                    ((MLKEM_ETA1 - 2) * 8))] buf) -> 
(reg mut ptr u16[MLKEM_N]) {
  
  #[inline]
  rp = __cbd2(rp, buf[0 : ((MLKEM_ETA2 * MLKEM_N) / 4)]);
  return (rp);
}

fn _poly_getnoise_eta2 (#[spill_to_mmx] reg mut ptr u16[MLKEM_N] rp,
                       reg const ptr u8[MLKEM_SYMBYTES] seed, reg u8 nonce) -> 
(reg mut ptr u16[MLKEM_N]) {
  stack u8[1] nonce_s;
  stack u8[((MLKEM_ETA2 * MLKEM_N) / 4)] buf;
  
  () = #spill(rp); /* :k */
  nonce_s[0] = nonce; /* u8 */
  buf = _shake256_A128__A32_A1(buf, seed, nonce_s);
  () = #unspill(rp); /* :k */
  #[inline]
  rp = __poly_cbd_eta1(rp, buf);
  return (rp);
}

fn _poly_getnoise_eta1_4x (reg mut ptr u16[MLKEM_N] r0,
                          reg mut ptr u16[MLKEM_N] r1,
                          reg mut ptr u16[MLKEM_N] r2,
                          reg mut ptr u16[MLKEM_N] r3,
                          reg const ptr u8[MLKEM_SYMBYTES] seed,
                          reg u8 nonce) -> (reg mut ptr u16[MLKEM_N],
                                           reg mut ptr u16[MLKEM_N],
                                           reg mut ptr u16[MLKEM_N],
                                           reg mut ptr u16[MLKEM_N]) {
  reg mut ptr u8[128] buf0;
  stack u8[128] buf0_s;
  reg mut ptr u8[128] buf1;
  stack u8[128] buf1_s;
  reg mut ptr u8[128] buf2;
  stack u8[128] buf2_s;
  reg mut ptr u8[128] buf3;
  stack u8[128] buf3_s;
  stack u8[4] nonces;
  
  buf0 = buf0_s; /* u8[128] */
  buf1 = buf1_s; /* u8[128] */
  buf2 = buf2_s; /* u8[128] */
  buf3 = buf3_s; /* u8[128] */
  () = #spill(r0, r1, r2, r3); /* :k */
  nonces[0] = nonce; /* u8 */
  nonce = (nonce +8u ((8u) 1)); /* u8 */
  nonces[1] = nonce; /* u8 */
  nonce = (nonce +8u ((8u) 1)); /* u8 */
  nonces[2] = nonce; /* u8 */
  nonce = (nonce +8u ((8u) 1)); /* u8 */
  nonces[3] = nonce; /* u8 */
  (buf0, buf1, buf2, buf3) =
    _shake256x4_A128__A32_A1(buf0, buf1, buf2, buf3, seed, nonces);
  _ /* u64 */ = #init_msf(); /* :k */
  () = #unspill(r0, r1, r2, r3); /* :k */
  #[inline]
  r0 = __poly_cbd_eta1(r0, buf0);
  #[inline]
  r1 = __poly_cbd_eta1(r1, buf1);
  #[inline]
  r2 = __poly_cbd_eta1(r2, buf2);
  #[inline]
  r3 = __poly_cbd_eta1(r3, buf3);
  return (r0, r1, r2, r3);
}

inline
fn __invntt___butterfly64x (reg u256 rl0, reg u256 rl1, reg u256 rl2,
                           reg u256 rl3, reg u256 rh0, reg u256 rh1,
                           reg u256 rh2, reg u256 rh3, reg u256 zl0,
                           reg u256 zl1, reg u256 zh0, reg u256 zh1,
                           reg u256 qx16) -> (reg u256, reg u256, reg u256,
                                             reg u256, reg u256, reg u256,
                                             reg u256, reg u256) {
  reg u256 t0;
  reg u256 t1;
  reg u256 t2;
  reg u256 t3;
  
  t0 = #VPSUB_16u16(rl0, rh0); /*  */
  t1 = #VPSUB_16u16(rl1, rh1); /*  */
  t2 = #VPSUB_16u16(rl2, rh2); /*  */
  rl0 = #VPADD_16u16(rh0, rl0); /*  */
  rl1 = #VPADD_16u16(rh1, rl1); /*  */
  rh0 = #VPMULL_16u16(zl0, t0); /*  */
  rl2 = #VPADD_16u16(rh2, rl2); /*  */
  rh1 = #VPMULL_16u16(zl0, t1); /*  */
  t3 = #VPSUB_16u16(rl3, rh3); /*  */
  rl3 = #VPADD_16u16(rh3, rl3); /*  */
  rh2 = #VPMULL_16u16(zl1, t2); /*  */
  rh3 = #VPMULL_16u16(zl1, t3); /*  */
  t0 = #VPMULH_16u16(zh0, t0); /*  */
  t1 = #VPMULH_16u16(zh0, t1); /*  */
  t2 = #VPMULH_16u16(zh1, t2); /*  */
  t3 = #VPMULH_16u16(zh1, t3); /*  */
  rh0 = #VPMULH_16u16(qx16, rh0); /*  */
  rh1 = #VPMULH_16u16(qx16, rh1); /*  */
  rh2 = #VPMULH_16u16(qx16, rh2); /*  */
  rh3 = #VPMULH_16u16(qx16, rh3); /*  */
  rh0 = #VPSUB_16u16(t0, rh0); /*  */
  rh1 = #VPSUB_16u16(t1, rh1); /*  */
  rh2 = #VPSUB_16u16(t2, rh2); /*  */
  rh3 = #VPSUB_16u16(t3, rh3); /*  */
  return (rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3);
}

fn _poly_invntt (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N]) {
  reg u256 qx16;
  inline int i;
  reg u256 zeta0;
  reg u256 zeta1;
  reg u256 zeta2;
  reg u256 zeta3;
  reg u256 r0;
  reg u256 r1;
  reg u256 r2;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r6;
  reg u256 r7;
  reg u256 vx16;
  reg u256 flox16;
  reg u256 fhix16;
  
  qx16 = /* global: */ jqx16[:u256 0]; /* u256 */
  for i = 0 to 2 {
    zeta0 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (0 + (392 * i))]; /* u256 */
    zeta1 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (64 + (392 * i))]; /* u256 */
    zeta2 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (32 + (392 * i))]; /* u256 */
    zeta3 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (96 + (392 * i))]; /* u256 */
    r0 = rp.[#unaligned :u256 ((32 * 0) + (256 * i))]; /* u256 */
    r1 = rp.[#unaligned :u256 ((32 * 1) + (256 * i))]; /* u256 */
    r2 = rp.[#unaligned :u256 ((32 * 2) + (256 * i))]; /* u256 */
    r3 = rp.[#unaligned :u256 ((32 * 3) + (256 * i))]; /* u256 */
    r4 = rp.[#unaligned :u256 ((32 * 4) + (256 * i))]; /* u256 */
    r5 = rp.[#unaligned :u256 ((32 * 5) + (256 * i))]; /* u256 */
    r6 = rp.[#unaligned :u256 ((32 * 6) + (256 * i))]; /* u256 */
    r7 = rp.[#unaligned :u256 ((32 * 7) + (256 * i))]; /* u256 */
    #[inline]
    (r0, r1, r4, r5, r2, r3, r6, r7) =
      __invntt___butterfly64x(r0, r1, r4, r5, r2, r3, r6, r7, zeta0, zeta1,
                              zeta2, zeta3, qx16);
    vx16 = /* global: */ jvx16[:u256 0]; /* u256 */
    zeta0 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (128 + (392 * i))]; /* u256 */
    zeta1 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (160 + (392 * i))]; /* u256 */
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    #[inline]
    r1 = __red16x(r1, qx16, vx16);
    #[inline]
    r4 = __red16x(r4, qx16, vx16);
    #[inline]
    r5 = __red16x(r5, qx16, vx16);
    #[inline]
    (r0, r1, r2, r3, r4, r5, r6, r7) =
      __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    #[inline]
    (r0, r1) = __shuffle1(r0, r1);
    #[inline]
    (r2, r3) = __shuffle1(r2, r3);
    #[inline]
    (r4, r5) = __shuffle1(r4, r5);
    #[inline]
    (r6, r7) = __shuffle1(r6, r7);
    zeta0 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (192 + (392 * i))]; /* u256 */
    zeta1 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (224 + (392 * i))]; /* u256 */
    #[inline]
    (r0, r2, r4, r6, r1, r3, r5, r7) =
      __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    #[inline]
    (r0, r2) = __shuffle2(r0, r2);
    #[inline]
    (r4, r6) = __shuffle2(r4, r6);
    #[inline]
    (r1, r3) = __shuffle2(r1, r3);
    #[inline]
    (r5, r7) = __shuffle2(r5, r7);
    zeta0 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (256 + (392 * i))]; /* u256 */
    zeta1 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (288 + (392 * i))]; /* u256 */
    #[inline]
    (r0, r4, r1, r5, r2, r6, r3, r7) =
      __invntt___butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    #[inline]
    (r0, r4) = __shuffle4(r0, r4);
    #[inline]
    (r1, r5) = __shuffle4(r1, r5);
    #[inline]
    (r2, r6) = __shuffle4(r2, r6);
    #[inline]
    (r3, r7) = __shuffle4(r3, r7);
    zeta0 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (320 + (392 * i))]; /* u256 */
    zeta1 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (352 + (392 * i))]; /* u256 */
    #[inline]
    (r0, r1, r2, r3, r4, r5, r6, r7) =
      __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    #[inline]
    (r0, r1) = __shuffle8(r0, r1);
    #[inline]
    (r2, r3) = __shuffle8(r2, r3);
    #[inline]
    (r4, r5) = __shuffle8(r4, r5);
    #[inline]
    (r6, r7) = __shuffle8(r6, r7);
    zeta0 =
      #VPBROADCAST_8u32(/* global: */ jzetas_inv_exp.[#unaligned :u32 
                        (384 + (392 * i))]); /*  */
    zeta1 =
      #VPBROADCAST_8u32(/* global: */ jzetas_inv_exp.[#unaligned :u32 
                        (388 + (392 * i))]); /*  */
    #[inline]
    (r0, r2, r4, r6, r1, r3, r5, r7) =
      __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    if (i == 0) {
      rp.[#unaligned :u256 ((32 * 0) + (256 * i))] = r0; /* u256 */
      rp.[#unaligned :u256 ((32 * 1) + (256 * i))] = r2; /* u256 */
      rp.[#unaligned :u256 ((32 * 2) + (256 * i))] = r4; /* u256 */
      rp.[#unaligned :u256 ((32 * 3) + (256 * i))] = r6; /* u256 */
    }
    rp.[#unaligned :u256 ((32 * 4) + (256 * i))] = r1; /* u256 */
    rp.[#unaligned :u256 ((32 * 5) + (256 * i))] = r3; /* u256 */
    rp.[#unaligned :u256 ((32 * 6) + (256 * i))] = r5; /* u256 */
    rp.[#unaligned :u256 ((32 * 7) + (256 * i))] = r7; /* u256 */
  }
  zeta0 =
    #VPBROADCAST_8u32(/* global: */ jzetas_inv_exp.[#unaligned :u32 784]); /*  */
  zeta1 =
    #VPBROADCAST_8u32(/* global: */ jzetas_inv_exp.[#unaligned :u32 788]); /*  */
  for i = 0 to 2 {
    if (i == 0) {
      r7 = r6; /* u256 */
      r6 = r4; /* u256 */
      r5 = r2; /* u256 */
      r4 = r0; /* u256 */
    } else {
      r4 = rp.[#unaligned :u256 ((32 * 8) + (128 * i))]; /* u256 */
      r5 = rp.[#unaligned :u256 ((32 * 9) + (128 * i))]; /* u256 */
      r6 = rp.[#unaligned :u256 ((32 * 10) + (128 * i))]; /* u256 */
      r7 = rp.[#unaligned :u256 ((32 * 11) + (128 * i))]; /* u256 */
    }
    r0 = rp.[#unaligned :u256 ((32 * 0) + (128 * i))]; /* u256 */
    r1 = rp.[#unaligned :u256 ((32 * 1) + (128 * i))]; /* u256 */
    r2 = rp.[#unaligned :u256 ((32 * 2) + (128 * i))]; /* u256 */
    r3 = rp.[#unaligned :u256 ((32 * 3) + (128 * i))]; /* u256 */
    #[inline]
    (r0, r1, r2, r3, r4, r5, r6, r7) =
      __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    flox16 = /* global: */ jflox16[:u256 0]; /* u256 */
    fhix16 = /* global: */ jfhix16[:u256 0]; /* u256 */
    rp.[#unaligned :u256 ((32 * 8) + (128 * i))] = r4; /* u256 */
    rp.[#unaligned :u256 ((32 * 9) + (128 * i))] = r5; /* u256 */
    rp.[#unaligned :u256 ((32 * 10) + (128 * i))] = r6; /* u256 */
    rp.[#unaligned :u256 ((32 * 11) + (128 * i))] = r7; /* u256 */
    #[inline]
    r0 = __fqmulprecomp16x(r0, flox16, fhix16, qx16);
    #[inline]
    r1 = __fqmulprecomp16x(r1, flox16, fhix16, qx16);
    #[inline]
    r2 = __fqmulprecomp16x(r2, flox16, fhix16, qx16);
    #[inline]
    r3 = __fqmulprecomp16x(r3, flox16, fhix16, qx16);
    rp.[#unaligned :u256 ((32 * 0) + (128 * i))] = r0; /* u256 */
    rp.[#unaligned :u256 ((32 * 1) + (128 * i))] = r1; /* u256 */
    rp.[#unaligned :u256 ((32 * 2) + (128 * i))] = r2; /* u256 */
    rp.[#unaligned :u256 ((32 * 3) + (128 * i))] = r3; /* u256 */
  }
  return (rp);
}

inline
fn __butterfly64x (reg u256 rl0, reg u256 rl1, reg u256 rl2, reg u256 rl3,
                  reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3,
                  reg u256 zl0, reg u256 zl1, reg u256 zh0, reg u256 zh1,
                  reg u256 qx16) -> (reg u256, reg u256, reg u256, reg u256,
                                    reg u256, reg u256, reg u256, reg u256) {
  reg u256 t0;
  reg u256 t1;
  reg u256 t2;
  reg u256 t3;
  reg u256 t4;
  reg u256 t5;
  reg u256 t6;
  reg u256 t7;
  
  t0 = #VPMULL_16u16(zl0, rh0); /*  */
  t1 = #VPMULH_16u16(zh0, rh0); /*  */
  t2 = #VPMULL_16u16(zl0, rh1); /*  */
  t3 = #VPMULH_16u16(zh0, rh1); /*  */
  t4 = #VPMULL_16u16(zl1, rh2); /*  */
  t5 = #VPMULH_16u16(zh1, rh2); /*  */
  t6 = #VPMULL_16u16(zl1, rh3); /*  */
  t7 = #VPMULH_16u16(zh1, rh3); /*  */
  t0 = #VPMULH_16u16(t0, qx16); /*  */
  t2 = #VPMULH_16u16(t2, qx16); /*  */
  t4 = #VPMULH_16u16(t4, qx16); /*  */
  t6 = #VPMULH_16u16(t6, qx16); /*  */
  rh1 = #VPSUB_16u16(rl1, t3); /*  */
  rl1 = #VPADD_16u16(t3, rl1); /*  */
  rh0 = #VPSUB_16u16(rl0, t1); /*  */
  rl0 = #VPADD_16u16(t1, rl0); /*  */
  rh3 = #VPSUB_16u16(rl3, t7); /*  */
  rl3 = #VPADD_16u16(t7, rl3); /*  */
  rh2 = #VPSUB_16u16(rl2, t5); /*  */
  rl2 = #VPADD_16u16(t5, rl2); /*  */
  rh0 = #VPADD_16u16(t0, rh0); /*  */
  rl0 = #VPSUB_16u16(rl0, t0); /*  */
  rh1 = #VPADD_16u16(t2, rh1); /*  */
  rl1 = #VPSUB_16u16(rl1, t2); /*  */
  rh2 = #VPADD_16u16(t4, rh2); /*  */
  rl2 = #VPSUB_16u16(rl2, t4); /*  */
  rh3 = #VPADD_16u16(t6, rh3); /*  */
  rl3 = #VPSUB_16u16(rl3, t6); /*  */
  return (rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3);
}

fn _poly_ntt (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N]) {
  reg u256 qx16;
  reg u256 zeta0;
  reg u256 zeta1;
  reg u256 r0;
  reg u256 r1;
  reg u256 r2;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r6;
  reg u256 r7;
  inline int i;
  reg u256 zeta2;
  reg u256 zeta3;
  reg u256 vx16;
  
  qx16 = /* global: */ jqx16[:u256 0]; /* u256 */
  zeta0 = #VPBROADCAST_8u32(/* global: */ jzetas_exp[:u32 0]); /*  */
  zeta1 = #VPBROADCAST_8u32(/* global: */ jzetas_exp[:u32 1]); /*  */
  r0 = rp.[#unaligned :u256 (32 * 0)]; /* u256 */
  r1 = rp.[#unaligned :u256 (32 * 1)]; /* u256 */
  r2 = rp.[#unaligned :u256 (32 * 2)]; /* u256 */
  r3 = rp.[#unaligned :u256 (32 * 3)]; /* u256 */
  r4 = rp.[#unaligned :u256 (32 * 8)]; /* u256 */
  r5 = rp.[#unaligned :u256 (32 * 9)]; /* u256 */
  r6 = rp.[#unaligned :u256 (32 * 10)]; /* u256 */
  r7 = rp.[#unaligned :u256 (32 * 11)]; /* u256 */
  #[inline]
  (r0, r1, r2, r3, r4, r5, r6, r7) =
    __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1,
                   zeta1, qx16);
  rp.[#unaligned :u256 (32 * 0)] = r0; /* u256 */
  rp.[#unaligned :u256 (32 * 1)] = r1; /* u256 */
  rp.[#unaligned :u256 (32 * 2)] = r2; /* u256 */
  rp.[#unaligned :u256 (32 * 3)] = r3; /* u256 */
  rp.[#unaligned :u256 (32 * 8)] = r4; /* u256 */
  rp.[#unaligned :u256 (32 * 9)] = r5; /* u256 */
  rp.[#unaligned :u256 (32 * 10)] = r6; /* u256 */
  rp.[#unaligned :u256 (32 * 11)] = r7; /* u256 */
  r0 = rp.[#unaligned :u256 (32 * 4)]; /* u256 */
  r1 = rp.[#unaligned :u256 (32 * 5)]; /* u256 */
  r2 = rp.[#unaligned :u256 (32 * 6)]; /* u256 */
  r3 = rp.[#unaligned :u256 (32 * 7)]; /* u256 */
  r4 = rp.[#unaligned :u256 (32 * 12)]; /* u256 */
  r5 = rp.[#unaligned :u256 (32 * 13)]; /* u256 */
  r6 = rp.[#unaligned :u256 (32 * 14)]; /* u256 */
  r7 = rp.[#unaligned :u256 (32 * 15)]; /* u256 */
  #[inline]
  (r0, r1, r2, r3, r4, r5, r6, r7) =
    __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1,
                   zeta1, qx16);
  rp.[#unaligned :u256 (32 * 12)] = r4; /* u256 */
  rp.[#unaligned :u256 (32 * 13)] = r5; /* u256 */
  rp.[#unaligned :u256 (32 * 14)] = r6; /* u256 */
  rp.[#unaligned :u256 (32 * 15)] = r7; /* u256 */
  for i = 0 to 2 {
    zeta0 =
      #VPBROADCAST_8u32(/* global: */ jzetas_exp.[#unaligned :u32 (8 +
                                                                  (392 * i))]); /*  */
    zeta1 =
      #VPBROADCAST_8u32(/* global: */ jzetas_exp.[#unaligned :u32 (12 +
                                                                  (392 * i))]); /*  */
    if (i == 0) {
      r4 = r0; /* u256 */
      r5 = r1; /* u256 */
      r6 = r2; /* u256 */
      r7 = r3; /* u256 */
    } else {
      r4 = rp.[#unaligned :u256 ((32 * 4) + (256 * i))]; /* u256 */
      r5 = rp.[#unaligned :u256 ((32 * 5) + (256 * i))]; /* u256 */
      r6 = rp.[#unaligned :u256 ((32 * 6) + (256 * i))]; /* u256 */
      r7 = rp.[#unaligned :u256 ((32 * 7) + (256 * i))]; /* u256 */
    }
    r0 = rp.[#unaligned :u256 ((32 * 0) + (256 * i))]; /* u256 */
    r1 = rp.[#unaligned :u256 ((32 * 1) + (256 * i))]; /* u256 */
    r2 = rp.[#unaligned :u256 ((32 * 2) + (256 * i))]; /* u256 */
    r3 = rp.[#unaligned :u256 ((32 * 3) + (256 * i))]; /* u256 */
    #[inline]
    (r0, r1, r2, r3, r4, r5, r6, r7) =
      __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1,
                     zeta1, qx16);
    zeta0 =
      /* global: */ jzetas_exp.[#unaligned :u256 (16 + (392 * i))]; /* u256 */
    zeta1 =
      /* global: */ jzetas_exp.[#unaligned :u256 (48 + (392 * i))]; /* u256 */
    #[inline]
    (r0, r4) = __shuffle8(r0, r4);
    #[inline]
    (r1, r5) = __shuffle8(r1, r5);
    #[inline]
    (r2, r6) = __shuffle8(r2, r6);
    #[inline]
    (r3, r7) = __shuffle8(r3, r7);
    #[inline]
    (r0, r4, r1, r5, r2, r6, r3, r7) =
      __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1,
                     zeta1, qx16);
    zeta0 =
      /* global: */ jzetas_exp.[#unaligned :u256 (80 + (392 * i))]; /* u256 */
    zeta1 =
      /* global: */ jzetas_exp.[#unaligned :u256 (112 + (392 * i))]; /* u256 */
    #[inline]
    (r0, r2) = __shuffle4(r0, r2);
    #[inline]
    (r4, r6) = __shuffle4(r4, r6);
    #[inline]
    (r1, r3) = __shuffle4(r1, r3);
    #[inline]
    (r5, r7) = __shuffle4(r5, r7);
    #[inline]
    (r0, r2, r4, r6, r1, r3, r5, r7) =
      __butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1,
                     zeta1, qx16);
    zeta0 =
      /* global: */ jzetas_exp.[#unaligned :u256 (144 + (392 * i))]; /* u256 */
    zeta1 =
      /* global: */ jzetas_exp.[#unaligned :u256 (176 + (392 * i))]; /* u256 */
    #[inline]
    (r0, r1) = __shuffle2(r0, r1);
    #[inline]
    (r2, r3) = __shuffle2(r2, r3);
    #[inline]
    (r4, r5) = __shuffle2(r4, r5);
    #[inline]
    (r6, r7) = __shuffle2(r6, r7);
    #[inline]
    (r0, r1, r2, r3, r4, r5, r6, r7) =
      __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1,
                     zeta1, qx16);
    zeta0 =
      /* global: */ jzetas_exp.[#unaligned :u256 (208 + (392 * i))]; /* u256 */
    zeta1 =
      /* global: */ jzetas_exp.[#unaligned :u256 (240 + (392 * i))]; /* u256 */
    #[inline]
    (r0, r4) = __shuffle1(r0, r4);
    #[inline]
    (r1, r5) = __shuffle1(r1, r5);
    #[inline]
    (r2, r6) = __shuffle1(r2, r6);
    #[inline]
    (r3, r7) = __shuffle1(r3, r7);
    #[inline]
    (r0, r4, r1, r5, r2, r6, r3, r7) =
      __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1,
                     zeta1, qx16);
    zeta0 =
      /* global: */ jzetas_exp.[#unaligned :u256 (272 + (392 * i))]; /* u256 */
    zeta2 =
      /* global: */ jzetas_exp.[#unaligned :u256 (304 + (392 * i))]; /* u256 */
    zeta1 =
      /* global: */ jzetas_exp.[#unaligned :u256 (336 + (392 * i))]; /* u256 */
    zeta3 =
      /* global: */ jzetas_exp.[#unaligned :u256 (368 + (392 * i))]; /* u256 */
    #[inline]
    (r0, r4, r2, r6, r1, r5, r3, r7) =
      __butterfly64x(r0, r4, r2, r6, r1, r5, r3, r7, zeta0, zeta1, zeta2,
                     zeta3, qx16);
    vx16 = /* global: */ jvx16[:u256 0]; /* u256 */
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    #[inline]
    r4 = __red16x(r4, qx16, vx16);
    #[inline]
    r2 = __red16x(r2, qx16, vx16);
    #[inline]
    r6 = __red16x(r6, qx16, vx16);
    #[inline]
    r1 = __red16x(r1, qx16, vx16);
    #[inline]
    r5 = __red16x(r5, qx16, vx16);
    #[inline]
    r3 = __red16x(r3, qx16, vx16);
    #[inline]
    r7 = __red16x(r7, qx16, vx16);
    rp.[#unaligned :u256 ((32 * 0) + (256 * i))] = r0; /* u256 */
    rp.[#unaligned :u256 ((32 * 1) + (256 * i))] = r4; /* u256 */
    rp.[#unaligned :u256 ((32 * 2) + (256 * i))] = r1; /* u256 */
    rp.[#unaligned :u256 ((32 * 3) + (256 * i))] = r5; /* u256 */
    rp.[#unaligned :u256 ((32 * 4) + (256 * i))] = r2; /* u256 */
    rp.[#unaligned :u256 ((32 * 5) + (256 * i))] = r6; /* u256 */
    rp.[#unaligned :u256 ((32 * 6) + (256 * i))] = r3; /* u256 */
    rp.[#unaligned :u256 ((32 * 7) + (256 * i))] = r7; /* u256 */
  }
  return (rp);
}

inline
fn __poly_reduce (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N]) {
  reg u256 qx16;
  reg u256 vx16;
  inline int i;
  reg u256 r;
  
  qx16 = /* global: */ jqx16[:u256 0]; /* u256 */
  vx16 = /* global: */ jvx16[:u256 0]; /* u256 */
  for i = 0 to 16 {
    r = rp.[#unaligned :u256 (32 * i)]; /* u256 */
    #[inline]
    r = __red16x(r, qx16, vx16);
    rp.[#unaligned :u256 (32 * i)] = r; /* u256 */
  }
  return (rp);
}

fn _poly_sub (reg mut ptr u16[MLKEM_N] rp, reg const ptr u16[MLKEM_N] ap,
             reg const ptr u16[MLKEM_N] bp) -> (reg mut ptr u16[MLKEM_N]) {
  inline int i;
  reg u256 a;
  reg u256 b;
  reg u256 r;
  
  for i = 0 to 16 {
    a = ap.[#unaligned :u256 (32 * i)]; /* u256 */
    b = bp.[#unaligned :u256 (32 * i)]; /* u256 */
    r = #VPSUB_16u16(a, b); /*  */
    rp.[#unaligned :u256 (32 * i)] = r; /* u256 */
  }
  return (rp);
}

fn _i_poly_tobytes (reg mut ptr u8[MLKEM_POLYBYTES] rp,
                   reg mut ptr u16[MLKEM_N] a) -> (reg mut ptr u8[MLKEM_POLYBYTES],
                                                  reg mut ptr u16[MLKEM_N]) {
  inline int i;
  reg u256 t0;
  reg u256 t1;
  reg u256 t2;
  reg u256 t3;
  reg u256 t4;
  reg u256 t5;
  reg u256 t6;
  reg u256 t7;
  reg u256 tt;
  reg u256 ttt;
  
  a = _poly_csubq(a);
  for i = 0 to 2 {
    t0 = a[:u256 (8 * i)]; /* u256 */
    t1 = a[:u256 ((8 * i) + 1)]; /* u256 */
    t2 = a[:u256 ((8 * i) + 2)]; /* u256 */
    t3 = a[:u256 ((8 * i) + 3)]; /* u256 */
    t4 = a[:u256 ((8 * i) + 4)]; /* u256 */
    t5 = a[:u256 ((8 * i) + 5)]; /* u256 */
    t6 = a[:u256 ((8 * i) + 6)]; /* u256 */
    t7 = a[:u256 ((8 * i) + 7)]; /* u256 */
    tt = #VPSLL_16u16(t1, ((128u) 12)); /*  */
    tt = (tt |256u t0); /* u256 */
    t0 = #VPSRL_16u16(t1, ((128u) 4)); /*  */
    t1 = #VPSLL_16u16(t2, ((128u) 8)); /*  */
    t0 = (t0 |256u t1); /* u256 */
    t1 = #VPSRL_16u16(t2, ((128u) 8)); /*  */
    t2 = #VPSLL_16u16(t3, ((128u) 4)); /*  */
    t1 = (t1 |256u t2); /* u256 */
    t2 = #VPSLL_16u16(t5, ((128u) 12)); /*  */
    t2 = (t2 |256u t4); /* u256 */
    t3 = #VPSRL_16u16(t5, ((128u) 4)); /*  */
    t4 = #VPSLL_16u16(t6, ((128u) 8)); /*  */
    t3 = (t3 |256u t4); /* u256 */
    t4 = #VPSRL_16u16(t6, ((128u) 8)); /*  */
    t5 = #VPSLL_16u16(t7, ((128u) 4)); /*  */
    t4 = (t4 |256u t5); /* u256 */
    #[inline]
    (ttt, t0) = __shuffle1(tt, t0);
    #[inline]
    (tt, t2) = __shuffle1(t1, t2);
    #[inline]
    (t1, t4) = __shuffle1(t3, t4);
    #[inline]
    (t3, tt) = __shuffle2(ttt, tt);
    #[inline]
    (ttt, t0) = __shuffle2(t1, t0);
    #[inline]
    (t1, t4) = __shuffle2(t2, t4);
    #[inline]
    (t2, ttt) = __shuffle4(t3, ttt);
    #[inline]
    (t3, tt) = __shuffle4(t1, tt);
    #[inline]
    (t1, t4) = __shuffle4(t0, t4);
    #[inline]
    (t0, t3) = __shuffle8(t2, t3);
    #[inline]
    (t2, ttt) = __shuffle8(t1, ttt);
    #[inline]
    (t1, t4) = __shuffle8(tt, t4);
    rp.[#unaligned :u256 (192 * i)] = t0; /* u256 */
    rp.[#unaligned :u256 ((192 * i) + 32)] = t2; /* u256 */
    rp.[#unaligned :u256 ((192 * i) + 64)] = t1; /* u256 */
    rp.[#unaligned :u256 ((192 * i) + 96)] = t3; /* u256 */
    rp.[#unaligned :u256 ((192 * i) + 128)] = ttt; /* u256 */
    rp.[#unaligned :u256 ((192 * i) + 160)] = t4; /* u256 */
  }
  return (rp, a);
}

fn _i_poly_tomsg (reg mut ptr u8[MLKEM_INDCPA_MSGBYTES] rp,
                 reg mut ptr u16[MLKEM_N] a) -> (reg mut ptr u8[MLKEM_INDCPA_MSGBYTES],
                                                reg mut ptr u16[MLKEM_N]) {
  reg mut ptr u16[16] px16;
  reg u256 hq;
  reg u256 hhq;
  inline int i;
  reg u256 f0;
  reg u256 f1;
  reg u256 g0;
  reg u256 g1;
  reg u32 c;
  
  a = _poly_csubq(a);
  px16 = /* global: */ hqx16_m1; /* u16[16] */
  hq = px16[:u256 0]; /* u256 */
  px16 = /* global: */ hhqx16; /* u16[16] */
  hhq = px16[:u256 0]; /* u256 */
  for i = 0 to (MLKEM_N / 32) {
    f0 = a[:u256 (2 * i)]; /* u256 */
    f1 = a[:u256 ((2 * i) + 1)]; /* u256 */
    f0 = #VPSUB_16u16(hq, f0); /*  */
    f1 = #VPSUB_16u16(hq, f1); /*  */
    g0 = #VPSRA_16u16(f0, ((128u) 15)); /*  */
    g1 = #VPSRA_16u16(f1, ((128u) 15)); /*  */
    f0 = #VPXOR_256(f0, g0); /*  */
    f1 = #VPXOR_256(f1, g1); /*  */
    f0 = #VPSUB_16u16(f0, hhq); /*  */
    f1 = #VPSUB_16u16(f1, hhq); /*  */
    f0 = #VPACKSS_16u16(f0, f1); /*  */
    f0 = #VPERMQ(f0, ((8u) 216)); /*  */
    c = #VPMOVMSKB_u256u32(f0); /*  */
    rp[:u32 i] = c; /* u32 */
  }
  return (rp, a);
}

u16 pc_mask_s = ((16u) 31);

u16 pc_shift1_s = ((16u) 1024);

u16 pc_shift2_s = ((16u) 8193);

u32 pc_shift3_s = ((32u) 67108865);

u64 pc_sllvdidx_s = ((64u) 12);

u8[32] pc_shufbidx_s = {((8u) 0), ((8u) 1), ((8u) 2), ((8u) 3), ((8u) 4),
                        ((8u) (- 1)), ((8u) (- 1)), ((8u) (- 1)),
                        ((8u) (- 1)), ((8u) (- 1)), ((8u) 8), ((8u) 9),
                        ((8u) 10), ((8u) 11), ((8u) 12), ((8u) (- 1)),
                        ((8u) 9), ((8u) 10), ((8u) 11), ((8u) 12),
                        ((8u) (- 1)), ((8u) 0), ((8u) 1), ((8u) 2), ((8u) 3),
                        ((8u) 4), ((8u) (- 1)), ((8u) (- 1)), ((8u) (- 1)),
                        ((8u) (- 1)), ((8u) (- 1)), ((8u) 8)};

inline
fn _i_poly_compress (reg mut ptr u8[MLKEM_POLYCOMPRESSEDBYTES] rp,
                    reg mut ptr u16[MLKEM_N] a) -> (reg mut ptr u8[MLKEM_POLYCOMPRESSEDBYTES],
                                                   reg mut ptr u16[MLKEM_N]) {
  reg u256 v;
  reg u256 shift1;
  reg u256 mask;
  reg u256 shift2;
  reg u256 shift3;
  reg u256 sllvdidx;
  reg u256 shufbidx;
  inline int i;
  reg u256 f0;
  reg u256 f1;
  reg u128 t0;
  reg u128 t1;
  
  a = _poly_csubq(a);
  v = /* global: */ jvx16[:u256 0]; /* u256 */
  shift1 = #VPBROADCAST_16u16(/* global: */ pc_shift1_s); /*  */
  mask = #VPBROADCAST_16u16(/* global: */ pc_mask_s); /*  */
  shift2 = #VPBROADCAST_16u16(/* global: */ pc_shift2_s); /*  */
  shift3 = #VPBROADCAST_8u32(/* global: */ pc_shift3_s); /*  */
  sllvdidx = #VPBROADCAST_4u64(/* global: */ pc_sllvdidx_s); /*  */
  shufbidx = /* global: */ pc_shufbidx_s[:u256 0]; /* u256 */
  for i = 0 to (MLKEM_N / 32) {
    f0 = a[:u256 (2 * i)]; /* u256 */
    f1 = a[:u256 ((2 * i) + 1)]; /* u256 */
    f0 = #VPMULH_16u16(f0, v); /*  */
    f1 = #VPMULH_16u16(f1, v); /*  */
    f0 = #VPMULHRS_16u16(f0, shift1); /*  */
    f1 = #VPMULHRS_16u16(f1, shift1); /*  */
    f0 = #VPAND_256(f0, mask); /*  */
    f1 = #VPAND_256(f1, mask); /*  */
    f0 = #VPACKUS_16u16(f0, f1); /*  */
    f0 = #VPMADDUBSW_256(f0, shift2); /*  */
    f0 = #VPMADDWD_256(f0, shift3); /*  */
    f0 = #VPSLLV_8u32(f0, sllvdidx); /*  */
    f0 = #VPSRLV_4u64(f0, sllvdidx); /*  */
    f0 = #VPSHUFB_256(f0, shufbidx); /*  */
    t0 = f0; /* u128 */
    t1 = #VEXTRACTI128(f0, ((8u) 1)); /*  */
    t0 = #BLENDV_16u8(t0, t1, shufbidx); /*  */
    rp.[#unaligned :u128 (20 * i)] = t0; /* u128 */
    rp.[#unaligned :u32 ((20 * i) + 16)] = t1; /* u32 */
  }
  return (rp, a);
}

u8[32] pd_jshufbidx = {((8u) 0), ((8u) 0), ((8u) 0), ((8u) 1), ((8u) 1),
                       ((8u) 1), ((8u) 1), ((8u) 2), ((8u) 2), ((8u) 3),
                       ((8u) 3), ((8u) 3), ((8u) 3), ((8u) 4), ((8u) 4),
                       ((8u) 4), ((8u) 5), ((8u) 5), ((8u) 5), ((8u) 6),
                       ((8u) 6), ((8u) 6), ((8u) 6), ((8u) 7), ((8u) 7),
                       ((8u) 8), ((8u) 8), ((8u) 8), ((8u) 8), ((8u) 9),
                       ((8u) 9), ((8u) 9)};

u16[16] pd_mask_s = {((16u) 31), ((16u) 992), ((16u) 124), ((16u) 3968),
                     ((16u) 496), ((16u) 62), ((16u) 1984), ((16u) 248),
                     ((16u) 31), ((16u) 992), ((16u) 124), ((16u) 3968),
                     ((16u) 496), ((16u) 62), ((16u) 1984), ((16u) 248)};

u16[16] pd_shift_s = {((16u) 1024), ((16u) 32), ((16u) 256), ((16u) 8),
                      ((16u) 64), ((16u) 512), ((16u) 16), ((16u) 128),
                      ((16u) 1024), ((16u) 32), ((16u) 256), ((16u) 8),
                      ((16u) 64), ((16u) 512), ((16u) 16), ((16u) 128)};

fn _i_poly_decompress (reg mut ptr u16[MLKEM_N] rp,
                      reg const ptr u8[MLKEM_POLYCOMPRESSEDBYTES] a) -> 
(reg mut ptr u16[MLKEM_N]) {
  reg u256 q;
  reg u256 shufbidx;
  reg u256 mask;
  reg u256 shift;
  inline int i;
  reg u128 t;
  reg u16 ti;
  stack u128 sh;
  reg u256 f;
  
  q = /* global: */ jqx16[:u256 0]; /* u256 */
  shufbidx = /* global: */ pd_jshufbidx[:u256 0]; /* u256 */
  mask = /* global: */ pd_mask_s[:u256 0]; /* u256 */
  shift = /* global: */ pd_shift_s[:u256 0]; /* u256 */
  for i = 0 to (MLKEM_N / 16) {
    t = ((128u) a.[#unaligned :u64 (10 * i)]); /* u128 */
    ti = a.[#unaligned :u16 ((10 * i) + 8)]; /* u16 */
    t = #VPINSR_8u16(t, ti, ((8u) 4)); /*  */
    sh = t; /* u128 */
    f = #VPBROADCAST_2u128(sh); /*  */
    f = #VPSHUFB_256(f, shufbidx); /*  */
    f = #VPAND_256(f, mask); /*  */
    f = #VPMULL_16u16(f, shift); /*  */
    f = #VPMULHRS_16u16(f, q); /*  */
    rp[:u256 i] = f; /* u256 */
  }
  return (rp);
}

inline
fn __polyvec_add2 (stack u16[MLKEM_VECN] r, stack u16[MLKEM_VECN] b) -> 
(stack u16[MLKEM_VECN]) {
  inline int i;
  
  for i = 0 to MLKEM_K {
    r[(MLKEM_N * i) : MLKEM_N] =
      _poly_add2(r[(MLKEM_N * i) : MLKEM_N], b[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __polyvec_csubq (stack u16[MLKEM_VECN] r) -> (stack u16[MLKEM_VECN]) {
  inline int i;
  
  for i = 0 to MLKEM_K {
    r[(MLKEM_N * i) : MLKEM_N] = _poly_csubq(r[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __polyvec_invntt (stack u16[MLKEM_VECN] r) -> (stack u16[MLKEM_VECN]) {
  inline int i;
  
  for i = 0 to MLKEM_K {
    r[(MLKEM_N * i) : MLKEM_N] = _poly_invntt(r[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __polyvec_ntt (stack u16[MLKEM_VECN] r) -> (stack u16[MLKEM_VECN]) {
  inline int i;
  
  for i = 0 to MLKEM_K {
    r[(MLKEM_N * i) : MLKEM_N] = _poly_ntt(r[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __polyvec_reduce (stack u16[MLKEM_VECN] r) -> (stack u16[MLKEM_VECN]) {
  inline int i;
  
  for i = 0 to MLKEM_K {
    #[inline]
    r[(MLKEM_N * i) : MLKEM_N] = __poly_reduce(r[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __i_polyvec_frombytes (reg const ptr u8[MLKEM_POLYVECBYTES] a) -> 
(stack u16[MLKEM_VECN]) {
  stack u16[MLKEM_VECN] r;
  inline int i;
  
  for i = 0 to MLKEM_K {
    r[(MLKEM_N * i) : MLKEM_N] =
      _i_poly_frombytes(r[(MLKEM_N * i) : MLKEM_N],
                        a[(MLKEM_POLYBYTES * i) : MLKEM_POLYBYTES]);
  }
  return (r);
}

inline
fn __i_polyvec_tobytes (reg mut ptr u8[MLKEM_POLYVECBYTES] r,
                       stack u16[MLKEM_VECN] a) -> (reg mut ptr u8[MLKEM_POLYVECBYTES]) {
  inline int i;
  
  for i = 0 to MLKEM_K {
    (r[(MLKEM_POLYBYTES * i) : MLKEM_POLYBYTES], a[(MLKEM_N * i) : MLKEM_N]) =
      _i_poly_tobytes(r[(MLKEM_POLYBYTES * i) : MLKEM_POLYBYTES],
                      a[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __polyvec_pointwise_acc (stack u16[MLKEM_N] r, stack u16[MLKEM_VECN] a,
                           stack u16[MLKEM_VECN] b) -> (stack u16[MLKEM_N]) {
  inline int i;
  stack u16[MLKEM_N] t;
  
  r = _poly_basemul(r, a[0 : MLKEM_N], b[0 : MLKEM_N]);
  for i = 1 to MLKEM_K {
    t =
      _poly_basemul(t, a[(MLKEM_N * i) : MLKEM_N], b[(MLKEM_N * i) : MLKEM_N]);
    r = _poly_add2(r, t);
  }
  return (r);
}

u8[32] pvd_shufbidx_s = {((8u) 0), ((8u) 1), ((8u) 1), ((8u) 2), ((8u) 2),
                         ((8u) 3), ((8u) 4), ((8u) 5), ((8u) 5), ((8u) 6),
                         ((8u) 6), ((8u) 7), ((8u) 8), ((8u) 9), ((8u) 9),
                         ((8u) 10), ((8u) 3), ((8u) 4), ((8u) 4), ((8u) 5),
                         ((8u) 5), ((8u) 6), ((8u) 7), ((8u) 8), ((8u) 8),
                         ((8u) 9), ((8u) 9), ((8u) 10), ((8u) 11), ((8u) 12),
                         ((8u) 12), ((8u) 13)};

u32[8] pvd_srlvdidx_s = {((32u) 0), ((32u) 1), ((32u) 0), ((32u) 0),
                         ((32u) 0), ((32u) 1), ((32u) 0), ((32u) 0)};

u64[4] pvd_srlvqidx_s = {((64u) 0), ((64u) 2), ((64u) 0), ((64u) 2)};

u16[16] pvd_shift_s = {((16u) 32), ((16u) 4), ((16u) 1), ((16u) 32),
                       ((16u) 8), ((16u) 1), ((16u) 32), ((16u) 4),
                       ((16u) 32), ((16u) 4), ((16u) 1), ((16u) 32),
                       ((16u) 8), ((16u) 1), ((16u) 32), ((16u) 4)};

u16 pvd_mask_s = ((16u) 32752);

inline
fn __i_polyvec_decompress (reg mut ptr u16[MLKEM_VECN] r,
                          reg const ptr u8[MLKEM_CIPHERTEXTBYTES] rp) -> 
(reg mut ptr u16[MLKEM_VECN]) {
  reg u256 q;
  reg u256 shufbidx;
  reg u256 srlvdidx;
  reg u256 srlvqidx;
  reg u256 shift;
  reg u256 mask;
  inline int k;
  inline int i;
  reg u256 f;
  
  q = /* global: */ jqx16[:u256 0]; /* u256 */
  shufbidx = /* global: */ pvd_shufbidx_s[:u256 0]; /* u256 */
  srlvdidx = /* global: */ pvd_srlvdidx_s[:u256 0]; /* u256 */
  srlvqidx = /* global: */ pvd_srlvqidx_s[:u256 0]; /* u256 */
  shift = /* global: */ pvd_shift_s[:u256 0]; /* u256 */
  mask = #VPBROADCAST_16u16(/* global: */ pvd_mask_s); /*  */
  for k = 0 to MLKEM_K {
    for i = 0 to (MLKEM_N / 16) {
      f = rp.[#unaligned :u256 ((352 * k) + (22 * i))]; /* u256 */
      f = #VPERMQ(f, ((8u) 148)); /*  */
      f = #VPSHUFB_256(f, shufbidx); /*  */
      f = #VPSRLV_8u32(f, srlvdidx); /*  */
      f = #VPSRLV_4u64(f, srlvqidx); /*  */
      f = #VPMULL_16u16(f, shift); /*  */
      f = #VPSRL_16u16(f, ((128u) 1)); /*  */
      f = #VPAND_256(f, mask); /*  */
      f = #VPMULHRS_16u16(f, q); /*  */
      r[:u256 ((16 * k) + i)] = f; /* u256 */
    }
  }
  return (r);
}

u16 pvc_off_s = ((16u) 36);

u16 pvc_shift1_s = ((16u) 8192);

u16 pvc_mask_s = ((16u) 2047);

u64 pvc_shift2_s = ((64u) 576460756732608513);

u64 pvc_sllvdidx_s = ((64u) 10);

u64[4] pvc_srlvqidx = {((64u) 10), ((64u) 30), ((64u) 10), ((64u) 30)};

u8[32] pvc_shufbidx_s = {((8u) 0), ((8u) 1), ((8u) 2), ((8u) 3), ((8u) 4),
                         ((8u) 5), ((8u) 6), ((8u) 7), ((8u) 8), ((8u) 9),
                         ((8u) 10), ((8u) (- 1)), ((8u) (- 1)), ((8u) (- 1)),
                         ((8u) (- 1)), ((8u) (- 1)), ((8u) 5), ((8u) 6),
                         ((8u) 7), ((8u) 8), ((8u) 9), ((8u) 10),
                         ((8u) (- 1)), ((8u) (- 1)), ((8u) (- 1)),
                         ((8u) (- 1)), ((8u) 0), ((8u) 0), ((8u) 1),
                         ((8u) 2), ((8u) 3), ((8u) 4)};

inline
fn __i_polyvec_compress (reg mut ptr u8[(MLKEM_POLYVECCOMPRESSEDBYTES + 2)] rp,
                        stack u16[MLKEM_VECN] a) -> (reg mut ptr u8[(MLKEM_POLYVECCOMPRESSEDBYTES +
                                                                    2)]) {
  reg u256 v;
  reg u256 v8;
  reg u256 off;
  reg u256 shift1;
  reg u256 mask;
  reg u256 shift2;
  reg u256 sllvdidx;
  reg u256 srlvqidx;
  reg u256 shufbidx;
  inline int i;
  reg u256 f0;
  reg u256 f1;
  reg u256 f2;
  reg u128 t0;
  reg u128 t1;
  
  #[inline]
  a = __polyvec_csubq(a);
  v = /* global: */ jvx16[:u256 0]; /* u256 */
  v8 = #VPSLL_16u16(v, ((128u) 3)); /*  */
  off = #VPBROADCAST_16u16(/* global: */ pvc_off_s); /*  */
  shift1 = #VPBROADCAST_16u16(/* global: */ pvc_shift1_s); /*  */
  mask = #VPBROADCAST_16u16(/* global: */ pvc_mask_s); /*  */
  shift2 = #VPBROADCAST_4u64(/* global: */ pvc_shift2_s); /*  */
  sllvdidx = #VPBROADCAST_4u64(/* global: */ pvc_sllvdidx_s); /*  */
  srlvqidx = /* global: */ pvc_srlvqidx[:u256 0]; /* u256 */
  shufbidx = /* global: */ pvc_shufbidx_s[:u256 0]; /* u256 */
  for i = 0 to (MLKEM_VECN / 16) {
    f0 = a[:u256 i]; /* u256 */
    f1 = #VPMULL_16u16(f0, v8); /*  */
    f2 = #VPADD_16u16(f0, off); /*  */
    f0 = #VPSLL_16u16(f0, ((128u) 3)); /*  */
    f0 = #VPMULH_16u16(f0, v); /*  */
    f2 = #VPSUB_16u16(f1, f2); /*  */
    f1 = #VPANDN_256(f1, f2); /*  */
    f1 = #VPSRL_16u16(f1, ((128u) 15)); /*  */
    f0 = #VPSUB_16u16(f0, f1); /*  */
    f0 = #VPMULHRS_16u16(f0, shift1); /*  */
    f0 = #VPAND_256(f0, mask); /*  */
    f0 = #VPMADDWD_256(f0, shift2); /*  */
    f0 = #VPSLLV_8u32(f0, sllvdidx); /*  */
    f1 = #VPSRLDQ_256(f0, ((8u) 8)); /*  */
    f0 = #VPSRLV_4u64(f0, srlvqidx); /*  */
    f1 = #VPSLL_4u64(f1, ((128u) 34)); /*  */
    f0 = #VPADD_4u64(f0, f1); /*  */
    f0 = #VPSHUFB_256(f0, shufbidx); /*  */
    t0 = f0; /* u128 */
    t1 = #VEXTRACTI128(f0, ((8u) 1)); /*  */
    t0 = #BLENDV_16u8(t0, t1, shufbidx); /*  */
    rp.[#unaligned :u128 (22 * i)] = t0; /* u128 */
    rp.[#unaligned :u64 ((22 * i) + 16)] = t1; /* u64 */
  }
  return (rp);
}

u8[32] sample_load_shuffle = {((8u) 0), ((8u) 1), ((8u) 1), ((8u) 2),
                              ((8u) 3), ((8u) 4), ((8u) 4), ((8u) 5),
                              ((8u) 6), ((8u) 7), ((8u) 7), ((8u) 8),
                              ((8u) 9), ((8u) 10), ((8u) 10), ((8u) 11),
                              ((8u) 4), ((8u) 5), ((8u) 5), ((8u) 6),
                              ((8u) 7), ((8u) 8), ((8u) 8), ((8u) 9),
                              ((8u) 10), ((8u) 11), ((8u) 11), ((8u) 12),
                              ((8u) 13), ((8u) 14), ((8u) 14), ((8u) 15)};

u256 sample_ones = (32u8)[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1];

u256 sample_mask = (16u16)[4095, 4095, 4095, 4095, 4095, 4095, 4095, 4095,
                   4095, 4095, 4095, 4095, 4095, 4095, 4095, 4095];

u256 sample_q = (16u16)[MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q,
                MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q,
                MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q];

u8[(256 * 8)] sample_shuffle_table = {((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 6),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 6), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 6), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 8), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 8), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 6),
                                      ((8u) 8), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 6),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 6), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 8),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 6),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 6), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 6), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 8),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 8),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 6),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 6), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 6), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 8),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 8),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 8),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 8),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 8),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 8),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 8),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 8),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 2), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) 14)};

u8[64] gen_matrix_indexes = {((8u) 0), ((8u) 0), ((8u) 1), ((8u) 0),
                             ((8u) 2), ((8u) 0), ((8u) 3), ((8u) 0),
                             ((8u) 0), ((8u) 1), ((8u) 1), ((8u) 1),
                             ((8u) 2), ((8u) 1), ((8u) 3), ((8u) 1),
                             ((8u) 0), ((8u) 2), ((8u) 1), ((8u) 2),
                             ((8u) 2), ((8u) 2), ((8u) 3), ((8u) 2),
                             ((8u) 0), ((8u) 3), ((8u) 1), ((8u) 3),
                             ((8u) 2), ((8u) 3), ((8u) 3), ((8u) 3),
                             ((8u) 0), ((8u) 0), ((8u) 0), ((8u) 1),
                             ((8u) 0), ((8u) 2), ((8u) 0), ((8u) 3),
                             ((8u) 1), ((8u) 0), ((8u) 1), ((8u) 1),
                             ((8u) 1), ((8u) 2), ((8u) 1), ((8u) 3),
                             ((8u) 2), ((8u) 0), ((8u) 2), ((8u) 1),
                             ((8u) 2), ((8u) 2), ((8u) 2), ((8u) 3),
                             ((8u) 3), ((8u) 0), ((8u) 3), ((8u) 1),
                             ((8u) 3), ((8u) 2), ((8u) 3), ((8u) 3)};

param int BUF_size = 536;

inline
fn __gen_matrix_buf_rejection_filter48 (reg mut ptr u16[MLKEM_N] pol,
                                       reg u64 counter,
                                       reg const ptr u8[BUF_size] buf,
                                       reg u64 buf_offset,
                                       reg u256 load_shuffle, reg u256 mask,
                                       reg u256 bounds,
                                       reg const ptr u8[2048] sst,
                                       reg u256 ones, #[msf] reg u64 ms) -> 
(reg mut ptr u16[MLKEM_N], reg u64) {
  reg u256 f0;
  reg u256 f1;
  reg u256 g0;
  reg u256 g1;
  reg u64 good;
  reg u64 t0_0;
  reg u256 shuffle_0;
  reg bool _of_;
  reg bool _cf_;
  reg bool _sf_;
  reg bool _zf_;
  reg u64 t0_1;
  reg u128 shuffle_0_1;
  reg u64 t1_0;
  reg u256 shuffle_1;
  reg u64 t1_1;
  reg u128 shuffle_1_1;
  reg u256 shuffle_t;
  
  f0 =
    #VPERMQ(buf.[#unaligned :u256 (((uint /* of u64 */) buf_offset) + 0)],
            (4u2)[2, 1, 1, 0]); /*  */
  f1 =
    #VPERMQ(buf.[#unaligned :u256 (((uint /* of u64 */) buf_offset) + 24)],
            (4u2)[2, 1, 1, 0]); /*  */
  f0 = #VPSHUFB_256(f0, load_shuffle); /*  */
  f1 = #VPSHUFB_256(f1, load_shuffle); /*  */
  g0 = #VPSRL_16u16(f0, ((128u) 4)); /*  */
  g1 = #VPSRL_16u16(f1, ((128u) 4)); /*  */
  f0 = #VPBLEND_16u16(f0, g0, ((8u) 170)); /*  */
  f1 = #VPBLEND_16u16(f1, g1, ((8u) 170)); /*  */
  f0 = #VPAND_256(f0, mask); /*  */
  f1 = #VPAND_256(f1, mask); /*  */
  g0 = #VPCMPGT_16u16(bounds, f0); /*  */
  g1 = #VPCMPGT_16u16(bounds, f1); /*  */
  g0 = #VPACKSS_16u16(g0, g1); /*  */
  #[declassify]
  good = #VPMOVMSKB_u256u64(g0); /*  */
  good = #protect_64(good, ms); /* :k */
  t0_0 = good; /* u64 */
  t0_0 = (t0_0 &64u ((64u) 255)); /* u64 */
  shuffle_0 = (256u)#VMOV_64(sst[:u64 t0_0]); /*  */
  (_of_, _cf_, _sf_, _ /* bool */, _zf_, t0_0) = #POPCNT_64(t0_0); /*  */
  t0_0 = (t0_0 +64u counter); /* u64 */
  t0_1 = good; /* u64 */
  t0_1 = (t0_1 >>64u ((8u) 16)); /* u64 */
  t0_1 = (t0_1 &64u ((64u) 255)); /* u64 */
  shuffle_0_1 = #VMOV_64(sst[:u64 t0_1]); /*  */
  (_of_, _cf_, _sf_, _ /* bool */, _zf_, t0_1) = #POPCNT_64(t0_1); /*  */
  t0_1 = (t0_1 +64u t0_0); /* u64 */
  t1_0 = good; /* u64 */
  t1_0 = (t1_0 >>64u ((8u) 8)); /* u64 */
  t1_0 = (t1_0 &64u ((64u) 255)); /* u64 */
  shuffle_1 = (256u)#VMOV_64(sst[:u64 t1_0]); /*  */
  (_of_, _cf_, _sf_, _ /* bool */, _zf_, t1_0) = #POPCNT_64(t1_0); /*  */
  t1_0 = (t1_0 +64u t0_1); /* u64 */
  t1_1 = good; /* u64 */
  t1_1 = (t1_1 >>64u ((8u) 24)); /* u64 */
  t1_1 = (t1_1 &64u ((64u) 255)); /* u64 */
  shuffle_1_1 = #VMOV_64(sst[:u64 t1_1]); /*  */
  (_of_, _cf_, _sf_, _ /* bool */, _zf_, t1_1) = #POPCNT_64(t1_1); /*  */
  t1_1 = (t1_1 +64u t1_0); /* u64 */
  shuffle_0 = #VINSERTI128(shuffle_0, shuffle_0_1, ((8u) 1)); /*  */
  shuffle_1 = #VINSERTI128(shuffle_1, shuffle_1_1, ((8u) 1)); /*  */
  shuffle_t = #VPADD_32u8(shuffle_0, ones); /*  */
  shuffle_0 = #VPUNPCKL_32u8(shuffle_0, shuffle_t); /*  */
  shuffle_t = #VPADD_32u8(shuffle_1, ones); /*  */
  shuffle_1 = #VPUNPCKL_32u8(shuffle_1, shuffle_t); /*  */
  f0 = #VPSHUFB_256(f0, shuffle_0); /*  */
  f1 = #VPSHUFB_256(f1, shuffle_1); /*  */
  pol.[#unaligned :u128 (((64u) 2) *64u counter)] = f0; /* u128 */
  pol.[#unaligned :u128 (((64u) 2) *64u t0_0)] =
    #VEXTRACTI128(f0, ((8u) 1)); /*  */
  pol.[#unaligned :u128 (((64u) 2) *64u t0_1)] = f1; /* u128 */
  pol.[#unaligned :u128 (((64u) 2) *64u t1_0)] =
    #VEXTRACTI128(f1, ((8u) 1)); /*  */
  counter = t1_1; /* u64 */
  return (pol, counter);
}

inline
fn __write_u128_boundchk (reg mut ptr u16[MLKEM_N] pol, reg u64 ctr,
                         reg u128 data, #[msf] reg u64 ms) -> (reg mut ptr u16[MLKEM_N],
                                                              #[msf] reg u64) {
  reg bool condition_8;
  reg u64 data_u64;
  reg bool condition_4;
  reg bool condition_2;
  reg bool condition_1;
  
  condition_8 = (ctr <=u ((64u) (MLKEM_N - 8))); /* bool */
  if condition_8 {
    ms = #update_msf(condition_8, ms); /* :k */
    pol.[#unaligned :u128 (2 * ((uint /* of u64 */) ctr))] = data; /* u128 */
    ctr = (ctr +64u ((64u) 8)); /* u64 */
  } else {
    ms = #update_msf((! condition_8), ms); /* :k */
    data_u64 = #MOVV_64(data); /*  */
    condition_4 = (ctr <=u ((64u) (MLKEM_N - 4))); /* bool */
    if condition_4 {
      ms = #update_msf(condition_4, ms); /* :k */
      pol.[#unaligned :u64 (2 * ((uint /* of u64 */) ctr))] =
        data_u64; /* u64 */
      data_u64 = #VPEXTR_64(data, ((8u) 1)); /*  */
      ctr = (ctr +64u ((64u) 4)); /* u64 */
    } else {
      ms = #update_msf((! condition_4), ms); /* :k */
    }
    condition_2 = (ctr <=u ((64u) (MLKEM_N - 2))); /* bool */
    if condition_2 {
      ms = #update_msf(condition_2, ms); /* :k */
      pol.[#unaligned :u32 (2 * ((uint /* of u64 */) ctr))] =
        data_u64; /* u32 */
      data_u64 = (data_u64 >>64u ((8u) 32)); /* u64 */
      ctr = (ctr +64u ((64u) 2)); /* u64 */
    } else {
      ms = #update_msf((! condition_2), ms); /* :k */
    }
    condition_1 = (ctr <=u ((64u) (MLKEM_N - 1))); /* bool */
    if condition_1 {
      ms = #update_msf(condition_1, ms); /* :k */
      pol.[#unaligned (2 * ((uint /* of u64 */) ctr))] = data_u64; /* u16 */
    } else {
      ms = #update_msf((! condition_1), ms); /* :k */
    }
  }
  return (pol, ms);
}

inline
fn __gen_matrix_buf_rejection_filter24 (reg mut ptr u16[MLKEM_N] pol,
                                       reg u64 counter,
                                       reg const ptr u8[BUF_size] buf,
                                       reg u64 buf_offset,
                                       reg u256 load_shuffle, reg u256 mask,
                                       reg u256 bounds,
                                       reg const ptr u8[2048] sst,
                                       reg u256 ones, #[msf] reg u64 ms) -> 
(reg mut ptr u16[MLKEM_N], reg u64, #[msf] reg u64) {
  reg u256 f0;
  reg u256 g0;
  reg u256 g1;
  reg u64 good;
  reg u64 t0_0;
  reg u256 shuffle_0;
  reg bool _of_;
  reg bool _cf_;
  reg bool _sf_;
  reg bool _zf_;
  reg u64 t0_1;
  reg u128 shuffle_0_1;
  reg u256 shuffle_t;
  reg u128 t128;
  
  f0 =
    #VPERMQ(buf.[#unaligned :u256 (((uint /* of u64 */) buf_offset) + 0)],
            (4u2)[2, 1, 1, 0]); /*  */
  f0 = #VPSHUFB_256(f0, load_shuffle); /*  */
  g0 = #VPSRL_16u16(f0, ((128u) 4)); /*  */
  f0 = #VPBLEND_16u16(f0, g0, ((8u) 170)); /*  */
  f0 = #VPAND_256(f0, mask); /*  */
  g0 = #VPCMPGT_16u16(bounds, f0); /*  */
  g1 = #set0_256(); /*  */
  g0 = #VPACKSS_16u16(g0, g1); /*  */
  #[declassify]
  good = #VPMOVMSKB_u256u64(g0); /*  */
  good = #protect_64(good, ms); /* :k */
  t0_0 = good; /* u64 */
  t0_0 = (t0_0 &64u ((64u) 255)); /* u64 */
  shuffle_0 = (256u)#VMOV_64(sst[:u64 t0_0]); /*  */
  (_of_, _cf_, _sf_, _ /* bool */, _zf_, t0_0) = #POPCNT_64(t0_0); /*  */
  t0_0 = (t0_0 +64u counter); /* u64 */
  t0_1 = good; /* u64 */
  t0_1 = (t0_1 >>64u ((8u) 16)); /* u64 */
  t0_1 = (t0_1 &64u ((64u) 255)); /* u64 */
  shuffle_0_1 = #VMOV_64(sst[:u64 t0_1]); /*  */
  (_of_, _cf_, _sf_, _ /* bool */, _zf_, t0_1) = #POPCNT_64(t0_1); /*  */
  t0_1 = (t0_1 +64u t0_0); /* u64 */
  shuffle_0 = #VINSERTI128(shuffle_0, shuffle_0_1, ((8u) 1)); /*  */
  shuffle_t = #VPADD_32u8(shuffle_0, ones); /*  */
  shuffle_0 = #VPUNPCKL_32u8(shuffle_0, shuffle_t); /*  */
  f0 = #VPSHUFB_256(f0, shuffle_0); /*  */
  t128 = f0; /* u128 */
  #[inline]
  (pol, ms) = __write_u128_boundchk(pol, counter, t128, ms);
  t128 = #VEXTRACTI128(f0, ((8u) 1)); /*  */
  #[inline]
  (pol, ms) = __write_u128_boundchk(pol, t0_0, t128, ms);
  counter = t0_1; /* u64 */
  return (pol, counter, ms);
}

fn _gen_matrix_buf_rejection (reg mut ptr u16[MLKEM_N] pol, reg u64 counter,
                             reg const ptr u8[BUF_size] buf,
                             reg u64 buf_offset) -> (reg mut ptr u16[MLKEM_N],
                                                    reg u64) {
  #[msf]
  reg u64 ms;
  reg u256 load_shuffle;
  reg u256 mask;
  reg u256 bounds;
  reg u256 ones;
  reg mut ptr u8[2048] sst;
  stack u64 saved_buf_offset;
  reg bool condition_loop;
  
  ms = #init_msf(); /* :k */
  load_shuffle = /* global: */ sample_load_shuffle[:u256 0]; /* u256 */
  mask = /* global: */ sample_mask; /* u256 */
  bounds = /* global: */ sample_q; /* u256 */
  ones = /* global: */ sample_ones; /* u256 */
  sst = /* global: */ sample_shuffle_table; /* u8[2048] */
  saved_buf_offset = buf_offset; /* u64 */
  buf_offset = buf_offset; /* u64 */
  while {
    condition_loop =
      (buf_offset <u ((64u) (((3 * 168) - 48) + 1))); /* bool */
  } (condition_loop) {
    ms = #update_msf(condition_loop, ms); /* :k */
    condition_loop = (counter <u ((64u) ((MLKEM_N - 32) + 1))); /* bool */
    if condition_loop {
      ms = #update_msf(condition_loop, ms); /* :k */
      #[inline]
      (pol, counter) =
        __gen_matrix_buf_rejection_filter48(pol, counter, buf, buf_offset,
                                            load_shuffle, mask, bounds, sst,
                                            ones, ms);
      saved_buf_offset = (saved_buf_offset +64u ((64u) 48)); /* u64 */
      buf_offset = saved_buf_offset; /* u64 */
      buf_offset = #protect_64(buf_offset, ms); /* :k */
    } else {
      ms = #update_msf((! condition_loop), ms); /* :k */
      buf_offset = ((64u) (3 * 168)); /* u64 */
    }
  }
  ms = #update_msf((! condition_loop), ms); /* :k */
  buf_offset = saved_buf_offset; /* u64 */
  buf_offset = #protect_64(buf_offset, ms); /* :k */
  while {
    condition_loop =
      (buf_offset <u ((64u) (((3 * 168) - 24) + 1))); /* bool */
  } (condition_loop) {
    ms = #update_msf(condition_loop, ms); /* :k */
    condition_loop = (counter <u ((64u) MLKEM_N)); /* bool */
    if condition_loop {
      ms = #update_msf(condition_loop, ms); /* :k */
      () = #spill(buf_offset); /* :k */
      #[inline]
      (pol, counter, ms) =
        __gen_matrix_buf_rejection_filter24(pol, counter, buf, buf_offset,
                                            load_shuffle, mask, bounds, sst,
                                            ones, ms);
      () = #unspill(buf_offset); /* :k */
      buf_offset = #protect_64(buf_offset, ms); /* :k */
      buf_offset = (buf_offset +64u ((64u) 24)); /* u64 */
    } else {
      ms = #update_msf((! condition_loop), ms); /* :k */
      buf_offset = ((64u) (3 * 168)); /* u64 */
    }
  }
  return (pol, counter);
}

param int IDX_TABLE_SIZE = (32 * (MLKEM_K - 2));

inline
fn gen_matrix_get_indexes (reg u64 b, reg u64 _t) -> (reg u64) {
  reg u64 t;
  reg mut ptr u8[IDX_TABLE_SIZE] idxs;
  
  idxs = /* global: */ gen_matrix_indexes; /* u8[IDX_TABLE_SIZE] */
  t = _t; /* u64 */
  t = (t <<64u ((8u) (MLKEM_K + 1))); /* u64 */
  b = (b +64u t); /* u64 */
  t = idxs.[#unaligned :u64 b]; /* u64 */
  return (t);
}

fn __gen_matrix_fill_polynomial (reg mut ptr u16[MLKEM_N] pol,
                                reg mut ptr u8[BUF_size] buf) -> (reg mut ptr u16[MLKEM_N],
                                                                 reg mut ptr u8[BUF_size]) {
  reg u64 buf_offset;
  reg u64 counter;
  
  buf_offset = ((64u) 0); /* u64 */
  counter = ((64u) 0); /* u64 */
  (pol, counter) = _gen_matrix_buf_rejection(pol, counter, buf, buf_offset);
  buf_offset = ((64u) (2 * 168)); /* u64 */
  while ((counter <u ((64u) MLKEM_N))) {
    buf = _shake128_next_state(buf);
    (pol, counter) =
      _gen_matrix_buf_rejection(pol, counter, buf, buf_offset);
  }
  return (pol, buf);
}

fn _gen_matrix_sample_four_polynomials (reg mut ptr u16[(4 * MLKEM_N)] polx4,
                                       reg mut ptr u8[(BUF_size * 4)] buf,
                                       reg const ptr u8[32] rho,
                                       reg u64 pos_entry, reg u64 transposed) -> 
(reg mut ptr u16[(4 * MLKEM_N)], reg mut ptr u8[(BUF_size * 4)]) {
  stack u8[8] indexes;
  reg mut ptr u256[25] stx4;
  stack u256[25] state;
  reg mut ptr u16[MLKEM_N] pol;
  
  #[inline]
  indexes.[#unaligned :u64 0] =
    gen_matrix_get_indexes(pos_entry, transposed);
  stx4 = state; /* u256[25] */
  stx4 = _shake128x4_absorb_A32_A2(stx4, rho, indexes);
  (_ /* u256[?] */, buf) = _shake128x4_squeeze3blocks(stx4, buf);
  pol = polx4[(0 * MLKEM_N) : MLKEM_N]; /* u16[MLKEM_N] */
  (pol, buf[(BUF_size * 0) : BUF_size]) =
    __gen_matrix_fill_polynomial(pol, buf[(BUF_size * 0) : BUF_size]);
  polx4[(0 * MLKEM_N) : MLKEM_N] = pol; /* u16[MLKEM_N] */
  pol = polx4[(1 * MLKEM_N) : MLKEM_N]; /* u16[MLKEM_N] */
  (pol, buf[(BUF_size * 1) : BUF_size]) =
    __gen_matrix_fill_polynomial(pol, buf[(BUF_size * 1) : BUF_size]);
  polx4[(1 * MLKEM_N) : MLKEM_N] = pol; /* u16[MLKEM_N] */
  pol = polx4[(2 * MLKEM_N) : MLKEM_N]; /* u16[MLKEM_N] */
  (pol, buf[(BUF_size * 2) : BUF_size]) =
    __gen_matrix_fill_polynomial(pol, buf[(BUF_size * 2) : BUF_size]);
  polx4[(2 * MLKEM_N) : MLKEM_N] = pol; /* u16[MLKEM_N] */
  pol = polx4[(3 * MLKEM_N) : MLKEM_N]; /* u16[MLKEM_N] */
  (pol, buf[(BUF_size * 3) : BUF_size]) =
    __gen_matrix_fill_polynomial(pol, buf[(BUF_size * 3) : BUF_size]);
  polx4[(3 * MLKEM_N) : MLKEM_N] = pol; /* u16[MLKEM_N] */
  return (polx4, buf);
}

inline
fn __gen_matrix_sample_one_polynomial (reg mut ptr u16[MLKEM_N] pol,
                                      reg mut ptr u8[BUF_size] buf,
                                      reg const ptr u8[32] rho, reg u16 rc) -> 
(reg mut ptr u16[MLKEM_N], reg mut ptr u8[BUF_size]) {
  stack u8[2] pos;
  reg u256[7] stavx2;
  
  pos[:u16 0] = rc; /* u16 */
  stavx2 = _shake128_absorb_A32_A2(rho, pos);
  buf = _shake128_squeeze3blocks(buf, stavx2);
  (pol, buf) = __gen_matrix_fill_polynomial(pol, buf);
  return (pol, buf);
}

fn _gen_matrix_avx2 (reg mut ptr u16[((MLKEM_K * MLKEM_K) * MLKEM_N)] matrix,
                    reg const ptr u8[32] rho, #[spill_to_mmx]
                    reg u64 transposed) -> (reg mut ptr u16[((MLKEM_K *
                                                             MLKEM_K) *
                                                            MLKEM_N)]) {
  reg mut ptr u8[(BUF_size * 4)] buf;
  stack u8[(BUF_size * 4)] buf_s;
  inline int i;
  reg u64 pos_entry;
  reg mut ptr u16[(4 * MLKEM_N)] polx4;
  inline int j;
  
  () = #spill(transposed); /* :k */
  buf = buf_s; /* u8[(BUF_size * 4)] */
  for i = 0 to 4 {
    pos_entry = ((64u) (8 * i)); /* u64 */
    polx4 =
      matrix[((4 * i) * MLKEM_N) : (4 * MLKEM_N)]; /* u16[(4 * MLKEM_N)] */
    () = #unspill(transposed); /* :k */
    (polx4, buf) =
      _gen_matrix_sample_four_polynomials(polx4, buf, rho, pos_entry,
                                          transposed);
    matrix[((i * 4) * MLKEM_N) : (4 * MLKEM_N)] =
      polx4; /* u16[(4 * MLKEM_N)] */
  }
  for i = 0 to MLKEM_K {
    for j = 0 to MLKEM_K {
      matrix[((i * MLKEM_VECN) + (j * MLKEM_N)) : MLKEM_N] =
        _nttunpack(matrix[((i * MLKEM_VECN) + (j * MLKEM_N)) : MLKEM_N]);
    }
  }
  return (matrix);
}

inline
fn __indcpa_keypair (#[spill_to_mmx] reg mut ptr u8[MLKEM_PUBLICKEYBYTES] pk,
                    #[spill_to_mmx] reg mut ptr u8[MLKEM_POLYVECBYTES] sk,
                    reg const ptr u8[MLKEM_SYMBYTES] randomnessp) -> 
(reg mut ptr u8[MLKEM_PUBLICKEYBYTES], reg mut ptr u8[MLKEM_POLYVECBYTES]) {
  inline int i;
  reg u64 t64;
  stack u8[33] inbuf;
  stack u8[64] buf;
  stack u8[MLKEM_SYMBYTES] publicseed;
  stack u8[MLKEM_SYMBYTES] noiseseed;
  reg u64 transposed;
  stack u16[(MLKEM_K * MLKEM_VECN)] aa;
  reg u8 nonce;
  stack u16[MLKEM_VECN] skpv;
  stack u16[MLKEM_VECN] e;
  stack u16[MLKEM_VECN] pkpv;
  
  () = #spill(pk, sk); /* :k */
  for i = 0 to (MLKEM_SYMBYTES / 8) {
    t64 = randomnessp[:u64 i]; /* u64 */
    inbuf[:u64 i] = t64; /* u64 */
  }
  inbuf[32] = ((8u) MLKEM_K); /* u8 */
  buf = _sha3_512A_A33(buf, inbuf);
  for i = 0 to (MLKEM_SYMBYTES / 8) {
    #[declassify]
    t64 = buf[:u64 i]; /* u64 */
    publicseed[:u64 i] = t64; /* u64 */
    t64 = buf[:u64 (i + (MLKEM_SYMBYTES / 8))]; /* u64 */
    noiseseed[:u64 i] = t64; /* u64 */
  }
  transposed = ((64u) 0); /* u64 */
  aa = _gen_matrix_avx2(aa, publicseed, transposed);
  nonce = ((8u) 0); /* u8 */
  (skpv[0 : MLKEM_N], skpv[MLKEM_N : MLKEM_N], skpv[(2 * MLKEM_N) : MLKEM_N],
   skpv[(3 * MLKEM_N) : MLKEM_N]) =
    _poly_getnoise_eta1_4x(skpv[0 : MLKEM_N], skpv[MLKEM_N : MLKEM_N],
                           skpv[(2 * MLKEM_N) : MLKEM_N],
                           skpv[(3 * MLKEM_N) : MLKEM_N], noiseseed, nonce);
  nonce = ((8u) 4); /* u8 */
  (e[0 : MLKEM_N], e[MLKEM_N : MLKEM_N], e[(2 * MLKEM_N) : MLKEM_N],
   e[(3 * MLKEM_N) : MLKEM_N]) =
    _poly_getnoise_eta1_4x(e[0 : MLKEM_N], e[MLKEM_N : MLKEM_N],
                           e[(2 * MLKEM_N) : MLKEM_N],
                           e[(3 * MLKEM_N) : MLKEM_N], noiseseed, nonce);
  #[inline]
  skpv = __polyvec_ntt(skpv);
  #[inline]
  e = __polyvec_ntt(e);
  for i = 0 to MLKEM_K {
    #[inline]
    pkpv[(i * MLKEM_N) : MLKEM_N] =
      __polyvec_pointwise_acc(pkpv[(i * MLKEM_N) : MLKEM_N],
                              aa[(i * MLKEM_VECN) : MLKEM_VECN], skpv);
    pkpv[(i * MLKEM_N) : MLKEM_N] =
      _poly_frommont(pkpv[(i * MLKEM_N) : MLKEM_N]);
  }
  #[inline]
  pkpv = __polyvec_add2(pkpv, e);
  #[inline]
  pkpv = __polyvec_reduce(pkpv);
  () = #unspill(pk, sk); /* :k */
  #[inline]
  sk = __i_polyvec_tobytes(sk, skpv);
  #[inline]
  pk[0 : MLKEM_POLYVECBYTES] =
    __i_polyvec_tobytes(pk[0 : MLKEM_POLYVECBYTES], pkpv);
  for i = 0 to (MLKEM_SYMBYTES / 8) {
    t64 = publicseed[:u64 i]; /* u64 */
    pk.[#unaligned :u64 ((i + (MLKEM_POLYVECBYTES / 8)) * 8)] =
      t64; /* u64 */
  }
  return (pk, sk);
}

inline
fn __indcpa_enc (#[spill_to_mmx] reg mut ptr u8[MLKEM_CIPHERTEXTBYTES] ct,
                reg const ptr u8[MLKEM_INDCPA_MSGBYTES] msgp,
                reg const ptr u8[MLKEM_PUBLICKEYBYTES] pk,
                reg const ptr u8[MLKEM_SYMBYTES] noiseseed) -> (reg mut ptr u8[MLKEM_CIPHERTEXTBYTES]) {
  stack u16[MLKEM_VECN] pkpv;
  inline int w;
  reg u64 t64;
  stack u8[MLKEM_SYMBYTES] publicseed;
  stack u16[MLKEM_N] k;
  reg u64 transposed;
  stack u16[(MLKEM_K * MLKEM_VECN)] aat;
  reg u8 nonce;
  stack u16[MLKEM_VECN] sp;
  stack u16[MLKEM_VECN] ep;
  stack u16[MLKEM_N] epp;
  stack u16[MLKEM_VECN] bp;
  stack u16[MLKEM_N] v;
  
  () = #spill(ct); /* :k */
  #[inline]
  pkpv = __i_polyvec_frombytes(pk[0 : MLKEM_POLYVECBYTES]);
  for w = 0 to (MLKEM_SYMBYTES / 8) {
    #[declassify]
    t64 =
      pk.[#unaligned :u64 (((MLKEM_POLYVECBYTES / 8) + w) * 8)]; /* u64 */
    publicseed[:u64 w] = t64; /* u64 */
  }
  k = _i_poly_frommsg(k, msgp);
  transposed = ((64u) 1); /* u64 */
  aat = _gen_matrix_avx2(aat, publicseed, transposed);
  nonce = ((8u) 0); /* u8 */
  (sp[0 : MLKEM_N], sp[MLKEM_N : MLKEM_N], sp[(2 * MLKEM_N) : MLKEM_N],
   sp[(3 * MLKEM_N) : MLKEM_N]) =
    _poly_getnoise_eta1_4x(sp[0 : MLKEM_N], sp[MLKEM_N : MLKEM_N],
                           sp[(2 * MLKEM_N) : MLKEM_N],
                           sp[(3 * MLKEM_N) : MLKEM_N], noiseseed, nonce);
  nonce = ((8u) 4); /* u8 */
  (ep[0 : MLKEM_N], ep[MLKEM_N : MLKEM_N], ep[(2 * MLKEM_N) : MLKEM_N],
   ep[(3 * MLKEM_N) : MLKEM_N]) =
    _poly_getnoise_eta1_4x(ep[0 : MLKEM_N], ep[MLKEM_N : MLKEM_N],
                           ep[(2 * MLKEM_N) : MLKEM_N],
                           ep[(3 * MLKEM_N) : MLKEM_N], noiseseed, nonce);
  nonce = ((8u) 8); /* u8 */
  epp = _poly_getnoise_eta2(epp, noiseseed, nonce);
  #[inline]
  sp = __polyvec_ntt(sp);
  for w = 0 to MLKEM_K {
    #[inline]
    bp[(w * MLKEM_N) : MLKEM_N] =
      __polyvec_pointwise_acc(bp[(w * MLKEM_N) : MLKEM_N],
                              aat[(w * MLKEM_VECN) : MLKEM_VECN], sp);
  }
  #[inline]
  v = __polyvec_pointwise_acc(v, pkpv, sp);
  #[inline]
  bp = __polyvec_invntt(bp);
  v = _poly_invntt(v);
  #[inline]
  bp = __polyvec_add2(bp, ep);
  v = _poly_add2(v, epp);
  v = _poly_add2(v, k);
  #[inline]
  bp = __polyvec_reduce(bp);
  #[inline]
  v = __poly_reduce(v);
  () = #unspill(ct); /* :k */
  #[inline]
  ct[0 : (MLKEM_POLYVECCOMPRESSEDBYTES + 2)] =
    __i_polyvec_compress(ct[0 : (MLKEM_POLYVECCOMPRESSEDBYTES + 2)], bp);
  #[inline]
  (ct[MLKEM_POLYVECCOMPRESSEDBYTES : MLKEM_POLYCOMPRESSEDBYTES], v) =
    _i_poly_compress(ct[MLKEM_POLYVECCOMPRESSEDBYTES : MLKEM_POLYCOMPRESSEDBYTES],
                     v);
  return (ct);
}

inline
fn __indcpa_dec (reg mut ptr u8[MLKEM_INDCPA_MSGBYTES] msgp,
                reg const ptr u8[MLKEM_CIPHERTEXTBYTES] ct,
                reg const ptr u8[MLKEM_POLYVECBYTES] sk) -> (reg mut ptr u8[MLKEM_INDCPA_MSGBYTES]) {
  stack u16[MLKEM_VECN] bp;
  stack u16[MLKEM_N] v;
  stack u16[MLKEM_VECN] skpv;
  stack u16[MLKEM_N] t;
  stack u16[MLKEM_N] mp;
  
  #[inline]
  bp = __i_polyvec_decompress(bp, ct);
  v =
    _i_poly_decompress(v,
                       ct[MLKEM_POLYVECCOMPRESSEDBYTES : MLKEM_POLYCOMPRESSEDBYTES]);
  #[inline]
  skpv = __i_polyvec_frombytes(sk);
  #[inline]
  bp = __polyvec_ntt(bp);
  #[inline]
  t = __polyvec_pointwise_acc(t, skpv, bp);
  t = _poly_invntt(t);
  mp = _poly_sub(mp, v, t);
  #[inline]
  mp = __poly_reduce(mp);
  (msgp, mp) = _i_poly_tomsg(msgp, mp);
  return (msgp);
}

inline
fn __verify (reg const ptr u8[MLKEM_INDCPA_CIPHERTEXTBYTES] ct,
            reg const ptr u8[MLKEM_INDCPA_CIPHERTEXTBYTES] ctpc) -> (reg u64) {
  reg u64 cnd;
  reg u64 t64;
  reg u256 h;
  inline int i;
  reg u256 f;
  reg u256 g;
  reg bool zf;
  
  cnd = ((64u) 0); /* u64 */
  t64 = ((64u) 1); /* u64 */
  h = #set0_256(); /*  */
  for i = 0 to (MLKEM_INDCPA_CIPHERTEXTBYTES / 32) {
    f = ctpc.[#unaligned :u256 (32 * i)]; /* u256 */
    g = ct.[#unaligned :u256 (32 * i)]; /* u256 */
    f = #VPXOR_256(f, g); /*  */
    h = #VPOR_256(h, f); /*  */
  }
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
    #VPTEST_256(h, h); /*  */
  cnd = ((! zf) ? t64 : cnd); /* u64 */
  return (cnd);
}

inline
fn __cmov (reg mut ptr u8[MLKEM_SYMBYTES] dst,
          reg const ptr u8[MLKEM_SYMBYTES] src, reg u64 cnd) -> (reg mut ptr u8[MLKEM_SYMBYTES]) {
  stack u64 scnd;
  reg u256 m;
  reg u256 f;
  reg u256 g;
  
  cnd = (-64u cnd); /* u64 */
  scnd = cnd; /* u64 */
  m = #VPBROADCAST_4u64(scnd); /*  */
  f = src.[#unaligned :u256 0]; /* u256 */
  g = dst.[#unaligned :u256 0]; /* u256 */
  f = #BLENDV_32u8(f, g, m); /*  */
  dst.[#unaligned :u256 0] = f; /* u256 */
  return (dst);
}

inline
fn __crypto_kem_keypair_jazz (reg mut ptr u8[MLKEM_PUBLICKEYBYTES] pk,
                             reg mut ptr u8[MLKEM_SECRETKEYBYTES] sk,
                             reg const ptr u8[(MLKEM_SYMBYTES * 2)] randomnessp) -> 
(reg mut ptr u8[MLKEM_PUBLICKEYBYTES], reg mut ptr u8[MLKEM_SECRETKEYBYTES]) {
  #[mmx]
  reg mut ptr u8[(MLKEM_SYMBYTES * 2)] s_randomnessp;
  reg mut ptr u8[MLKEM_SYMBYTES] randomnessp1;
  reg mut ptr u8[MLKEM_POLYVECBYTES] skcpa;
  stack mut ptr u8[MLKEM_SECRETKEYBYTES] sk_s;
  inline int i;
  reg u64 t64;
  
  s_randomnessp = randomnessp; /* u8[(MLKEM_SYMBYTES * 2)] */
  randomnessp1 = randomnessp[0 : MLKEM_SYMBYTES]; /* u8[MLKEM_SYMBYTES] */
  skcpa = sk[0 : MLKEM_POLYVECBYTES]; /* u8[MLKEM_POLYVECBYTES] */
  sk_s = sk; /* u8[MLKEM_SECRETKEYBYTES] */
  #[inline]
  (pk, skcpa) = __indcpa_keypair(pk, skcpa, randomnessp1);
  sk = sk_s; /* u8[MLKEM_SECRETKEYBYTES] */
  sk[0 : MLKEM_POLYVECBYTES] = skcpa; /* u8[MLKEM_POLYVECBYTES] */
  for i = 0 to (MLKEM_INDCPA_PUBLICKEYBYTES / 8) {
    t64 = pk[:u64 i]; /* u64 */
    sk.[#unaligned :u64 (((MLKEM_POLYVECBYTES / 8) + i) * 8)] =
      t64; /* u64 */
  }
  sk[(MLKEM_POLYVECBYTES + MLKEM_PUBLICKEYBYTES) : 32] =
    _sha3_256A_A1568(sk[(MLKEM_POLYVECBYTES + MLKEM_PUBLICKEYBYTES) : 32], pk);
  randomnessp = s_randomnessp; /* u8[(MLKEM_SYMBYTES * 2)] */
  for i = 0 to (MLKEM_SYMBYTES / 8) {
    t64 = randomnessp[:u64 ((MLKEM_SYMBYTES / 8) + i)]; /* u64 */
    sk.[#unaligned :u64 (((((MLKEM_POLYVECBYTES + MLKEM_PUBLICKEYBYTES) +
                           MLKEM_SYMBYTES) /
                          8) +
                         i) *
                        8)] = t64; /* u64 */
  }
  return (pk, sk);
}

inline
fn __crypto_kem_enc_jazz (reg mut ptr u8[MLKEM_CIPHERTEXTBYTES] ct,
                         #[spill_to_mmx] reg mut ptr u8[MLKEM_SYMBYTES] shk,
                         #[spill_to_mmx]
                         reg const ptr u8[MLKEM_PUBLICKEYBYTES] pk,
                         reg const ptr u8[MLKEM_SYMBYTES] randomnessp) -> 
(reg mut ptr u8[MLKEM_CIPHERTEXTBYTES], reg mut ptr u8[MLKEM_SYMBYTES]) {
  stack u8[(MLKEM_SYMBYTES * 2)] buf;
  stack u8[(MLKEM_SYMBYTES * 2)] kr;
  
  () = #spill(pk, shk); /* :k */
  buf[0 : MLKEM_SYMBYTES] = #copy_64(randomnessp); /*  */
  buf[MLKEM_SYMBYTES : MLKEM_SYMBYTES] =
    _sha3_256A_A1568(buf[MLKEM_SYMBYTES : MLKEM_SYMBYTES], pk);
  kr = _sha3_512A_A64(kr, buf);
  () = #unspill(pk); /* :k */
  #[inline]
  ct =
    __indcpa_enc(ct, buf[0 : MLKEM_INDCPA_MSGBYTES], pk,
                 kr[MLKEM_SYMBYTES : MLKEM_SYMBYTES]);
  () = #unspill(shk); /* :k */
  shk = #copy_64(kr[0 : MLKEM_SYMBYTES]); /*  */
  return (ct, shk);
}

inline
fn __crypto_kem_dec_jazz (#[spill_to_mmx] reg mut ptr u8[MLKEM_SYMBYTES] shk,
                         #[spill_to_mmx]
                         reg const ptr u8[MLKEM_CIPHERTEXTBYTES] ct,
                         #[spill_to_mmx]
                         reg const ptr u8[MLKEM_SECRETKEYBYTES] sk) -> 
(reg mut ptr u8[MLKEM_SYMBYTES]) {
  stack u8[(MLKEM_SYMBYTES + MLKEM_CIPHERTEXTBYTES)] zp_ct;
  stack u8[(2 * MLKEM_SYMBYTES)] buf;
  stack u8[(2 * MLKEM_SYMBYTES)] kr;
  stack u8[MLKEM_INDCPA_CIPHERTEXTBYTES] ctc;
  reg u64 cnd;
  
  () = #spill(shk, ct); /* :k */
  zp_ct[0 : MLKEM_SYMBYTES] =
    #copy_64(sk[(MLKEM_SECRETKEYBYTES - MLKEM_SYMBYTES) : MLKEM_SYMBYTES]); /*  */
  #[inline]
  buf[0 : MLKEM_INDCPA_MSGBYTES] =
    __indcpa_dec(buf[0 : MLKEM_INDCPA_MSGBYTES], ct,
                 sk[0 : MLKEM_POLYVECBYTES]);
  buf[MLKEM_SYMBYTES : MLKEM_SYMBYTES] =
    #copy_64(sk[(MLKEM_INDCPA_SECRETKEYBYTES + MLKEM_INDCPA_PUBLICKEYBYTES) : MLKEM_SYMBYTES]); /*  */
  kr = _sha3_512A_A64(kr, buf);
  #[inline]
  ctc =
    __indcpa_enc(ctc, buf[0 : MLKEM_SYMBYTES],
                 sk[MLKEM_POLYVECBYTES : MLKEM_PUBLICKEYBYTES],
                 kr[MLKEM_SYMBYTES : MLKEM_SYMBYTES]);
  () = #unspill(ct); /* :k */
  #[inline]
  cnd = __verify(ct, ctc);
  zp_ct[MLKEM_SYMBYTES : MLKEM_CIPHERTEXTBYTES] = #copy_64(ct); /*  */
  () = #unspill(shk); /* :k */
  shk = _shake256_A32__A1600(shk, zp_ct);
  #[inline]
  shk = __cmov(shk, kr[0 : MLKEM_SYMBYTES], cnd);
  return (shk);
}

#[sct="\n{ ptr: transient, val: secret } \195\151\n{ ptr: transient, val: secret } \195\151\n{ ptr: transient, val: secret } \226\134\146\n{ ptr: public, val: secret } \195\151\n{ ptr: public, val: secret } \195\151\npublic\n"]
export
fn jade_kem_mlkem_mlkem1024_amd64_avx2_keypair_derand (#[secret]
                                                      reg mut ptr u8[MLKEM_PUBLICKEYBYTES] public_key,
                                                      #[secret]
                                                      reg mut ptr u8[MLKEM_SECRETKEYBYTES] secret_key,
                                                      #[secret]
                                                      reg const ptr u8[
                                                      (2 * MLKEM_SYMBYTES)] coins) -> 
(#[secret] reg mut ptr u8[MLKEM_PUBLICKEYBYTES], #[secret]
reg mut ptr u8[MLKEM_SECRETKEYBYTES], #[public] reg u64) {
  reg u64 r;
  reg bool _of_;
  reg bool _cf_;
  reg bool _sf_;
  reg bool _zf_;
  
  _ /* u64 */ = #init_msf(); /* :k */
  #[inline]
  (public_key, secret_key) =
    __crypto_kem_keypair_jazz(public_key, secret_key, coins);
  (_of_, _cf_, _sf_, _ /* bool */, _zf_, r) = #set0_64(); /*  */
  return (public_key, secret_key, r);
}

#[sct="\n{ ptr: transient, val: secret } \195\151\n{ ptr: transient, val: secret } \195\151\n{ ptr: transient, val: secret } \195\151\n{ ptr: transient, val: secret } \226\134\146\n{ ptr: public, val: secret } \195\151\n{ ptr: public, val: secret } \195\151\npublic\n"]
export
fn jade_kem_mlkem_mlkem1024_amd64_avx2_enc_derand (#[secret]
                                                  reg mut ptr u8[MLKEM_CIPHERTEXTBYTES] ciphertext,
                                                  #[secret]
                                                  reg mut ptr u8[MLKEM_SYMBYTES] shared_secret,
                                                  #[secret]
                                                  reg const ptr u8[MLKEM_PUBLICKEYBYTES] public_key,
                                                  #[secret]
                                                  reg const ptr u8[MLKEM_SYMBYTES] coins) -> 
(#[secret] reg mut ptr u8[MLKEM_CIPHERTEXTBYTES], #[secret]
reg mut ptr u8[MLKEM_SYMBYTES], #[public] reg u64) {
  reg u64 r;
  reg bool _of_;
  reg bool _cf_;
  reg bool _sf_;
  reg bool _zf_;
  
  _ /* u64 */ = #init_msf(); /* :k */
  public_key = public_key; /* u8[MLKEM_PUBLICKEYBYTES] */
  #[inline]
  (ciphertext, shared_secret) =
    __crypto_kem_enc_jazz(ciphertext, shared_secret, public_key, coins);
  (_of_, _cf_, _sf_, _ /* bool */, _zf_, r) = #set0_64(); /*  */
  return (ciphertext, shared_secret, r);
}

#[sct="\n{ ptr: transient, val: secret } \195\151\n{ ptr: transient, val: secret } \226\134\146\n{ ptr: public, val: secret } \195\151\n{ ptr: public, val: secret } \195\151\npublic\n"]
export
fn jade_kem_mlkem_mlkem1024_amd64_avx2_keypair (#[secret]
                                               reg mut ptr u8[MLKEM_PUBLICKEYBYTES] public_key,
                                               #[secret]
                                               reg mut ptr u8[MLKEM_SECRETKEYBYTES] secret_key) -> 
(#[secret] reg mut ptr u8[MLKEM_PUBLICKEYBYTES], #[secret]
reg mut ptr u8[MLKEM_SECRETKEYBYTES], #[public] reg u64) {
  reg u64 r;
  reg mut ptr u8[(MLKEM_SYMBYTES * 2)] randomnessp;
  stack u8[(MLKEM_SYMBYTES * 2)] randomness;
  reg bool _of_;
  reg bool _cf_;
  reg bool _sf_;
  reg bool _zf_;
  
  public_key = public_key; /* u8[MLKEM_PUBLICKEYBYTES] */
  secret_key = secret_key; /* u8[MLKEM_SECRETKEYBYTES] */
  randomnessp = randomness; /* u8[(MLKEM_SYMBYTES * 2)] */
  randomnessp = #randombytes(randomnessp);
  #[inline]
  (public_key, secret_key) =
    __crypto_kem_keypair_jazz(public_key, secret_key, randomnessp);
  (_of_, _cf_, _sf_, _ /* bool */, _zf_, r) = #set0_64(); /*  */
  return (public_key, secret_key, r);
}

#[sct="\n{ ptr: transient, val: secret } \195\151\n{ ptr: transient, val: secret } \195\151\n{ ptr: transient, val: secret } \226\134\146\n{ ptr: public, val: secret } \195\151\n{ ptr: public, val: secret } \195\151\npublic\n"]
export
fn jade_kem_mlkem_mlkem1024_amd64_avx2_enc (#[secret]
                                           reg mut ptr u8[MLKEM_CIPHERTEXTBYTES] ciphertext,
                                           #[secret]
                                           reg mut ptr u8[MLKEM_SYMBYTES] shared_secret,
                                           #[secret]
                                           reg const ptr u8[MLKEM_PUBLICKEYBYTES] public_key) -> 
(#[secret] reg mut ptr u8[MLKEM_CIPHERTEXTBYTES], #[secret]
reg mut ptr u8[MLKEM_SYMBYTES], #[public] reg u64) {
  reg u64 r;
  reg mut ptr u8[MLKEM_SYMBYTES] randomnessp;
  stack u8[MLKEM_SYMBYTES] randomness;
  reg bool _of_;
  reg bool _cf_;
  reg bool _sf_;
  reg bool _zf_;
  
  ciphertext = ciphertext; /* u8[MLKEM_CIPHERTEXTBYTES] */
  shared_secret = shared_secret; /* u8[MLKEM_SYMBYTES] */
  public_key = public_key; /* u8[MLKEM_PUBLICKEYBYTES] */
  randomnessp = randomness; /* u8[MLKEM_SYMBYTES] */
  randomnessp = #randombytes(randomnessp);
  #[inline]
  (ciphertext, shared_secret) =
    __crypto_kem_enc_jazz(ciphertext, shared_secret, public_key, randomnessp);
  (_of_, _cf_, _sf_, _ /* bool */, _zf_, r) = #set0_64(); /*  */
  return (ciphertext, shared_secret, r);
}

#[sct="\n{ ptr: transient, val: secret } \195\151\n{ ptr: transient, val: secret } \195\151\n{ ptr: transient, val: secret } \226\134\146\n{ ptr: public, val: secret } \195\151\npublic\n"]
export
fn jade_kem_mlkem_mlkem1024_amd64_avx2_dec (#[secret]
                                           reg mut ptr u8[MLKEM_SYMBYTES] shared_secret,
                                           #[secret]
                                           reg const ptr u8[MLKEM_CIPHERTEXTBYTES] ciphertext,
                                           #[secret]
                                           reg const ptr u8[MLKEM_SECRETKEYBYTES] secret_key) -> 
(#[secret] reg mut ptr u8[MLKEM_SYMBYTES], #[public] reg u64) {
  reg u64 r;
  reg bool _of_;
  reg bool _cf_;
  reg bool _sf_;
  reg bool _zf_;
  
  _ /* u64 */ = #init_msf(); /* :k */
  #[inline]
  shared_secret =
    __crypto_kem_dec_jazz(shared_secret, ciphertext, secret_key);
  (_of_, _cf_, _sf_, _ /* bool */, _zf_, r) = #set0_64(); /*  */
  return (shared_secret, r);
}


