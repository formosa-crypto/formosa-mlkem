// DEPENDENCIES
// require "keccak1600_avx2.jinc"
// param int ASIZE = 1002;

require "../common/subreadwrite_array_ASIZE.jinc"

/*
   ONE-SHOT (FIXED-SIZE) ARRAY ABSORB
   ==================================
*/

inline fn __addstate_array_avx2
( reg u256[7] st
, reg const ptr u8[ASIZE] buf
, reg u64 offset
, inline int LEN
, inline int TRAILB
) -> reg u256[7] /* st */
   , reg u64 /* offset */
{
  reg u64 t64;
  reg u256 r0, r1, r2, r3, r4, r5, r6;
  reg u128 t128_0, t128_1;
  inline int DELTA;
  DELTA = 0;

  DELTA, LEN, TRAILB, t64 = __aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  t128_0 = (128u) t64;
  r0 = #VPBROADCAST_4u64(t128_0);
  st[0] ^= r0;

  DELTA, LEN, TRAILB, r1 = __aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] ^= r1;

  if (0 < LEN ) {
    DELTA, LEN, TRAILB, t64 = __aread_subu64(buf,offset, DELTA,  LEN, TRAILB);
    t128_1 = (128u) t64;

    DELTA, LEN, TRAILB, r3 = __aread_subu256(buf, offset, DELTA, LEN, TRAILB);

    DELTA, LEN, TRAILB, t64 = __aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = (128u) t64;

    DELTA, LEN, TRAILB, r4 = __aread_subu256(buf, offset, DELTA, LEN, TRAILB);

    DELTA, LEN, TRAILB, t64 = __aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, 1);

    DELTA, LEN, TRAILB, r5 = __aread_subu256(buf, offset, DELTA, LEN, TRAILB);

    DELTA, LEN, TRAILB, t64 = __aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, 1);
    r2 = (2u128)[t128_1, t128_0];
    st[2] ^= r2;

    DELTA, LEN, TRAILB, r6 = __aread_subu256(buf, offset, DELTA, LEN, TRAILB);

    st = __addstate_r3456_avx2( st, r3, r4, r5, r6);
  }
  offset += DELTA;
  return st, offset;
}

inline fn __absorb_array_avx2
( reg u256[7] st
, reg const ptr u8[ASIZE] buf
, reg u64 offset
, inline int LEN
, inline int RATE8
, inline int TRAILB /* closes state if !=0 (i.e. adds trailbyte and padding) */
) -> reg u256[7] /* st */
   , reg u64 /* offset */
{
  reg u64 i;
  inline int ALL, ITERS;

  ALL = LEN + (TRAILB!=0 ? 1 : 0);

  // continue by processing full blocks
  ITERS = LEN / RATE8; // number of full blocks
  if (0 < ITERS) {
    i = 0;
    while ( i < ITERS ) {
      st, offset = __addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i += 1;
    }
  }
    
  // last incomplete block
  LEN = LEN % RATE8;
  st, offset = __addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB!=0) { st = __addratebit_avx2(st, RATE8); }

  return st, offset;
} 

/*
   INCREMENTAL (FIXED-SIZE) MEMORY ABSORB
   ======================================
*/

inline fn __pstate_array_avx2
( reg mut ptr u64[25] pst
, inline int AT /* bytes (0 <= AT < 200) */
, reg const ptr u8[ASIZE] buf
, reg u64 offset
, inline int LEN
, inline int TRAILB
) -> reg ptr u64[25] /* pst */
   , inline int /* AT */
   , reg u64 /* offset */
{
  inline int DELTA, LO, ALL;
  reg u64 at, t64;
  reg u256 t256;
  reg u128 t128;

  DELTA = 0;
  ALL = AT+LEN; // total bytes to process (excluding trail byte, if !=0)
  LO = AT % 8; // leftover bytes
  at = AT / 8; // current pstate position

  if ( 0 < LO ) { // process first word...
    if ( LO+LEN < 8) { // ...not enough to fill a word (just update it)
      if ( TRAILB != 0 ) { ALL += 1; }
      DELTA, _, TRAILB, t64 = __aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 <<= 8*LO;
      pst[at] ^= t64;
      LO = 0;
      AT = 0;
      LEN = 0;
    } else { // process first word
      if ( 8 <= LEN ) {
        t64 = buf.[:u64 offset + DELTA];
        DELTA += (8-LO);
      } else {
        DELTA, _, _, t64 = __aread_subu64(buf, offset, DELTA, 8-LO, 0);
      }
      LEN -= 8-LO;
      AT += 8-LO;
      t64 <<= 8*LO;
      pst[at] ^= t64;
      at += 1;
    }
  }

  // continue processing remaining bytes
  if (32 <= LEN) {
    offset += DELTA;
    DELTA = 0;
    while ( at < AT/8+4*(LEN/32)) {
      t256 = buf.[:u256 offset];
      offset += 32;
      pst.[:u256 8*at] = t256;
      at += 4;
    }
    LEN = LEN % 32;
  }
  if (16 <= LEN) {
    t128 = buf.[:u128 offset + DELTA];
    DELTA += 16;
    pst.[:u128 8*at] = t128;
    at += 2;
    LEN -= 16;
  }
  if (8 <= LEN) {
    t64 = buf.[:u64 offset + DELTA];
    DELTA += 8;
    pst.[:u64 8*at] = t64;
    at += 1;
    LEN -= 8;
  }

  // process last word (possibly closing the state)
  LO = (AT+LEN) % 8;
  if ( 0 < LO || TRAILB != 0 ) {
    if ( TRAILB != 0 ) { ALL += 1; }
    DELTA, _, TRAILB, t64 = __aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[:u64 (ALL/8)] = t64;
  }
  offset += DELTA;
  return pst, ALL, offset;
} 

inline fn __pabsorb_array_avx2
( reg mut ptr u64[25] pst
, inline int AT
, reg u256[7] st
, reg const ptr u8[ASIZE] buf
, reg u64 offset
, inline int LEN
, inline int RATE8
, inline int TRAILB /* closes state if !=0 (i.e. adds trailbyte and padding) */
) -> reg ptr u64[25] /* pst */
   , inline int /* AT */
   , reg u256[7] /* st */
   , reg u64 /* offset */
{
  reg u64 i;
  inline int ALL, ITERS;

  ALL = AT + LEN;
  if ( (AT+LEN) < RATE8 ) { // not enough to fill a block!
    pst, AT, offset = __pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) { // add pstate and closes the state
      i = AT/8 + 1;
      if (AT <= 5*8) { // only st[0..1] is affected
        while (i < 5) { pst[i] = 0; i += 1; }
        st = __addpst01_avx2(st, pst);
        st = __addratebit_avx2(st, RATE8);
      } else { // all state is affected
        while (i < RATE8/8) { pst[i] = 0; i += 1; }
        pst[:u8 RATE8-1] ^= 0x80;
        st = _addpstate_avx2(st, pst);
      }
    }
  } else { // at least a block is filled
    if ( AT != 0 ) { // start by filling the first block
      pst, _, offset = __pstate_array_avx2(pst, AT, buf, offset, RATE8-AT, 0);
      LEN = LEN - (RATE8-AT);
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0;
    }

    // continue by processing full blocks
    ITERS = LEN / RATE8; // number of full blocks
    i = 0;
    while ( i < ITERS ) {
      st, offset = __addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i += 1;
    }

    // last incomplete block
    LEN = ALL % RATE8;
    if (TRAILB!=0) {
      st, offset = __addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      st = __addratebit_avx2(st, RATE8);
    } else if ( LEN != 0) {
      pst, AT, offset = __pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
/*
      if (TRAILB != 0) { // add pstate and closes the state
        i = AT/8 + 1;
        if (AT <= 5*8) { // only st[0..1] is affected
          while (i < 5) { pst[i] = 0; i += 1; }
          st = __addpst01(st, pst);
          st = __addratebit_avx2(st, RATE8);
        } else { // all state is affected
          while (i < RATE8/8) { pst[i] = 0; i += 1; }
          pst[:u8 RATE8-1] ^= 0x80;
          st = _addpstate_avx2(st, pst);
        }
      }
*/
    }
  }
  return pst, AT, st, offset;
} 

/*
   ONE-SHOT (FIXED-SIZE) MEMORY SQUEEZE
   ====================================
*/

inline fn __dumpstate_array_avx2
( reg mut ptr u8[ASIZE] buf
, reg u64 offset
, inline int LEN
, reg u256[7] st
) -> reg ptr u8[ASIZE] /* buf */
   , reg u64 /* offset */
{
  reg u64 t;
  reg u128 t128_0, t128_1;
  reg u256 t256_0, t256_1, t256_2, t256_3, t256_4;
  inline int DELTA;

  DELTA = 0;

  // reg0
  if (8 <= LEN) {
    buf, DELTA, _ = __awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN -= 8;
  } else {
    buf, DELTA, LEN = __awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }

  // reg1
  buf, DELTA, LEN = __awrite_subu256(buf, offset, DELTA, LEN, st[1]);

  // reg2 (5)
  if (0 <s LEN) {
    t128_0 = (128u) st[2];
    t128_1 = #VEXTRACTI128(st[2], 1);
    t = (64u) t128_1;
    buf, DELTA, LEN = __awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1);

    if (0 <s LEN) { // regs 3,4,5,6
      // [ 16, 7, 8, 19 ]
      t256_0 = #VPBLEND_8u32(st[3], st[4], (8u1)[1,1,1,1,0,0,0,0]);
      // [ 11, 22, 23, 14 ]
      t256_1 = #VPBLEND_8u32(st[4], st[3], (8u1)[1,1,1,1,0,0,0,0]);
      // [ 21, 17, 18, 24 ]
      t256_2 = #VPBLEND_8u32(st[5], st[6], (8u1)[1,1,1,1,0,0,0,0]);
      // [ 6, 12, 13, 9 ]
      t256_3 = #VPBLEND_8u32(st[6], st[5], (8u1)[1,1,1,1,0,0,0,0]);

      // [ 6, 7, 8, 9 ]
      t256_4 = #VPBLEND_8u32(t256_0, t256_3, (8u1)[1,1,0,0,0,0,1,1]);
      buf, DELTA, LEN = __awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      
      // reg2 (10)
      if (0 <s LEN) {
	t = (64u) t128_0;
	buf, DELTA, LEN = __awrite_subu64(buf, offset, DELTA, LEN, t);
	t128_0 = #VPUNPCKH_2u64(t128_0, t128_0);
      }

      if (0 <s LEN) { // [ 11, 12, 13, 14 ]
	t256_4 = #VPBLEND_8u32(t256_3, t256_1, (8u1)[1,1,0,0,0,0,1,1]);
	buf, DELTA, LEN = __awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }

      // reg2 (15)
      if (0 <s LEN) {
	t = (64u) t128_1;
	buf, DELTA, LEN = __awrite_subu64(buf, offset, DELTA, LEN, t);
      }

      if (0 <s LEN) { // [ 16, 17, 18, 19 ]
	t256_4 = #VPBLEND_8u32(t256_2, t256_0, (8u1)[1,1,0,0,0,0,1,1]);
	buf, DELTA, LEN = __awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }

      // reg2 (20)
      if (0 <s LEN) {
	t = (64u) t128_0;
	buf, DELTA, LEN = __awrite_subu64(buf, offset, DELTA, LEN, t);
      }

      if (0 <s LEN) { // [ 21, 22, 23, 24 ]
	t256_4 = #VPBLEND_8u32(t256_1, t256_2, (8u1)[1,1,0,0,0,0,1,1]);
	buf, DELTA, LEN = __awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset += DELTA;
  return buf, offset;
}

inline fn __squeeze_array_avx2
( reg mut ptr u8[ASIZE] buf
, reg u64 offset
, inline int LEN
, reg u256[7] st
, inline int RATE8
) -> reg ptr u8[ASIZE] /* buf */
   , reg u256[7] /* st */
{
  reg u64 i;
  inline int ITERS, LO;
  ITERS = LEN/RATE8;
  LO = LEN%RATE8;
  if (0 <s LEN) {
    if (0 < ITERS) {
      i = 0;
      while (i < ITERS) {
        st = _keccakf1600_avx2(st);
        buf, offset = __dumpstate_array_avx2(buf, offset, RATE8, st);
        i += 1;
      }
    }
    if (0 < LO) {
        st = _keccakf1600_avx2(st);
        buf, offset = __dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return buf, st;
}

