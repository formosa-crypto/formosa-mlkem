require "keccakf1600_avx2.jinc"
require "../common/keccak1600_generic.jinc"
require "../common/subreadwrite_imem.jinc"


inline fn __u64_to_u256
( reg u64 x
, inline int L
) -> reg u256
{
  reg u256 t256;
  reg u128 t128;

  if (L % 2 == 0) {
    t128 = (128u) x;
  } else {
    t128 = #set0_128();
    t128 = #VPINSR_2u64(t128, x, 1);
  }
  t256 = #set0_256();
  if (L / 2 == 0) {
    t256 = #VINSERTI128(t256, t128, 0);
  } else {
    t256 = #VINSERTI128(t256, t128, 1);
  }

  return t256;
}

/*
   STATE INIT
   ==========
*/
inline fn __state_init_avx2() -> reg u256[7]
{
  inline int i;
  reg u256[7] st;

  for i=0 to 7 { st[i] = #set0_256(); }

  return st;
}

/*
   PSTATE - UNPERMUTED KECCAK STATE
   ================================
*/
inline fn __pstate_init_avx2
( reg mut ptr u64[25] pst
) -> reg ptr u64[25], reg u256[7]
{
  inline int i;
  reg u64 z64;
  reg u256 z256;
  reg u256[7] st;

  z256 = #set0_256();
  for i=0 to 25/4 { pst[:u256 i] = z256; }
  z64 = 0;
  pst[24] = z64;

  st = __state_init_avx2();

  return pst, st;
}

inline fn __perm_reg3456_avx2
( reg u256 r3 r4 r5 r6
) -> reg u256 /* st[3] */
   , reg u256 /* st[4] */
   , reg u256 /* st[5] */
   , reg u256 /* st[6] */
{
  reg u256 t256_0, t256_1, t256_2;
  reg u256 st3, st4, st5, st6;
  // [ 16 7 8 19 ]
  t256_0 = #VPBLEND_8u32(r3, r5, (8u1)[1,1,0,0,0,0,1,1]);
  // [ 11 22 23 14 ]
  t256_1 = #VPBLEND_8u32(r6, r4, (8u1)[1,1,0,0,0,0,1,1]);
  // [ 6 12 13 9 ]
  t256_2 = #VPBLEND_8u32(r4, r3, (8u1)[1,1,0,0,0,0,1,1]);
  // [ 16 7 23 14 ]
  st3 = #VPBLEND_8u32(t256_0, t256_1, (8u1)[1,1,1,1,0,0,0,0]);
  // [ 11 22 8 19 ]
  st4 = #VPBLEND_8u32(t256_1, t256_0, (8u1)[1,1,1,1,0,0,0,0]);
  // [ 21 17 18 24 ]
  t256_0 = #VPBLEND_8u32(r5, r6, (8u1)[1,1,0,0,0,0,1,1]);
  // [ 21 17 13 9 ]
  st5 = #VPBLEND_8u32(t256_0, t256_2, (8u1)[1,1,1,1,0,0,0,0]);
  // [ 6 12 18 24 ]
  st6 = #VPBLEND_8u32(t256_2, t256_0, (8u1)[1,1,1,1,0,0,0,0]);

  return st3, st4, st5, st6;
}

inline fn __unperm_reg3456_avx2
( reg u256 st3 st4 st5 st6
) -> reg u256 /* r3 */
   , reg u256 /* r4 */
   , reg u256 /* r5 */
   , reg u256 /* r6 */
{
  reg u256 t256_0, t256_1, t256_2, t256_3;
  reg u256 r3, r4, r5, r6;
  // [ 16, 7, 8, 19 ]
  t256_0 = #VPBLEND_8u32(st3, st4, (8u1)[1,1,1,1,0,0,0,0]);
  // [ 11, 22, 23, 14 ]
  t256_1 = #VPBLEND_8u32(st4, st3, (8u1)[1,1,1,1,0,0,0,0]);
  // [ 21, 17, 18, 24 ]
  t256_2 = #VPBLEND_8u32(st5, st6, (8u1)[1,1,1,1,0,0,0,0]);
  // [ 6, 12, 13, 9 ]
  t256_3 = #VPBLEND_8u32(st6, st5, (8u1)[1,1,1,1,0,0,0,0]);
  // [ 6, 7, 8, 9 ]
  r3 = #VPBLEND_8u32(t256_0, t256_3, (8u1)[1,1,0,0,0,0,1,1]);
  // [ 11, 12, 13, 14 ]
  r4 = #VPBLEND_8u32(t256_3, t256_1, (8u1)[1,1,0,0,0,0,1,1]);
  // [ 16, 17, 18, 19 ]
  r5 = #VPBLEND_8u32(t256_2, t256_0, (8u1)[1,1,0,0,0,0,1,1]);
  // [ 21, 22, 23, 24 ]
  r6 = #VPBLEND_8u32(t256_1, t256_2, (8u1)[1,1,0,0,0,0,1,1]);

  return r3, r4, r5, r6;
}

/*
   STATE READ
   ==========
*/
inline fn __state_from_pstate_avx2
( reg const ptr u64[25] pst
) -> reg u256[7]
{
  reg u256[7] st;
  reg u128 t128_0, t128_1;
  reg u64 t;

  st[0] = #VPBROADCAST_4u64(pst.[:u64 0]);
  st[1] = pst.[:u256 8];

  // [ 5 - ]
  t128_0 = #VMOV(pst.[:u64 5*8]);
  // [ 6 7 8 9 ]
  st[3] = pst.[:u256 6*8];
  // [ 10 - ]
  t128_1 = #VMOV(pst.[:u64 10*8]);
  // [ 11 12 13 14 ]
  st[4] = pst.[:u256 11*8];
  // [ 5 15 ]
  t = pst.[:u64 15*8];
  t128_0 = #VPINSR_2u64(t128_0, t, 1);
  // [ 16 17 18 19 ]
  st[5] = pst.[:u256 16*8];
  // [ 10 20 ]
  t = pst.[:u64 20*8];
  t128_1 = #VPINSR_2u64(t128_1, t, 1);
  // [ 10 20 5 15 ]
  st[2] = (2u128)[t128_0, t128_1];
  // [ 21 22 23 24 ]
  st[6] = pst.[:u256 21*8];
  st[3], st[4], st[5], st[6] = __perm_reg3456_avx2(st[3], st[4], st[5], st[6]);

  return st;
}


inline fn __addstate_r3456_avx2
( reg u256[7] st
, reg u256 r3 r4 r5 r6
) -> reg u256[7]
{
  r3, r4, r5, r6 = __perm_reg3456_avx2(r3, r4, r5, r6);
  st[3] ^= r3;
  st[4] ^= r4;
  st[5] ^= r5;
  st[6] ^= r6;
/*
  reg u256 t256_0, t256_1, t256_2;
  // [ 16 7 8 19 ]
  t256_0 = #VPBLEND_8u32(r3, r5, (8u1)[1,1,0,0,0,0,1,1]);
  // [ 11 22 23 14 ]
  t256_1 = #VPBLEND_8u32(r6, r4, (8u1)[1,1,0,0,0,0,1,1]);
  // [ 6 12 13 9 ]
  t256_2 = #VPBLEND_8u32(r4, r3, (8u1)[1,1,0,0,0,0,1,1]);
  // [ 16 7 23 14 ]
  r3 = #VPBLEND_8u32(t256_0, t256_1, (8u1)[1,1,1,1,0,0,0,0]);
  st[3] ^= r3;
  // [ 11 22 8 19 ]
  r4 = #VPBLEND_8u32(t256_1, t256_0, (8u1)[1,1,1,1,0,0,0,0]);
  st[4] ^= r4;
  // [ 21 17 18 24 ]
  t256_0 = #VPBLEND_8u32(r5, r6, (8u1)[1,1,0,0,0,0,1,1]);
  // [ 21 17 13 9 ]
  r5 = #VPBLEND_8u32(t256_0, t256_2, (8u1)[1,1,1,1,0,0,0,0]);
  st[5] ^= r5;
  // [ 6 12 18 24 ]
  r6 = #VPBLEND_8u32(t256_2, t256_0, (8u1)[1,1,1,1,0,0,0,0]);
  st[6] ^= r6;
*/
  return st;
}

inline fn __addpst01_avx2
( reg u256[7] st
, reg const ptr u64[25] pst
) -> reg u256[7]
{
  reg u256 t256;
  t256 = #VPBROADCAST_4u64(pst.[:u64 0]);
  st[0] ^= t256;
  t256 = pst.[:u256 8*1];
  st[1] ^= t256;
  return st;
}

inline fn __addpst23456_avx2  // remaining entries
( reg u256[7] st
, reg const ptr u64[25] pst
) -> reg u256[7]
{
  reg u256 r2, r3, r4, r5, r6;
  reg u128 t128_0, t128_1;
  reg u64 t;

  // [ 5 - ]
  t128_0 = #VMOV(pst.[:u64 5*8]);
  // [ 6 7 8 9 ]
  r3 = pst.[:u256 6*8];
  // [ 10 - ]
  t128_1 = #VMOV(pst.[:u64 10*8]);
  // [ 11 12 13 14 ]
  r4 = pst.[:u256 11*8];
  // [ 5 15 ]
  t = pst.[:u64 15*8];
  t128_0 = #VPINSR_2u64(t128_0, t, 1);
  // [ 16 17 18 19 ]
  r5 = pst.[:u256 16*8];
  // [ 10 20 ]
  t = pst.[:u64 20*8];
  t128_1 = #VPINSR_2u64(t128_1, t, 1);
  // [ 10 20 5 15 ]
  r2 = (2u128)[t128_0, t128_1];
  st[2] ^= r2;
  // [ 21 22 23 24 ]
  r6 = pst.[:u256 21*8];

  st = __addstate_r3456_avx2(st, r3, r4, r5, r6);

  return st;
}

fn _addpstate_avx2
( reg u256[7] st
, reg const ptr u64[25] pst
) -> reg u256[7]
{
  st = __addpst01_avx2(st, pst);
  st = __addpst23456_avx2(st, pst);
  return st;
}

/*
   ADD RATE BIT
   ============
*/

inline fn __stavx2_pos_avx2(inline int POS) -> inline int, inline int {
  inline int R, L;
  //0: [ 0  0  0  0  ]
  R = 0; L = 0;
  if (0 < POS) {
  //1: [ 1  2  3  4  ]
    if (POS <= 4) { R = 1; L = POS-1; }
  //2: [ 10 20 5  15 ]
    else if (POS == 10) { R = 2; L = 0; }
    else if (POS == 20) { R = 2; L = 1; }
    else if (POS == 5 ) { R = 2; L = 2; }
    else if (POS == 15) { R = 2; L = 3; }
  //3: [ 16 7  23 14 ]
    else if (POS == 16) { R = 3; L = 0; }
    else if (POS == 7 ) { R = 3; L = 1; }
    else if (POS == 23) { R = 3; L = 2; }
    else if (POS == 14) { R = 3; L = 3; }
  //4: [ 11 22 8  19 ]
    else if (POS == 11) { R = 4; L = 0; }
    else if (POS == 22) { R = 4; L = 1; }
    else if (POS == 8 ) { R = 4; L = 2; }
    else if (POS == 19) { R = 4; L = 3; }
  //5: [ 21 17 13 9  ]
    else if (POS == 21) { R = 5; L = 0; }
    else if (POS == 17) { R = 5; L = 1; }
    else if (POS == 13) { R = 5; L = 2; }
    else if (POS == 9 ) { R = 5; L = 3; }
  //6: [ 6  12 18 24 ]  
    else if (POS == 6 ) { R = 6; L = 0; }
    else if (POS == 12) { R = 6; L = 1; }
    else if (POS == 18) { R = 6; L = 2; }
    else if (POS == 24) { R = 6; L = 3; }
  }
  return R,L;
}

inline fn __addratebit_avx2
( reg u256[7] st
, inline int RATE8
) -> reg u256[7]
{
  inline int R, L;
  reg u256 t256;

  reg u64 t64;
  t64 = 1;
  t64 <<= (8*RATE8-1) % 64; // obs: should be 63 for all admissible rates!
  R,L = __stavx2_pos_avx2((RATE8-1)/8);
  if (R==0) {
    t256 = #VPBROADCAST_4u64(t64);
  } else {
    t256 = __u64_to_u256(t64, L);
  }
  st[R] ^= t256;
  return st;
}

