param int KYBER_K = 3;
param int KYBER_Q = 3329;
param int KYBER_N = 256;
param int KYBER_VECN = KYBER_K * KYBER_N;

param int KYBER_SYMBYTES = 32;
param int KYBER_SSBYTES = 32;

param int KYBER_ETA1 = 2;
param int KYBER_ETA2 = 2;

param int KYBER_POLYBYTES = 384;
param int KYBER_POLYVECBYTES = (KYBER_K * KYBER_POLYBYTES);

param int KYBER_POLYCOMPRESSEDBYTES = 128;
param int KYBER_POLYVECCOMPRESSEDBYTES = (KYBER_K * 320);

param int KYBER_INDCPA_MSGBYTES = KYBER_SYMBYTES;
param int KYBER_INDCPA_PUBLICKEYBYTES = KYBER_POLYVECBYTES + KYBER_SYMBYTES;
param int KYBER_INDCPA_SECRETKEYBYTES = KYBER_POLYVECBYTES;
param int KYBER_INDCPA_BYTES = KYBER_POLYVECCOMPRESSEDBYTES + KYBER_POLYCOMPRESSEDBYTES;

param int KYBER_PUBLICKEYBYTES = KYBER_INDCPA_PUBLICKEYBYTES;
param int KYBER_SECRETKEYBYTES = KYBER_INDCPA_SECRETKEYBYTES + KYBER_INDCPA_PUBLICKEYBYTES + 2*KYBER_SYMBYTES;
param int KYBER_CIPHERTEXTBYTES = KYBER_INDCPA_BYTES;

param int KECCAK_ROUNDS=24;

param int KECCAK_ROUNDS=24;


u256[24] KECCAK_IOTAS =
{  (4u64)[0x0000000000000001, 0x0000000000000001, 0x0000000000000001, 0x0000000000000001]
  ,(4u64)[0x0000000000008082, 0x0000000000008082, 0x0000000000008082, 0x0000000000008082]
  ,(4u64)[0x800000000000808a, 0x800000000000808a, 0x800000000000808a, 0x800000000000808a]
  ,(4u64)[0x8000000080008000, 0x8000000080008000, 0x8000000080008000, 0x8000000080008000]
  ,(4u64)[0x000000000000808b, 0x000000000000808b, 0x000000000000808b, 0x000000000000808b]
  ,(4u64)[0x0000000080000001, 0x0000000080000001, 0x0000000080000001, 0x0000000080000001]
  ,(4u64)[0x8000000080008081, 0x8000000080008081, 0x8000000080008081, 0x8000000080008081]
  ,(4u64)[0x8000000000008009, 0x8000000000008009, 0x8000000000008009, 0x8000000000008009]
  ,(4u64)[0x000000000000008a, 0x000000000000008a, 0x000000000000008a, 0x000000000000008a]
  ,(4u64)[0x0000000000000088, 0x0000000000000088, 0x0000000000000088, 0x0000000000000088]
  ,(4u64)[0x0000000080008009, 0x0000000080008009, 0x0000000080008009, 0x0000000080008009]
  ,(4u64)[0x000000008000000a, 0x000000008000000a, 0x000000008000000a, 0x000000008000000a]
  ,(4u64)[0x000000008000808b, 0x000000008000808b, 0x000000008000808b, 0x000000008000808b]
  ,(4u64)[0x800000000000008b, 0x800000000000008b, 0x800000000000008b, 0x800000000000008b]
  ,(4u64)[0x8000000000008089, 0x8000000000008089, 0x8000000000008089, 0x8000000000008089]
  ,(4u64)[0x8000000000008003, 0x8000000000008003, 0x8000000000008003, 0x8000000000008003]
  ,(4u64)[0x8000000000008002, 0x8000000000008002, 0x8000000000008002, 0x8000000000008002]
  ,(4u64)[0x8000000000000080, 0x8000000000000080, 0x8000000000000080, 0x8000000000000080]
  ,(4u64)[0x000000000000800a, 0x000000000000800a, 0x000000000000800a, 0x000000000000800a]
  ,(4u64)[0x800000008000000a, 0x800000008000000a, 0x800000008000000a, 0x800000008000000a]
  ,(4u64)[0x8000000080008081, 0x8000000080008081, 0x8000000080008081, 0x8000000080008081]
  ,(4u64)[0x8000000000008080, 0x8000000000008080, 0x8000000000008080, 0x8000000000008080]
  ,(4u64)[0x0000000080000001, 0x0000000080000001, 0x0000000080000001, 0x0000000080000001]
  ,(4u64)[0x8000000080008008, 0x8000000080008008, 0x8000000080008008, 0x8000000080008008]
};


u256[6] KECCAK_RHOTATES_LEFT = 
{
  (4u64)[41, 36, 18,  3],
  (4u64)[27, 28, 62,  1],
  (4u64)[39, 56,  6, 45],
  (4u64)[ 8, 55, 61, 10],
  (4u64)[20, 25, 15,  2],
  (4u64)[14, 21, 43, 44]
};


u256[6] KECCAK_RHOTATES_RIGHT =
{
  (4u64)[64-41, 64-36, 64-18, 64- 3],
  (4u64)[64-27, 64-28, 64-62, 64- 1],
  (4u64)[64-39, 64-56, 64- 6, 64-45],
  (4u64)[64- 8, 64-55, 64-61, 64-10],
  (4u64)[64-20, 64-25, 64-15, 64- 2],
  (4u64)[64-14, 64-21, 64-43, 64-44]
};


u64[25] KECCAK_A_JAGGED = 
{
   0,  4,  5,  6,  7,
  10, 24, 13, 18, 23,
   8, 16, 25, 22, 15,
  11, 12, 21, 26, 19,
   9, 20, 17, 14, 27
};


inline fn __keccakf1600_avx2(reg u256[7] state) -> reg u256[7]
{
  reg u256[9] t;
  reg u256 c00 c14 d00 d14;

  reg bool zf;
  reg u64 r iotas_o;

  reg ptr u256[24] iotas_p;
  reg ptr u256[6] rhotates_left_p;
  reg ptr u256[6] rhotates_right_p;

  iotas_p = KECCAK_IOTAS;
  iotas_o = 0;
  rhotates_left_p = KECCAK_RHOTATES_LEFT;
  rhotates_right_p = KECCAK_RHOTATES_RIGHT;

  r = KECCAK_ROUNDS;
  while
  {
	  //######################################## Theta
	  c00 = #VPSHUFD_256(state[2], (4u2)[1,0,3,2]);
	  c14 = state[5] ^ state[3];
	  t[2] = state[4] ^ state[6];
	  c14 = c14 ^ state[1];
	  c14 = c14 ^ t[2];
	  t[4] = #VPERMQ(c14, (4u2)[2,1,0,3]);
	  c00 = c00 ^ state[2];
	  t[0] = #VPERMQ(c00, (4u2)[1,0,3,2]);
	  t[1] = c14 >>4u64 63;
	  t[2] = c14 +4u64 c14;
	  t[1] = t[1] | t[2];
	  d14 = #VPERMQ(t[1], (4u2)[0,3,2,1]);
	  d00 = t[1] ^ t[4];
	  d00 = #VPERMQ(d00, (4u2)[0,0,0,0]);
	  c00 = c00 ^ state[0];
	  c00 = c00 ^ t[0];
	  t[0] = c00 >>4u64 63;
	  t[1] = c00 +4u64 c00;
	  t[1] = t[1] | t[0];
	  state[2] = state[2] ^ d00;
	  state[0] = state[0] ^ d00;
	  d14 = #VPBLEND_8u32(d14, t[1], (8u1)[1,1,0,0,0,0,0,0]);
	  t[4] = #VPBLEND_8u32(t[4], c00, (8u1)[0,0,0,0,0,0,1,1]);
	  d14 = d14 ^ t[4];

	  //######################################## Rho + Pi + pre-Chi shuffle
    t[3] = #VPSLLV_4u64(state[2], rhotates_left_p[0] );
	  state[2] = #VPSRLV_4u64(state[2], rhotates_right_p[0] );
	  state[2] = state[2] | t[3];
	  state[3] = state[3] ^ d14;
	  t[4] = #VPSLLV_4u64(state[3], rhotates_left_p[2] );
	  state[3] = #VPSRLV_4u64(state[3], rhotates_right_p[2] );
	  state[3] = state[3] | t[4];
	  state[4] = state[4] ^ d14;
	  t[5] = #VPSLLV_4u64(state[4], rhotates_left_p[3] );
	  state[4] = #VPSRLV_4u64(state[4], rhotates_right_p[3] );
	  state[4] = state[4] | t[5];
	  state[5] = state[5] ^ d14;
	  t[6] = #VPSLLV_4u64(state[5], rhotates_left_p[4] );
	  state[5] = #VPSRLV_4u64(state[5], rhotates_right_p[4] );
	  state[5] = state[5] | t[6];
	  state[6] = state[6] ^ d14;
	  t[3] = #VPERMQ(state[2], (4u2)[2,0,3,1]);
	  t[4] = #VPERMQ(state[3], (4u2)[2,0,3,1]);
	  t[7] = #VPSLLV_4u64(state[6], rhotates_left_p[5] );
	  t[1] = #VPSRLV_4u64(state[6], rhotates_right_p[5] );
	  t[1] = t[1] | t[7];
	  state[1] = state[1] ^ d14;
	  t[5] = #VPERMQ(state[4], (4u2)[0,1,2,3]);
	  t[6] = #VPERMQ(state[5], (4u2)[1,3,0,2]);
	  t[8] = #VPSLLV_4u64(state[1], rhotates_left_p[1] );
	  t[2] = #VPSRLV_4u64(state[1], rhotates_right_p[1] );
	  t[2] = t[2] | t[8];

	  //######################################## Chi
	  t[7] = #VPSRLDQ_256(t[1], 8);
	  t[0] = !t[1] & t[7];
	  state[3] = #VPBLEND_8u32(t[2], t[6], (8u1)[0,0,0,0,1,1,0,0]);
	  t[8] = #VPBLEND_8u32(t[4], t[2], (8u1)[0,0,0,0,1,1,0,0]);
	  state[5] = #VPBLEND_8u32(t[3], t[4], (8u1)[0,0,0,0,1,1,0,0]);
	  t[7] = #VPBLEND_8u32(t[2], t[3], (8u1)[0,0,0,0,1,1,0,0]);
	  state[3] = #VPBLEND_8u32(state[3], t[4], (8u1)[0,0,1,1,0,0,0,0]);
	  t[8] = #VPBLEND_8u32(t[8], t[5], (8u1)[0,0,1,1,0,0,0,0]);
	  state[5] = #VPBLEND_8u32(state[5], t[2], (8u1)[0,0,1,1,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[6], (8u1)[0,0,1,1,0,0,0,0]);
	  state[3] = #VPBLEND_8u32(state[3], t[5], (8u1)[1,1,0,0,0,0,0,0]);
	  t[8] = #VPBLEND_8u32(t[8], t[6], (8u1)[1,1,0,0,0,0,0,0]);
	  state[5] = #VPBLEND_8u32(state[5], t[6], (8u1)[1,1,0,0,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[4], (8u1)[1,1,0,0,0,0,0,0]);
	  state[3] = !state[3] & t[8];
	  state[5] = !state[5] & t[7];
	  state[6] = #VPBLEND_8u32(t[5], t[2], (8u1)[0,0,0,0,1,1,0,0]);
	  t[8] = #VPBLEND_8u32(t[3], t[5], (8u1)[0,0,0,0,1,1,0,0]);
	  state[3] = state[3] ^ t[3];
	  state[6] = #VPBLEND_8u32(state[6], t[3], (8u1)[0,0,1,1,0,0,0,0]);
	  t[8] = #VPBLEND_8u32(t[8], t[4], (8u1)[0,0,1,1,0,0,0,0]);
	  state[5] = state[5] ^ t[5];
	  state[6] = #VPBLEND_8u32(state[6], t[4], (8u1)[1,1,0,0,0,0,0,0]);
	  t[8] = #VPBLEND_8u32(t[8], t[2], (8u1)[1,1,0,0,0,0,0,0]);
	  state[6] = !state[6] & t[8];
	  state[6] = state[6] ^ t[6];
	  state[4] = #VPERMQ(t[1], (4u2)[0,1,3,2]);
	  t[8] = #VPBLEND_8u32(state[4], state[0], (8u1)[0,0,1,1,0,0,0,0]);
	  state[1] = #VPERMQ(t[1], (4u2)[0,3,2,1]);
	  state[1] = #VPBLEND_8u32(state[1], state[0], (8u1)[1,1,0,0,0,0,0,0]);
	  state[1] = !state[1] & t[8];
	  state[2] = #VPBLEND_8u32(t[4], t[5], (8u1)[0,0,0,0,1,1,0,0]);
	  t[7] = #VPBLEND_8u32(t[6], t[4], (8u1)[0,0,0,0,1,1,0,0]);
	  state[2] = #VPBLEND_8u32(state[2], t[6], (8u1)[0,0,1,1,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[3], (8u1)[0,0,1,1,0,0,0,0]);
	  state[2] = #VPBLEND_8u32(state[2], t[3], (8u1)[1,1,0,0,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[5], (8u1)[1,1,0,0,0,0,0,0]);
	  state[2] = !state[2] & t[7];
	  state[2] = state[2] ^ t[2];
	  t[0] = #VPERMQ(t[0], (4u2)[0,0,0,0]);
	  state[3] = #VPERMQ(state[3], (4u2)[0,1,2,3]);
	  state[5] = #VPERMQ(state[5], (4u2)[2,0,3,1]);
	  state[6] = #VPERMQ(state[6], (4u2)[1,3,0,2]);
	  state[4] = #VPBLEND_8u32(t[6], t[3], (8u1)[0,0,0,0,1,1,0,0]);
	  t[7] = #VPBLEND_8u32(t[5], t[6], (8u1)[0,0,0,0,1,1,0,0]);
	  state[4] = #VPBLEND_8u32(state[4], t[5], (8u1)[0,0,1,1,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[2], (8u1)[0,0,1,1,0,0,0,0]);
	  state[4] = #VPBLEND_8u32(state[4], t[2], (8u1)[1,1,0,0,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[3], (8u1)[1,1,0,0,0,0,0,0]);
	  state[4] = !state[4] & t[7];
	  state[0] = state[0] ^ t[0];
	  state[1] = state[1] ^ t[1];
	  state[4] = state[4] ^ t[4];

	  //######################################## Iota
	  state[0] = state[0] ^ iotas_p.[(int) iotas_o];
    iotas_o += 32;

    _,_,_,zf,r = #DEC_64(r);
  }(!zf)

  return state;
}



inline fn __keccak_init_avx2() -> reg u256[7]
{
  inline int i;
  reg u256[7] state;

  for i=0 to 7
  { state[i] = #set0_256(); }

  return state;
}


inline fn __init_s_state_avx2() -> stack u64[28]
{
  inline int i;
  stack u64[28] s_state;
  reg u256 zero;

  zero = #set0_256();
  for i=0 to 7
  { s_state[u256 i] = zero; }

  return s_state;
}


inline fn __add_full_block_avx2(
  reg u256[7] state,
  stack u64[28] s_state,
  reg ptr u64[25] a_jagged_p,
  reg u64 in inlen,
  reg u64 rate
) -> reg u256[7], stack u64[28], reg u64, reg u64
{

  inline int i;
  reg u64 j l t rate8;
  reg u8 c;

  rate8 = rate;
  rate8 >>= 3;
  j = 0;
  while ( j < rate8 )
  {
    t = [in + 8*j];
    l = a_jagged_p[(int) j];
    s_state[(int) l] = t;
    j += 1;
  }

  //TODO: check & change to #VPBROADCAST_4u64
  t = s_state[0];
  s_state[1] = t;
  s_state[2] = t;
  s_state[3] = t;

  for i = 0 to 7
  { state[i] ^= s_state[u256 i]; }

  in += rate;
  inlen -= rate;

  return state, s_state, in, inlen;
}


// TODO: refactor when this feature is available: https://github.com/haslab/libjbn/wiki/Feature-request-%231#procedural-parameters
inline fn __add_final_block_avx2(
  reg  u256[7] state,
  stack u64[28] s_state,
  reg ptr u64[25] a_jagged_p,
  reg   u64 in inlen,
  reg   u8  trail_byte,
  reg   u64 rate
) -> reg u256[7]
{
  inline int i;
  reg u64 j l t inlen8;
  reg u8 c;

  s_state = __init_s_state_avx2();

  inlen8 = inlen;
  inlen8 >>= 3;
  j = 0;
  while ( j < inlen8 )
  {
    t = [in + 8*j];
    l = a_jagged_p[(int) j];
    s_state[(int) l] = t;
    j += 1;
  }
  l = a_jagged_p[(int) j];
  l <<= 3;
  j <<= 3;

  while ( j < inlen )
  {
    c = (u8)[in + j];
    s_state[u8 (int) l] = c;
    j += 1;
    l += 1;
  }

  s_state[u8 (int) l] = trail_byte;

  // j  = (rate-1) >> 3;
  j = rate; j -= 1; j >>= 3;
  l  = a_jagged_p[(int) j];
  l <<= 3;
  // l += ((rate-1) & 0x7)
  j = rate; j -= 1; j &= 0x7;
  l += j;

  s_state[u8 (int) l] ^= 0x80;

  t = s_state[0];
  s_state[1] = t;
  s_state[2] = t;
  s_state[3] = t;

  for i = 0 to 7
  { state[i] ^= s_state[u256 i]; }

  return state;
}


// obs: @pre: len <= rate_in_bytes
inline fn __xtr_full_block_avx2(
  reg u256[7] state,
  reg ptr u64[25] a_jagged_p,
  reg u64 out,
  reg u64 len
) -> reg u64
{
  inline int i;
  stack u64[28] s_state;
  reg u64 j l t len8;
  reg u8 c;

  for i = 0 to 7
  { s_state[u256 i] = state[i]; }

  len8 = len;
  len8 >>= 3;
  j = 0;
  while ( j < len8 )
  {
    l = a_jagged_p[(int) j];
    t = s_state[(int) l];
    [out + 8*j] = t;
    j += 1;
  }

  out += len;

  return out;
}


// obs: @pre: len <= rate_in_bytes
inline fn __xtr_bytes_avx2(
  reg u256[7] state,
  reg ptr u64[25] a_jagged_p,
  reg u64 out,
  reg u64 len
) -> reg u64
{
  inline int i;
  stack u64[28] s_state;
  reg u64 j l t len8;
  reg u8 c;

  for i = 0 to 7
  { s_state[u256 i] = state[i]; }

  len8 = len;
  len8 >>= 3;
  j = 0;
  while ( j < len8 )
  { l = a_jagged_p[(int) j];
    t = s_state[(int) l];
    [out + 8*j] = t;
    j += 1;
  }
  l = a_jagged_p[(int)j];
  j <<= 3;
  l <<= 3;

  while ( j < len )
  {
    c = s_state[u8 (int) l];
    (u8)[out + j] = c;
    j += 1;
    l += 1;
  }

  out += len;

  return out;
}


inline fn __absorb_avx2(
  reg u256[7] state,
  reg u64 in inlen,
  reg u8  trail_byte,
  reg u64 rate
) -> reg u256[7]
{
  stack u64[28] s_state;
  reg ptr u64[25] a_jagged_p;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  // intermediate blocks
  while ( inlen >= rate )
  {
    state, s_state, in, inlen = __add_full_block_avx2(state, s_state, a_jagged_p, in, inlen, rate);
    state = __keccakf1600_avx2(state);
  }

  // final block
  state = __add_final_block_avx2(state, s_state, a_jagged_p, in, inlen, trail_byte, rate);

  return state;
}


inline fn __squeeze_avx2(reg u256[7] state, reg u64 out outlen rate)
{
  reg ptr u64[25] a_jagged_p;

  a_jagged_p = KECCAK_A_JAGGED;

  // intermediate blocks
  while ( outlen > rate )
  {
    state = __keccakf1600_avx2(state);
    out = __xtr_full_block_avx2(state, a_jagged_p, out, rate);
    outlen -= rate;
  }

  state = __keccakf1600_avx2(state);
  out = __xtr_bytes_avx2(state, a_jagged_p, out, outlen);
}


inline fn __keccak1600_avx2(reg u64 out outlen in inlen, reg u8 trail_byte, reg u64 rate)
{
  reg u256[7] state;

  state = __keccak_init_avx2();

  // absorb
  state = __absorb_avx2(state, in, inlen, trail_byte, rate);

  // squeeze
  __squeeze_avx2(state, out, outlen, rate);
}


fn _keccak1600_avx2(reg u64 out outlen in inlen, reg u8 trail_byte, reg u64 rate)
{
  __keccak1600_avx2(out, outlen, in, inlen, trail_byte, rate);
}


param int SHAKE128_RATE = 168;
param int SHAKE256_RATE = 136;
param int SHA3_256_RATE = 136;
param int SHA3_512_RATE = 72;

#[returnaddress="stack"]
fn _sha3_256(reg ptr u8[32] out, reg u64 in inlen) -> reg ptr u8[32]
{
  reg u256[7] state;
  stack u64[28] s_state;
  reg ptr u64[25] a_jagged_p;
  reg u64 t l;
  reg u8 trail_byte;
  inline int i;

  state = __keccak_init_avx2();

  trail_byte = 0x06;
  t = SHA3_256_RATE;
  state = __absorb_avx2(state, in, inlen, trail_byte, t);

  state = __keccakf1600_avx2(state);

  for i=0 to 7 { s_state[u256 i] = state[i]; }
  
  a_jagged_p = KECCAK_A_JAGGED;

  for i=0 to 4
  {
    l = a_jagged_p[i];
    t = s_state[(int) l];
    out[u64 i] = t;
  }

  return out;
}


#[returnaddress="stack"]
fn _sha3_256_32(reg ptr u8[32] out, reg ptr u8[KYBER_SYMBYTES] in) -> reg ptr u8[32]
{
  reg u256[7] state;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  state[0] = #VPBROADCAST_4u64(in[u64 0]);

  for i=1 to KYBER_SYMBYTES/8
  {
    t = in[u64 i];
    l = a_jagged_p[i];
    s_state[(int) l] = t;
  }

  l = a_jagged_p[KYBER_SYMBYTES/8];
  l <<= 3;
  s_state[u8 (int)l] = 0x06;

  l = a_jagged_p[(SHA3_256_RATE-1)/8];
  l <<= 3;
  t = SHA3_256_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] ^= 0x80;

  for i=1 to 7 { state[i] = s_state[u256 i]; }
  
  state = __keccakf1600_avx2(state);

  for i=0 to 7 { s_state[u256 i] = state[i]; }

  for i=0 to 4
  {
    l = a_jagged_p[i];
    t = s_state[(int)l];
    out[u64 i] = t;
  }

  return out;
}


#[returnaddress="stack"]
fn _shake256_64(reg u64 out outlen, reg const ptr u8[64] in)
{
  reg u256[7] state;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  
  for i=1 to 8
  {
    l = a_jagged_p[i];
    t = in[u64 i];
    s_state[(int)l] = t;
  }

  l = a_jagged_p[8];
  l <<= 3;

  s_state[u8 (int)l] = 0x1f;

  l = a_jagged_p[(SHAKE256_RATE-1)/8];
  l <<= 3;
  t = SHAKE256_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] ^= 0x80;

  for i=1 to 7
  { state[i] = s_state[u256 i]; }
  
  t = SHAKE256_RATE;
  __squeeze_avx2(state, out, outlen, t);
}


#[returnaddress="stack"]
fn _shake256_128_33(reg ptr u8[128] out, reg const ptr u8[33] in) -> stack u8[128]
{
  reg u256[7] state;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  reg u8 c;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  state[0] = #VPBROADCAST_4u64(in[u64 0]);

  for i = 1 to 4
  {
    t = in[u64 i];
    l = a_jagged_p[i];
    s_state[(int)l] = t;
  }

  c = in[u8 32];
  l = a_jagged_p[4];
  l <<= 3;
  s_state[u8 (int)l] = c;
  
  l += 1;
  s_state[u8 (int)l] = 0x1f;

  l = a_jagged_p[(SHAKE256_RATE-1)/8];
  l <<= 3;
  t = SHAKE256_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] ^= 0x80;

  for i=1 to 7
  { state[i] = s_state[u256 i]; }

  state = __keccakf1600_avx2(state);

  for i=0 to 7
  { s_state[u256 i] = state[i]; }

  for i = 0 to 16
  {
    l = a_jagged_p[i];
    t = s_state[(int)l];
    out[u64 i] = t;
  }

  return out;
}

#[returnaddress="stack"]
fn _sha3_512_64(reg ptr u8[64] out, reg const ptr u8[64] in) -> stack u8[64]
{
  reg u256[7] state;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  
  for i = 1 to 8
  {
    t = in[u64 i];
    l = a_jagged_p[i];
    s_state[(int)l] = t;
  }

  l = a_jagged_p[8];
  l <<= 3;
  s_state[u8 (int)l] = 0x06;

  l = a_jagged_p[(SHA3_512_RATE-1)/8];
  l <<= 3;
  t = SHA3_512_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] ^= 0x80;

  for i=1 to 7
  { state[i] = s_state[u256 i]; }
  
  state = __keccakf1600_avx2(state);

  for i=0 to 7
  { s_state[u256 i] = state[i]; }

  for i = 0 to 8
  {
    l = a_jagged_p[i];
    t = s_state[(int) l];
    out[u64 i] = t;
  }

  return out;
}

#[returnaddress="stack"]
fn _sha3_512_32(reg ptr u8[64] out, reg const ptr u8[32] in) -> stack u8[64]
{
  reg u256[7] state;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  
  for i = 1 to 4
  {
    t = in[u64 i];
    l = a_jagged_p[i];
    s_state[(int)l] = t;
  }

  l = a_jagged_p[4];
  l <<= 3;
  s_state[u8 (int)l] = 0x06;

  l = a_jagged_p[(SHA3_512_RATE-1)/8];
  l <<= 3;
  t = SHA3_512_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] ^= 0x80;

  for i=1 to 7
  { state[i] = s_state[u256 i]; }
  
  state = __keccakf1600_avx2(state);

  for i=0 to 7
  { s_state[u256 i] = state[i]; }

  for i = 0 to 8
  {
    l = a_jagged_p[i];
    t = s_state[(int) l];
    out[u64 i] = t;
  }

  return out;
}

//FIXME: remove inline when register arrays are supported in "real" functions
inline
fn __shake128_absorb34(reg u256[7] state, reg const ptr u8[34] in) -> reg u256[7]
{
  reg u128 t128;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 l t;
  reg u8 t8;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();
  
  state[0] = #VPBROADCAST_4u64(in[u64 0]);

  for i=1 to 4
  {
    t = in[u64 i];
    l = a_jagged_p[i];
    s_state[(int)l] = t;
  }

  t8 = in[32];
  l = a_jagged_p[(int) 4];
  l <<= 3;
  s_state[u8 (int)l] = t8;

  t8 = in[33];
  l += 1;
  s_state[u8 (int)l] = t8;

  l += 1;
  s_state[u8 (int)l] = 0x1f;

  l = a_jagged_p[(SHAKE128_RATE-1)/8];
  l <<= 3;
  t = SHAKE128_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] = 0x80;

  for i=1 to 7 { state[i] = s_state[u256 i]; }
  
  return state;
}

inline
fn __shake128_squeezeblock(reg u256[7] state, reg ptr u8[SHAKE128_RATE] out) -> reg u256[7], reg ptr u8[SHAKE128_RATE]
{
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  inline int i;

  state = __keccakf1600_avx2(state);

  for i=0 to 7
  { s_state[u256 i] = state[i]; }

  a_jagged_p = KECCAK_A_JAGGED;

  for i = 0 to SHAKE128_RATE/8
  {
    l = a_jagged_p[i];
    t = s_state[(int) l];
    out[u64 i] = t;
  }

  return state, out;
}



u64[24] KECCAK1600_RC =
{  0x0000000000000001
  ,0x0000000000008082
  ,0x800000000000808a
  ,0x8000000080008000
  ,0x000000000000808b
  ,0x0000000080000001
  ,0x8000000080008081
  ,0x8000000000008009
  ,0x000000000000008a
  ,0x0000000000000088
  ,0x0000000080008009
  ,0x000000008000000a
  ,0x000000008000808b
  ,0x800000000000008b
  ,0x8000000000008089
  ,0x8000000000008003
  ,0x8000000000008002
  ,0x8000000000000080
  ,0x000000000000800a
  ,0x800000008000000a
  ,0x8000000080008081
  ,0x8000000000008080
  ,0x0000000080000001
  ,0x8000000080008008
};

inline fn __index_spec(inline int x y) -> inline int
{
  inline int r;
  r = (x % 5) + 5 * (y % 5);
  return r;
}


inline fn __keccak_rho_offsets_spec(inline int i) -> inline int
{
  inline int r x y z t;

  r = 0;
  x = 1;
  y = 0;

  for t = 0 to 24 {
    if (i == x + 5 * y) {
      r = ((t + 1) * (t + 2) / 2) % 64;
    }
    z = (2 * x + 3 * y) % 5;
    x = y;
    y = z;
  }

  return r;
}


inline fn __rhotates_spec(inline int x y) -> inline int
{
  inline int i r;
  i = __index_spec(x, y);
  r = __keccak_rho_offsets_spec(i);
  return r;
}


inline fn __theta_spec(stack u64[25] a) -> stack u64[25]
{
  inline int x y;
  reg u64[5] c d;

  for x = 0 to 5 {
    c[x] = 0;
    for y = 0 to 5 {
      c[x] ^= a[x + 5 * y];
    }
  }

  for x = 0 to 5 {
    d[x] = c[(x + 1) % 5];
    _, _, d[x] = #ROL_64(d[x], 1);
    d[x] ^= c[(x + 4) % 5];
  }

  for x = 0 to 5 {
    for y = 0 to 5 {
      a[x + 5 * y] ^= d[x];
    }
  }

  return a;
}


inline fn __rho_spec(stack u64[25] a) -> stack u64[25]
{
  inline int x y i z;

  for x = 0 to 5 {
    for y = 0 to 5 {
      i = __index_spec(x, y);
      z = __keccak_rho_offsets_spec(i);
      _, _, a[i] = #ROL_64(a[i], z);
    }
  }

  return a;
}


inline fn __pi_spec(stack u64[25] a) -> stack u64[25]
{
  inline int x y i;
  stack u64[25] b;
  reg u64 t;

  for i = 0 to 25
  { t = a[i]; b[i] = t; }

  for x = 0 to 5 {
    for y = 0 to 5 {
      t = b[x + 5 * y];
      i = __index_spec(y, 2 * x + 3 * y);
      a[i] = t;
    }
  }
  return a;
}


inline fn __chi_spec(stack u64[25] a) -> stack u64[25]
{
  inline int x y i;
  reg u64[5] c;

  for y = 0 to 5 {
    for x = 0 to 5 {
      i = __index_spec(x + 1, y);
      c[x] = a[i];
      c[x] = !c[x];
      i = __index_spec(x + 2, y);
      c[x] &= a[i];
      i = __index_spec(x, y);
      c[x] ^= a[i];
    }
    for x = 0 to 5 {
      a[x + 5 * y] = c[x];
    }
  }
  return a;
}


inline fn __iota_spec(stack u64[25] a, reg u64 c) -> stack u64[25]
{
  a[0] ^= c;
  return a;
}


inline fn __keccakP1600_round_spec(stack u64[25] state, reg u64 c) -> stack u64[25]
{
  state = __theta_spec(state);
  state = __rho_spec(state);
  state = __pi_spec(state);
  state = __chi_spec(state);
  state = __iota_spec(state, c);
  return state;
}


inline fn __keccakf1600_spec(stack u64[25] state) -> stack u64[25]
{
  reg u64 round RC;
  reg ptr u64[24] kRCp;

  kRCp = KECCAK1600_RC;
  round = 0;
  while(round < 24)
  {
    RC = kRCp[(int) round];
    state = __keccakP1600_round_spec(state, RC);
    round += 1;
  }

  return state;
}



param int KECCAK_ROUNDS=24;

u256[24] KECCAK1600_RC_4x = {
  (4u64)[0x0000000000000001, 0x0000000000000001, 0x0000000000000001, 0x0000000000000001],
  (4u64)[0x0000000000008082, 0x0000000000008082, 0x0000000000008082, 0x0000000000008082],
  (4u64)[0x800000000000808a, 0x800000000000808a, 0x800000000000808a, 0x800000000000808a],
  (4u64)[0x8000000080008000, 0x8000000080008000, 0x8000000080008000, 0x8000000080008000],
  (4u64)[0x000000000000808b, 0x000000000000808b, 0x000000000000808b, 0x000000000000808b],
  (4u64)[0x0000000080000001, 0x0000000080000001, 0x0000000080000001, 0x0000000080000001],
  (4u64)[0x8000000080008081, 0x8000000080008081, 0x8000000080008081, 0x8000000080008081],
  (4u64)[0x8000000000008009, 0x8000000000008009, 0x8000000000008009, 0x8000000000008009],
  (4u64)[0x000000000000008a, 0x000000000000008a, 0x000000000000008a, 0x000000000000008a],
  (4u64)[0x0000000000000088, 0x0000000000000088, 0x0000000000000088, 0x0000000000000088],
  (4u64)[0x0000000080008009, 0x0000000080008009, 0x0000000080008009, 0x0000000080008009],
  (4u64)[0x000000008000000a, 0x000000008000000a, 0x000000008000000a, 0x000000008000000a],
  (4u64)[0x000000008000808b, 0x000000008000808b, 0x000000008000808b, 0x000000008000808b],
  (4u64)[0x800000000000008b, 0x800000000000008b, 0x800000000000008b, 0x800000000000008b],
  (4u64)[0x8000000000008089, 0x8000000000008089, 0x8000000000008089, 0x8000000000008089],
  (4u64)[0x8000000000008003, 0x8000000000008003, 0x8000000000008003, 0x8000000000008003],
  (4u64)[0x8000000000008002, 0x8000000000008002, 0x8000000000008002, 0x8000000000008002],
  (4u64)[0x8000000000000080, 0x8000000000000080, 0x8000000000000080, 0x8000000000000080],
  (4u64)[0x000000000000800a, 0x000000000000800a, 0x000000000000800a, 0x000000000000800a],
  (4u64)[0x800000008000000a, 0x800000008000000a, 0x800000008000000a, 0x800000008000000a],
  (4u64)[0x8000000080008081, 0x8000000080008081, 0x8000000080008081, 0x8000000080008081],
  (4u64)[0x8000000000008080, 0x8000000000008080, 0x8000000000008080, 0x8000000000008080],
  (4u64)[0x0000000080000001, 0x0000000080000001, 0x0000000080000001, 0x0000000080000001],
  (4u64)[0x8000000080008008, 0x8000000080008008, 0x8000000080008008, 0x8000000080008008]
};

u256 ROL56 = 0x181F1E1D1C1B1A191017161514131211080F0E0D0C0B0A090007060504030201;
u256 ROL8  = 0x1E1D1C1B1A19181F16151413121110170E0D0C0B0A09080F0605040302010007;

// C[x] = A[x,0] ^ A[x,1] ^ A[x,2] ^ A[x,3] ^ A[x,4]
inline fn __theta_sum_4x_avx2(reg ptr u256[25] a) -> reg u256[5]
{
  inline int x y;
  reg u256[5] c;

  // C[x] = A[x, 0]
  for x=0 to 5
  { c[x] = a[x + 0]; }

  // C[x] ^= A[x,1] ^ A[x,2] ^ A[x,3] ^ A[x,4]
  for y=1 to 5
  { for x=0 to 5
    { c[x] ^= a[x + y*5]; }
  }

  return c;
}

inline fn __rol_4x_avx2(reg u256[5] a, inline int x r, reg u256 r8 r56) -> reg u256[5]
{
	reg u256 t;

  if(r == 8)
  {	a[x] = #VPSHUFB_256(a[x], r8); }
  else { if(r == 56)
  { a[x] = #VPSHUFB_256(a[x], r56); }
  else
  { t     = #VPSLL_4u64(a[x], r);
	  a[x]  = #VPSRL_4u64(a[x], 64 - r);
	  a[x] |= t; }
  }

	return a; 
}

// D[x] = C[x-1] ^ ROT(C[x+1], 1) 
inline fn __theta_rol_4x_avx2(reg u256[5] c, reg u256 r8 r56) -> reg u256[5]
{
  inline int x;
  reg u256[5] d;

  for x = 0 to 5
  { // D[x] = C[x + 1]
    d[x] = c[(x + 1) % 5];

    // D[x] = ROT(D[x], 1)
    d = __rol_4x_avx2(d, x, 1, r8, r56);

    // D[x] ^= C[x-1]
    d[x] ^= c[(x - 1 + 5) % 5];
  }

  return d;
}


// B[x] = ROT( (A[x',y'] ^ D[x']), r[x',y'] ) with (x',y') = M^-1 (x,y)
//
// M = (0 1)  M^-1 = (1 3)  x' = 1x + 3y
//     (2 3)         (1 0)  y' = 1x + 0y
//
inline fn __rol_sum_4x_avx2(
  reg ptr u256[25] a,
  reg u256[5] d,
  inline int y,
  reg u256 r8 r56
) -> reg u256[5]
{
  inline int r x x_ y_;
  reg u256[5] b;

  for x = 0 to 5
  {
    x_ = (x + 3*y) % 5;
    y_ = x;
    r = __rhotates_spec(x_, y_);

    // B[x] = A[x',y']
    b[x] = a[x_ + y_*5];

    // B[x] ^= D[x'];
    b[x] ^= d[x_];

    // B[x] = ROT( B[x], r[x',y'] );
    if(r != 0)
    { b = __rol_4x_avx2(b, x, r, r8, r56); }
  }

  return b;
}


// E[x, y] = B[x] ^ ( (!B[x+1]) & B[x+2] )
// -- when x and y are 0: E[0,0] ^= RC[i];
inline fn __set_row_4x_avx2(
  reg ptr u256[25] e,
  reg u256[5] b,
  inline int y,
  reg u256 rc
) -> reg ptr u256[25]
{
  inline int x x1 x2;
  reg u256 t;

  for x=0 to 5
  { 
    x1 = (x + 1) % 5;
    x2 = (x + 2) % 5;

    t = #VPANDN_256(b[x1], b[x2]);

    t ^= b[x];
    if( x==0 && y==0 ){ t ^= rc; }
    e[x + y*5] = t;
  }

  return e;
}


inline fn __round_4x_avx2(reg ptr u256[25] e a, reg u256 rc r8 r56) -> reg ptr u256[25]
{
  inline int y;
  reg u256[5] b c d;

  c = __theta_sum_4x_avx2(a);
  d = __theta_rol_4x_avx2(c, r8, r56);

  for y = 0 to 5
  { b = __rol_sum_4x_avx2(a, d, y, r8, r56);
    e = __set_row_4x_avx2(e, b, y, rc);
  }

  return e;
}


inline fn __keccakf1600_4x_avx2(reg ptr u256[25] a) -> reg ptr u256[25]
{
  reg ptr u256[24] RC;
  stack u256[25] s_e;
  reg ptr u256[25] e;
  reg u256 rc r8 r56;
  reg u64 c;

  e = s_e;
  RC = KECCAK1600_RC_4x;

  r8 = ROL8;
  r56 = ROL56;

  c = 0;
  while
  {
    rc = RC.[(int) c];
    e = __round_4x_avx2(e, a, rc, r8, r56);

    rc = RC.[(int) c + 32];
    a = __round_4x_avx2(a, e, rc, r8, r56);

    c += 64;
  } (c < (KECCAK_ROUNDS*32))

  return a;
}

#[returnaddress="stack"]
fn _keccakf1600_4x_avx2(reg ptr u256[25] a) -> reg ptr u256[25]
{
  a = __keccakf1600_4x_avx2(a);
  return a;
}


inline fn _keccakf1600_4x_avx2_(reg ptr u256[25] a) -> reg ptr u256[25]
{
  a = a;
  a = _keccakf1600_4x_avx2(a);
  a = a;
  return a;
}


u256 SHAKE_SEP = (4u64)[0x8000000000000000, 0x8000000000000000, 0x8000000000000000, 0x8000000000000000];

#[returnaddress="stack"]
fn _shake128_absorb4x_34(reg ptr u256[25] s, reg ptr u8[34] m0 m1 m2 m3) -> reg ptr u256[25]
{
	inline int i;
  reg u256 t0 t1;
  reg u16 t16;
	reg u64 t64;

  for i = 0 to 25
  {
    t0 = #set0_256();
    s[i] = t0;
  }

	for i = 0 to 4
  {
    t64 = m0[u64 i];
    s[u64 4 * i] ^= t64;
    t64 = m1[u64 i];
    s[u64 4 * i + 1] ^= t64;
    t64 = m2[u64 i];
    s[u64 4 * i + 2] ^= t64;
    t64 = m3[u64 i];
    s[u64 4 * i + 3] ^= t64;
  }

  t16 = m0.[u16 32];
  s[u16 64] ^= t16;
  s[u8 130] ^= 0x1F;

  t16 = m1.[u16 32];
  s[u16 68] ^= t16;
  s[u8 138] ^= 0x1F;

  t16 = m2.[u16 32];
  s[u16 72] ^= t16;
  s[u8 146] ^= 0x1F;

  t16 = m3.[u16 32];
  s[u16 76] ^= t16;
  s[u8 154] ^= 0x1F;

  t0 = SHAKE_SEP;
  t1 = s[SHAKE128_RATE / 8 - 1];
  t0 = t0 ^ t1;
  s[SHAKE128_RATE / 8 - 1] = t0;

	return s;
}


#[returnaddress="stack"]
fn _shake256_absorb4x_33(reg ptr u256[25] s, reg ptr u8[33] m0 m1 m2 m3) -> reg ptr u256[25]
{
	inline int i;
  reg u256 t0 t1;
	reg u64 t64;
  reg u8 t8;

  for i = 0 to 25
  {
    t0 = #set0_256();
    s[i] = t0;
  }

	for i = 0 to 4
  {
    t64 = m0[u64 i];
    s[u64 4 * i] ^= t64;
    t64 = m1[u64 i];
    s[u64 4 * i + 1] ^= t64;
    t64 = m2[u64 i];
    s[u64 4 * i + 2] ^= t64;
    t64 = m3[u64 i];
    s[u64 4 * i + 3] ^= t64;
  }

  t8 = m0[32];
  s[u8 128] ^= t8;
  s[u8 129] ^= 0x1F;

  t8 = m1[32];
  s[u8 136] ^= t8;
  s[u8 137] ^= 0x1F;

  t8 = m2[32];
  s[u8 144] ^= t8;
  s[u8 145] ^= 0x1F;

  t8 = m3[32];
  s[u8 152] ^= t8;
  s[u8 153] ^= 0x1F;

  t0 = SHAKE_SEP;
  t1 = s[SHAKE256_RATE / 8 - 1];
  t0 = t0 ^ t1;
  s[SHAKE256_RATE / 8 - 1] = t0;

	return s;
}


inline fn __shake256_squeezeblock4x(
  reg ptr u256[25] state,
  reg ptr u8[SHAKE256_RATE] h0 h1 h2 h3)
  ->
  reg ptr u256[25],
  reg ptr u8[SHAKE256_RATE],
  reg ptr u8[SHAKE256_RATE],
  reg ptr u8[SHAKE256_RATE],
  reg ptr u8[SHAKE256_RATE]
{
  reg u256 t256;
  reg u128 t128;
  inline int i;

  state = _keccakf1600_4x_avx2(state);

	for i = 0 to (SHAKE256_RATE / 8) {
    t256 = state[i];
    t128 = (128u)t256;
		h0[u64 i] = #VMOVLPD(t128);
		h1[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI128(t256, 1);
		h2[u64 i] = #VMOVLPD(t128);
		h3[u64 i] = #VMOVHPD(t128);
	}

  return state, h0, h1, h2, h3;
}
inline 
fn __shuffle8(reg u256 a b) -> reg u256, reg u256
{
  reg u256 r0 r1; 
  r0 = #VPERM2I128(a,b,0x20);
  r1 = #VPERM2I128(a,b,0x31);
  return r0, r1;
}

inline 
fn __shuffle4(reg u256 a b) -> reg u256, reg u256
{
  reg u256 r0 r1; 
  r0 = #VPUNPCKL_4u64(a,b);
  r1 = #VPUNPCKH_4u64(a,b);
  return r0, r1;
}

inline 
fn __shuffle2(reg u256 a b) -> reg u256, reg u256
{
  reg u256 t0 t1;
  t0 = #VMOVSLDUP_256(b);
  t0 = #VPBLEND_8u32(a, t0, 0xAA);
  a = #VPSRL_4u64(a,32);
  t1 = #VPBLEND_8u32(a, b, 0xAA);
  return t0, t1;
}


inline 
fn __shuffle1(reg u256 a b) -> reg u256, reg u256
{
  reg u256 r0 r1 t0 t1; 
  t0 = #VPSLL_8u32(b,16);
  r0 = #VPBLEND_16u16(a,t0,0xAA);
  t1 = #VPSRL_8u32(a,16);
  r1 = #VPBLEND_16u16(t1,b,0xAA);
  return r0, r1;
}


// Transform from AVX order to bitreversed order
inline 
fn __nttpack128(reg u256 r0 r1 r2 r3 r4 r5 r6 r7)
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  r0, r1 = __shuffle1(r0, r1);
  r2, r3 = __shuffle1(r2, r3);
  r4, r5 = __shuffle1(r4, r5);
  r6, r7 = __shuffle1(r6, r7);

  r0, r2 = __shuffle2(r0, r2);
  r4, r6 = __shuffle2(r4, r6);
  r1, r3 = __shuffle2(r1, r3);
  r5, r7 = __shuffle2(r5, r7);

  r0, r4 = __shuffle4(r0, r4);
  r1, r5 = __shuffle4(r1, r5);
  r2, r6 = __shuffle4(r2, r6);
  r3, r7 = __shuffle4(r3, r7);

  r0, r1 = __shuffle8(r0, r1);
  r2, r3 = __shuffle8(r2, r3);
  r4, r5 = __shuffle8(r4, r5);
  r6, r7 = __shuffle8(r6, r7);

  return r0, r2, r4, r6, r1, r3, r5, r7;
}


// Transform from bitreversed order to AVX order
inline
fn __nttunpack128(reg u256 r0 r1 r2 r3 r4 r5 r6 r7)
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  r0, r4 = __shuffle8(r0, r4);
  r1, r5 = __shuffle8(r1, r5);
  r2, r6 = __shuffle8(r2, r6);
  r3, r7 = __shuffle8(r3, r7);

  r0, r2 = __shuffle4(r0, r2);
  r4, r6 = __shuffle4(r4, r6);
  r1, r3 = __shuffle4(r1, r3);
  r5, r7 = __shuffle4(r5, r7);

  r0, r1 = __shuffle2(r0, r1);
  r2, r3 = __shuffle2(r2, r3);
  r4, r5 = __shuffle2(r4, r5);
  r6, r7 = __shuffle2(r6, r7);

  r0, r4 = __shuffle1(r0, r4);
  r1, r5 = __shuffle1(r1, r5);
  r2, r6 = __shuffle1(r2, r6);
  r3, r7 = __shuffle1(r3, r7);

  return r0, r4, r1, r5, r2, r6, r3, r7;
}

fn _nttpack(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 r0 r1 r2 r3 r4 r5 r6 r7;

  r0 = rp.[u256 32*0];
  r1 = rp.[u256 32*1];
  r2 = rp.[u256 32*2];
  r3 = rp.[u256 32*3];
  r4 = rp.[u256 32*4];
  r5 = rp.[u256 32*5];
  r6 = rp.[u256 32*6];
  r7 = rp.[u256 32*7];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*0] = r0;
  rp.[u256 32*1] = r1;
  rp.[u256 32*2] = r2;
  rp.[u256 32*3] = r3;
  rp.[u256 32*4] = r4;
  rp.[u256 32*5] = r5;
  rp.[u256 32*6] = r6;
  rp.[u256 32*7] = r7;

  r0 = rp.[u256 32*8];
  r1 = rp.[u256 32*9];
  r2 = rp.[u256 32*10];
  r3 = rp.[u256 32*11];
  r4 = rp.[u256 32*12];
  r5 = rp.[u256 32*13];
  r6 = rp.[u256 32*14];
  r7 = rp.[u256 32*15];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*8] = r0;
  rp.[u256 32*9] = r1;
  rp.[u256 32*10] = r2;
  rp.[u256 32*11] = r3;
  rp.[u256 32*12] = r4;
  rp.[u256 32*13] = r5;
  rp.[u256 32*14] = r6;
  rp.[u256 32*15] = r7;

  return rp;
}

fn _nttunpack(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 r0 r1 r2 r3 r4 r5 r6 r7;

  r0 = rp.[u256 32*0];
  r1 = rp.[u256 32*1];
  r2 = rp.[u256 32*2];
  r3 = rp.[u256 32*3];
  r4 = rp.[u256 32*4];
  r5 = rp.[u256 32*5];
  r6 = rp.[u256 32*6];
  r7 = rp.[u256 32*7];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttunpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*0] = r0;
  rp.[u256 32*1] = r1;
  rp.[u256 32*2] = r2;
  rp.[u256 32*3] = r3;
  rp.[u256 32*4] = r4;
  rp.[u256 32*5] = r5;
  rp.[u256 32*6] = r6;
  rp.[u256 32*7] = r7;

  r0 = rp.[u256 32*8];
  r1 = rp.[u256 32*9];
  r2 = rp.[u256 32*10];
  r3 = rp.[u256 32*11];
  r4 = rp.[u256 32*12];
  r5 = rp.[u256 32*13];
  r6 = rp.[u256 32*14];
  r7 = rp.[u256 32*15];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttunpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*8] = r0;
  rp.[u256 32*9] = r1;
  rp.[u256 32*10] = r2;
  rp.[u256 32*11] = r3;
  rp.[u256 32*12] = r4;
  rp.[u256 32*13] = r5;
  rp.[u256 32*14] = r6;
  rp.[u256 32*15] = r7;

  return rp;
}
u16[128] jzetas = {2285, 2571, 2970, 1812, 1493, 1422, 287, 202, 3158, 622, 1577, 182, 962, 2127, 1855, 1468, 
                  573, 2004, 264, 383, 2500, 1458, 1727, 3199, 2648, 1017, 732, 608, 1787, 411, 3124, 1758, 
                  1223, 652, 2777, 1015, 2036, 1491, 3047, 1785, 516, 3321, 3009, 2663, 1711, 2167, 126, 1469, 
                  2476, 3239, 3058, 830, 107, 1908, 3082, 2378, 2931, 961, 1821, 2604, 448, 2264, 677, 2054, 
                  2226, 430, 555, 843, 2078, 871, 1550, 105, 422, 587, 177, 3094, 3038, 2869, 1574, 1653, 
                  3083, 778, 1159, 3182, 2552, 1483, 2727, 1119, 1739, 644, 2457, 349, 418, 329, 3173, 3254, 
                  817, 1097, 603, 610, 1322, 2044, 1864, 384, 2114, 3193, 1218, 1994, 2455, 220, 2142, 1670, 
                  2144, 1799, 2051, 794, 1819, 2475, 2459, 478, 3221, 3021, 996, 991, 958, 1869, 1522, 1628};


u16[128] jzetas_inv = {1701, 1807, 1460, 2371, 2338, 2333, 308, 108, 2851, 870, 854, 1510, 2535, 1278, 1530, 1185, 
                       1659, 1187, 3109, 874, 1335, 2111, 136, 1215, 2945, 1465, 1285, 2007, 2719, 2726, 2232, 2512, 
                       75, 156, 3000, 2911, 2980, 872, 2685, 1590, 2210, 602, 1846, 777, 147, 2170, 2551, 246, 
                       1676, 1755, 460, 291, 235, 3152, 2742, 2907, 3224, 1779, 2458, 1251, 2486, 2774, 2899, 1103, 
                       1275, 2652, 1065, 2881, 725, 1508, 2368, 398, 951, 247, 1421, 3222, 2499, 271, 90, 853, 
                       1860, 3203, 1162, 1618, 666, 320, 8, 2813, 1544, 282, 1838, 1293, 2314, 552, 2677, 2106, 
                       1571, 205, 2918, 1542, 2721, 2597, 2312, 681, 130, 1602, 1871, 829, 2946, 3065, 1325, 2756, 
                       1861, 1474, 1202, 2367, 3147, 1752, 2707, 171, 3127, 3042, 1907, 1836, 1517, 359, 758, 1441};

u16[400] jzetas_exp = {31499, 31499,  2571,  2571, 14746, 14746,  2970,  2970, 13525, 13525, 13525, 13525, 13525, 13525, 13525, 13525,
                       53134, 53134, 53134, 53134, 53134, 53134, 53134, 53134, 1493,  1493,  1493,  1493,  1493,  1493,  1493,  1493,
                        1422,  1422,  1422,  1422,  1422,  1422,  1422,  1422, 44630, 44630, 44630, 44630, 27758, 27758, 27758, 27758,
                       61737, 61737, 61737, 61737, 49846, 49846, 49846, 49846, 3158,  3158,  3158,  3158,   622,   622,   622,   622,
                        1577,  1577,  1577,  1577,   182,   182,   182,   182, 59709, 59709, 17364, 17364, 39176, 39176, 36479, 36479,
                        5572,  5572, 64434, 64434, 21439, 21439, 39295, 39295, 573,   573,  2004,  2004,   264,   264,   383,   383,
                        2500,  2500,  1458,  1458,  1727,  1727,  3199,  3199, 59847, 59020,  1497, 30967, 41972, 20179, 20711, 25081,
                       52740, 26617, 16065, 53095,  9135, 64887, 39550, 27837, 1223,   652,  2777,  1015,  2036,  1491,  3047,  1785,
                         516,  3321,  3009,  2663,  1711,  2167,   126,  1469, 65202, 54059, 33310, 20494, 37798,   945, 50654,  6182,
                       32011, 10631, 29176, 36775, 47051, 17561, 51106, 60261, 2226,   555,  2078,  1550,   422,   177,  3038,  1574,
                        3083,  1159,  2552,  2727,  1739,  2457,   418,  3173, 11182, 13387, 51303, 43881, 13131, 60950, 23093,  5493,
                       33034, 30318, 46795, 12639, 20100, 18525, 19529, 52918, 430,   843,   871,   105,   587,  3094,  2869,  1653,
                         778,  3182,  1483,  1119,   644,   349,   329,  3254, 788,   788,  1812,  1812, 28191, 28191, 28191, 28191,
                       28191, 28191, 28191, 28191, 48842, 48842, 48842, 48842, 48842, 48842, 48842, 48842,   287,   287,   287,   287,
                         287,   287,   287,   287,   202,   202,   202,   202, 202,   202,   202,   202, 10690, 10690, 10690, 10690,
                        1359,  1359,  1359,  1359, 54335, 54335, 54335, 54335, 31164, 31164, 31164, 31164,   962,   962,   962,   962,
                        2127,  2127,  2127,  2127,  1855,  1855,  1855,  1855, 1468,  1468,  1468,  1468, 37464, 37464, 24313, 24313,
                       55004, 55004,  8800,  8800, 18427, 18427,  8859,  8859, 26676, 26676, 49374, 49374,  2648,  2648,  1017,  1017,
                         732,   732,   608,   608,  1787,  1787,   411,   411, 3124,  3124,  1758,  1758, 19884, 37287, 49650, 56638,
                       37227,  9076, 35338, 18250, 13427, 14017, 36381, 52780, 16832,  4312, 41381, 47622,  2476,  3239,  3058,   830,
                         107,  1908,  3082,  2378,  2931,   961,  1821,  2604, 448,  2264,   677,  2054, 34353, 25435, 58154, 24392,
                       44610, 10946, 24215, 16990, 10336, 57603, 43035, 10907, 31637, 28644, 23998, 48114,   817,   603,  1322,  1864,
                        2114,  1218,  2455,  2142,  2144,  2051,  1819,  2459, 3221,   996,   958,  1522, 20297,  2146, 15356, 33152,
                       59257, 50634, 54492, 14470, 44039, 45338, 23211, 48094, 41677, 45279,  7757, 23132,  1097,   610,  2044,   384,
                        3193,  1994,   220,  1670,  1799,   794,  2475,   478, 3021,   991,  1869,  1628,     0,     0,     0,     0};

u16[400] jzetas_inv_exp = {42405, 57780, 20258, 23860, 17443, 42326, 20199, 21498, 51067, 11045, 14903,  6280, 32385, 50181, 63391, 45240,
                            1701,  1460,  2338,   308,  2851,   854,  2535,  1530, 1659,  3109,  1335,   136,  2945,  1285,  2719,  2232,
                           17423, 41539, 36893, 33900, 54630, 22502,  7934, 55201, 48547, 41322, 54591, 20927, 41145,  7383, 40102, 31184,
                            1807,  2371,  2333,   108,   870,  1510,  1278,  1185, 1187,   874,  2111,  1215,  1465,  2007,  2726,  2512,
                           17915, 24156, 61225, 48705, 12757, 29156, 51520, 52110, 47287, 30199, 56461, 28310,  8899, 15887, 28250, 45653,
                            1275,  2652,  1065,  2881,   725,  1508,  2368,   398, 951,   247,  1421,  3222,  2499,   271,    90,   853,
                           16163, 16163, 38861, 38861, 56678, 56678, 47110, 47110, 56737, 56737, 10533, 10533, 41224, 41224, 28073, 28073,
                            1571,  1571,   205,   205,  2918,  2918,  1542,  1542, 2721,  2721,  2597,  2597,  2312,  2312,   681,   681,
                           34373, 34373, 34373, 34373, 11202, 11202, 11202, 11202, 64178, 64178, 64178, 64178, 54847, 54847, 54847, 54847,
                            1861,  1861,  1861,  1861,  1474,  1474,  1474,  1474, 1202,  1202,  1202,  1202,  2367,  2367,  2367,  2367,
                           16695, 16695, 16695, 16695, 16695, 16695, 16695, 16695, 37346, 37346, 37346, 37346, 37346, 37346, 37346, 37346,
                            3127,  3127,  3127,  3127,  3127,  3127,  3127,  3127, 3042,  3042,  3042,  3042,  3042,  3042,  3042,  3042,
                           64749, 64749,  1517,  1517, 12619, 46008, 47012, 45437, 52898, 18742, 35219, 32503, 60044, 42444,  4587, 52406,
                           21656, 14234, 52150, 54355,    75,  3000,  2980,  2685, 2210,  1846,   147,  2551,  1676,   460,   235,  2742,
                            3224,  2458,  2486,  2899,  5276, 14431, 47976, 18486, 28762, 36361, 54906, 33526, 59355, 14883, 64592, 27739,
                           45043, 32227, 11478,   335,   156,  2911,   872,  1590, 602,   777,  2170,   246,  1755,   291,  3152,  2907,
                            1779,  1251,  2774,  1103, 37700, 25987,   650, 56402, 12442, 49472, 38920, 12797, 40456, 44826, 45358, 23565,
                           34570, 64040,  6517,  5690,  1860,  3203,  1162,  1618, 666,   320,     8,  2813,  1544,   282,  1838,  1293,
                            2314,   552,  2677,  2106, 26242, 26242, 44098, 44098, 1103,  1103, 59965, 59965, 29058, 29058, 26361, 26361,
                           48173, 48173,  5828,  5828,   130,   130,  1602,  1602, 1871,  1871,   829,   829,  2946,  2946,  3065,  3065,
                            1325,  1325,  2756,  2756, 15691, 15691, 15691, 15691, 3800,  3800,  3800,  3800, 37779, 37779, 37779, 37779,
                           20907, 20907, 20907, 20907,  3147,  3147,  3147,  3147, 1752,  1752,  1752,  1752,  2707,  2707,  2707,  2707,
                             171,   171,   171,   171, 12403, 12403, 12403, 12403, 12403, 12403, 12403, 12403, 52012, 52012, 52012, 52012,
                           52012, 52012, 52012, 52012,  1907,  1907,  1907,  1907, 1907,  1907,  1907,  1907,  1836,  1836,  1836,  1836,
                            1836,  1836,  1836,  1836, 50791, 50791,   359,   359, 60300, 60300,  1932,  1932,     0,     0,     0,     0
};

u16[16] jqx16 = {KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q,
                 KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q};

u16[16] jqinvx16 = {62209, 62209, 62209, 62209, 62209, 62209, 62209, 62209, 
                    62209, 62209, 62209, 62209, 62209, 62209, 62209, 62209};

u16[16] jvx16 = {20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159,
                 20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159};

u16[16] jfhix16 = {1441, 1441, 1441, 1441, 1441, 1441, 1441, 1441,
                   1441, 1441, 1441, 1441, 1441, 1441, 1441, 1441};

u16[16] jflox16 = {55457, 55457, 55457, 55457, 55457, 55457, 55457, 55457,
                   55457, 55457, 55457, 55457, 55457, 55457, 55457, 55457};

u16[16] maskx16 = {4095, 4095, 4095, 4095, 4095, 4095, 4095, 4095,
                   4095, 4095, 4095, 4095, 4095, 4095, 4095, 4095};

u16[16] hqx16_p1 = {1665, 1665, 1665, 1665, 1665, 1665, 1665, 1665,
                 1665, 1665, 1665, 1665, 1665, 1665, 1665, 1665};

u16[16] hqx16_m1 =  {1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664,
                 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664};

u16[16] hhqx16 = {832, 832, 832, 832, 832, 832, 832, 832,
                  832, 832, 832, 832, 832, 832, 832, 832};

u16[16] mqinvx16 = {80635, 80635, 80635, 80635, 80635, 80635, 80635, 80635,
                    80635, 80635, 80635, 80635, 80635, 80635, 80635, 80635};

u16[16] jdmontx16 = {1353, 1353, 1353, 1353, 1353, 1353, 1353, 1353,
                    1353, 1353, 1353, 1353, 1353, 1353, 1353, 1353};
param int QINV    = 62209;     /* q^(-1) mod 2^16 */
param int MONT    = 2285;      /* 2^16 % Q */
param int BARR    = 20159;     /* (1U << 26)/KYBER_Q + 1 */

inline 
fn __csubq(reg u256 r qx16) -> reg u256
{
  reg u256 t;
  r = #VPSUB_16u16(r, qx16);
  t = #VPSRA_16u16(r, 15);
  t = #VPAND_256(t, qx16);
  r = #VPADD_16u16(t, r);
  return r;
}

inline 
fn __red16x(reg u256 r qx16 vx16) -> reg u256
{
  reg u256 x;
  x = #VPMULH_16u16(r, vx16);
  x = #VPSRA_16u16(x, 10);
  x = #VPMULL_16u16(x, qx16);
  r = #VPSUB_16u16(r, x);
  return r;
}

inline 
fn __fqmulprecomp16x(reg u256 b al ah qx16) -> reg u256
{
  reg u256 x;
  x = #VPMULL_16u16(al, b);
  b = #VPMULH_16u16(ah, b);
  x = #VPMULH_16u16(x, qx16);
  b = #VPSUB_16u16(b, x);
  return b;
}

inline
fn __fqmulx16(reg u256 a b qx16 qinvx16) -> reg u256
{
  reg u256 rd rhi rlo;
  rhi = #VPMULH_16u16(a, b);
  rlo = #VPMULL_16u16(a, b);

  rlo = #VPMULL_16u16(rlo, qinvx16);
  rlo = #VPMULH_16u16(rlo, qx16);
  rd = #VPSUB_16u16(rhi, rlo);

  return rd;
}

inline
fn __fqmul(reg u16 a, reg u16 b) -> reg u16
{
  reg u32 ad;
  reg u32 bd;
  reg u32 c;
  reg u32 t;
  reg u16 r;
  reg u32 u;

  ad = (32s)a;
  bd = (32s)b;

  c = ad * bd;

  u = c * QINV;
  u <<= 16;
  //u = #SAR_32(u, 16);
  u >>s= 16;
  t = u * KYBER_Q;
  t = c - t;
  //t = #SAR_32(t, 16);
  t >>s= 16;
  r = t;
  return r;
}

inline
fn __barrett_reduce(reg u16 a) -> reg u16
{
  reg u32 t;
  reg u16 r;
  t = (32s)a;
  t = t * BARR;
  //t = #SAR_32(t, 26);
  t >>s= 26;
  t *= KYBER_Q;
  r = t;
  r = a;
  r -= t;
  return r;
}

fn _poly_add2(reg ptr u16[KYBER_N] rp bp) -> stack u16[KYBER_N]
{
  inline int i;
  reg u256 a;
  reg u256 b;
  reg u256 r;

  for i = 0 to 16 {
    a = rp.[u256 32*i];
    b = bp.[u256 32*i];
    r = #VPADD_16u16(a, b);
    rp.[u256 32*i] = r;
  }

  return rp;
}

fn _poly_csubq(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 r qx16;
  inline int i;
  
  qx16 = jqx16[u256 0];

  for i=0 to 16 {
    r = rp.[u256 32*i];
    r = __csubq(r, qx16);
    rp.[u256 32*i] = r;
  }

  return rp;
}

inline
fn __w256_interleave_u16(reg u256 al ah) -> reg u256, reg u256 {
 reg u256 a0 a1;

 a0  = #VPUNPCKL_16u16(al, ah);
 a1  = #VPUNPCKH_16u16(al, ah);

 return a0, a1;
}

inline
fn __w256_deintereleave_u16(reg u256 _zero a0 a1) -> reg u256, reg u256 {
  reg u256 al ah;

  al = #VPBLEND_16u16(a0,_zero,0xAA);
  ah = #VPBLEND_16u16(a1,_zero,0xAA);
  al = #VPACKUS_8u32(al, ah);
  a0 = #VPSRL_8u32(a0,16);
  a1 = #VPSRL_8u32(a1,16);
  ah = #VPACKUS_8u32(a0, a1);

  return al, ah;
}

inline
fn __mont_red(reg u256 lo hi qx16 qinvx16) -> reg u256 {
  reg u256 m;

  m  = #VPMULL_16u16(lo, qinvx16);
  m  = #VPMULH_16u16(m, qx16);
  lo = #VPSUB_16u16(hi, m);

  return lo;
}

inline
fn __wmul_16u16(reg u256 x y) -> reg u256, reg u256 {
 reg u256 xyL xyH xy0 xy1;
 xyL = #VPMULL_16u16(x, y);
 xyH = #VPMULH_16u16(x, y);
 xy0, xy1 = __w256_interleave_u16(xyL, xyH);

 return xy0, xy1;
}

inline 
fn __schoolbook16x(reg u256 are aim bre bim zeta zetaqinv qx16 qinvx16, inline int sign) -> reg u256, reg u256
{ reg u256 zaim ac0 ac1 zbd0 zbd1 ad0 ad1 bc0 bc1 x0 x1 y0 y1 _zero;

  zaim = __fqmulprecomp16x(aim, zetaqinv, zeta, qx16);

  ac0, ac1 = __wmul_16u16(are, bre);
  ad0, ad1 = __wmul_16u16(are, bim);
  bc0, bc1 = __wmul_16u16(aim, bre);
  zbd0, zbd1 = __wmul_16u16(zaim, bim);

  if (sign == 0) {
    x0 = #VPADD_8u32(ac0, zbd0);
    x1 = #VPADD_8u32(ac1, zbd1);
  } else {
    x0 = #VPSUB_8u32(ac0, zbd0);
    x1 = #VPSUB_8u32(ac1, zbd1);
  }
  y0 = #VPADD_8u32(bc0, ad0);
  y1 = #VPADD_8u32(bc1, ad1);

  _zero = #set0_256();
  x0, x1 = __w256_deintereleave_u16(_zero, x0, x1);
  y0, y1 = __w256_deintereleave_u16(_zero, y0, y1);
  x0 = __mont_red(x0, x1, qx16, qinvx16);
  y0 = __mont_red(y0, y1, qx16, qinvx16);
  return x0, y0;
}

fn _poly_basemul(reg ptr u16[KYBER_N] rp ap bp) -> reg ptr u16[KYBER_N]
{
  reg u256 zeta zetaqinv qx16 qinvx16 are aim bre bim;
  
  qx16    = jqx16.[u256 0];
  qinvx16 = jqinvx16.[u256 0];
  
  zetaqinv = jzetas_exp.[u256 272];
  zeta = jzetas_exp.[u256 304];

  are = ap.[u256 32*0];
  aim = ap.[u256 32*1];
  bre = bp.[u256 32*0];
  bim = bp.[u256 32*1];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*0] = are;
  rp.[u256 32*1] = aim;

  are = ap.[u256 32*2];
  aim = ap.[u256 32*3];
  bre = bp.[u256 32*2];
  bim = bp.[u256 32*3];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*2] = are;
  rp.[u256 32*3] = aim;

  zetaqinv = jzetas_exp.[u256 336];
  zeta = jzetas_exp.[u256 368];

  are = ap.[u256 32*4];
  aim = ap.[u256 32*5];
  bre = bp.[u256 32*4];
  bim = bp.[u256 32*5];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*4] = are;
  rp.[u256 32*5] = aim;

  are = ap.[u256 32*6];
  aim = ap.[u256 32*7];
  bre = bp.[u256 32*6];
  bim = bp.[u256 32*7];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*6] = are;
  rp.[u256 32*7] = aim;

  zetaqinv = jzetas_exp.[u256 664];
  zeta = jzetas_exp.[u256 696];

  are = ap.[u256 32*8];
  aim = ap.[u256 32*9];
  bre = bp.[u256 32*8];
  bim = bp.[u256 32*9];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*8] = are;
  rp.[u256 32*9] = aim;

  are = ap.[u256 32*10];
  aim = ap.[u256 32*11];
  bre = bp.[u256 32*10];
  bim = bp.[u256 32*11];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*10] = are;
  rp.[u256 32*11] = aim;

  zetaqinv = jzetas_exp.[u256 728];
  zeta = jzetas_exp.[u256 760];

  are = ap.[u256 32*12];
  aim = ap.[u256 32*13];
  bre = bp.[u256 32*12];
  bim = bp.[u256 32*13];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*12] = are;
  rp.[u256 32*13] = aim;

  are = ap.[u256 32*14];
  aim = ap.[u256 32*15];
  bre = bp.[u256 32*14];
  bim = bp.[u256 32*15];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*14] = are;
  rp.[u256 32*15] = aim;

  return rp;
}


u16 pc_shift1_s = 0x200;
u16 pc_mask_s = 0x0F;
u16 pc_shift2_s = 0x1001;
u32[8] pc_permidx_s = {0,4,1,5,2,6,3,7};

fn _poly_compress(reg u64 rp, reg ptr u16[KYBER_N] a) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3 v shift1 mask shift2 permidx;
  reg u128 t0 t1 t3;
  reg ptr u16[16] x16p;
  reg u64 t64;
  reg u32 t32;
  reg u16 t16;

  a = _poly_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  shift1 = #VPBROADCAST_16u16(pc_shift1_s);
  mask = #VPBROADCAST_16u16(pc_mask_s);
  shift2 = #VPBROADCAST_16u16(pc_shift2_s);
  permidx = pc_permidx_s[u256 0];

  for i=0 to KYBER_N/64
  {
    f0 = a[u256 4*i];
    f1 = a[u256 4*i + 1];
    f2 = a[u256 4*i + 2];
    f3 = a[u256 4*i + 3];
    f0 = #VPMULH_16u16(f0, v);
    f1 = #VPMULH_16u16(f1, v);
    f2 = #VPMULH_16u16(f2, v);
    f3 = #VPMULH_16u16(f3, v);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f1 = #VPMULHRS_16u16(f1, shift1);
    f2 = #VPMULHRS_16u16(f2, shift1);
    f3 = #VPMULHRS_16u16(f3, shift1);
    f0 = #VPAND_256(f0, mask);
    f1 = #VPAND_256(f1, mask);
    f2 = #VPAND_256(f2, mask);
    f3 = #VPAND_256(f3, mask);
    f0 = #VPACKUS_16u16(f0, f1);
    f2 = #VPACKUS_16u16(f2, f3);
    f0 = #VPMADDUBSW_256(f0, shift2);
    f2 = #VPMADDUBSW_256(f2, shift2);
    f0 = #VPACKUS_16u16(f0, f2);
    f0 = #VPERMD(permidx, f0);
    (u256)[rp + 32*i] = f0;
  }

  return a;
}

// FIXME: E_EPTR
fn _poly_compress_1(reg ptr u8[KYBER_POLYCOMPRESSEDBYTES] rp, reg ptr u16[KYBER_N] a) -> reg ptr u8[KYBER_POLYCOMPRESSEDBYTES], reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3 v shift1 mask shift2 permidx;
  reg u128 t0 t1 t3;
  reg ptr u16[16] x16p;
  reg u64 t64;
  reg u32 t32;
  reg u16 t16;

  a = _poly_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  shift1 = #VPBROADCAST_16u16(pc_shift1_s);
  mask = #VPBROADCAST_16u16(pc_mask_s);
  shift2 = #VPBROADCAST_16u16(pc_shift2_s);
  permidx = pc_permidx_s[u256 0];

  for i=0 to KYBER_N/64
  {
    f0 = a[u256 4*i];
    f1 = a[u256 4*i + 1];
    f2 = a[u256 4*i + 2];
    f3 = a[u256 4*i + 3];
    f0 = #VPMULH_16u16(f0, v);
    f1 = #VPMULH_16u16(f1, v);
    f2 = #VPMULH_16u16(f2, v);
    f3 = #VPMULH_16u16(f3, v);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f1 = #VPMULHRS_16u16(f1, shift1);
    f2 = #VPMULHRS_16u16(f2, shift1);
    f3 = #VPMULHRS_16u16(f3, shift1);
    f0 = #VPAND_256(f0, mask);
    f1 = #VPAND_256(f1, mask);
    f2 = #VPAND_256(f2, mask);
    f3 = #VPAND_256(f3, mask);
    f0 = #VPACKUS_16u16(f0, f1);
    f2 = #VPACKUS_16u16(f2, f3);
    f0 = #VPMADDUBSW_256(f0, shift2);
    f2 = #VPMADDUBSW_256(f2, shift2);
    f0 = #VPACKUS_16u16(f0, f2);
    f0 = #VPERMD(permidx, f0);
    rp.[u256 32*i] = f0;
  }

  return rp, a;
}



u8[32] pd_jshufbidx = {0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,
                       4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7};
u32 pd_mask_s = 0x00F0000F;
u32 pd_shift_s = 0x800800;

fn _poly_decompress(reg ptr u16[KYBER_N] rp, reg u64 ap) -> stack u16[KYBER_N]
{
  inline int i;
  reg u256 f q shufbidx mask shift;
  reg u128 h;
  reg ptr u16[16] x16p;
  reg ptr u8[32] x32p;
  stack u128 sh;

  x16p = jqx16;
  q = x16p[u256 0];
  x32p = pd_jshufbidx;
  shufbidx = x32p[u256 0];
  mask = #VPBROADCAST_8u32(pd_mask_s);
  shift = #VPBROADCAST_8u32(pd_shift_s);

  f = #set0_256();

  for i=0 to KYBER_N/16
  {
    h = (128u)(u64)[ap + 8*i];
    sh = h;
    f = #VPBROADCAST_2u128(sh);
    f = #VPSHUFB_256(f, shufbidx);
    f = #VPAND_256(f, mask);
    f = #VPMULL_16u16(f, shift);
    f = #VPMULHRS_16u16(f, q);
    rp[u256 i] = f;
  }

  return rp;
}


fn _poly_frombytes(reg ptr u16[KYBER_N] rp, reg u64 ap) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 tt mask;
  reg ptr u16[16] maskp;

  maskp = maskx16;
  mask = maskp[u256 0];

  for i=0 to 2
  {
    t0 = (u256)[ap + 192*i];
    t1 = (u256)[ap + 192*i + 32];
    t2 = (u256)[ap + 192*i + 64];
    t3 = (u256)[ap + 192*i + 96];
    t4 = (u256)[ap + 192*i + 128];
    t5 = (u256)[ap + 192*i + 160];

    tt, t3 = __shuffle8(t0, t3);
    t0, t4 = __shuffle8(t1, t4);
    t1, t5 = __shuffle8(t2, t5);

    t2, t4 = __shuffle4(tt, t4);
    tt, t1 = __shuffle4(t3, t1);
    t3, t5 = __shuffle4(t0, t5);

    t0, t1 = __shuffle2(t2, t1);
    t2, t3 = __shuffle2(t4, t3);
    t4, t5 = __shuffle2(tt, t5);

    t6, t3 = __shuffle1(t0, t3);
    t0, t4 = __shuffle1(t1, t4);
    t1, t5 = __shuffle1(t2, t5);

    t7 = #VPSRL_16u16(t6, 12);
    t8 = #VPSLL_16u16(t3, 4);
    t7 = #VPOR_256(t7, t8);
    t6 = #VPAND_256(mask, t6);
    t7 = #VPAND_256(mask, t7);

    t8 = #VPSRL_16u16(t3, 8);
    t9 = #VPSLL_16u16(t0, 8);
    t8 = #VPOR_256(t8,t9);
    t8 = #VPAND_256(mask,t8);

    t9 = #VPSRL_16u16(t0, 4);
    t9 = #VPAND_256(mask, t9);

    t10 = #VPSRL_16u16(t4, 12);
    t11 = #VPSLL_16u16(t1, 4);
    t10 = #VPOR_256(t10, t11);
    t4 = #VPAND_256(mask,t4);
    t10 = #VPAND_256(mask, t10);

    t11 = #VPSRL_16u16(t1, 8);
    tt = #VPSLL_16u16(t5, 8);
    t11 = #VPOR_256(t11, tt);
    t11 = #VPAND_256(mask, t11);

    tt = #VPSRL_16u16(t5, 4);
    tt = #VPAND_256(mask, tt);

    rp[u256 8*i] = t6;
    rp[u256 8*i + 1] = t7;
    rp[u256 8*i + 2] = t8;
    rp[u256 8*i + 3] = t9;
    rp[u256 8*i + 4] = t4;
    rp[u256 8*i + 5] = t10;
    rp[u256 8*i + 6] = t11;
    rp[u256 8*i + 7] = tt;
  }

  return rp;
}

param int DMONT   = 1353;      /* (1ULL << 32) % KYBER_Q */

fn _poly_frommont(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 t qx16 qinvx16 dmontx16;
  inline int i;
  reg ptr u16[16] x16p;

  x16p = jqx16;
  qx16 = x16p[u256 0];
  x16p = jqinvx16;
  qinvx16 = x16p[u256 0];
  x16p = jdmontx16;
  dmontx16 = x16p[u256 0];

  for i=0 to KYBER_N/16
  {
    t = rp[u256 i];
    t = __fqmulx16(t, dmontx16, qx16, qinvx16);
    rp[u256 i] = t;
  }

  return rp; 
}

u32[4] pfm_shift_s = {3, 2, 1, 0};
u8[16] pfm_idx_s = {0, 1, 4, 5, 8, 9, 12, 13,
                2, 3, 6, 7, 10, 11, 14, 15};

fn _poly_frommsg(reg ptr u16[KYBER_N] rp, reg u64 ap) -> stack u16[KYBER_N]
{
  inline int i;
  reg u256 f g0 g1 g2 g3 g4 h0 h1 h2 h3;
  reg u256 shift idx hqs;
  reg ptr u16[16] x16p;

  x16p = hqx16_p1;
  hqs = x16p[u256 0];
  shift = #VPBROADCAST_2u128(pfm_shift_s[u128 0]);
  idx = #VPBROADCAST_2u128(pfm_idx_s[u128 0]);

  f = (u256)[ap];

  for i=0 to 4
  {
    g3 =  #VPSHUFD_256(f, 0x55*i);
    g3 = #VPSLLV_8u32(g3, shift);
    g3 = #VPSHUFB_256(g3, idx);
    g0 = #VPSLL_16u16(g3,12);
    g1 = #VPSLL_16u16(g3,8);
    g2 = #VPSLL_16u16(g3,4);
    g0 = #VPSRA_16u16(g0,15);
    g1 = #VPSRA_16u16(g1,15);
    g2 = #VPSRA_16u16(g2,15);
    g3 = #VPSRA_16u16(g3,15);
    g0 = #VPAND_256(g0,hqs);
    g1 = #VPAND_256(g1,hqs);
    g2 = #VPAND_256(g2,hqs);
    g3 = #VPAND_256(g3,hqs);
    h0 = #VPUNPCKL_4u64(g0,g1);
    h2 = #VPUNPCKH_4u64(g0,g1);
    h1 = #VPUNPCKL_4u64(g2,g3);
    h3 = #VPUNPCKH_4u64(g2,g3);
    g0 = #VPERM2I128(h0,h1,0x20);
    g2 = #VPERM2I128(h0,h1,0x31);
    g1 = #VPERM2I128(h2,h3,0x20);
    g3 = #VPERM2I128(h2,h3,0x31);
    rp[u256 2*i] = g0;
    rp[u256 2*i + 1] = g1;
    rp[u256 2*i + 8] = g2;
    rp[u256 2*i + 8 + 1] = g3;
  }

  return rp;
}

// FIXME: E_EPTR
fn _poly_frommsg_1(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_INDCPA_MSGBYTES] ap) -> stack u16[KYBER_N]
{
  inline int i;
  reg u256 f g0 g1 g2 g3 g4 h0 h1 h2 h3;
  reg u256 shift idx hqs;
  reg ptr u16[16] x16p;

  x16p = hqx16_p1;
  hqs = x16p[u256 0];
  shift = #VPBROADCAST_2u128(pfm_shift_s[u128 0]);
  idx = #VPBROADCAST_2u128(pfm_idx_s[u128 0]);

  f = ap[u256 0];

  for i=0 to 4
  {
    g3 = #VPSHUFD_256(f, 0x55*i);
    g3 = #VPSLLV_8u32(g3, shift);
    g3 = #VPSHUFB_256(g3, idx);
    g0 = #VPSLL_16u16(g3,12);
    g1 = #VPSLL_16u16(g3,8);
    g2 = #VPSLL_16u16(g3,4);
    g0 = #VPSRA_16u16(g0,15);
    g1 = #VPSRA_16u16(g1,15);
    g2 = #VPSRA_16u16(g2,15);
    g3 = #VPSRA_16u16(g3,15);
    g0 = #VPAND_256(g0,hqs);
    g1 = #VPAND_256(g1,hqs);
    g2 = #VPAND_256(g2,hqs);
    g3 = #VPAND_256(g3,hqs);
    h0 = #VPUNPCKL_4u64(g0,g1);
    h2 = #VPUNPCKH_4u64(g0,g1);
    h1 = #VPUNPCKL_4u64(g2,g3);
    h3 = #VPUNPCKH_4u64(g2,g3);
    g0 = #VPERM2I128(h0,h1,0x20);
    g2 = #VPERM2I128(h0,h1,0x31);
    g1 = #VPERM2I128(h2,h3,0x20);
    g3 = #VPERM2I128(h2,h3,0x31);
    rp[u256 2*i] = g0;
    rp[u256 2*i + 1] = g1;
    rp[u256 2*i + 8] = g2;
    rp[u256 2*i + 8 + 1] = g3;
  }

  return rp;
}

param int NOISE_NBLOCKS = (KYBER_ETA1 * KYBER_N/4 + SHAKE256_RATE - 1)/SHAKE256_RATE;

u8[32] cbd_jshufbidx = {0, 1, 2, -1, 3, 4, 5, -1, 6, 7, 8, -1, 9, 10, 11, -1,
                        4, 5, 6, -1, 7, 8, 9, -1, 10, 11, 12, -1, 13, 14, 15, -1};

inline
fn __cbd3(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8] buf) -> reg ptr u16[KYBER_N]{
  inline int i;
  reg u256 f0 f1 f2 f3;
  reg u256 mask249 mask6DB mask07 mask70 mask3 shufbidx;
  stack u32 mask249_s mask6DB_s mask07_s mask70_s;
  stack u16 mask3_s;

  mask249_s = 0x249249;
  mask6DB_s = 0x6DB6DB;
  mask07_s = 7;
  mask70_s = (7 << 16);
  mask3_s = 3;

  mask249 = #VPBROADCAST_8u32(mask249_s);
  mask6DB = #VPBROADCAST_8u32(mask6DB_s);
  mask07  = #VPBROADCAST_8u32(mask07_s);
  mask70  = #VPBROADCAST_8u32(mask70_s);
  mask3   = #VPBROADCAST_16u16(mask3_s);
  shufbidx = cbd_jshufbidx[u256 0];

  for i=0 to KYBER_N/32
  {
    f0 = buf.[u256 24*i];
    f0 = #VPERMQ(f0, 0x94);
    f0 = #VPSHUFB_256(f0, shufbidx);

    f1 = #VPSRL_8u32(f0, 1);
    f2 = #VPSRL_8u32(f0, 2);
    f0 = #VPAND_256(mask249, f0);
    f1 = #VPAND_256(mask249, f1);
    f2 = #VPAND_256(mask249, f2);
    f0 = #VPADD_8u32(f0, f1);
    f0 = #VPADD_8u32(f0, f2);

    f1 = #VPSRL_8u32(f0, 3);
    f0 = #VPADD_8u32(f0, mask6DB);
    f0 = #VPSUB_8u32(f0, f1);

    f1 = #VPSLL_8u32(f0, 10);
    f2 = #VPSRL_8u32(f0, 12);
    f3 = #VPSRL_8u32(f0, 2);
    f0 = #VPAND_256(f0, mask07);
    f1 = #VPAND_256(f1, mask70);
    f2 = #VPAND_256(f2, mask07);
    f3 = #VPAND_256(f3, mask70);
    f0 = #VPADD_16u16(f0, f1);
    f1 = #VPADD_16u16(f2, f3);
    f0 = #VPSUB_16u16(f0, mask3);
    f1 = #VPSUB_16u16(f1, mask3);

    f2 = #VPUNPCKL_8u32(f0, f1);
    f3 = #VPUNPCKH_8u32(f0, f1);

    f0 = #VPERM2I128(f2, f3, 0x20);
    f1 = #VPERM2I128(f2, f3, 0x31);

    rp[u256 2*i] = f0;
    rp[u256 2*i + 1] = f1;
  }

  return rp;
}


inline
fn __cbd2(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_ETA2*KYBER_N/4] buf) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3;
  reg u256 mask55 mask33 mask03 mask0F;
  reg u128 t;
  stack u32 mask55_s mask33_s mask03_s mask0F_s;

  mask55_s = 0x55555555;
  mask33_s = 0x33333333;
  mask03_s = 0x03030303;
  mask0F_s = 0x0F0F0F0F;

  mask55 = #VPBROADCAST_8u32(mask55_s);
  mask33 = #VPBROADCAST_8u32(mask33_s);
  mask03 = #VPBROADCAST_8u32(mask03_s);
  mask0F = #VPBROADCAST_8u32(mask0F_s);

  for i=0 to KYBER_N/64
  {
    f0 = buf[u256 i];

    f1 = #VPSRL_16u16(f0, 1);
    f0 = #VPAND_256(mask55, f0);
    f1 = #VPAND_256(mask55, f1);
    f0 = #VPADD_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 2);
    f0 = #VPAND_256(mask33, f0);
    f1 = #VPAND_256(mask33, f1);
    f0 = #VPADD_32u8(f0, mask33);
    f0 = #VPSUB_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 4);
    f0 = #VPAND_256(mask0F, f0);
    f1 = #VPAND_256(mask0F, f1);
    f0 = #VPSUB_32u8(f0, mask03);
    f1 = #VPSUB_32u8(f1, mask03);

    f2 = #VPUNPCKL_32u8(f0, f1);
    f3 = #VPUNPCKH_32u8(f0, f1);

    t = (128u)f2;
    f0 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f2, 1);
    f1 = #VPMOVSX_16u8_16u16(t);
    t = (128u)f3;
    f2 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f3, 1);
    f3 = #VPMOVSX_16u8_16u16(t);
    rp[u256 4*i] = f0;
    rp[u256 4*i + 1] = f2;
    rp[u256 4*i + 2] = f1;
    rp[u256 4*i + 3] = f3;
  }

  return rp;
}

/* buf 32 bytes longer for cbd3 (KYBER_ETA1 == 3) */
inline
fn __poly_cbd_eta1(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8] buf) -> reg ptr u16[KYBER_N]
{
  if(KYBER_ETA1 == 2) { // resolved at compile-time
    rp = __cbd2(rp, buf[0:KYBER_ETA2*KYBER_N/4]);
  } else {
    rp = __cbd3(rp, buf);
  }

  return rp;
}

inline
fn __poly_cbd_eta2(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_ETA2*KYBER_N/4] buf) -> reg ptr u16[KYBER_N]
{
  if(KYBER_ETA2 == 2) {
    rp = __cbd2(rp, buf);
  }
  return rp;
}


#[returnaddress="stack"]
fn _poly_getnoise(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_SYMBYTES] seed, reg u8 nonce) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3;
  reg u256 mask55 mask33 mask03 mask0F;
  reg u128 t;
  reg u64 t64;
  stack ptr u16[KYBER_N] srp;
  stack u8[128] buf;
  stack u8[33] extseed;
  stack u32 mask55_s mask33_s mask03_s mask0F_s;

  mask55_s = 0x55555555;
  mask33_s = 0x33333333;
  mask03_s = 0x03030303;
  mask0F_s = 0x0F0F0F0F;

  srp = rp;

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = seed[u64 i];
    extseed[u64 i] = t64;
  }
  extseed[KYBER_SYMBYTES] = nonce;

  buf = _shake256_128_33(buf, extseed);

  mask55 = #VPBROADCAST_8u32(mask55_s);
  mask33 = #VPBROADCAST_8u32(mask33_s);
  mask03 = #VPBROADCAST_8u32(mask03_s);
  mask0F = #VPBROADCAST_8u32(mask0F_s);

  rp = srp;

  for i=0 to KYBER_N/64
  {
    f0 = buf[u256 i];

    f1 = #VPSRL_16u16(f0, 1);
    f0 = #VPAND_256(mask55, f0);
    f1 = #VPAND_256(mask55, f1);
    f0 = #VPADD_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 2);
    f0 = #VPAND_256(mask33, f0);
    f1 = #VPAND_256(mask33, f1);
    f0 = #VPADD_32u8(f0, mask33);
    f0 = #VPSUB_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 4);
    f0 = #VPAND_256(mask0F, f0);
    f1 = #VPAND_256(mask0F, f1);
    f0 = #VPSUB_32u8(f0, mask03);
    f1 = #VPSUB_32u8(f1, mask03);

    f2 = #VPUNPCKL_32u8(f0, f1);
    f3 = #VPUNPCKH_32u8(f0, f1);

    t = (128u)f2;
    f0 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f2, 1);
    f1 = #VPMOVSX_16u8_16u16(t);
    t = (128u)f3;
    f2 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f3, 1);
    f3 = #VPMOVSX_16u8_16u16(t);
    rp[u256 4*i] = f0;
    rp[u256 4*i + 1] = f2;
    rp[u256 4*i + 2] = f1;
    rp[u256 4*i + 3] = f3;
  }

  return rp;
}

inline
fn __shake256_squeezenblocks4x(reg ptr u256[25] state, reg ptr u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3) -> reg ptr u256[25], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE]
{
  inline int i;

  for i = 0 to NOISE_NBLOCKS
  {
    state, buf0[i*SHAKE256_RATE:SHAKE256_RATE], buf1[i*SHAKE256_RATE:SHAKE256_RATE], buf2[i*SHAKE256_RATE:SHAKE256_RATE], buf3[i*SHAKE256_RATE:SHAKE256_RATE] = __shake256_squeezeblock4x(state, buf0[i*SHAKE256_RATE:SHAKE256_RATE], buf1[i*SHAKE256_RATE:SHAKE256_RATE], buf2[i*SHAKE256_RATE:SHAKE256_RATE], buf3[i*SHAKE256_RATE:SHAKE256_RATE]);
  }

  return state, buf0, buf1, buf2, buf3;
}

#[returnaddress="stack"]
fn _poly_getnoise_eta1_4x(
  reg ptr u16[KYBER_N] r0 r1 r2 r3,
  reg ptr u8[KYBER_SYMBYTES] seed,
  reg u8 nonce)
  ->
  reg ptr u16[KYBER_N],
  reg ptr u16[KYBER_N],
  reg ptr u16[KYBER_N],
  reg ptr u16[KYBER_N]
{
  reg u256 f;
  stack u256[25] state;
  stack u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3;
  stack ptr u16[KYBER_N] r0s;

  r0s = r0;

  f = seed[u256 0];
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;

  buf0.[32] = nonce;
  nonce += 1;
  buf1.[32] = nonce;
  nonce += 1;
  buf2.[32] = nonce;
  nonce += 1;
  buf3.[32] = nonce;

  state = _shake256_absorb4x_33(state, buf0[0:33], buf1[0:33], buf2[0:33], buf3[0:33]);
  state, buf0, buf1, buf2, buf3 = __shake256_squeezenblocks4x(state, buf0, buf1, buf2, buf3);
  
  r0 = r0s;
  r0 = __poly_cbd_eta1(r0, buf0[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);
  r1 = __poly_cbd_eta1(r1, buf1[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);
  r2 = __poly_cbd_eta1(r2, buf2[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);
  r3 = __poly_cbd_eta1(r3, buf3[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);

  return r0, r1, r2, r3;
}

#[returnaddress="stack"]
fn _poly_getnoise_eta1122_4x(
  reg ptr u16[KYBER_N] r0 r1 r2 r3,
  reg ptr u8[KYBER_SYMBYTES] seed,
  reg u8 nonce)
  ->
  reg ptr u16[KYBER_N],
  reg ptr u16[KYBER_N],
  reg ptr u16[KYBER_N],
  reg ptr u16[KYBER_N]
{
  reg u256 f;
  stack u256[25] state;
  stack u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3;
  stack ptr u16[KYBER_N] r0s;

  r0s = r0;

  f = seed[u256 0];
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;

  buf0.[32] = nonce;
  nonce += 1;
  buf1.[32] = nonce;
  nonce += 1;
  buf2.[32] = nonce;
  nonce += 1;
  buf3.[32] = nonce;

  state = _shake256_absorb4x_33(state, buf0[0:33], buf1[0:33], buf2[0:33], buf3[0:33]);
  state, buf0, buf1, buf2, buf3 = __shake256_squeezenblocks4x(state, buf0, buf1, buf2, buf3);

  r0 = r0s;
  r0 = __poly_cbd_eta1(r0, buf0[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);
  r1 = __poly_cbd_eta1(r1, buf1[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);
  r2 = __poly_cbd_eta2(r2, buf2[0:KYBER_ETA2*KYBER_N/4]);
  r3 = __poly_cbd_eta2(r3, buf3[0:KYBER_ETA2*KYBER_N/4]);

  return r0, r1, r2, r3;
}


inline
fn __invntt___butterfly64x(reg u256 rl0 rl1 rl2 rl3 rh0 rh1 rh2 rh3 zl0 zl1 zh0 zh1 qx16) 
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7;

  t0  = #VPSUB_16u16(rl0, rh0);
  t1  = #VPSUB_16u16(rl1, rh1);
  t2  = #VPSUB_16u16(rl2, rh2);

  rl0 = #VPADD_16u16(rh0, rl0);
  rl1 = #VPADD_16u16(rh1, rl1);
  rh0 = #VPMULL_16u16(zl0, t0);

  rl2 = #VPADD_16u16(rh2, rl2);
  rh1 = #VPMULL_16u16(zl0, t1);
  t3  = #VPSUB_16u16(rl3, rh3);

  rl3 = #VPADD_16u16(rh3, rl3);
  rh2 = #VPMULL_16u16(zl1, t2);
  rh3 = #VPMULL_16u16(zl1, t3);
  
  t0  = #VPMULH_16u16(zh0, t0);
  t1  = #VPMULH_16u16(zh0, t1);

  t2  = #VPMULH_16u16(zh1, t2);
  t3  = #VPMULH_16u16(zh1, t3);

  // Reduce
  rh0  = #VPMULH_16u16(qx16, rh0);
  rh1  = #VPMULH_16u16(qx16, rh1);
  rh2  = #VPMULH_16u16(qx16, rh2);
  rh3  = #VPMULH_16u16(qx16, rh3);
  
  rh0  = #VPSUB_16u16(t0, rh0);
  rh1  = #VPSUB_16u16(t1, rh1);
  rh2  = #VPSUB_16u16(t2, rh2);
  rh3  = #VPSUB_16u16(t3, rh3);

  return rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3;
}

fn _poly_invntt(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 zeta0 zeta1 zeta2 zeta3 r0 r1 r2 r3 r4 r5 r6 r7 qx16 vx16 flox16 fhix16;
  reg ptr u16[400] zetasp;
  reg ptr u16[16] qx16p;
  inline int i;

  zetasp = jzetas_inv_exp;
  qx16 = jqx16[u256 0];

  for i=0 to 2 
  {
    // level 0:
    zeta0 = zetasp.[u256 0+392*i];
    zeta1 = zetasp.[u256 64+392*i];
    zeta2 = zetasp.[u256 32+392*i];
    zeta3 = zetasp.[u256 96+392*i];

    r0 = rp.[u256 32*0+256*i];
    r1 = rp.[u256 32*1+256*i];
    r2 = rp.[u256 32*2+256*i];
    r3 = rp.[u256 32*3+256*i];
    r4 = rp.[u256 32*4+256*i];
    r5 = rp.[u256 32*5+256*i];
    r6 = rp.[u256 32*6+256*i];
    r7 = rp.[u256 32*7+256*i];

    r0, r1, r4, r5, r2, r3, r6, r7 = __invntt___butterfly64x(r0, r1, r4, r5, r2, r3, r6, r7, zeta0, zeta1, zeta2, zeta3, qx16);

    // level 1:
    vx16 = jvx16[u256 0];
    zeta0 = zetasp.[u256 128+392*i];
    zeta1 = zetasp.[u256 160+392*i];
    r0 = __red16x(r0, qx16, vx16);
    r1 = __red16x(r1, qx16, vx16);
    r4 = __red16x(r4, qx16, vx16);
    r5 = __red16x(r5, qx16, vx16);

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);
    
    r0, r1 = __shuffle1(r0, r1);
    r2, r3 = __shuffle1(r2, r3);
    r4, r5 = __shuffle1(r4, r5);
    r6, r7 = __shuffle1(r6, r7);

    // level 2:
    zeta0 = zetasp.[u256 192+392*i];
    zeta1 = zetasp.[u256 224+392*i];


    r0, r2, r4, r6, r1, r3, r5, r7 = __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r2 = __shuffle2(r0, r2);
    r4, r6 = __shuffle2(r4, r6);
    r1, r3 = __shuffle2(r1, r3);
    r5, r7 = __shuffle2(r5, r7);

    // level 3:
    zeta0 = zetasp.[u256 256+392*i];
    zeta1 = zetasp.[u256 288+392*i];

    r0, r4, r1, r5, r2, r6, r3, r7 = __invntt___butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r4 = __shuffle4(r0, r4);
    r1, r5 = __shuffle4(r1, r5);
    r2, r6 = __shuffle4(r2, r6);
    r3, r7 = __shuffle4(r3, r7);

    // level 4:
    zeta0 = zetasp.[u256 320+392*i];
    zeta1 = zetasp.[u256 352+392*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r1 = __shuffle8(r0, r1);
    r2, r3 = __shuffle8(r2, r3);
    r4, r5 = __shuffle8(r4, r5);
    r6, r7 = __shuffle8(r6, r7);

    // level 5:
    zeta0 = #VPBROADCAST_8u32(zetasp.[u32 384+392*i]);
    zeta1 = #VPBROADCAST_8u32(zetasp.[u32 388+392*i]);

    r0, r2, r4, r6, r1, r3, r5, r7 = __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    if (i==0) {
     rp.[u256 32*0+256*i] = r0;
     rp.[u256 32*1+256*i] = r2;
     rp.[u256 32*2+256*i] = r4;
     rp.[u256 32*3+256*i] = r6;
    }
    rp.[u256 32*4+256*i] = r1;
    rp.[u256 32*5+256*i] = r3;
    rp.[u256 32*6+256*i] = r5;
    rp.[u256 32*7+256*i] = r7;
  }

  zeta0 = #VPBROADCAST_8u32(zetasp.[u32 784]);
  zeta1 = #VPBROADCAST_8u32(zetasp.[u32 788]);

  for i=0 to 2
  {
    if (i == 0) {
     r7 = r6;
     r6 = r4;
     r5 = r2;
     r4 = r0;
    } else {
     r4 = rp.[u256 32*8+128*i];
     r5 = rp.[u256 32*9+128*i];
     r6 = rp.[u256 32*10+128*i];
     r7 = rp.[u256 32*11+128*i];
    }
    r0 = rp.[u256 32*0+128*i];
    r1 = rp.[u256 32*1+128*i];
    r2 = rp.[u256 32*2+128*i];
    r3 = rp.[u256 32*3+128*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    flox16 = jflox16[u256 0];
    fhix16 = jfhix16[u256 0];

    rp.[u256 32*8+128*i]  = r4;
    rp.[u256 32*9+128*i]  = r5;
    rp.[u256 32*10+128*i] = r6;
    rp.[u256 32*11+128*i] = r7;

    r0 = __fqmulprecomp16x(r0, flox16, fhix16, qx16);
    r1 = __fqmulprecomp16x(r1, flox16, fhix16, qx16);
    r2 = __fqmulprecomp16x(r2, flox16, fhix16, qx16);
    r3 = __fqmulprecomp16x(r3, flox16, fhix16, qx16);

    rp.[u256 32*0+128*i] = r0;
    rp.[u256 32*1+128*i] = r1;
    rp.[u256 32*2+128*i] = r2;
    rp.[u256 32*3+128*i] = r3;
  }

  return rp;
}

inline
fn __butterfly64x(reg u256 rl0 rl1 rl2 rl3 rh0 rh1 rh2 rh3 zl0 zl1 zh0 zh1 qx16) 
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7;

  t0 = #VPMULL_16u16(zl0, rh0);
  t1 = #VPMULH_16u16(zh0, rh0);
  t2 = #VPMULL_16u16(zl0, rh1);
  t3 = #VPMULH_16u16(zh0, rh1);
  t4 = #VPMULL_16u16(zl1, rh2);
  t5 = #VPMULH_16u16(zh1, rh2);
  t6 = #VPMULL_16u16(zl1, rh3);
  t7 = #VPMULH_16u16(zh1, rh3);

  t0 = #VPMULH_16u16(t0, qx16);
  t2 = #VPMULH_16u16(t2, qx16);
  t4 = #VPMULH_16u16(t4, qx16);
  t6 = #VPMULH_16u16(t6, qx16);

  //rh1 = #VPSUB_16u16(t3, rl1);
  rh1 = #VPSUB_16u16(rl1, t3);
  rl1 = #VPADD_16u16(t3, rl1);
  //rh0 = #VPSUB_16u16(t1, rl0);
  rh0 = #VPSUB_16u16(rl0, t1);
  rl0 = #VPADD_16u16(t1, rl0);
  //rh3 = #VPSUB_16u16(t7, rl3);
  rh3 = #VPSUB_16u16(rl3, t7);
  rl3 = #VPADD_16u16(t7, rl3);
  //rh2 = #VPSUB_16u16(t5, rl2);
  rh2 = #VPSUB_16u16(rl2, t5);
  rl2 = #VPADD_16u16(t5, rl2);

  rh0 = #VPADD_16u16(t0, rh0);
  //rl0 = #VPSUB_16u16(t0, rl0);
  rl0 = #VPSUB_16u16(rl0, t0);
  rh1 = #VPADD_16u16(t2, rh1);
  //rl1 = #VPSUB_16u16(t2, rl1);
  rl1 = #VPSUB_16u16(rl1, t2);
  rh2 = #VPADD_16u16(t4, rh2);
  //rl2 = #VPSUB_16u16(t4, rl2);
  rl2 = #VPSUB_16u16(rl2, t4);
  rh3 = #VPADD_16u16(t6, rh3);
  //rl3 = #VPSUB_16u16(t6, rl3);
  rl3 = #VPSUB_16u16(rl3, t6);

  return rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3;
}

fn _poly_ntt(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 zeta0 zeta1 zeta2 zeta3 r0 r1 r2 r3 r4 r5 r6 r7 qx16 vx16;
  reg u32 t;
  reg u16 w;
  reg ptr u16[400] zetasp;
  inline int i;

  zetasp = jzetas_exp;
  qx16 = jqx16[u256 0];

  zeta0 = #VPBROADCAST_8u32(zetasp[u32 0]);
  zeta1 = #VPBROADCAST_8u32(zetasp[u32 1]);

  r0 = rp.[u256 32*0];
  r1 = rp.[u256 32*1];
  r2 = rp.[u256 32*2];
  r3 = rp.[u256 32*3];
  r4 = rp.[u256 32*8];
  r5 = rp.[u256 32*9];
  r6 = rp.[u256 32*10];
  r7 = rp.[u256 32*11];

  r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

  rp.[u256 32*0] = r0;
  rp.[u256 32*1] = r1;
  rp.[u256 32*2] = r2;
  rp.[u256 32*3] = r3;
  rp.[u256 32*8] = r4;
  rp.[u256 32*9] = r5;
  rp.[u256 32*10] = r6;
  rp.[u256 32*11] = r7;

  r0 = rp.[u256 32*4];
  r1 = rp.[u256 32*5];
  r2 = rp.[u256 32*6];
  r3 = rp.[u256 32*7];
  r4 = rp.[u256 32*12];
  r5 = rp.[u256 32*13];
  r6 = rp.[u256 32*14];
  r7 = rp.[u256 32*15];

  r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

  /*
   rp.[u256 32*4] = r0;
   rp.[u256 32*5] = r1;
   rp.[u256 32*6] = r2;
   rp.[u256 32*7] = r3;
  */
  rp.[u256 32*12] = r4;
  rp.[u256 32*13] = r5;
  rp.[u256 32*14] = r6;
  rp.[u256 32*15] = r7;

  for i=0 to 2 {

    // level 1
    zeta0 = #VPBROADCAST_8u32(zetasp.[u32 8 + 392*i]);
    zeta1 = #VPBROADCAST_8u32(zetasp.[u32 12 + 392*i]);

    if ( i == 0) {
     r4 = r0;
     r5 = r1;
     r6 = r2;
     r7 = r3;
    } else {
     r4 = rp.[u256 32*4+256*i];
     r5 = rp.[u256 32*5+256*i];
     r6 = rp.[u256 32*6+256*i];
     r7 = rp.[u256 32*7+256*i];
    }
    r0 = rp.[u256 32*0+256*i];
    r1 = rp.[u256 32*1+256*i];
    r2 = rp.[u256 32*2+256*i];
    r3 = rp.[u256 32*3+256*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 2
    zeta0 = zetasp.[u256 16 + 392*i];
    zeta1 = zetasp.[u256 48 + 392*i];

    r0, r4 = __shuffle8(r0, r4);
    r1, r5 = __shuffle8(r1, r5);
    r2, r6 = __shuffle8(r2, r6);
    r3, r7 = __shuffle8(r3, r7);

    r0, r4, r1, r5, r2, r6, r3, r7 = __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 3
    zeta0 = zetasp.[u256 80 + 392*i];
    zeta1 = zetasp.[u256 112 + 392*i];

    r0, r2 = __shuffle4(r0, r2);
    r4, r6 = __shuffle4(r4, r6);
    r1, r3 = __shuffle4(r1, r3);
    r5, r7 = __shuffle4(r5, r7);

    r0, r2, r4, r6, r1, r3, r5, r7 = __butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 4
    zeta0 = zetasp.[u256 144 + 392*i];
    zeta1 = zetasp.[u256 176 + 392*i];

    r0, r1 = __shuffle2(r0, r1);
    r2, r3 = __shuffle2(r2, r3);
    r4, r5 = __shuffle2(r4, r5);
    r6, r7 = __shuffle2(r6, r7);

    r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 5
    zeta0 = zetasp.[u256 208 + 392*i];
    zeta1 = zetasp.[u256 240 + 392*i];

    r0, r4 = __shuffle1(r0, r4);
    r1, r5 = __shuffle1(r1, r5);
    r2, r6 = __shuffle1(r2, r6);
    r3, r7 = __shuffle1(r3, r7);

    r0, r4, r1, r5, r2, r6, r3, r7 = __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 6
    zeta0 = zetasp.[u256 272 + 392*i];
    zeta2 = zetasp.[u256 304 + 392*i];
    zeta1 = zetasp.[u256 336 + 392*i];
    zeta3 = zetasp.[u256 368 + 392*i];

    r0, r4, r2, r6, r1, r5, r3, r7 = __butterfly64x(r0, r4, r2, r6, r1, r5, r3, r7, zeta0, zeta1, zeta2, zeta3, qx16);

    vx16 = jvx16[u256 0];

    r0 = __red16x(r0, qx16, vx16);
    r4 = __red16x(r4, qx16, vx16);
    r2 = __red16x(r2, qx16, vx16);
    r6 = __red16x(r6, qx16, vx16);
    r1 = __red16x(r1, qx16, vx16);
    r5 = __red16x(r5, qx16, vx16);
    r3 = __red16x(r3, qx16, vx16);
    r7 = __red16x(r7, qx16, vx16);

    rp.[u256 32*0+256*i] = r0;
    rp.[u256 32*1+256*i] = r4;
    rp.[u256 32*2+256*i] = r1;
    rp.[u256 32*3+256*i] = r5;
    rp.[u256 32*4+256*i] = r2;
    rp.[u256 32*5+256*i] = r6;
    rp.[u256 32*6+256*i] = r3;
    rp.[u256 32*7+256*i] = r7;
  }

  return rp;
}

inline
fn __poly_reduce(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 r qx16 vx16;
  
  qx16 = jqx16[u256 0];
  vx16 = jvx16[u256 0];

  for i=0 to 16 
  {
    r = rp.[u256 32*i];
    r = __red16x(r, qx16, vx16);
    rp.[u256 32*i] = r;
  }
  return rp;
}

fn _poly_sub(reg ptr u16[KYBER_N] rp ap bp) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 a;
  reg u256 b;
  reg u256 r;

  for i = 0 to 16 {
    a = ap.[u256 32*i];
    b = bp.[u256 32*i];
    r = #VPSUB_16u16(a, b);
    rp.[u256 32*i] = r;
  }

  return rp;
}

fn _poly_tobytes(reg u64 rp, reg ptr u16[KYBER_N] a) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7 qx16 tt ttt;
  reg ptr u16[16] jqx16_p;

  jqx16_p = jqx16;
  qx16 = jqx16_p[u256 0];

  for i = 0 to 2
  {
    t0 = a[u256 8*i];
    t1 = a[u256 8*i + 1];
    t2 = a[u256 8*i + 2];
    t3 = a[u256 8*i + 3];
    t4 = a[u256 8*i + 4];
    t5 = a[u256 8*i + 5];
    t6 = a[u256 8*i + 6];
    t7 = a[u256 8*i + 7];

    t0 = __csubq(t0, qx16);
    t1 = __csubq(t1, qx16);
    t2 = __csubq(t2, qx16);
    t3 = __csubq(t3, qx16);
    t4 = __csubq(t4, qx16);
    t5 = __csubq(t5, qx16);
    t6 = __csubq(t6, qx16);
    t7 = __csubq(t7, qx16);

    tt = #VPSLL_16u16(t1, 12);
    tt |= t0;

    t0 = #VPSRL_16u16(t1, 4);
    t1 = #VPSLL_16u16(t2, 8);
    t0 |= t1;

    t1 = #VPSRL_16u16(t2, 8);
    t2 = #VPSLL_16u16(t3, 4);
    t1 |= t2;

    t2 = #VPSLL_16u16(t5, 12);
    t2 |= t4;

    t3 = #VPSRL_16u16(t5, 4);
    t4 = #VPSLL_16u16(t6, 8);
    t3 |= t4;

    t4 = #VPSRL_16u16(t6, 8);
    t5 = #VPSLL_16u16(t7, 4);
    t4 |= t5;

    ttt, t0 = __shuffle1(tt, t0);
    tt, t2 = __shuffle1(t1, t2);
    t1, t4 = __shuffle1(t3, t4);

    t3, tt= __shuffle2(ttt, tt);
    ttt, t0 = __shuffle2(t1, t0);
    t1, t4 = __shuffle2(t2, t4);

    t2, ttt = __shuffle4(t3, ttt);
    t3, tt = __shuffle4(t1, tt);
    t1, t4 = __shuffle4(t0, t4);

    t0, t3 = __shuffle8(t2, t3);
    t2, ttt = __shuffle8(t1, ttt);
    t1, t4 = __shuffle8(tt, t4);

    (u256)[rp + 192*i] = t0;
    (u256)[rp + 192*i + 32] = t2;
    (u256)[rp + 192*i + 64] = t1;
    (u256)[rp + 192*i + 96] = t3;
    (u256)[rp + 192*i + 128] = ttt;
    (u256)[rp + 192*i + 160] = t4;
  }

  return a;
}

fn _poly_tomsg(reg u64 rp, reg ptr u16[KYBER_N] a) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 g0 g1 hq hhq;
  reg ptr u16[16] px16;
  reg u32 c;

  a = _poly_csubq(a);

  px16 = hqx16_m1;
  hq = px16[u256 0];

  px16 = hhqx16;
  hhq = px16[u256 0];

  for i=0 to KYBER_N/32
  {
    f0 = a[u256 2*i];
    f1 = a[u256 2*i + 1];
    f0 = #VPSUB_16u16(hq, f0);
    f1 = #VPSUB_16u16(hq, f1);
    g0 = #VPSRA_16u16(f0, 15);
    g1 = #VPSRA_16u16(f1, 15);
    f0 = #VPXOR_256(f0, g0);
    f1 = #VPXOR_256(f1, g1);
    f0 = #VPSUB_16u16(f0, hhq);
    f1 = #VPSUB_16u16(f1, hhq);
    f0 = #VPACKSS_16u16(f0, f1);
    f0 = #VPERMQ(f0, 0xD8);
    c = #VPMOVMSKB_u256u32(f0);
    (u32)[rp+4*i] = c;
  }
  return a;
}

// FIXME: E_EPTR
fn _poly_tomsg_1(reg ptr u8[KYBER_INDCPA_MSGBYTES] rp, reg ptr u16[KYBER_N] a) -> reg ptr u8[KYBER_INDCPA_MSGBYTES], reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 g0 g1 hq hhq;
  reg ptr u16[16] px16;
  reg u32 c;

  a = _poly_csubq(a);

  px16 = hqx16_m1;
  hq = px16[u256 0];

  px16 = hhqx16;
  hhq = px16[u256 0];

  for i=0 to KYBER_N/32
  {
    f0 = a[u256 2*i];
    f1 = a[u256 2*i + 1];
    f0 = #VPSUB_16u16(hq, f0);
    f1 = #VPSUB_16u16(hq, f1);
    g0 = #VPSRA_16u16(f0, 15);
    g1 = #VPSRA_16u16(f1, 15);
    f0 = #VPXOR_256(f0, g0);
    f1 = #VPXOR_256(f1, g1);
    f0 = #VPSUB_16u16(f0, hhq);
    f1 = #VPSUB_16u16(f1, hhq);
    f0 = #VPACKSS_16u16(f0, f1);
    f0 = #VPERMQ(f0, 0xD8);
    c = #VPMOVMSKB_u256u32(f0);
    rp[u32 i] = c;
  }
  return rp, a;
}

inline
fn __polyvec_add2(stack u16[KYBER_VECN] r, stack u16[KYBER_VECN] b) -> stack u16[KYBER_VECN]
{
  inline int i;

  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N]         = _poly_add2(r[i*KYBER_N:KYBER_N], b[i*KYBER_N:KYBER_N]);
  }

  return r;
}

inline
fn __polyvec_csubq(stack u16[KYBER_VECN] r) -> stack u16[KYBER_VECN]
{
  inline int i;

  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N] = _poly_csubq(r[i*KYBER_N:KYBER_N]);
  }

  return r;
}

u32 pvd_q_s = 0x0d013404;
u8[32] pvd_shufbdidx_s = {0, 1, 1, 2, 2, 3, 3, 4,
                     5, 6, 6, 7, 7, 8, 8, 9,
                     2, 3, 3, 4, 4, 5, 5, 6,
                     7, 8, 8, 9, 9, 10, 10, 11};
u64 pvd_sllvdidx_s = 0x04;
u32 pvd_mask_s = 0x7fe01ff8;

inline
fn __polyvec_decompress(reg u64 rp) -> stack u16[KYBER_VECN]
{
  inline int i k;
  reg u256 f q shufbidx sllvdidx mask;
  stack u16[KYBER_VECN] r;

  q = #VPBROADCAST_8u32(pvd_q_s);
  shufbidx = pvd_shufbdidx_s[u256 0];
  sllvdidx = #VPBROADCAST_4u64(pvd_sllvdidx_s);
  mask = #VPBROADCAST_8u32(pvd_mask_s);

  for k=0 to KYBER_K
  {
    for i=0 to KYBER_N/16
    {
      f = (u256)[rp + 320 * k + 20 * i];
      f = #VPERMQ(f, 0x94);
      f = #VPSHUFB_256(f, shufbidx);
      f = #VPSLLV_8u32(f, sllvdidx);
      f = #VPSRL_16u16(f, 1);
      f = #VPAND_256(f, mask);
      f = #VPMULHRS_16u16(f, q);
      r[u256 16*k + i] = f;
    }
  }

  return r;
}

u16 pvc_off_s = 0x0f;
u16 pvc_shift1_s = 0x1000;
u16 pvc_mask_s = 0x03ff;
u64 pvc_shift2_s = 0x0400000104000001;
u64 pvc_sllvdidx_s = 0x0C;
u8[32] pvc_shufbidx_s = {0, 1, 2, 3, 4, 8, 9, 10, 11, 12, -1, -1, -1, -1, -1, -1,
                         9, 10, 11, 12, -1, -1, -1, -1, -1, -1, 0, 1, 2, 3, 4, 8};

inline
fn __polyvec_compress(reg u64 rp, stack u16[KYBER_VECN] a)
{
  inline int i;
  reg u256 f0 f1 f2 v v8 off shift1 mask shift2 sllvdidx shufbidx;
  reg u128 t0 t1;
  reg ptr u16[16] x16p;
  reg ptr u8[32] x8p;

  a = __polyvec_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  v8 = #VPSLL_16u16(v, 3);
  off = #VPBROADCAST_16u16(pvc_off_s);
  shift1 = #VPBROADCAST_16u16(pvc_shift1_s);
  mask = #VPBROADCAST_16u16(pvc_mask_s);
  shift2 = #VPBROADCAST_4u64(pvc_shift2_s);
  sllvdidx = #VPBROADCAST_4u64(pvc_sllvdidx_s);
  shufbidx = pvc_shufbidx_s[u256 0];

  for i=0 to KYBER_VECN/16
  {
    f0 = a[u256 i];
    f1 = #VPMULL_16u16(f0, v8);
    f2 = #VPADD_16u16(f0, off);
    f0 = #VPSLL_16u16(f0, 3);
    f0 = #VPMULH_16u16(f0, v);
    f2 = #VPSUB_16u16(f1, f2);
    f1 = #VPANDN_256(f1, f2);
    f1 = #VPSRL_16u16(f1, 15);
    f0 = #VPSUB_16u16(f0, f1);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f0 = #VPAND_256(f0, mask);
    f0 = #VPMADDWD_256(f0, shift2);
    f0 = #VPSLLV_8u32(f0, sllvdidx);
    f0 = #VPSRL_4u64(f0, 12);
    f0 = #VPSHUFB_256(f0, shufbidx);
    t0 = (128u)f0;
    t1 = #VEXTRACTI128(f0, 1);
    t0 = #VPBLEND_8u16(t0, t1, 0xE0);
    (u128)[rp + 20*i] = t0;
    (u32)[rp + 20*i + 16] = #VPEXTR_32(t1, 0);
  }
}

// FIXME: E_PTR
inline
fn __polyvec_compress_1(reg ptr u8[KYBER_POLYVECCOMPRESSEDBYTES] rp, stack u16[KYBER_VECN] a) -> reg ptr u8[KYBER_POLYVECCOMPRESSEDBYTES]
{
  inline int i;
  reg u256 f0 f1 f2 v v8 off shift1 mask shift2 sllvdidx shufbidx;
  reg u128 t0 t1;
  reg ptr u16[16] x16p;
  reg ptr u8[32] x8p;

  a = __polyvec_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  v8 = #VPSLL_16u16(v, 3);
  off = #VPBROADCAST_16u16(pvc_off_s);
  shift1 = #VPBROADCAST_16u16(pvc_shift1_s);
  mask = #VPBROADCAST_16u16(pvc_mask_s);
  shift2 = #VPBROADCAST_4u64(pvc_shift2_s);
  sllvdidx = #VPBROADCAST_4u64(pvc_sllvdidx_s);
  shufbidx = pvc_shufbidx_s[u256 0];

  for i=0 to KYBER_VECN/16
  {
    f0 = a[u256 i];
    f1 = #VPMULL_16u16(f0, v8);
    f2 = #VPADD_16u16(f0, off);
    f0 = #VPSLL_16u16(f0, 3);
    f0 = #VPMULH_16u16(f0, v);
    f2 = #VPSUB_16u16(f1, f2);
    f1 = #VPANDN_256(f1, f2);
    f1 = #VPSRL_16u16(f1, 15);
    f0 = #VPSUB_16u16(f0, f1);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f0 = #VPAND_256(f0, mask);
    f0 = #VPMADDWD_256(f0, shift2);
    f0 = #VPSLLV_8u32(f0, sllvdidx);
    f0 = #VPSRL_4u64(f0, 12);
    f0 = #VPSHUFB_256(f0, shufbidx);
    t0 = (128u)f0;
    t1 = #VEXTRACTI128(f0, 1);
    t0 = #VPBLEND_8u16(t0, t1, 0xE0);
    rp.[u128 20*i] = t0;
    rp.[u32 20*i + 16] = #VPEXTR_32(t1, 0);
  }

  return rp;
}

inline
fn __polyvec_frombytes(reg u64 ap) -> stack u16[KYBER_VECN]
{
  stack u16[KYBER_VECN] r;
  reg u64 pp;
  inline int i;

  pp = ap;
  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N] = _poly_frombytes(r[i*KYBER_N:KYBER_N], pp);
    pp += KYBER_POLYBYTES;
  }

  return r;
}


inline
fn __polyvec_invntt(stack u16[KYBER_VECN] r) -> stack u16[KYBER_VECN]
{
  inline int i;

  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N] = _poly_invntt(r[i*KYBER_N:KYBER_N]);
  }

  return r;
}


inline
fn __polyvec_ntt(stack u16[KYBER_VECN] r) -> stack u16[KYBER_VECN]
{
  inline int i;
  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N] = _poly_ntt(r[i*KYBER_N:KYBER_N]);
  }

  return r;
}


inline
fn __polyvec_reduce(stack u16[KYBER_VECN] r) -> stack u16[KYBER_VECN]
{
  inline int i;

  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N] = __poly_reduce(r[i*KYBER_N:KYBER_N]);
  }

  return r;
}


inline
fn __polyvec_pointwise_acc(stack u16[KYBER_N] r, stack u16[KYBER_VECN] a b) -> stack u16[KYBER_N]
{
  stack u16[KYBER_N] t;
  inline int i;

  r = _poly_basemul(r, a[0:KYBER_N], b[0:KYBER_N]);
  for i=1 to KYBER_K
  {
    t = _poly_basemul(t, a[i*KYBER_N:KYBER_N], b[i*KYBER_N:KYBER_N]);
    r = _poly_add2(r, t);
  }

  // r = __poly_reduce(r);
  
  return r;
}


inline
fn __polyvec_tobytes(reg u64 rp, stack u16[KYBER_VECN] a)
{
  reg u64 pp;
  inline int i;

  pp = rp;
  for i=0 to KYBER_K
  {
    a[i*KYBER_N:KYBER_N] = _poly_tobytes(pp, a[i*KYBER_N:KYBER_N]);
    pp += KYBER_POLYBYTES;
  }
}


inline fn __shake128_squeezeblock4x(
  reg ptr u256[25] state,
  reg ptr u8[SHAKE128_RATE] h0 h1 h2 h3)
  ->
  reg ptr u256[25],
  reg ptr u8[SHAKE128_RATE],
  reg ptr u8[SHAKE128_RATE],
  reg ptr u8[SHAKE128_RATE],
  reg ptr u8[SHAKE128_RATE]
{
  reg u256 t256;
  reg u128 t128;
  inline int i;

  state = _keccakf1600_4x_avx2(state);

	for i = 0 to (SHAKE128_RATE / 8) {
    t256 = state[i];
    t128 = (128u)t256;
		h0[u64 i] = #VMOVLPD(t128);
		h1[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI128(t256, 1);
		h2[u64 i] = #VMOVLPD(t128);
		h3[u64 i] = #VMOVHPD(t128);
	}

  return state, h0, h1, h2, h3;
}

param int GENMATRIX_NBLOCKS = ((12*KYBER_N/8*4096/KYBER_Q + SHAKE128_RATE)/SHAKE128_RATE);
param int REJ_UNIFORM_AVX_BUFLEN = GENMATRIX_NBLOCKS * SHAKE128_RATE;

u8[2048] ru_idx = {-1, -1, -1, -1, -1, -1, -1, -1,
                0, -1, -1, -1, -1, -1, -1, -1,
                2, -1, -1, -1, -1, -1, -1, -1,
                0,  2, -1, -1, -1, -1, -1, -1,
                4, -1, -1, -1, -1, -1, -1, -1,
                0,  4, -1, -1, -1, -1, -1, -1,
                2,  4, -1, -1, -1, -1, -1, -1,
                0,  2,  4, -1, -1, -1, -1, -1,
                6, -1, -1, -1, -1, -1, -1, -1,
                0,  6, -1, -1, -1, -1, -1, -1,
                2,  6, -1, -1, -1, -1, -1, -1,
                0,  2,  6, -1, -1, -1, -1, -1,
                4,  6, -1, -1, -1, -1, -1, -1,
                0,  4,  6, -1, -1, -1, -1, -1,
                2,  4,  6, -1, -1, -1, -1, -1,
                0,  2,  4,  6, -1, -1, -1, -1,
                8, -1, -1, -1, -1, -1, -1, -1,
                0,  8, -1, -1, -1, -1, -1, -1,
                2,  8, -1, -1, -1, -1, -1, -1,
                0,  2,  8, -1, -1, -1, -1, -1,
                4,  8, -1, -1, -1, -1, -1, -1,
                0,  4,  8, -1, -1, -1, -1, -1,
                2,  4,  8, -1, -1, -1, -1, -1,
                0,  2,  4,  8, -1, -1, -1, -1,
                6,  8, -1, -1, -1, -1, -1, -1,
                0,  6,  8, -1, -1, -1, -1, -1,
                2,  6,  8, -1, -1, -1, -1, -1,
                0,  2,  6,  8, -1, -1, -1, -1,
                4,  6,  8, -1, -1, -1, -1, -1,
                0,  4,  6,  8, -1, -1, -1, -1,
                2,  4,  6,  8, -1, -1, -1, -1,
                0,  2,  4,  6,  8, -1, -1, -1,
                10, -1, -1, -1, -1, -1, -1, -1,
                0, 10, -1, -1, -1, -1, -1, -1,
                2, 10, -1, -1, -1, -1, -1, -1,
                0,  2, 10, -1, -1, -1, -1, -1,
                4, 10, -1, -1, -1, -1, -1, -1,
                0,  4, 10, -1, -1, -1, -1, -1,
                2,  4, 10, -1, -1, -1, -1, -1,
                0,  2,  4, 10, -1, -1, -1, -1,
                6, 10, -1, -1, -1, -1, -1, -1,
                0,  6, 10, -1, -1, -1, -1, -1,
                2,  6, 10, -1, -1, -1, -1, -1,
                0,  2,  6, 10, -1, -1, -1, -1,
                4,  6, 10, -1, -1, -1, -1, -1,
                0,  4,  6, 10, -1, -1, -1, -1,
                2,  4,  6, 10, -1, -1, -1, -1,
                0,  2,  4,  6, 10, -1, -1, -1,
                8, 10, -1, -1, -1, -1, -1, -1,
                0,  8, 10, -1, -1, -1, -1, -1,
                2,  8, 10, -1, -1, -1, -1, -1,
                0,  2,  8, 10, -1, -1, -1, -1,
                4,  8, 10, -1, -1, -1, -1, -1,
                0,  4,  8, 10, -1, -1, -1, -1,
                2,  4,  8, 10, -1, -1, -1, -1,
                0,  2,  4,  8, 10, -1, -1, -1,
                6,  8, 10, -1, -1, -1, -1, -1,
                0,  6,  8, 10, -1, -1, -1, -1,
                2,  6,  8, 10, -1, -1, -1, -1,
                0,  2,  6,  8, 10, -1, -1, -1,
                4,  6,  8, 10, -1, -1, -1, -1,
                0,  4,  6,  8, 10, -1, -1, -1,
                2,  4,  6,  8, 10, -1, -1, -1,
                0,  2,  4,  6,  8, 10, -1, -1,
                12, -1, -1, -1, -1, -1, -1, -1,
                0, 12, -1, -1, -1, -1, -1, -1,
                2, 12, -1, -1, -1, -1, -1, -1,
                0,  2, 12, -1, -1, -1, -1, -1,
                4, 12, -1, -1, -1, -1, -1, -1,
                0,  4, 12, -1, -1, -1, -1, -1,
                2,  4, 12, -1, -1, -1, -1, -1,
                0,  2,  4, 12, -1, -1, -1, -1,
                6, 12, -1, -1, -1, -1, -1, -1,
                0,  6, 12, -1, -1, -1, -1, -1,
                2,  6, 12, -1, -1, -1, -1, -1,
                0,  2,  6, 12, -1, -1, -1, -1,
                4,  6, 12, -1, -1, -1, -1, -1,
                0,  4,  6, 12, -1, -1, -1, -1,
                2,  4,  6, 12, -1, -1, -1, -1,
                0,  2,  4,  6, 12, -1, -1, -1,
                8, 12, -1, -1, -1, -1, -1, -1,
                0,  8, 12, -1, -1, -1, -1, -1,
                2,  8, 12, -1, -1, -1, -1, -1,
                0,  2,  8, 12, -1, -1, -1, -1,
                4,  8, 12, -1, -1, -1, -1, -1,
                0,  4,  8, 12, -1, -1, -1, -1,
                2,  4,  8, 12, -1, -1, -1, -1,
                0,  2,  4,  8, 12, -1, -1, -1,
                6,  8, 12, -1, -1, -1, -1, -1,
                0,  6,  8, 12, -1, -1, -1, -1,
                2,  6,  8, 12, -1, -1, -1, -1,
                0,  2,  6,  8, 12, -1, -1, -1,
                4,  6,  8, 12, -1, -1, -1, -1,
                0,  4,  6,  8, 12, -1, -1, -1,
                2,  4,  6,  8, 12, -1, -1, -1,
                0,  2,  4,  6,  8, 12, -1, -1,
                10, 12, -1, -1, -1, -1, -1, -1,
                0, 10, 12, -1, -1, -1, -1, -1,
                2, 10, 12, -1, -1, -1, -1, -1,
                0,  2, 10, 12, -1, -1, -1, -1,
                4, 10, 12, -1, -1, -1, -1, -1,
                0,  4, 10, 12, -1, -1, -1, -1,
                2,  4, 10, 12, -1, -1, -1, -1,
                0,  2,  4, 10, 12, -1, -1, -1,
                6, 10, 12, -1, -1, -1, -1, -1,
                0,  6, 10, 12, -1, -1, -1, -1,
                2,  6, 10, 12, -1, -1, -1, -1,
                0,  2,  6, 10, 12, -1, -1, -1,
                4,  6, 10, 12, -1, -1, -1, -1,
                0,  4,  6, 10, 12, -1, -1, -1,
                2,  4,  6, 10, 12, -1, -1, -1,
                0,  2,  4,  6, 10, 12, -1, -1,
                8, 10, 12, -1, -1, -1, -1, -1,
                0,  8, 10, 12, -1, -1, -1, -1,
                2,  8, 10, 12, -1, -1, -1, -1,
                0,  2,  8, 10, 12, -1, -1, -1,
                4,  8, 10, 12, -1, -1, -1, -1,
                0,  4,  8, 10, 12, -1, -1, -1,
                2,  4,  8, 10, 12, -1, -1, -1,
                0,  2,  4,  8, 10, 12, -1, -1,
                6,  8, 10, 12, -1, -1, -1, -1,
                0,  6,  8, 10, 12, -1, -1, -1,
                2,  6,  8, 10, 12, -1, -1, -1,
                0,  2,  6,  8, 10, 12, -1, -1,
                4,  6,  8, 10, 12, -1, -1, -1,
                0,  4,  6,  8, 10, 12, -1, -1,
                2,  4,  6,  8, 10, 12, -1, -1,
                0,  2,  4,  6,  8, 10, 12, -1,
                14, -1, -1, -1, -1, -1, -1, -1,
                0, 14, -1, -1, -1, -1, -1, -1,
                2, 14, -1, -1, -1, -1, -1, -1,
                0,  2, 14, -1, -1, -1, -1, -1,
                4, 14, -1, -1, -1, -1, -1, -1,
                0,  4, 14, -1, -1, -1, -1, -1,
                2,  4, 14, -1, -1, -1, -1, -1,
                0,  2,  4, 14, -1, -1, -1, -1,
                6, 14, -1, -1, -1, -1, -1, -1,
                0,  6, 14, -1, -1, -1, -1, -1,
                2,  6, 14, -1, -1, -1, -1, -1,
                0,  2,  6, 14, -1, -1, -1, -1,
                4,  6, 14, -1, -1, -1, -1, -1,
                0,  4,  6, 14, -1, -1, -1, -1,
                2,  4,  6, 14, -1, -1, -1, -1,
                0,  2,  4,  6, 14, -1, -1, -1,
                8, 14, -1, -1, -1, -1, -1, -1,
                0,  8, 14, -1, -1, -1, -1, -1,
                2,  8, 14, -1, -1, -1, -1, -1,
                0,  2,  8, 14, -1, -1, -1, -1,
                4,  8, 14, -1, -1, -1, -1, -1,
                0,  4,  8, 14, -1, -1, -1, -1,
                2,  4,  8, 14, -1, -1, -1, -1,
                0,  2,  4,  8, 14, -1, -1, -1,
                6,  8, 14, -1, -1, -1, -1, -1,
                0,  6,  8, 14, -1, -1, -1, -1,
                2,  6,  8, 14, -1, -1, -1, -1,
                0,  2,  6,  8, 14, -1, -1, -1,
                4,  6,  8, 14, -1, -1, -1, -1,
                0,  4,  6,  8, 14, -1, -1, -1,
                2,  4,  6,  8, 14, -1, -1, -1,
                0,  2,  4,  6,  8, 14, -1, -1,
                10, 14, -1, -1, -1, -1, -1, -1,
                0, 10, 14, -1, -1, -1, -1, -1,
                2, 10, 14, -1, -1, -1, -1, -1,
                0,  2, 10, 14, -1, -1, -1, -1,
                4, 10, 14, -1, -1, -1, -1, -1,
                0,  4, 10, 14, -1, -1, -1, -1,
                2,  4, 10, 14, -1, -1, -1, -1,
                0,  2,  4, 10, 14, -1, -1, -1,
                6, 10, 14, -1, -1, -1, -1, -1,
                0,  6, 10, 14, -1, -1, -1, -1,
                2,  6, 10, 14, -1, -1, -1, -1,
                0,  2,  6, 10, 14, -1, -1, -1,
                4,  6, 10, 14, -1, -1, -1, -1,
                0,  4,  6, 10, 14, -1, -1, -1,
                2,  4,  6, 10, 14, -1, -1, -1,
                0,  2,  4,  6, 10, 14, -1, -1,
                8, 10, 14, -1, -1, -1, -1, -1,
                0,  8, 10, 14, -1, -1, -1, -1,
                2,  8, 10, 14, -1, -1, -1, -1,
                0,  2,  8, 10, 14, -1, -1, -1,
                4,  8, 10, 14, -1, -1, -1, -1,
                0,  4,  8, 10, 14, -1, -1, -1,
                2,  4,  8, 10, 14, -1, -1, -1,
                0,  2,  4,  8, 10, 14, -1, -1,
                6,  8, 10, 14, -1, -1, -1, -1,
                0,  6,  8, 10, 14, -1, -1, -1,
                2,  6,  8, 10, 14, -1, -1, -1,
                0,  2,  6,  8, 10, 14, -1, -1,
                4,  6,  8, 10, 14, -1, -1, -1,
                0,  4,  6,  8, 10, 14, -1, -1,
                2,  4,  6,  8, 10, 14, -1, -1,
                0,  2,  4,  6,  8, 10, 14, -1,
                12, 14, -1, -1, -1, -1, -1, -1,
                0, 12, 14, -1, -1, -1, -1, -1,
                2, 12, 14, -1, -1, -1, -1, -1,
                0,  2, 12, 14, -1, -1, -1, -1,
                4, 12, 14, -1, -1, -1, -1, -1,
                0,  4, 12, 14, -1, -1, -1, -1,
                2,  4, 12, 14, -1, -1, -1, -1,
                0,  2,  4, 12, 14, -1, -1, -1,
                6, 12, 14, -1, -1, -1, -1, -1,
                0,  6, 12, 14, -1, -1, -1, -1,
                2,  6, 12, 14, -1, -1, -1, -1,
                0,  2,  6, 12, 14, -1, -1, -1,
                4,  6, 12, 14, -1, -1, -1, -1,
                0,  4,  6, 12, 14, -1, -1, -1,
                2,  4,  6, 12, 14, -1, -1, -1,
                0,  2,  4,  6, 12, 14, -1, -1,
                8, 12, 14, -1, -1, -1, -1, -1,
                0,  8, 12, 14, -1, -1, -1, -1,
                2,  8, 12, 14, -1, -1, -1, -1,
                0,  2,  8, 12, 14, -1, -1, -1,
                4,  8, 12, 14, -1, -1, -1, -1,
                0,  4,  8, 12, 14, -1, -1, -1,
                2,  4,  8, 12, 14, -1, -1, -1,
                0,  2,  4,  8, 12, 14, -1, -1,
                6,  8, 12, 14, -1, -1, -1, -1,
                0,  6,  8, 12, 14, -1, -1, -1,
                2,  6,  8, 12, 14, -1, -1, -1,
                0,  2,  6,  8, 12, 14, -1, -1,
                4,  6,  8, 12, 14, -1, -1, -1,
                0,  4,  6,  8, 12, 14, -1, -1,
                2,  4,  6,  8, 12, 14, -1, -1,
                0,  2,  4,  6,  8, 12, 14, -1,
                10, 12, 14, -1, -1, -1, -1, -1,
                0, 10, 12, 14, -1, -1, -1, -1,
                2, 10, 12, 14, -1, -1, -1, -1,
                0,  2, 10, 12, 14, -1, -1, -1,
                4, 10, 12, 14, -1, -1, -1, -1,
                0,  4, 10, 12, 14, -1, -1, -1,
                2,  4, 10, 12, 14, -1, -1, -1,
                0,  2,  4, 10, 12, 14, -1, -1,
                6, 10, 12, 14, -1, -1, -1, -1,
                0,  6, 10, 12, 14, -1, -1, -1,
                2,  6, 10, 12, 14, -1, -1, -1,
                0,  2,  6, 10, 12, 14, -1, -1,
                4,  6, 10, 12, 14, -1, -1, -1,
                0,  4,  6, 10, 12, 14, -1, -1,
                2,  4,  6, 10, 12, 14, -1, -1,
                0,  2,  4,  6, 10, 12, 14, -1,
                8, 10, 12, 14, -1, -1, -1, -1,
                0,  8, 10, 12, 14, -1, -1, -1,
                2,  8, 10, 12, 14, -1, -1, -1,
                0,  2,  8, 10, 12, 14, -1, -1,
                4,  8, 10, 12, 14, -1, -1, -1,
                0,  4,  8, 10, 12, 14, -1, -1,
                2,  4,  8, 10, 12, 14, -1, -1,
                0,  2,  4,  8, 10, 12, 14, -1,
                6,  8, 10, 12, 14, -1, -1, -1,
                0,  6,  8, 10, 12, 14, -1, -1,
                2,  6,  8, 10, 12, 14, -1, -1,
                0,  2,  6,  8, 10, 12, 14, -1,
                4,  6,  8, 10, 12, 14, -1, -1,
                0,  4,  6,  8, 10, 12, 14, -1,
                2,  4,  6,  8, 10, 12, 14, -1,
                0,  2,  4,  6,  8, 10, 12, 14};

inline
fn __shake128_squeezenblocks(reg u256[7] state, stack u8[REJ_UNIFORM_AVX_BUFLEN] out)
      -> reg u256[7], stack u8[REJ_UNIFORM_AVX_BUFLEN]
{
  inline int i;

  for i = 0 to GENMATRIX_NBLOCKS 
  {
      state, out[i*SHAKE128_RATE:SHAKE128_RATE] = __shake128_squeezeblock(state, out[i*SHAKE128_RATE:SHAKE128_RATE]);
  }
  return state, out;
}

inline
fn __shake128_squeezenblocks4x(reg ptr u256[25] state, reg ptr u8[REJ_UNIFORM_AVX_BUFLEN] h0 h1 h2 h3)
  -> reg ptr u256[25], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN]
{
  inline int i;

  for i = 0 to GENMATRIX_NBLOCKS
  {
    state, h0[i*SHAKE128_RATE:SHAKE128_RATE], h1[i*SHAKE128_RATE:SHAKE128_RATE], h2[i*SHAKE128_RATE:SHAKE128_RATE], h3[i*SHAKE128_RATE:SHAKE128_RATE] = __shake128_squeezeblock4x(state, h0[i*SHAKE128_RATE:SHAKE128_RATE], h1[i*SHAKE128_RATE:SHAKE128_RATE], h2[i*SHAKE128_RATE:SHAKE128_RATE], h3[i*SHAKE128_RATE:SHAKE128_RATE]);
  }

  return state, h0, h1, h2, h3;
}

inline
fn __rej_uniform(reg ptr u16[KYBER_N] rp, reg u64 offset, reg ptr u8[SHAKE128_RATE] buf, inline int buflen) ->  reg u64, stack u16[KYBER_N]
{
  reg u16 val0 val1;
  reg u16 t;
  reg u64 pos ctr;
  reg u8 fl1 fl2;
  reg bool b;

  ctr = offset;
  pos = 0;

  ?{ "<=u" = b }= #CMP_64(ctr, KYBER_N - 1);
  fl1 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(pos, buflen - 3);
  fl2 = #SETcc(b);

  _, _, _, _, b = #TEST_8(fl1, fl2);

  while(!b)
  {
    val0 = (16u)buf[(int)pos];
    pos += 1;

    t   = (16u)buf[(int)pos];
    val1 = t;
    val1 >>= 4;

    t &= 0x0F;
    t <<= 8;
    val0 |= t;
    pos += 1;

    t   = (16u)buf[(int)pos];
    t <<= 4;
    val1 |= t;
    pos += 1;

    if(val0 < KYBER_Q)
    {
      rp[(int)ctr] = val0;
      ctr += 1;
    }

    if(ctr < KYBER_N)
    {
      if(val1 < KYBER_Q)
      {
        rp[(int)ctr] = val1;
        ctr += 1;
      }
    }

    ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 1);
    fl1 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(pos, buflen - 3);
    fl2 = #SETcc(b);

    _, _, _, _, b = #TEST_8(fl1, fl2);
  }

  return ctr, rp;
}

u8 ru_ones_s = 1;
u16 ru_mask_s = 0x0FFF;
u8[32] ru_idx8_s = {0, 1, 1, 2, 3, 4, 4, 5,
                 6, 7, 7, 8, 9, 10, 10, 11,
                 4, 5, 5, 6, 7, 8, 8, 9,
                 10, 11, 11, 12, 13, 14, 14, 15};

fn _rej_uniform_avx(reg ptr u16[KYBER_N] rp, reg ptr u8[REJ_UNIFORM_AVX_BUFLEN] buf) -> reg u64, reg ptr u16[KYBER_N]
{
  reg u256 f0 f1 g0 g1 g2 g3;
  reg u256 bound ones mask idx8;
  reg u128 f t l h;
  reg u64 pos ctr t64 t64_1 t64_2 t64_3;
  reg u64 good;
  reg u16 val0 val1 t16;
  reg ptr u8[2048] idxp;
  reg u8 fl1 fl2;
  reg bool b;

  idxp = ru_idx;

  bound = jqx16[u256 0];
  ctr = 0;
  pos = 0;
  ones = #VPBROADCAST_32u8(ru_ones_s);
  mask = #VPBROADCAST_16u16(ru_mask_s);
  idx8 = ru_idx8_s[u256 0];

  ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 32);
  fl1 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 56);
  fl2 = #SETcc(b);

   _, _, _, _, b = #TEST_8(fl1, fl2);

  while(!b)
  {
    f0 = #VPERMQ(buf.[u256 (int)pos], 0x94);
    f1 = #VPERMQ(buf.[u256 24 + (int)pos], 0x94);
    f0 = #VPSHUFB_256(f0, idx8);
    f1 = #VPSHUFB_256(f1, idx8);
    g0 = #VPSRL_16u16(f0, 4);
    g1 = #VPSRL_16u16(f1, 4);
    f0 = #VPBLEND_16u16(f0, g0, 0xAA);
    f1 = #VPBLEND_16u16(f1, g1, 0xAA);
    f0 = #VPAND_256(f0, mask);
    f1 = #VPAND_256(f1, mask);

    g0 = #VPCMPGT_16u16(bound, f0);
    g1 = #VPCMPGT_16u16(bound, f1);

    g0 = #VPACKSS_16u16(g0, g1);
    good = #VPMOVMSKB_u256u64(g0);

    t64 = good;
    t64 &= 0xFF;
    g0 = (256u) #VMOV(idxp[u64 (int)t64]);

    t64_1 = good;
    t64_1 >>= 16;
    t64_1 &= 0xFF;
    l = #VMOV(idxp[u64 (int)t64_1]);

    t64_2 = good;
    t64_2 >>= 8;
    t64_2 &= 0xFF;
    g1 = (256u) #VMOV(idxp[u64 (int)t64_2]);

    t64_3 = good;
    t64_3 >>= 24;
    t64_3 &= 0xFF;
    h = #VMOV(idxp[u64 (int)t64_3]);

    g0 = #VINSERTI128(g0, l, 1);

    _, _, _, _, _, t64 = #POPCNT_64(t64);
    _, _, _, _, _, t64_1 = #POPCNT_64(t64_1);
    t64 += ctr;

    g1 = #VINSERTI128(g1, h, 1);

    t64_1 += t64;
    _, _, _, _, _, t64_2 = #POPCNT_64(t64_2);
    t64_2 += t64_1;
    _, _, _, _, _, t64_3 = #POPCNT_64(t64_3);
    t64_3 += t64_2;

    g2 = #VPADD_32u8(g0, ones);
    g0 = #VPUNPCKL_32u8(g0, g2);
    g3 = #VPADD_32u8(g1, ones);
    g1 = #VPUNPCKL_32u8(g1, g3);

    f0 = #VPSHUFB_256(f0, g0);
    f1 = #VPSHUFB_256(f1, g1);

    rp.[u128 2*(int)ctr] = (128u)f0;
    rp.[u128 2*(int)t64] = #VEXTRACTI128(f0, 1);
    rp.[u128 2*(int)t64_1] = (128u)f1;
    rp.[u128 2*(int)t64_2] = #VEXTRACTI128(f1, 1);

    ctr = t64_3;

    ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 32);
    fl1 = #SETcc(b);

    pos += 48;
    ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 56);
    fl2 = #SETcc(b);

     _, _, _, _, b = #TEST_8(fl1, fl2);
  }

  ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 8);
  fl1 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 16);
  fl2 = #SETcc(b);

   _, _, _, _, b = #TEST_8(fl1, fl2);

  t64 = 0x5555;
  while(!b)
  {
    f = buf.[u128 (int)pos];
    f = #VPSHUFB_128(f, idx8);
    t = #VPSRL_8u16(f, 4);
    f = #VPBLEND_8u16(f, t, 0xAA);
    f = #VPAND_128(f, mask);

    t = #VPCMPGT_8u16(bound, f);
    good = #VPMOVMSKB_u128u64(t);

    good = #PEXT_64(good, t64);
    l = #VMOV(idxp[u64 (int)good]);
    _, _, _, _, _, good =  #POPCNT_64(good);

    h = #VPADD_16u8(l, ones);
    l = #VPUNPCKL_16u8(l, h);
    f = #VPSHUFB_128(f, l);

    rp.[u128 2*(int)ctr] = f;
    ctr += good;

    pos += 12;
    ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 8);
    fl1 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 16);
    fl2 = #SETcc(b);

     _, _, _, _, b = #TEST_8(fl1, fl2);
  }

  ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 1);
  fl1 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 3);
  fl2 = #SETcc(b);

   _, _, _, _, b = #TEST_8(fl1, fl2);

  while(!b)
  {
    val0 = (16u)buf[(int)pos];
    pos += 1;
    t16 = (16u)buf[(int)pos];
    pos += 1;
    val1 = t16;

    t16 <<= 8;
    val0 |= t16;
    val0 &= 0xFFF;

    val1 >>= 4;
    t16 = (16u)buf[(int)pos];
    pos += 1;
    t16 <<= 4;
    val1 |= t16;

    if(val0 < KYBER_Q)
    {
      rp[(int)ctr] = val0;
      ctr += 1;
    }
    if(val1 < KYBER_Q)
    {
      if(ctr < KYBER_N)
      {
        rp[(int)ctr] = val1;
        ctr += 1;
      }
    }

    ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 1);
    fl1 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 3);
    fl2 = #SETcc(b);

    _, _, _, _, b = #TEST_8(fl1, fl2);
  }

  return ctr, rp;
}

inline
fn __gen_matrix(stack u8[KYBER_SYMBYTES] seed, inline int transposed) -> stack u16[KYBER_K*KYBER_VECN]
{
  stack u8[REJ_UNIFORM_AVX_BUFLEN] buf0;
  stack u8[REJ_UNIFORM_AVX_BUFLEN] buf1;
  stack u8[REJ_UNIFORM_AVX_BUFLEN] buf2;
  stack u8[REJ_UNIFORM_AVX_BUFLEN] buf3;
  reg u256[7] statev;
  stack u64[28] s_state;
  stack u256[25] state;
  stack u16[KYBER_K*KYBER_VECN] rr;
  stack u256 fs;
  reg u256 f;
  reg u64 ctr0 ctr1 ctr2 ctr3 tmp;
  stack u64 ctr0_s;
  reg u8 flg0 flg1 bflg;
  reg bool b;
  reg bool zf;

  inline int i, j;

  f = seed[u256 0];
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;
  fs = f;

  if(transposed == 1)
  {
    buf0[KYBER_SYMBYTES]   = 0;
    buf0[KYBER_SYMBYTES+1] = 0;
    buf1[KYBER_SYMBYTES]   = 0;
    buf1[KYBER_SYMBYTES+1] = 1;
    buf2[KYBER_SYMBYTES]   = 0;
    buf2[KYBER_SYMBYTES+1] = 2;
    buf3[KYBER_SYMBYTES]   = 1;
    buf3[KYBER_SYMBYTES+1] = 0;
  }
  else
  {
    buf0[KYBER_SYMBYTES]   = 0;
    buf0[KYBER_SYMBYTES+1] = 0;
    buf1[KYBER_SYMBYTES]   = 1;
    buf1[KYBER_SYMBYTES+1] = 0;
    buf2[KYBER_SYMBYTES]   = 2;
    buf2[KYBER_SYMBYTES+1] = 0;
    buf3[KYBER_SYMBYTES]   = 0;
    buf3[KYBER_SYMBYTES+1] = 1;
  }

  state = _shake128_absorb4x_34(state, buf0[0:34], buf1[0:34], buf2[0:34], buf3[0:34]);
  state, buf0, buf1, buf2, buf3 = __shake128_squeezenblocks4x(state, buf0, buf1, buf2, buf3);

  tmp, rr[0*KYBER_VECN+0*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[0*KYBER_VECN+0*KYBER_N:KYBER_N], buf0);
  ctr0 = tmp;
  tmp, rr[0*KYBER_VECN+1*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[0*KYBER_VECN+1*KYBER_N:KYBER_N], buf1);
  ctr1 = tmp;
  tmp, rr[0*KYBER_VECN+2*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[0*KYBER_VECN+2*KYBER_N:KYBER_N], buf2);
  ctr2 = tmp;
  ctr3, rr[1*KYBER_VECN+0*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[1*KYBER_VECN+0*KYBER_N:KYBER_N], buf3);

  ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
  flg0 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(ctr1, KYBER_N - 1);
  flg1 = #SETcc(b);

  _, _, _, _, _, bflg = #OR_8(flg0, flg1);

  ?{ "<=u" = b } = #CMP_64(ctr2, KYBER_N - 1);
  flg0 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(ctr3, KYBER_N - 1);
  flg1 = #SETcc(b);

  _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
  _, _, _, _, zf, _ = #OR_8(flg0, bflg);

  while (!zf) {
    state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE] = __shake128_squeezeblock4x(state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE]);

    ctr0, rr[0*KYBER_VECN+0*KYBER_N:KYBER_N] = __rej_uniform(rr[0*KYBER_VECN+0*KYBER_N:KYBER_N], ctr0, buf0[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr1, rr[0*KYBER_VECN+1*KYBER_N:KYBER_N] = __rej_uniform(rr[0*KYBER_VECN+1*KYBER_N:KYBER_N], ctr1, buf1[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr2, rr[0*KYBER_VECN+2*KYBER_N:KYBER_N] = __rej_uniform(rr[0*KYBER_VECN+2*KYBER_N:KYBER_N], ctr2, buf2[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr3, rr[1*KYBER_VECN+0*KYBER_N:KYBER_N] = __rej_uniform(rr[1*KYBER_VECN+0*KYBER_N:KYBER_N], ctr3, buf3[0:SHAKE128_RATE], SHAKE128_RATE);

    ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
    flg0 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(ctr1, KYBER_N - 1);
    flg1 = #SETcc(b);

    _, _, _, _, _, bflg = #OR_8(flg0, flg1);

    ?{ "<=u" = b } = #CMP_64(ctr2, KYBER_N - 1);
    flg0 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(ctr3, KYBER_N - 1);
    flg1 = #SETcc(b);

    _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
    _, _, _, _, zf, _ = #OR_8(flg0, bflg);
  }
  
  f = fs;
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;
  fs = f;

  if(transposed == 1)
  {
    buf0[KYBER_SYMBYTES]   = 1;
    buf0[KYBER_SYMBYTES+1] = 1;
    buf1[KYBER_SYMBYTES]   = 1;
    buf1[KYBER_SYMBYTES+1] = 2;
    buf2[KYBER_SYMBYTES]   = 2;
    buf2[KYBER_SYMBYTES+1] = 0;
    buf3[KYBER_SYMBYTES]   = 2;
    buf3[KYBER_SYMBYTES+1] = 1;
  }
  else
  {
    buf0[KYBER_SYMBYTES]   = 1;
    buf0[KYBER_SYMBYTES+1] = 1;
    buf1[KYBER_SYMBYTES]   = 2;
    buf1[KYBER_SYMBYTES+1] = 1;
    buf2[KYBER_SYMBYTES]   = 0;
    buf2[KYBER_SYMBYTES+1] = 2;
    buf3[KYBER_SYMBYTES]   = 1;
    buf3[KYBER_SYMBYTES+1] = 2;
  }

  state = _shake128_absorb4x_34(state, buf0[0:34], buf1[0:34], buf2[0:34], buf3[0:34]);
  state, buf0, buf1, buf2, buf3 = __shake128_squeezenblocks4x(state, buf0, buf1, buf2, buf3);

  tmp, rr[1*KYBER_VECN+1*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[1*KYBER_VECN+1*KYBER_N:KYBER_N], buf0);
  ctr0 = tmp;
  tmp, rr[1*KYBER_VECN+2*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[1*KYBER_VECN+2*KYBER_N:KYBER_N], buf1);
  ctr1 = tmp;
  tmp, rr[2*KYBER_VECN+0*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[2*KYBER_VECN+0*KYBER_N:KYBER_N], buf2);
  ctr2 = tmp;
  ctr3, rr[2*KYBER_VECN+1*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[2*KYBER_VECN+1*KYBER_N:KYBER_N], buf3);

  ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
  flg0 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(ctr1, KYBER_N - 1);
  flg1 = #SETcc(b);

  _, _, _, _, _, bflg = #OR_8(flg0, flg1);

  ?{ "<=u" = b } = #CMP_64(ctr2, KYBER_N - 1);
  flg0 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(ctr3, KYBER_N - 1);
  flg1 = #SETcc(b);

  _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
  _, _, _, _, zf, _ = #OR_8(flg0, bflg);


  while(!zf) {
    state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE] = __shake128_squeezeblock4x(state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE]);

    ctr0, rr[1*KYBER_VECN+1*KYBER_N:KYBER_N] = __rej_uniform(rr[1*KYBER_VECN+1*KYBER_N:KYBER_N], ctr0, buf0[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr1, rr[1*KYBER_VECN+2*KYBER_N:KYBER_N] = __rej_uniform(rr[1*KYBER_VECN+2*KYBER_N:KYBER_N], ctr1, buf1[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr2, rr[2*KYBER_VECN+0*KYBER_N:KYBER_N] = __rej_uniform(rr[2*KYBER_VECN+0*KYBER_N:KYBER_N], ctr2, buf2[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr3, rr[2*KYBER_VECN+1*KYBER_N:KYBER_N] = __rej_uniform(rr[2*KYBER_VECN+1*KYBER_N:KYBER_N], ctr3, buf3[0:SHAKE128_RATE], SHAKE128_RATE);

    ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
    flg0 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(ctr1, KYBER_N - 1);
    flg1 = #SETcc(b);

    _, _, _, _, _, bflg = #OR_8(flg0, flg1);

    ?{ "<=u" = b } = #CMP_64(ctr2, KYBER_N - 1);
    flg0 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(ctr3, KYBER_N - 1);
    flg1 = #SETcc(b);

    _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
    _, _, _, _, zf, _ = #OR_8(flg0, bflg);
  }

  f = fs;
  buf0[u256 0] = f;
  buf0[KYBER_SYMBYTES]   = 2;
  buf0[KYBER_SYMBYTES+1] = 2;

  statev = __shake128_absorb34(statev, buf0[0:34]);
  statev, buf0 = __shake128_squeezenblocks(statev, buf0);

  // spill state to free registers for rejection sampling
  for i=0 to 7 { s_state[u256 i] = statev[i]; } 

  ctr0, rr[2*KYBER_VECN+2*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[2*KYBER_VECN+2*KYBER_N:KYBER_N], buf0);

  ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
  bflg = #SETcc(b);

  for i=0 to 7 { statev[i] = s_state[u256 i]; }
  
  while(bflg != 0) {
    statev, buf0[0:SHAKE128_RATE] = __shake128_squeezeblock(statev, buf0[0:SHAKE128_RATE]);

    ctr0, rr[2*KYBER_VECN+2*KYBER_N:KYBER_N] = __rej_uniform(rr[2*KYBER_VECN+2*KYBER_N:KYBER_N], ctr0, buf0[0:SHAKE128_RATE], SHAKE128_RATE);

    ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
    bflg = #SETcc(b);
  }

  for i = 0 to KYBER_K
  {
    for j = 0 to KYBER_K
    {
      rr[i*KYBER_VECN+j*KYBER_N:KYBER_N] = _nttunpack(rr[i*KYBER_VECN+j*KYBER_N:KYBER_N]);
    }
  }

  return rr;
}

inline
fn __indcpa_keypair_derand(reg u64 pkp, reg u64 skp, reg ptr u8[KYBER_SYMBYTES] coins)
{
  stack u64 spkp sskp;
  stack u16[KYBER_K*KYBER_VECN] aa;
  stack u16[KYBER_VECN] e pkpv skpv;
  stack u8[64] buf;
  stack u8[KYBER_SYMBYTES] publicseed noiseseed;
  reg u64 t64;
  reg u8 nonce;
  inline int i;

  spkp = pkp;
  sskp = skp;

  buf = _sha3_512_32(buf, coins);

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = buf[u64 i];
    publicseed[u64 i] = t64;
    t64 = buf[u64 i + KYBER_SYMBYTES/8];
    noiseseed[u64 i] = t64;
  }

  aa = __gen_matrix(publicseed, 0);

  nonce = 0;
  skpv[0:KYBER_N], skpv[KYBER_N:KYBER_N], skpv[2*KYBER_N:KYBER_N], e[0:KYBER_N] = _poly_getnoise_eta1_4x(skpv[0:KYBER_N], skpv[KYBER_N:KYBER_N], skpv[2*KYBER_N:KYBER_N], e[0:KYBER_N], noiseseed, nonce);

  nonce = 4;
  e[KYBER_N:KYBER_N], e[2*KYBER_N:KYBER_N], pkpv[0:KYBER_N], pkpv[KYBER_N:KYBER_N] = _poly_getnoise_eta1_4x(e[KYBER_N:KYBER_N], e[2*KYBER_N:KYBER_N], pkpv[0:KYBER_N], pkpv[KYBER_N:KYBER_N], noiseseed, nonce);

  skpv = __polyvec_ntt(skpv);
  e    = __polyvec_ntt(e);

  for i=0 to KYBER_K
  {
    pkpv[i*KYBER_N:KYBER_N] = __polyvec_pointwise_acc(pkpv[i*KYBER_N:KYBER_N], aa[i*KYBER_VECN:KYBER_VECN], skpv);
    pkpv[i*KYBER_N:KYBER_N] = _poly_frommont(pkpv[i*KYBER_N:KYBER_N]);
  }

  pkpv = __polyvec_add2(pkpv, e);
  pkpv = __polyvec_reduce(pkpv);

  pkp = spkp;
  skp = sskp;

  __polyvec_tobytes(skp, skpv);
  __polyvec_tobytes(pkp, pkpv);

  pkp += KYBER_POLYVECBYTES;

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = publicseed[u64 i];
    (u64)[pkp] = t64;
    pkp += 8;
  }
}

// FIXME: E_EPTR
inline
fn __indcpa_enc_0(stack u64 sctp, reg ptr u8[KYBER_INDCPA_MSGBYTES] msgp, reg u64 pkp, reg ptr u8[KYBER_SYMBYTES] noiseseed)
{
  stack u16[KYBER_VECN] pkpv sp ep bp;
  stack u16[KYBER_K*KYBER_VECN] aat;
  stack u16[KYBER_N] k epp v;
  stack u8[KYBER_SYMBYTES] publicseed;
  stack ptr u8[KYBER_SYMBYTES] s_noiseseed;
  reg ptr u8[KYBER_SYMBYTES] lnoiseseed;
  reg u64 i ctp t64;
  reg u8 nonce;
  inline int w;

  pkpv = __polyvec_frombytes(pkp);

  i = 0;
  pkp += KYBER_POLYVECBYTES;
  while (i < KYBER_SYMBYTES/8)
  {
    t64 = (u64)[pkp];
    publicseed[u64 (int)i] = t64;
    pkp += 8;
    i += 1;
  }

  k = _poly_frommsg_1(k, msgp);

  s_noiseseed = noiseseed;
  aat = __gen_matrix(publicseed, 1);
  lnoiseseed = s_noiseseed;

  nonce = 0;
  sp[0:KYBER_N], sp[KYBER_N:KYBER_N], sp[2*KYBER_N:KYBER_N], ep[0:KYBER_N] = _poly_getnoise_eta1_4x(sp[0:KYBER_N], sp[KYBER_N:KYBER_N], sp[2*KYBER_N:KYBER_N], ep[0:KYBER_N], lnoiseseed, nonce);

  nonce = 4;
  ep[KYBER_N:KYBER_N], ep[2*KYBER_N:KYBER_N], epp, bp[0:KYBER_N] = _poly_getnoise_eta1_4x(ep[KYBER_N:KYBER_N], ep[2*KYBER_N:KYBER_N], epp, bp[0:KYBER_N], lnoiseseed, nonce);

  sp = __polyvec_ntt(sp);

  for w=0 to KYBER_K
  {
    bp[w*KYBER_N:KYBER_N] = __polyvec_pointwise_acc(bp[w*KYBER_N:KYBER_N], aat[w*KYBER_VECN:KYBER_VECN], sp);
  }

  v = __polyvec_pointwise_acc(v, pkpv, sp);

  bp = __polyvec_invntt(bp);
  v = _poly_invntt(v);

  bp = __polyvec_add2(bp, ep);
  v = _poly_add2(v, epp);
  v = _poly_add2(v, k);
  bp = __polyvec_reduce(bp);
  v  = __poly_reduce(v);

  ctp = sctp;
  __polyvec_compress(ctp, bp);
  ctp += KYBER_POLYVECCOMPRESSEDBYTES;
  v = _poly_compress(ctp, v);
}

// FIXME: E_EPTR
inline
fn __indcpa_enc_1(reg ptr u8[KYBER_INDCPA_BYTES] ctp, reg ptr u8[KYBER_INDCPA_MSGBYTES] msgp, reg u64 pkp, reg ptr u8[KYBER_SYMBYTES] noiseseed) -> reg ptr u8[KYBER_INDCPA_BYTES]
{
  stack u16[KYBER_VECN] pkpv sp ep bp;
  stack u16[KYBER_K*KYBER_VECN] aat;
  stack u16[KYBER_N] k epp v;
  stack u8[KYBER_SYMBYTES] publicseed;
  stack ptr u8[KYBER_SYMBYTES] s_noiseseed;
  reg ptr u8[KYBER_SYMBYTES] lnoiseseed;
  stack ptr u8[KYBER_INDCPA_BYTES] sctp;
  reg u64 i t64;
  reg u8 nonce;
  inline int w;

  sctp = ctp;

  pkpv = __polyvec_frombytes(pkp);

  i = 0;
  pkp += KYBER_POLYVECBYTES;
  while (i < KYBER_SYMBYTES/8)
  {
    t64 = (u64)[pkp];
    publicseed[u64 (int)i] = t64;
    pkp += 8;
    i += 1;
  }

  k = _poly_frommsg_1(k, msgp);

  s_noiseseed = noiseseed;
  aat = __gen_matrix(publicseed, 1);
  lnoiseseed = s_noiseseed;

  nonce = 0;
  sp[0:KYBER_N], sp[KYBER_N:KYBER_N], sp[2*KYBER_N:KYBER_N], ep[0:KYBER_N] = _poly_getnoise_eta1_4x(sp[0:KYBER_N], sp[KYBER_N:KYBER_N], sp[2*KYBER_N:KYBER_N], ep[0:KYBER_N], lnoiseseed, nonce);

  nonce = 4;
  ep[KYBER_N:KYBER_N], ep[2*KYBER_N:KYBER_N], epp, bp[0:KYBER_N] = _poly_getnoise_eta1_4x(ep[KYBER_N:KYBER_N], ep[2*KYBER_N:KYBER_N], epp, bp[0:KYBER_N], lnoiseseed, nonce);

  sp = __polyvec_ntt(sp);
    
  for w=0 to KYBER_K
  {
    bp[w*KYBER_N:KYBER_N] = __polyvec_pointwise_acc(bp[w*KYBER_N:KYBER_N], aat[w*KYBER_VECN:KYBER_VECN], sp);
  }

  v = __polyvec_pointwise_acc(v, pkpv, sp);

  bp = __polyvec_invntt(bp);
  v = _poly_invntt(v);

  bp = __polyvec_add2(bp, ep);
  v = _poly_add2(v, epp);
  v = _poly_add2(v, k);
  bp = __polyvec_reduce(bp);
  v  = __poly_reduce(v);

  ctp = sctp;
  ctp[0:KYBER_POLYVECCOMPRESSEDBYTES] = __polyvec_compress_1(ctp[0:KYBER_POLYVECCOMPRESSEDBYTES], bp);
  ctp[KYBER_POLYVECCOMPRESSEDBYTES:KYBER_POLYCOMPRESSEDBYTES], v = _poly_compress_1(ctp[KYBER_POLYVECCOMPRESSEDBYTES:KYBER_POLYCOMPRESSEDBYTES], v);

  return ctp;
}


inline
fn __indcpa_dec(reg ptr u8[KYBER_INDCPA_MSGBYTES] msgp, reg u64 ctp, reg u64 skp) -> reg ptr u8[KYBER_INDCPA_MSGBYTES]
{
  stack u16[KYBER_N] t v mp;
  stack u16[KYBER_VECN] bp skpv;

  bp = __polyvec_decompress(ctp);
  ctp += KYBER_POLYVECCOMPRESSEDBYTES;
  v = _poly_decompress(v, ctp);

  skpv = __polyvec_frombytes(skp);
  
  bp = __polyvec_ntt(bp);
  t = __polyvec_pointwise_acc(t, skpv, bp);
  t = _poly_invntt(t);

  mp = _poly_sub(mp, v, t);
  mp = __poly_reduce(mp);
  
  msgp, mp = _poly_tomsg_1(msgp, mp);

  return msgp;
}

inline 
fn __tostack64u8(reg ptr u8[64] out, reg u64 inp) -> reg ptr u8[64] 
{
  reg u8 t;
  inline int i;

  for i=0 to 64 
  {
    t = (u8)[inp + i];
    out[i] = t;
  }
  return out;
}

inline 
fn __tostack32u8(reg ptr u8[32] out, reg u64 inp) -> reg ptr u8[32] 
{
  reg u8 t;
  inline int i;

  for i=0 to 32
  {
    t = (u8)[inp + i];
    out[i] = t;
  }
  return out;
}


inline 
fn __fromstack32u8(reg u64 outp, reg ptr u8[32] in)
{
  reg u8 t;
  inline int i;

  for i=0 to 32
  {
    t = in[i];
    (u8)[outp + i] = t;
  }
}
inline 
fn __verify(reg u64 ctp, reg ptr u8[KYBER_INDCPA_BYTES] ctpc) -> reg u64
{
  reg u256 f g h;
  reg u64 cnd t64;
  reg u8 t1 t2;
  reg bool zf;
  inline int i off;

  cnd = 0;
  t64 = 1;
  h = #set0_256();

  for i=0 to KYBER_INDCPA_BYTES/32
  {
    f = ctpc.[u256 32*i];
    g = (u256)[ctp + 32*i];
    f = #VPXOR_256(f, g);
    h = #VPOR_256(h, f);
  }

  _, _, _, _, zf = #VPTEST_256(h, h);

  cnd = t64 if !zf;

  off = KYBER_CIPHERTEXTBYTES/32 * 32;

  for i=off to KYBER_CIPHERTEXTBYTES
  {
    t1 = ctpc.[i];
    t2 = (u8)[ctp + i];
    t1 ^= t2;
    t64 = (64u)t1;
    cnd |= t64;
  }

  cnd = -cnd;
  cnd >>= 63;

  return cnd;
}

inline
fn __cmov(reg ptr u8[KYBER_SYMBYTES] dst, reg u64 src cnd) -> reg ptr u8[KYBER_SYMBYTES]
{
  reg u256 f g m;
  stack u64 scnd;
  reg u8 t1 t2 bcond;
  inline int i off;

  cnd = -cnd;
  scnd = cnd;

  m = #VPBROADCAST_4u64(scnd);

  for i=0 to KYBER_SYMBYTES/32
  {
    f = dst.[u256 32*i];
    g = (u256)[src + 32*i];
    f = #VPBLENDVB_256(f, g, m);
    dst.[u256 32*i] = f;
  }

  off = KYBER_SYMBYTES/32 * 32;

  bcond = (8u)cnd;
  for i=off to KYBER_SYMBYTES
  {
    t1 = dst.[i];
    t2 = (u8)[src + i];
    t2 = t2 ^ t1;
    t2 = t2 & cnd;
    t1 ^= t2;
    dst.[u8 i] = t1;
  }

  return dst;
}
// Note1: before this file is required, it is necessary to require the correspondent 'verify.jinc':
//        - for 'ref' implementations: crypto_kem/kyber/common/amd64/ref/verify.jinc
//        - for 'avx2' implementations: crypto_kem/kyber/common/amd64/avx2/verify.jinc
//
// Note2: due to the integration of hakyber implementations into libjade, this file is no longer
//        required by kyber768/amd64/ref/

#[returnaddress="stack"]
fn __crypto_kem_keypair_derand_jazz(reg u64 pkp, reg u64 skp, reg ptr u8[2*KYBER_SYMBYTES] coins)
{
  stack u8[32] h_pk;
  stack u64 s_skp s_pkp;
  stack ptr u8[2*KYBER_SYMBYTES] s_coins;
  reg u64 t64;
  inline int i;

  s_coins = coins;
  s_pkp = pkp;
  s_skp = skp;

  __indcpa_keypair_derand(pkp, skp, coins[0:32]);

  skp = s_skp;
  skp += KYBER_POLYVECBYTES;
  pkp = s_pkp;

  for i=0 to KYBER_INDCPA_PUBLICKEYBYTES/8
  {
    t64 = (u64)[pkp + 8*i];
    (u64)[skp] = t64;
    skp += 8;
  }

  s_skp = skp;
  pkp = s_pkp;
  t64 = KYBER_INDCPA_PUBLICKEYBYTES;
  h_pk = _sha3_256(h_pk, pkp, t64);
  skp = s_skp;
  coins = s_coins;

  __fromstack32u8(skp, h_pk);
    skp += 32;
  
  __fromstack32u8(skp, coins[32:32]);
}

#[returnaddress="stack"]
fn __crypto_kem_enc_derand_jazz(reg u64 ctp, reg u64 shkp, reg u64 pkp, reg ptr u8[KYBER_SYMBYTES] coins)
{
  stack u8[KYBER_SYMBYTES * 2] buf kr;
  stack u64 s_pkp s_ctp s_shkp;
  reg u64 t64;
  inline int i;

  s_pkp = pkp;
  s_ctp = ctp;
  s_shkp = shkp;

  buf[0:32] = _sha3_256_32(buf[0:32], coins);

  pkp = s_pkp;

  t64 = KYBER_PUBLICKEYBYTES;
  buf[KYBER_SYMBYTES:32] = _sha3_256(buf[KYBER_SYMBYTES:32], pkp, t64);

  kr = _sha3_512_64(kr, buf);

  pkp = s_pkp;

  __indcpa_enc_0(s_ctp, buf[0:KYBER_INDCPA_MSGBYTES], pkp, kr[KYBER_SYMBYTES:KYBER_SYMBYTES]);

  ctp = s_ctp;
  t64 = KYBER_INDCPA_BYTES;
  kr[KYBER_SYMBYTES:32] = _sha3_256(kr[KYBER_SYMBYTES:32], ctp, t64);

  shkp = s_shkp;
  t64 = KYBER_SSBYTES;
  _shake256_64(shkp, t64, kr);
}

inline
fn __crypto_kem_dec_jazz(reg u64 shkp, reg u64 ctp, reg u64 skp)
{
  stack u8[KYBER_INDCPA_BYTES] ctpc;
  stack u8[2*KYBER_SYMBYTES] kr buf;
  stack u64 s_skp s_ctp s_shkp;
  reg u64 pkp hp zp t64 cnd;
  inline int i;

  s_shkp = shkp;
  s_ctp = ctp;

  buf[0:KYBER_INDCPA_MSGBYTES] = __indcpa_dec(buf[0:KYBER_INDCPA_MSGBYTES], ctp, skp);

  hp = #LEA(skp + 32); //hp = skp + 32;
  hp += 24 * KYBER_K * KYBER_N>>3;

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = (u64)[hp + 8*i];
    buf.[u64 KYBER_SYMBYTES + 8*i] = t64;
  }

  s_skp = skp;

  kr = _sha3_512_64(kr, buf);

  pkp = s_skp;
  pkp += 12 * KYBER_K * KYBER_N>>3;

  ctpc = __indcpa_enc_1(ctpc, buf[0:KYBER_INDCPA_MSGBYTES], pkp, kr[KYBER_SYMBYTES:KYBER_SYMBYTES]);

  ctp = s_ctp;
  cnd = __verify(ctp, ctpc);

  zp = s_skp;
  zp += 64;
  zp += 24 * KYBER_K * KYBER_N>>3;
  kr[0:KYBER_SYMBYTES] = __cmov(kr[0:KYBER_SYMBYTES], zp, cnd);

  t64 = KYBER_INDCPA_BYTES;
  kr[KYBER_SYMBYTES:32] = _sha3_256(kr[KYBER_SYMBYTES:32], ctp, t64);

  shkp = s_shkp;
  t64 = KYBER_SSBYTES;
  _shake256_64(shkp, t64, kr);
}

export fn jade_kem_kyber_kyber768_amd64_avx2_keypair_derand(reg u64 public_key secret_key coins) -> reg u64
{
  reg u64 r;
  stack u8[2*KYBER_SYMBYTES] stack_coins;

  public_key = public_key;
  secret_key = secret_key;
  stack_coins = __tostack64u8(stack_coins, coins);
  __crypto_kem_keypair_derand_jazz(public_key, secret_key, stack_coins);
  ?{}, r = #set0();
  return r;
}

export fn jade_kem_kyber_kyber768_amd64_avx2_keypair(reg u64 public_key secret_key) -> reg u64
{
  reg u64 r;
  stack u8[2*KYBER_SYMBYTES] stack_coins;

  public_key = public_key;
  secret_key = secret_key;
  stack_coins = #randombytes(stack_coins);
  __crypto_kem_keypair_derand_jazz(public_key, secret_key, stack_coins);
  ?{}, r = #set0();
  return r;
}

export fn jade_kem_kyber_kyber768_amd64_avx2_enc_derand(reg u64 ciphertext shared_secret public_key coins) -> reg u64
{
  reg u64 r;
  stack u8[KYBER_SYMBYTES] stack_coins;
  
  ciphertext = ciphertext;
  shared_secret = shared_secret;
  public_key = public_key;
  stack_coins = __tostack32u8(stack_coins, coins);
  __crypto_kem_enc_derand_jazz(ciphertext, shared_secret, public_key, stack_coins);
  ?{}, r = #set0();
  return r;
}

export fn jade_kem_kyber_kyber768_amd64_avx2_enc(reg u64 ciphertext shared_secret public_key) -> reg u64
{
  reg u64 r;
  stack u8[KYBER_SYMBYTES] stack_coins;

  ciphertext = ciphertext;
  shared_secret = shared_secret;
  public_key = public_key;
  stack_coins = #randombytes(stack_coins);
  __crypto_kem_enc_derand_jazz(ciphertext, shared_secret, public_key, stack_coins);
  ?{}, r = #set0();
  return r;
}

export fn jade_kem_kyber_kyber768_amd64_avx2_dec(reg u64 shared_secret ciphertext secret_key) -> reg u64
{
  reg u64 r;
  __crypto_kem_dec_jazz(shared_secret, ciphertext, secret_key);
  ?{}, r = #set0();
  return r;
}
