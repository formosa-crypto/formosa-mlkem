param int KYBER_K = 3;

param int KYBER_Q = 3329;
param int KYBER_N = 256;
param int KYBER_VECN = KYBER_K * KYBER_N;

param int KYBER_SYMBYTES = 32;
param int KYBER_SSBYTES = 32;

param int KYBER_ETA1 = 2;
param int KYBER_ETA2 = 2;

param int KYBER_POLYBYTES = 384;
param int KYBER_POLYVECBYTES = (KYBER_K * KYBER_POLYBYTES);

param int KYBER_POLYCOMPRESSEDBYTES = 128;
param int KYBER_POLYVECCOMPRESSEDBYTES = (KYBER_K * 320);

param int KYBER_INDCPA_MSGBYTES = KYBER_SYMBYTES;
param int KYBER_INDCPA_PUBLICKEYBYTES = KYBER_POLYVECBYTES + KYBER_SYMBYTES;
param int KYBER_INDCPA_SECRETKEYBYTES = KYBER_POLYVECBYTES;
param int KYBER_INDCPA_BYTES = KYBER_POLYVECCOMPRESSEDBYTES + KYBER_POLYCOMPRESSEDBYTES;

param int KYBER_PUBLICKEYBYTES = KYBER_INDCPA_PUBLICKEYBYTES;
param int KYBER_SECRETKEYBYTES = KYBER_INDCPA_SECRETKEYBYTES + KYBER_INDCPA_PUBLICKEYBYTES + 2*KYBER_SYMBYTES;
param int KYBER_CIPHERTEXTBYTES = KYBER_INDCPA_BYTES;

param int KECCAK_ROUNDS=24;

param int KECCAK_ROUNDS=24;


u256[24] KECCAK_IOTAS =
{  (4u64)[0x0000000000000001, 0x0000000000000001, 0x0000000000000001, 0x0000000000000001]
  ,(4u64)[0x0000000000008082, 0x0000000000008082, 0x0000000000008082, 0x0000000000008082]
  ,(4u64)[0x800000000000808a, 0x800000000000808a, 0x800000000000808a, 0x800000000000808a]
  ,(4u64)[0x8000000080008000, 0x8000000080008000, 0x8000000080008000, 0x8000000080008000]
  ,(4u64)[0x000000000000808b, 0x000000000000808b, 0x000000000000808b, 0x000000000000808b]
  ,(4u64)[0x0000000080000001, 0x0000000080000001, 0x0000000080000001, 0x0000000080000001]
  ,(4u64)[0x8000000080008081, 0x8000000080008081, 0x8000000080008081, 0x8000000080008081]
  ,(4u64)[0x8000000000008009, 0x8000000000008009, 0x8000000000008009, 0x8000000000008009]
  ,(4u64)[0x000000000000008a, 0x000000000000008a, 0x000000000000008a, 0x000000000000008a]
  ,(4u64)[0x0000000000000088, 0x0000000000000088, 0x0000000000000088, 0x0000000000000088]
  ,(4u64)[0x0000000080008009, 0x0000000080008009, 0x0000000080008009, 0x0000000080008009]
  ,(4u64)[0x000000008000000a, 0x000000008000000a, 0x000000008000000a, 0x000000008000000a]
  ,(4u64)[0x000000008000808b, 0x000000008000808b, 0x000000008000808b, 0x000000008000808b]
  ,(4u64)[0x800000000000008b, 0x800000000000008b, 0x800000000000008b, 0x800000000000008b]
  ,(4u64)[0x8000000000008089, 0x8000000000008089, 0x8000000000008089, 0x8000000000008089]
  ,(4u64)[0x8000000000008003, 0x8000000000008003, 0x8000000000008003, 0x8000000000008003]
  ,(4u64)[0x8000000000008002, 0x8000000000008002, 0x8000000000008002, 0x8000000000008002]
  ,(4u64)[0x8000000000000080, 0x8000000000000080, 0x8000000000000080, 0x8000000000000080]
  ,(4u64)[0x000000000000800a, 0x000000000000800a, 0x000000000000800a, 0x000000000000800a]
  ,(4u64)[0x800000008000000a, 0x800000008000000a, 0x800000008000000a, 0x800000008000000a]
  ,(4u64)[0x8000000080008081, 0x8000000080008081, 0x8000000080008081, 0x8000000080008081]
  ,(4u64)[0x8000000000008080, 0x8000000000008080, 0x8000000000008080, 0x8000000000008080]
  ,(4u64)[0x0000000080000001, 0x0000000080000001, 0x0000000080000001, 0x0000000080000001]
  ,(4u64)[0x8000000080008008, 0x8000000080008008, 0x8000000080008008, 0x8000000080008008]
};


u256[6] KECCAK_RHOTATES_LEFT = 
{
  (4u64)[41, 36, 18,  3],
  (4u64)[27, 28, 62,  1],
  (4u64)[39, 56,  6, 45],
  (4u64)[ 8, 55, 61, 10],
  (4u64)[20, 25, 15,  2],
  (4u64)[14, 21, 43, 44]
};


u256[6] KECCAK_RHOTATES_RIGHT =
{
  (4u64)[64-41, 64-36, 64-18, 64- 3],
  (4u64)[64-27, 64-28, 64-62, 64- 1],
  (4u64)[64-39, 64-56, 64- 6, 64-45],
  (4u64)[64- 8, 64-55, 64-61, 64-10],
  (4u64)[64-20, 64-25, 64-15, 64- 2],
  (4u64)[64-14, 64-21, 64-43, 64-44]
};


u64[25] KECCAK_A_JAGGED = 
{
   0,  4,  5,  6,  7,
  10, 24, 13, 18, 23,
   8, 16, 25, 22, 15,
  11, 12, 21, 26, 19,
   9, 20, 17, 14, 27
};


inline fn __keccakf1600_avx2(reg u256[7] state) -> reg u256[7]
{
  reg u256[9] t;
  reg u256 c00 c14 d00 d14;

  reg bool zf;
  reg u64 r iotas_o;

  reg ptr u256[24] iotas_p;
  reg ptr u256[6] rhotates_left_p;
  reg ptr u256[6] rhotates_right_p;

  iotas_p = KECCAK_IOTAS;
  iotas_o = 0;
  rhotates_left_p = KECCAK_RHOTATES_LEFT;
  rhotates_right_p = KECCAK_RHOTATES_RIGHT;

  r = KECCAK_ROUNDS;
  while
  {
	  //######################################## Theta
	  c00 = #VPSHUFD_256(state[2], (4u2)[1,0,3,2]);
	  c14 = state[5] ^ state[3];
	  t[2] = state[4] ^ state[6];
	  c14 = c14 ^ state[1];
	  c14 = c14 ^ t[2];
	  t[4] = #VPERMQ(c14, (4u2)[2,1,0,3]);
	  c00 = c00 ^ state[2];
	  t[0] = #VPERMQ(c00, (4u2)[1,0,3,2]);
	  t[1] = c14 >>4u64 63;
	  t[2] = c14 +4u64 c14;
	  t[1] = t[1] | t[2];
	  d14 = #VPERMQ(t[1], (4u2)[0,3,2,1]);
	  d00 = t[1] ^ t[4];
	  d00 = #VPERMQ(d00, (4u2)[0,0,0,0]);
	  c00 = c00 ^ state[0];
	  c00 = c00 ^ t[0];
	  t[0] = c00 >>4u64 63;
	  t[1] = c00 +4u64 c00;
	  t[1] = t[1] | t[0];
	  state[2] = state[2] ^ d00;
	  state[0] = state[0] ^ d00;
	  d14 = #VPBLEND_8u32(d14, t[1], (8u1)[1,1,0,0,0,0,0,0]);
	  t[4] = #VPBLEND_8u32(t[4], c00, (8u1)[0,0,0,0,0,0,1,1]);
	  d14 = d14 ^ t[4];

	  //######################################## Rho + Pi + pre-Chi shuffle
    t[3] = #VPSLLV_4u64(state[2], rhotates_left_p[0] );
	  state[2] = #VPSRLV_4u64(state[2], rhotates_right_p[0] );
	  state[2] = state[2] | t[3];
	  state[3] = state[3] ^ d14;
	  t[4] = #VPSLLV_4u64(state[3], rhotates_left_p[2] );
	  state[3] = #VPSRLV_4u64(state[3], rhotates_right_p[2] );
	  state[3] = state[3] | t[4];
	  state[4] = state[4] ^ d14;
	  t[5] = #VPSLLV_4u64(state[4], rhotates_left_p[3] );
	  state[4] = #VPSRLV_4u64(state[4], rhotates_right_p[3] );
	  state[4] = state[4] | t[5];
	  state[5] = state[5] ^ d14;
	  t[6] = #VPSLLV_4u64(state[5], rhotates_left_p[4] );
	  state[5] = #VPSRLV_4u64(state[5], rhotates_right_p[4] );
	  state[5] = state[5] | t[6];
	  state[6] = state[6] ^ d14;
	  t[3] = #VPERMQ(state[2], (4u2)[2,0,3,1]);
	  t[4] = #VPERMQ(state[3], (4u2)[2,0,3,1]);
	  t[7] = #VPSLLV_4u64(state[6], rhotates_left_p[5] );
	  t[1] = #VPSRLV_4u64(state[6], rhotates_right_p[5] );
	  t[1] = t[1] | t[7];
	  state[1] = state[1] ^ d14;
	  t[5] = #VPERMQ(state[4], (4u2)[0,1,2,3]);
	  t[6] = #VPERMQ(state[5], (4u2)[1,3,0,2]);
	  t[8] = #VPSLLV_4u64(state[1], rhotates_left_p[1] );
	  t[2] = #VPSRLV_4u64(state[1], rhotates_right_p[1] );
	  t[2] = t[2] | t[8];

	  //######################################## Chi
	  t[7] = #VPSRLDQ_256(t[1], 8);
	  t[0] = !t[1] & t[7];
	  state[3] = #VPBLEND_8u32(t[2], t[6], (8u1)[0,0,0,0,1,1,0,0]);
	  t[8] = #VPBLEND_8u32(t[4], t[2], (8u1)[0,0,0,0,1,1,0,0]);
	  state[5] = #VPBLEND_8u32(t[3], t[4], (8u1)[0,0,0,0,1,1,0,0]);
	  t[7] = #VPBLEND_8u32(t[2], t[3], (8u1)[0,0,0,0,1,1,0,0]);
	  state[3] = #VPBLEND_8u32(state[3], t[4], (8u1)[0,0,1,1,0,0,0,0]);
	  t[8] = #VPBLEND_8u32(t[8], t[5], (8u1)[0,0,1,1,0,0,0,0]);
	  state[5] = #VPBLEND_8u32(state[5], t[2], (8u1)[0,0,1,1,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[6], (8u1)[0,0,1,1,0,0,0,0]);
	  state[3] = #VPBLEND_8u32(state[3], t[5], (8u1)[1,1,0,0,0,0,0,0]);
	  t[8] = #VPBLEND_8u32(t[8], t[6], (8u1)[1,1,0,0,0,0,0,0]);
	  state[5] = #VPBLEND_8u32(state[5], t[6], (8u1)[1,1,0,0,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[4], (8u1)[1,1,0,0,0,0,0,0]);
	  state[3] = !state[3] & t[8];
	  state[5] = !state[5] & t[7];
	  state[6] = #VPBLEND_8u32(t[5], t[2], (8u1)[0,0,0,0,1,1,0,0]);
	  t[8] = #VPBLEND_8u32(t[3], t[5], (8u1)[0,0,0,0,1,1,0,0]);
	  state[3] = state[3] ^ t[3];
	  state[6] = #VPBLEND_8u32(state[6], t[3], (8u1)[0,0,1,1,0,0,0,0]);
	  t[8] = #VPBLEND_8u32(t[8], t[4], (8u1)[0,0,1,1,0,0,0,0]);
	  state[5] = state[5] ^ t[5];
	  state[6] = #VPBLEND_8u32(state[6], t[4], (8u1)[1,1,0,0,0,0,0,0]);
	  t[8] = #VPBLEND_8u32(t[8], t[2], (8u1)[1,1,0,0,0,0,0,0]);
	  state[6] = !state[6] & t[8];
	  state[6] = state[6] ^ t[6];
	  state[4] = #VPERMQ(t[1], (4u2)[0,1,3,2]);
	  t[8] = #VPBLEND_8u32(state[4], state[0], (8u1)[0,0,1,1,0,0,0,0]);
	  state[1] = #VPERMQ(t[1], (4u2)[0,3,2,1]);
	  state[1] = #VPBLEND_8u32(state[1], state[0], (8u1)[1,1,0,0,0,0,0,0]);
	  state[1] = !state[1] & t[8];
	  state[2] = #VPBLEND_8u32(t[4], t[5], (8u1)[0,0,0,0,1,1,0,0]);
	  t[7] = #VPBLEND_8u32(t[6], t[4], (8u1)[0,0,0,0,1,1,0,0]);
	  state[2] = #VPBLEND_8u32(state[2], t[6], (8u1)[0,0,1,1,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[3], (8u1)[0,0,1,1,0,0,0,0]);
	  state[2] = #VPBLEND_8u32(state[2], t[3], (8u1)[1,1,0,0,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[5], (8u1)[1,1,0,0,0,0,0,0]);
	  state[2] = !state[2] & t[7];
	  state[2] = state[2] ^ t[2];
	  t[0] = #VPERMQ(t[0], (4u2)[0,0,0,0]);
	  state[3] = #VPERMQ(state[3], (4u2)[0,1,2,3]);
	  state[5] = #VPERMQ(state[5], (4u2)[2,0,3,1]);
	  state[6] = #VPERMQ(state[6], (4u2)[1,3,0,2]);
	  state[4] = #VPBLEND_8u32(t[6], t[3], (8u1)[0,0,0,0,1,1,0,0]);
	  t[7] = #VPBLEND_8u32(t[5], t[6], (8u1)[0,0,0,0,1,1,0,0]);
	  state[4] = #VPBLEND_8u32(state[4], t[5], (8u1)[0,0,1,1,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[2], (8u1)[0,0,1,1,0,0,0,0]);
	  state[4] = #VPBLEND_8u32(state[4], t[2], (8u1)[1,1,0,0,0,0,0,0]);
	  t[7] = #VPBLEND_8u32(t[7], t[3], (8u1)[1,1,0,0,0,0,0,0]);
	  state[4] = !state[4] & t[7];
	  state[0] = state[0] ^ t[0];
	  state[1] = state[1] ^ t[1];
	  state[4] = state[4] ^ t[4];

	  //######################################## Iota
	  state[0] = state[0] ^ iotas_p.[(int) iotas_o];
    iotas_o += 32;

    _,_,_,zf,r = #DEC_64(r);
  }(!zf)

  return state;
}



inline fn __keccak_init_avx2() -> reg u256[7]
{
  inline int i;
  reg u256[7] state;

  for i=0 to 7
  { state[i] = #set0_256(); }

  return state;
}


inline fn __init_s_state_avx2() -> stack u64[28]
{
  inline int i;
  stack u64[28] s_state;
  reg u256 zero;

  zero = #set0_256();
  for i=0 to 7
  { s_state[u256 i] = zero; }

  return s_state;
}


inline fn __add_full_block_avx2(
  reg u256[7] state,
  stack u64[28] s_state,
  reg ptr u64[25] a_jagged_p,
  reg u64 in inlen,
  reg u64 rate
) -> reg u256[7], stack u64[28], reg u64, reg u64
{

  inline int i;
  reg u64 j l t rate8;
  reg u8 c;

  rate8 = rate;
  rate8 >>= 3;
  j = 0;
  while ( j < rate8 )
  {
    t = [in + 8*j];
    l = a_jagged_p[(int) j];
    s_state[(int) l] = t;
    j += 1;
  }

  //TODO: check & change to #VPBROADCAST_4u64
  t = s_state[0];
  s_state[1] = t;
  s_state[2] = t;
  s_state[3] = t;

  for i = 0 to 7
  { state[i] ^= s_state[u256 i]; }

  in += rate;
  inlen -= rate;

  return state, s_state, in, inlen;
}


// TODO: refactor when this feature is available: https://github.com/haslab/libjbn/wiki/Feature-request-%231#procedural-parameters
inline fn __add_final_block_avx2(
  reg  u256[7] state,
  stack u64[28] s_state,
  reg ptr u64[25] a_jagged_p,
  reg   u64 in inlen,
  reg   u8  trail_byte,
  reg   u64 rate
) -> reg u256[7]
{
  inline int i;
  reg u64 j l t inlen8;
  reg u8 c;

  s_state = __init_s_state_avx2();

  inlen8 = inlen;
  inlen8 >>= 3;
  j = 0;
  while ( j < inlen8 )
  {
    t = [in + 8*j];
    l = a_jagged_p[(int) j];
    s_state[(int) l] = t;
    j += 1;
  }
  l = a_jagged_p[(int) j];
  l <<= 3;
  j <<= 3;

  while ( j < inlen )
  {
    c = (u8)[in + j];
    s_state[u8 (int) l] = c;
    j += 1;
    l += 1;
  }

  s_state[u8 (int) l] = trail_byte;

  // j  = (rate-1) >> 3;
  j = rate; j -= 1; j >>= 3;
  l  = a_jagged_p[(int) j];
  l <<= 3;
  // l += ((rate-1) & 0x7)
  j = rate; j -= 1; j &= 0x7;
  l += j;

  s_state[u8 (int) l] ^= 0x80;

  t = s_state[0];
  s_state[1] = t;
  s_state[2] = t;
  s_state[3] = t;

  for i = 0 to 7
  { state[i] ^= s_state[u256 i]; }

  return state;
}


// obs: @pre: len <= rate_in_bytes
inline fn __xtr_full_block_avx2(
  reg u256[7] state,
  reg ptr u64[25] a_jagged_p,
  reg u64 out,
  reg u64 len
) -> reg u64
{
  inline int i;
  stack u64[28] s_state;
  reg u64 j l t len8;
  reg u8 c;

  for i = 0 to 7
  { s_state[u256 i] = state[i]; }

  len8 = len;
  len8 >>= 3;
  j = 0;
  while ( j < len8 )
  {
    l = a_jagged_p[(int) j];
    t = s_state[(int) l];
    [out + 8*j] = t;
    j += 1;
  }

  out += len;

  return out;
}


// obs: @pre: len <= rate_in_bytes
inline fn __xtr_bytes_avx2(
  reg u256[7] state,
  reg ptr u64[25] a_jagged_p,
  reg u64 out,
  reg u64 len
) -> reg u64
{
  inline int i;
  stack u64[28] s_state;
  reg u64 j l t len8;
  reg u8 c;

  for i = 0 to 7
  { s_state[u256 i] = state[i]; }

  len8 = len;
  len8 >>= 3;
  j = 0;
  while ( j < len8 )
  { l = a_jagged_p[(int) j];
    t = s_state[(int) l];
    [out + 8*j] = t;
    j += 1;
  }
  l = a_jagged_p[(int)j];
  j <<= 3;
  l <<= 3;

  while ( j < len )
  {
    c = s_state[u8 (int) l];
    (u8)[out + j] = c;
    j += 1;
    l += 1;
  }

  out += len;

  return out;
}


inline fn __absorb_avx2(
  reg u256[7] state,
  reg u64 in inlen,
  reg u8  trail_byte,
  reg u64 rate
) -> reg u256[7]
{
  stack u64[28] s_state;
  reg ptr u64[25] a_jagged_p;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  // intermediate blocks
  while ( inlen >= rate )
  {
    state, s_state, in, inlen = __add_full_block_avx2(state, s_state, a_jagged_p, in, inlen, rate);
    state = __keccakf1600_avx2(state);
  }

  // final block
  state = __add_final_block_avx2(state, s_state, a_jagged_p, in, inlen, trail_byte, rate);

  return state;
}


inline fn __squeeze_avx2(reg u256[7] state, reg u64 out outlen rate)
{
  reg ptr u64[25] a_jagged_p;

  a_jagged_p = KECCAK_A_JAGGED;

  // intermediate blocks
  while ( outlen > rate )
  {
    state = __keccakf1600_avx2(state);
    out = __xtr_full_block_avx2(state, a_jagged_p, out, rate);
    outlen -= rate;
  }

  state = __keccakf1600_avx2(state);
  out = __xtr_bytes_avx2(state, a_jagged_p, out, outlen);
}


inline fn __keccak1600_avx2(reg u64 out outlen in inlen, reg u8 trail_byte, reg u64 rate)
{
  reg u256[7] state;

  state = __keccak_init_avx2();

  // absorb
  state = __absorb_avx2(state, in, inlen, trail_byte, rate);

  // squeeze
  __squeeze_avx2(state, out, outlen, rate);
}


fn _keccak1600_avx2(reg u64 out outlen in inlen, reg u8 trail_byte, reg u64 rate)
{
  __keccak1600_avx2(out, outlen, in, inlen, trail_byte, rate);
}


param int SHAKE128_RATE = 168;
param int SHAKE256_RATE = 136;
param int SHA3_256_RATE = 136;
param int SHA3_512_RATE = 72;

#[returnaddress="stack"]
fn _isha3_256(reg ptr u8[32] out, reg u64 in inlen) -> reg ptr u8[32]
{
  reg u256[7] state;
  stack u64[28] s_state;
  reg ptr u64[25] a_jagged_p;
  reg u64 t l;
  reg u8 trail_byte;
  inline int i;

  state = __keccak_init_avx2();

  trail_byte = 0x06;
  t = SHA3_256_RATE;
  state = __absorb_avx2(state, in, inlen, trail_byte, t);

  state = __keccakf1600_avx2(state);

  for i=0 to 7 { s_state[u256 i] = state[i]; }
  
  a_jagged_p = KECCAK_A_JAGGED;

  for i=0 to 4
  {
    l = a_jagged_p[i];
    t = s_state[(int) l];
    out[u64 i] = t;
  }

  return out;
}


#[returnaddress="stack"]
fn _isha3_256_32(reg ptr u8[32] out, reg ptr u8[KYBER_SYMBYTES] in) -> reg ptr u8[32]
{
  reg u256[7] state;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  state[0] = #VPBROADCAST_4u64(in[u64 0]);

  for i=1 to KYBER_SYMBYTES/8
  {
    t = in[u64 i];
    l = a_jagged_p[i];
    s_state[(int) l] = t;
  }

  l = a_jagged_p[KYBER_SYMBYTES/8];
  l <<= 3;
  s_state[u8 (int)l] = 0x06;

  l = a_jagged_p[(SHA3_256_RATE-1)/8];
  l <<= 3;
  t = SHA3_256_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] ^= 0x80;

  for i=1 to 7 { state[i] = s_state[u256 i]; }
  
  state = __keccakf1600_avx2(state);

  for i=0 to 7 { s_state[u256 i] = state[i]; }

  for i=0 to 4
  {
    l = a_jagged_p[i];
    t = s_state[(int)l];
    out[u64 i] = t;
  }

  return out;
}


#[returnaddress="stack"]
fn _shake256_64(reg u64 out outlen, reg const ptr u8[64] in)
{
  reg u256[7] state;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  
  for i=1 to 8
  {
    l = a_jagged_p[i];
    t = in[u64 i];
    s_state[(int)l] = t;
  }

  l = a_jagged_p[8];
  l <<= 3;

  s_state[u8 (int)l] = 0x1f;

  l = a_jagged_p[(SHAKE256_RATE-1)/8];
  l <<= 3;
  t = SHAKE256_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] ^= 0x80;

  for i=1 to 7
  { state[i] = s_state[u256 i]; }
  
  t = SHAKE256_RATE;
  __squeeze_avx2(state, out, outlen, t);
}


#[returnaddress="stack"]
fn _ishake256_128_33(reg ptr u8[128] out, reg const ptr u8[33] in) -> stack u8[128]
{
  reg u256[7] state;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  reg u8 c;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  state[0] = #VPBROADCAST_4u64(in[u64 0]);

  for i = 1 to 4
  {
    t = in[u64 i];
    l = a_jagged_p[i];
    s_state[(int)l] = t;
  }

  c = in[u8 32];
  l = a_jagged_p[4];
  l <<= 3;
  s_state[u8 (int)l] = c;
  
  l += 1;
  s_state[u8 (int)l] = 0x1f;

  l = a_jagged_p[(SHAKE256_RATE-1)/8];
  l <<= 3;
  t = SHAKE256_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] ^= 0x80;

  for i=1 to 7
  { state[i] = s_state[u256 i]; }

  state = __keccakf1600_avx2(state);

  for i=0 to 7
  { s_state[u256 i] = state[i]; }

  for i = 0 to 16
  {
    l = a_jagged_p[i];
    t = s_state[(int)l];
    out[u64 i] = t;
  }

  return out;
}

#[returnaddress="stack"]
fn _isha3_512_64(reg ptr u8[64] out, reg const ptr u8[64] in) -> stack u8[64]
{
  reg u256[7] state;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  
  for i = 1 to 8
  {
    t = in[u64 i];
    l = a_jagged_p[i];
    s_state[(int)l] = t;
  }

  l = a_jagged_p[8];
  l <<= 3;
  s_state[u8 (int)l] = 0x06;

  l = a_jagged_p[(SHA3_512_RATE-1)/8];
  l <<= 3;
  t = SHA3_512_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] ^= 0x80;

  for i=1 to 7
  { state[i] = s_state[u256 i]; }
  
  state = __keccakf1600_avx2(state);

  for i=0 to 7
  { s_state[u256 i] = state[i]; }

  for i = 0 to 8
  {
    l = a_jagged_p[i];
    t = s_state[(int) l];
    out[u64 i] = t;
  }

  return out;
}

#[returnaddress="stack"]
fn _isha3_512_32(reg ptr u8[64] out, reg const ptr u8[32] in) -> stack u8[64]
{
  reg u256[7] state;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  
  for i = 1 to 4
  {
    t = in[u64 i];
    l = a_jagged_p[i];
    s_state[(int)l] = t;
  }

  l = a_jagged_p[4];
  l <<= 3;
  s_state[u8 (int)l] = 0x06;

  l = a_jagged_p[(SHA3_512_RATE-1)/8];
  l <<= 3;
  t = SHA3_512_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] ^= 0x80;

  for i=1 to 7
  { state[i] = s_state[u256 i]; }
  
  state = __keccakf1600_avx2(state);

  for i=0 to 7
  { s_state[u256 i] = state[i]; }

  for i = 0 to 8
  {
    l = a_jagged_p[i];
    t = s_state[(int) l];
    out[u64 i] = t;
  }

  return out;
}

//FIXME: remove inline when register arrays are supported in "real" functions
inline
fn __shake128_absorb34(reg u256[7] state, reg const ptr u8[34] in) -> reg u256[7]
{
  reg u128 t128;
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 l t;
  reg u8 t8;
  inline int i;

  a_jagged_p = KECCAK_A_JAGGED;
  s_state = __init_s_state_avx2();
  
  state[0] = #VPBROADCAST_4u64(in[u64 0]);

  for i=1 to 4
  {
    t = in[u64 i];
    l = a_jagged_p[i];
    s_state[(int)l] = t;
  }

  t8 = in[32];
  l = a_jagged_p[(int) 4];
  l <<= 3;
  s_state[u8 (int)l] = t8;

  t8 = in[33];
  l += 1;
  s_state[u8 (int)l] = t8;

  l += 1;
  s_state[u8 (int)l] = 0x1f;

  l = a_jagged_p[(SHAKE128_RATE-1)/8];
  l <<= 3;
  t = SHAKE128_RATE - 1; t &= 0x7;
  l += t;
  s_state[u8 (int)l] = 0x80;

  for i=1 to 7 { state[i] = s_state[u256 i]; }
  
  return state;
}

inline
fn __shake128_squeezeblock(reg u256[7] state, reg ptr u8[SHAKE128_RATE] out) -> reg u256[7], reg ptr u8[SHAKE128_RATE]
{
  stack u64[28] s_state;
  stack u64[25] a_jagged_p;
  reg u64 t l;
  inline int i;

  state = __keccakf1600_avx2(state);

  for i=0 to 7
  { s_state[u256 i] = state[i]; }

  a_jagged_p = KECCAK_A_JAGGED;

  for i = 0 to SHAKE128_RATE/8
  {
    l = a_jagged_p[i];
    t = s_state[(int) l];
    out[u64 i] = t;
  }

  return state, out;
}

u256 rho56 = 0x181F1E1D1C1B1A191017161514131211080F0E0D0C0B0A090007060504030201;
u256 rho8 = 0x1E1D1C1B1A19181F16151413121110170E0D0C0B0A09080F0605040302010007;

u64[4] shake_sep = {9223372036854775808, 9223372036854775808, 9223372036854775808, 9223372036854775808};

inline fn __rol_4u64_rho56(reg u256 a) -> reg u256
{
	reg u256 r;

	r = #VPSHUFB_256(a, rho56);

	return r; 
}


inline fn __rol_4u64_rho8(reg u256 a) -> reg u256
{
	reg u256 r;

	r = #VPSHUFB_256(a, rho8);

	return r; 
}


inline fn __rol_4u64(reg u256 a, inline int o) -> reg u256
{
	reg u256 r;
	reg u256 t256;

	r = #VPSLL_4u64(a, o);
	t256 = #VPSRL_4u64(a, 64 - o);

	r |= t256;

	return r; 
}


param int ba=0;
param int be=1;
param int bi=2;
param int bo=3;
param int bu=4;
param int ga=5;
param int ge=6;
param int gi=7;
param int go=8;
param int gu=9;
param int ka=10;
param int ke=11;
param int ki=12;
param int ko=13;
param int ku=14;
param int ma=15;
param int me=16;
param int mi=17;
param int mo=18;
param int mu=19;
param int sa=20;
param int se=21;
param int si=22;
param int so=23;
param int su=24;

u256[24] KeccakF1600RoundConstants = {
  0x0000000000000001000000000000000100000000000000010000000000000001,
    0x0000000000008082000000000000808200000000000080820000000000008082,
    0x800000000000808a800000000000808a800000000000808a800000000000808a,
    0x8000000080008000800000008000800080000000800080008000000080008000,
    0x000000000000808b000000000000808b000000000000808b000000000000808b,
    0x0000000080000001000000008000000100000000800000010000000080000001,
    0x8000000080008081800000008000808180000000800080818000000080008081,
    0x8000000000008009800000000000800980000000000080098000000000008009,
    0x000000000000008a000000000000008a000000000000008a000000000000008a,
    0x0000000000000088000000000000008800000000000000880000000000000088,
    0x0000000080008009000000008000800900000000800080090000000080008009,
    0x000000008000000a000000008000000a000000008000000a000000008000000a,
    0x000000008000808b000000008000808b000000008000808b000000008000808b,
    0x800000000000008b800000000000008b800000000000008b800000000000008b,
    0x8000000000008089800000000000808980000000000080898000000000008089,
    0x8000000000008003800000000000800380000000000080038000000000008003,
    0x8000000000008002800000000000800280000000000080028000000000008002,
    0x8000000000000080800000000000008080000000000000808000000000000080,
    0x000000000000800a000000000000800a000000000000800a000000000000800a,
    0x800000008000000a800000008000000a800000008000000a800000008000000a,
    0x8000000080008081800000008000808180000000800080818000000080008081,
    0x8000000000008080800000000000808080000000000080808000000000008080,
    0x0000000080000001000000008000000100000000800000010000000080000001,
    0x8000000080008008800000008000800880000000800080088000000080008008
    };

inline fn __prepare_theta(reg ptr u256[25] A_4x) -> reg u256, reg u256, reg u256, reg u256, reg u256
{ 
    reg u256 Ca, Ce, Ci, Co, Cu;

    // Ca = XOR256(Aba, XOR256(Aga, XOR256(Aka, XOR256(Ama, Asa))));
    Ca = A_4x[sa];
    Ca ^= A_4x[ma];
    Ca ^=  A_4x[ka];
    Ca ^=  A_4x[ga];
    Ca ^=  A_4x[ba];

    // Ce = XOR256(Abe, XOR256(Age, XOR256(Ake, XOR256(Ame, Ase))));
    Ce = A_4x[se];
    Ce ^= A_4x[me];
    Ce ^= A_4x[ke];
    Ce ^= A_4x[ge];
    Ce ^= A_4x[be];

    // Ci = XOR256(Abi, XOR256(Agi, XOR256(Aki, XOR256(Ami, Asi))));
    Ci = A_4x[si];
    Ci ^= A_4x[mi];
    Ci ^= A_4x[ki];
    Ci ^= A_4x[gi];
    Ci ^= A_4x[bi];

    // Co = XOR256(Abo, XOR256(Ago, XOR256(Ako, XOR256(Amo, Aso))));
    Co = A_4x[so];
    Co ^= A_4x[mo];
    Co ^= A_4x[ko];
    Co ^= A_4x[go];
    Co ^= A_4x[bo];

    // Cu = XOR256(Abu, XOR256(Agu, XOR256(Aku, XOR256(Amu, Asu))));
    Cu = A_4x[su];
    Cu ^= A_4x[mu];
    Cu ^= A_4x[ku];
    Cu ^= A_4x[gu];
    Cu ^= A_4x[bu];

    return Ca, Ce, Ci, Co, Cu;
}

inline fn __first(reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu) ->  reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Da, De, Di, Do, Du;
    reg u256 Ca1, Ce1, Ci1, Co1, Cu1;

    Ce1 = __rol_4u64(Ce, 1);
    Da = Cu ^ Ce1;

    Ci1 = __rol_4u64(Ci, 1);
    De = Ca ^ Ci1;

    Co1 = __rol_4u64(Co, 1);
    Di = Ce ^ Co1;

    Cu1 = __rol_4u64(Cu, 1);
    Do = Ci ^ Cu1;

    Ca1 = __rol_4u64(Ca, 1);
    Du = Co ^ Ca1;

    return Da, De, Di, Do, Du;
}


inline fn __second_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bba, Bbe, Bbi, Bbo, Bbu;
    reg u256 t256;

    t256 = A_4x[ba];
    t256 ^= Da;
    A_4x[ba] = t256;
    Bba = t256;

    t256 = A_4x[ge];
    t256 ^= De;
    A_4x[ge] = t256;
    Bbe = __rol_4u64(t256, 44);

    t256 = A_4x[ki];
    t256 ^= Di;
    A_4x[ki] = t256;
    Bbi = __rol_4u64(t256, 43);

    // E##ba = XOR256(Bba, ANDnu256(Bbe, Bbi)); XOReq256(E##ba, CONST256_64(KeccakF1600RoundConstants[i]));
    t256 = #VPANDN_256(Bbe, Bbi);
    t256 ^= Bba;
    t256 ^= KeccakF1600RoundConstants[index];
    E_4x[ba] = t256;

    Ca = t256;

    t256 = A_4x[mo];
    t256 ^= Do;
    A_4x[mo] = t256;
    Bbo = __rol_4u64(t256, 21);

    //  E##be = XOR256(Bbe, ANDnu256(Bbi, Bbo));
    t256 = #VPANDN_256(Bbi, Bbo);
    t256 ^= Bbe;
    E_4x[be] = t256;

    Ce = t256;

    t256 = A_4x[su];
    t256 ^= Du;
    A_4x[su] = t256;
    Bbu = __rol_4u64(t256, 14);

    // E##bi = XOR256(Bbi, ANDnu256(Bbo, Bbu)); 
    t256 = #VPANDN_256(Bbo, Bbu);
    t256 ^= Bbi;
    E_4x[bi] = t256;

    Ci = t256;

    // E##bo = XOR256(Bbo, ANDnu256(Bbu, Bba));
    t256 = #VPANDN_256(Bbu, Bba);
    t256 ^= Bbo;
    E_4x[bo] = t256; 
    
    Co = t256; 
    
    // E##bu = XOR256(Bbu, ANDnu256(Bba, Bbe));
    t256 = #VPANDN_256(Bba, Bbe);
    t256 ^= Bbu; 
    E_4x[bu] = t256;

    Cu = t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __third_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bga, Bge, Bgi, Bgo, Bgu;
    reg u256 t256;

    t256 = A_4x[bo];
    t256 ^= Do;
    A_4x[bo] = t256;
    Bga = __rol_4u64(t256, 28);

    t256 = A_4x[gu];
    t256 ^= Du;
    A_4x[gu] = t256;
    Bge = __rol_4u64(t256, 20);

    t256 = A_4x[ka];
    t256 ^= Da;
    A_4x[ka] = t256;
    Bgi = __rol_4u64(t256, 3);

    // E##ga = XOR256(Bga, ANDnu256(Bge, Bgi))
    t256 = #VPANDN_256(Bge, Bgi);
    t256 ^= Bga;
    E_4x[ga] = t256;

    Ca ^= t256;

    t256 = A_4x[me];
    t256 ^= De;
    A_4x[me] = t256;
    Bgo = __rol_4u64(t256, 45);

    // E##ge = XOR256(Bge, ANDnu256(Bgi, Bgo))
    t256 = #VPANDN_256(Bgi, Bgo);
    t256 ^= Bge;
    E_4x[ge] = t256;

    Ce ^= t256;

    t256 = A_4x[si];
    t256 ^= Di;
    A_4x[si] = t256;
    Bgu = __rol_4u64(t256, 61);

    //  E##gi = XOR256(Bgi, ANDnu256(Bgo, Bgu))
    t256 = #VPANDN_256(Bgo, Bgu);
    t256 ^= Bgi;
    E_4x[gi] = t256;
    
    Ci ^= t256;

    // E##go = XOR256(Bgo, ANDnu256(Bgu, Bga));
    t256 = #VPANDN_256(Bgu, Bga);
    t256 ^= Bgo;
    E_4x[go] = t256;
    
    Co ^= t256;

    // E##gu = XOR256(Bgu, ANDnu256(Bga, Bge));
    t256 = #VPANDN_256(Bga, Bge);
    t256 ^= Bgu;
    E_4x[gu] = t256;
    
    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __fourth_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bka, Bke, Bki, Bko, Bku;
    reg u256 t256;

    t256 = A_4x[be];
    t256 ^= De;
    A_4x[be] = t256;
    Bka = __rol_4u64(t256, 1);

    t256 = A_4x[gi];
    t256 ^= Di;
    A_4x[gi] = t256;
    Bke = __rol_4u64(t256, 6);

    t256 = A_4x[ko];
    t256 ^= Do;
    A_4x[ko] = t256;
    Bki = __rol_4u64(t256, 25);

    // E##ka = XOR256(Bka, ANDnu256(Bke, Bki));
    t256 = #VPANDN_256(Bke, Bki);
    t256 ^= Bka;
    E_4x[ka] = t256;

    Ca ^= t256;
    
    t256 = A_4x[mu];
    t256 ^= Du;
    A_4x[mu] = t256;
    Bko = __rol_4u64_rho8(t256);

    // E##ke = XOR256(Bke, ANDnu256(Bki, Bko));
    t256 = #VPANDN_256(Bki, Bko);
    t256 ^= Bke;
    E_4x[ke] = t256;

    Ce ^= t256;
    
    t256 = A_4x[sa];
    t256 ^= Da;
    A_4x[sa] = t256;
    Bku = __rol_4u64(t256, 18);

    // E##ki = XOR256(Bki, ANDnu256(Bko, Bku))
    t256 = #VPANDN_256(Bko, Bku);
    t256 ^= Bki;
    E_4x[ki] = t256;

    Ci ^= t256;

    //  E##ko = XOR256(Bko, ANDnu256(Bku, Bka));
    t256 = #VPANDN_256(Bku, Bka);
    t256 ^= Bko;
    E_4x[ko] = t256;

    Co ^= t256;

    //  E##ku = XOR256(Bku, ANDnu256(Bka, Bke));
    t256 = #VPANDN_256(Bka, Bke);
    t256 ^= Bku;
    E_4x[ku] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __fifth_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bma, Bme, Bmi, Bmo, Bmu;
    reg u256 t256;

    t256 = A_4x[bu];
    t256 ^= Du;
    A_4x[bu] = t256;
    Bma = __rol_4u64(t256, 27);

    t256 = A_4x[ga];
    t256 ^= Da;
    A_4x[ga] = t256;
    Bme = __rol_4u64(t256, 36);

    t256 = A_4x[ke];
    t256 ^= De;
    A_4x[ke] = t256;
    Bmi = __rol_4u64(t256, 10);

    // E##ma = XOR256(Bma, ANDnu256(Bme, Bmi));
    t256 = #VPANDN_256(Bme, Bmi);
    t256 ^= Bma;
    E_4x[ma] = t256;

    Ca ^= t256;

    t256 = A_4x[mi];
    t256 ^= Di;
    A_4x[mi] = t256;
    Bmo = __rol_4u64(t256, 15);

    // E##me = XOR256(Bme, ANDnu256(Bmi, Bmo));
    t256 = #VPANDN_256(Bmi, Bmo);
    t256 ^= Bme;
    E_4x[me] = t256;

    Ce ^= t256;

    t256 = A_4x[so];
    t256 ^= Do;
    A_4x[so] = t256;
    Bmu = __rol_4u64_rho56(t256);

    // E##mi = XOR256(Bmi, ANDnu256(Bmo, Bmu));
    t256 = #VPANDN_256(Bmo, Bmu);
    t256 ^= Bmi;
    E_4x[mi] = t256;

    Ci ^= t256;

    // E##mo = XOR256(Bmo, ANDnu256(Bmu, Bma));
    t256 = #VPANDN_256(Bmu, Bma);
    t256 ^= Bmo;
    E_4x[mo] = t256;

    Co ^= t256;

    // E##mu = XOR256(Bmu, ANDnu256(Bma, Bme));
    t256 = #VPANDN_256(Bma, Bme);
    t256 ^= Bmu;
    E_4x[mu] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __sixth_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bsa, Bse, Bsi, Bso, Bsu;
    reg u256 t256;

    t256 = A_4x[bi];
    t256 ^= Di;
    A_4x[bi] = t256;
    Bsa = __rol_4u64(t256, 62);

    t256 = A_4x[go];
    t256 ^= Do;
    A_4x[go] = t256;
    Bse = __rol_4u64(t256, 55);

    t256 = A_4x[ku];
    t256 ^= Du;
    A_4x[ku] = t256;
    Bsi = __rol_4u64(t256, 39);

    // E##sa = XOR256(Bsa, ANDnu256(Bse, Bsi));
    t256 = #VPANDN_256(Bse, Bsi);
    t256 ^= Bsa;
    E_4x[sa] = t256;

    Ca ^= t256;

    t256 = A_4x[ma];
    t256 ^= Da;
    A_4x[ma] = t256;
    Bso = __rol_4u64(t256, 41);

    // E##se = XOR256(Bse, ANDnu256(Bsi, Bso))
    t256 = #VPANDN_256(Bsi, Bso);
    t256 ^= Bse;
    E_4x[se] = t256;  

    Ce ^= t256;

    t256 = A_4x[se];
    t256 ^= De;
    A_4x[se] = t256;
    Bsu = __rol_4u64(t256, 2);

    // E##si = XOR256(Bsi, ANDnu256(Bso, Bsu)); 
    t256 = #VPANDN_256(Bso, Bsu);
    t256 ^= Bsi;
    E_4x[si] = t256;

    Ci ^= t256;

    // E##so = XOR256(Bso, ANDnu256(Bsu, Bsa));
    t256 = #VPANDN_256(Bsu, Bsa);
    t256 ^= Bso;
    E_4x[so] = t256;

    Co ^= t256;

    // E##su = XOR256(Bsu, ANDnu256(Bsa, Bse));
    t256 = #VPANDN_256(Bsa, Bse);
    t256 ^= Bsu;
    E_4x[su] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __second_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bba, Bbe, Bbi, Bbo, Bbu;
    reg u256 t256;

    t256 = A_4x[ba];
    t256 ^= Da;
    A_4x[ba] = t256;
    Bba = t256;

    t256 = A_4x[ge];
    t256 ^= De;
    A_4x[ge] = t256;
    Bbe = __rol_4u64(t256, 44);

    t256 = A_4x[ki];
    t256 ^= Di;
    A_4x[ki] = t256;
    Bbi = __rol_4u64(t256, 43);

    // E##ba = XOR256(Bba, ANDnu256(Bbe, Bbi)); XOReq256(E##ba, CONST256_64(KeccakF1600RoundConstants[i]));
    t256 = #VPANDN_256(Bbe, Bbi);
    t256 ^= Bba;
    t256 ^= KeccakF1600RoundConstants[index];
    E_4x[ba] = t256;

    Ca = t256;

    t256 = A_4x[mo];
    t256 ^= Do;
    A_4x[mo] = t256;
    Bbo = __rol_4u64(t256, 21);

    //  E##be = XOR256(Bbe, ANDnu256(Bbi, Bbo));
    t256 = #VPANDN_256(Bbi, Bbo);
    t256 ^= Bbe;
    E_4x[be] = t256;

    Ce = t256;

    t256 = A_4x[su];
    t256 ^= Du;
    A_4x[su] = t256;
    Bbu = __rol_4u64(t256, 14);

    // E##bi = XOR256(Bbi, ANDnu256(Bbo, Bbu)); 
    t256 = #VPANDN_256(Bbo, Bbu);
    t256 ^= Bbi;
    E_4x[bi] = t256;

    Ci = t256;

    // E##bo = XOR256(Bbo, ANDnu256(Bbu, Bba));
    t256 = #VPANDN_256(Bbu, Bba);
    t256 ^= Bbo;
    E_4x[bo] = t256; 
    
    Co = t256; 
    
    // E##bu = XOR256(Bbu, ANDnu256(Bba, Bbe));
    t256 = #VPANDN_256(Bba, Bbe);
    t256 ^= Bbu; 
    E_4x[bu] = t256;

    Cu = t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __third_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bga, Bge, Bgi, Bgo, Bgu;
    reg u256 t256;

    t256 = A_4x[bo];
    t256 ^= Do;
    A_4x[bo] = t256;
    Bga = __rol_4u64(t256, 28);

    t256 = A_4x[gu];
    t256 ^= Du;
    A_4x[gu] = t256;
    Bge = __rol_4u64(t256, 20);

    t256 = A_4x[ka];
    t256 ^= Da;
    A_4x[ka] = t256;
    Bgi = __rol_4u64(t256, 3);   

    // E##ga = XOR256(Bga, ANDnu256(Bge, Bgi))
    t256 = #VPANDN_256(Bge, Bgi);
    t256 ^= Bga;
    E_4x[ga] = t256;

    Ca ^= t256;

    t256 = A_4x[me];
    t256 ^= De;
    A_4x[me] = t256;
    Bgo = __rol_4u64(t256, 45);

    // E##ge = XOR256(Bge, ANDnu256(Bgi, Bgo))
    t256 = #VPANDN_256(Bgi, Bgo);
    t256 ^= Bge;
    E_4x[ge] = t256;

    Ce ^= t256;

    t256 = A_4x[si];
    t256 ^= Di;
    A_4x[si] = t256;
    Bgu = __rol_4u64(t256, 61);

    //  E##gi = XOR256(Bgi, ANDnu256(Bgo, Bgu))
    t256 = #VPANDN_256(Bgo, Bgu);
    t256 ^= Bgi;
    E_4x[gi] = t256;
    
    Ci ^= t256;

    // E##go = XOR256(Bgo, ANDnu256(Bgu, Bga));
    t256 = #VPANDN_256(Bgu, Bga);
    t256 ^= Bgo;
    E_4x[go] = t256;
    
    Co ^= t256;

    // E##gu = XOR256(Bgu, ANDnu256(Bga, Bge));
    t256 = #VPANDN_256(Bga, Bge);
    t256 ^= Bgu;
    E_4x[gu] = t256;
    
    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __fourth_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bka, Bke, Bki, Bko, Bku;
    reg u256 t256;

    t256 = A_4x[be];
    t256 ^= De;
    A_4x[be] = t256;
    Bka = __rol_4u64(t256, 1);

    t256 = A_4x[gi];
    t256 ^= Di;
    A_4x[gi] = t256;
    Bke = __rol_4u64(t256, 6);

    t256 = A_4x[ko];
    t256 ^= Do;
    A_4x[ko] = t256;
    Bki = __rol_4u64(t256, 25);

    // E##ka = XOR256(Bka, ANDnu256(Bke, Bki));
    t256 = #VPANDN_256(Bke, Bki);
    t256 ^= Bka;
    E_4x[ka] = t256;

    Ca ^= t256;
    
    t256 = A_4x[mu];
    t256 ^= Du;
    A_4x[mu] = t256;
    Bko = __rol_4u64_rho8(t256);

    // E##ke = XOR256(Bke, ANDnu256(Bki, Bko));
    t256 = #VPANDN_256(Bki, Bko);
    t256 ^= Bke;
    E_4x[ke] = t256;

    Ce ^= t256;
    
    t256 = A_4x[sa];
    t256 ^= Da;
    A_4x[sa] = t256;
    Bku = __rol_4u64(t256, 18);

    // E##ki = XOR256(Bki, ANDnu256(Bko, Bku))
    t256 = #VPANDN_256(Bko, Bku);
    t256 ^= Bki;
    E_4x[ki] = t256;

    Ci ^= t256;

    //  E##ko = XOR256(Bko, ANDnu256(Bku, Bka));
    t256 = #VPANDN_256(Bku, Bka);
    t256 ^= Bko;
    E_4x[ko] = t256;

    Co ^= t256;

    //  E##ku = XOR256(Bku, ANDnu256(Bka, Bke));
    t256 = #VPANDN_256(Bka, Bke);
    t256 ^= Bku;
    E_4x[ku] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __fifth_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bma, Bme, Bmi, Bmo, Bmu;
    reg u256 t256;

    t256 = A_4x[bu];
    t256 ^= Du;
    A_4x[bu] = t256;
    Bma = __rol_4u64(t256, 27);

    t256 = A_4x[ga];
    t256 ^= Da;
    A_4x[ga] = t256;
    Bme = __rol_4u64(t256, 36);

    t256 = A_4x[ke];
    t256 ^= De;
    A_4x[ke] = t256;
    Bmi = __rol_4u64(t256, 10);

    // E##ma = XOR256(Bma, ANDnu256(Bme, Bmi));
    t256 = #VPANDN_256(Bme, Bmi);
    t256 ^= Bma;
    E_4x[ma] = t256;

    Ca ^= t256;

    t256 = A_4x[mi];
    t256 ^= Di;
    A_4x[mi] = t256;
    Bmo = __rol_4u64(t256, 15);

    // E##me = XOR256(Bme, ANDnu256(Bmi, Bmo));
    t256 = #VPANDN_256(Bmi, Bmo);
    t256 ^= Bme;
    E_4x[me] = t256;

    Ce ^= t256;

    t256 = A_4x[so];
    t256 ^= Do;
    A_4x[so] = t256;
    Bmu = __rol_4u64_rho56(t256);

    // E##mi = XOR256(Bmi, ANDnu256(Bmo, Bmu));
    t256 = #VPANDN_256(Bmo, Bmu);
    t256 ^= Bmi;
    E_4x[mi] = t256;

    Ci ^= t256;

    // E##mo = XOR256(Bmo, ANDnu256(Bmu, Bma));
    t256 = #VPANDN_256(Bmu, Bma);
    t256 ^= Bmo;
    E_4x[mo] = t256;

    Co ^= t256;

    // E##mu = XOR256(Bmu, ANDnu256(Bma, Bme));
    t256 = #VPANDN_256(Bma, Bme);
    t256 ^= Bmu;
    E_4x[mu] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __sixth_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bsa, Bse, Bsi, Bso, Bsu;
    reg u256 t256;

    t256 = A_4x[bi];
    t256 ^= Di;
    A_4x[bi] = t256;
    Bsa = __rol_4u64(t256, 62);

    t256 = A_4x[go];
    t256 ^= Do;
    A_4x[go] = t256;
    Bse = __rol_4u64(t256, 55);

    t256 = A_4x[ku];
    t256 ^= Du;
    A_4x[ku] = t256;
    Bsi = __rol_4u64(t256, 39);

    // E##sa = XOR256(Bsa, ANDnu256(Bse, Bsi));
    t256 = #VPANDN_256(Bse, Bsi);
    t256 ^= Bsa;
    E_4x[sa] = t256;

    Ca ^= t256;

    t256 = A_4x[ma];
    t256 ^= Da;
    A_4x[ma] = t256;
    Bso = __rol_4u64(t256, 41);

    // E##se = XOR256(Bse, ANDnu256(Bsi, Bso))
    t256 = #VPANDN_256(Bsi, Bso);
    t256 ^= Bse;
    E_4x[se] = t256;  

    Ce ^= t256;

    t256 = A_4x[se];
    t256 ^= De;
    A_4x[se] = t256;
    Bsu = __rol_4u64(t256, 2);

    // E##si = XOR256(Bsi, ANDnu256(Bso, Bsu)); 
    t256 = #VPANDN_256(Bso, Bsu);
    t256 ^= Bsi;
    E_4x[si] = t256;

    Ci ^= t256;

    // E##so = XOR256(Bso, ANDnu256(Bsu, Bsa));
    t256 = #VPANDN_256(Bsu, Bsa);
    t256 ^= Bso;
    E_4x[so] = t256;

    Co ^= t256;

    // E##su = XOR256(Bsu, ANDnu256(Bsa, Bse));
    t256 = #VPANDN_256(Bsa, Bse);
    t256 ^= Bsu;
    E_4x[su] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __second_last(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Bba, Bbe, Bbi, Bbo, Bbu;
    reg u256 t256;

    t256 = A_4x[ba];
    t256 ^= Da;
    A_4x[ba] = t256;
    Bba = t256;

    t256 = A_4x[ge];
    t256 ^= De;
    A_4x[ge] = t256;
    Bbe = __rol_4u64(t256, 44);

    t256 = A_4x[ki];
    t256 ^= Di;
    A_4x[ki] = t256;
    Bbi = __rol_4u64(t256, 43);

    // E##ba = XOR256(Bba, ANDnu256(Bbe, Bbi)); XOReq256(E##ba, CONST256_64(KeccakF1600RoundConstants[i]));
    t256 = #VPANDN_256(Bbe, Bbi);
    t256 ^= Bba;
    t256 ^= KeccakF1600RoundConstants[index];
    E_4x[ba] = t256;

    t256 = A_4x[mo];
    t256 ^= Do;
    A_4x[mo] = t256;
    Bbo = __rol_4u64(t256, 21);

    //  E##be = XOR256(Bbe, ANDnu256(Bbi, Bbo));
    t256 = #VPANDN_256(Bbi, Bbo);
    t256 ^= Bbe;
    E_4x[be] = t256;

    t256 = A_4x[su];
    t256 ^= Du;
    A_4x[su] = t256;
    Bbu = __rol_4u64(t256, 14);

    // E##bi = XOR256(Bbi, ANDnu256(Bbo, Bbu)); 
    t256 = #VPANDN_256(Bbo, Bbu);
    t256 ^= Bbi;
    E_4x[bi] = t256;

    // E##bo = XOR256(Bbo, ANDnu256(Bbu, Bba));
    t256 = #VPANDN_256(Bbu, Bba);
    t256 ^= Bbo;
    E_4x[bo] = t256;
    
    // E##bu = XOR256(Bbu, ANDnu256(Bba, Bbe));
    t256 = #VPANDN_256(Bba, Bbe);
    t256 ^= Bbu; 
    E_4x[bu] = t256;

    return A_4x, E_4x;
}

inline fn __third_last(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Bga, Bge, Bgi, Bgo, Bgu;
    reg u256 t256;

    t256 = A_4x[bo];
    t256 ^= Do;
    A_4x[bo] = t256;
    Bga = __rol_4u64(t256, 28);

    t256 = A_4x[gu];
    t256 ^= Du;
    A_4x[gu] = t256;
    Bge = __rol_4u64(t256, 20);

    t256 = A_4x[ka];
    t256 ^= Da;
    A_4x[ka] = t256;
    Bgi = __rol_4u64(t256, 3);   

    // E##ga = XOR256(Bga, ANDnu256(Bge, Bgi))
    t256 = #VPANDN_256(Bge, Bgi);
    t256 ^= Bga;
    E_4x[ga] = t256;

    t256 = A_4x[me];
    t256 ^= De;
    A_4x[me] = t256;
    Bgo = __rol_4u64(t256, 45);

    // E##ge = XOR256(Bge, ANDnu256(Bgi, Bgo))
    t256 = #VPANDN_256(Bgi, Bgo);
    t256 ^= Bge;
    E_4x[ge] = t256;

    t256 = A_4x[si];
    t256 ^= Di;
    A_4x[si] = t256;
    Bgu = __rol_4u64(t256, 61);

    //  E##gi = XOR256(Bgi, ANDnu256(Bgo, Bgu))
    t256 = #VPANDN_256(Bgo, Bgu);
    t256 ^= Bgi;
    E_4x[gi] = t256;

    // E##go = XOR256(Bgo, ANDnu256(Bgu, Bga));
    t256 = #VPANDN_256(Bgu, Bga);
    t256 ^= Bgo;
    E_4x[go] = t256;

    // E##gu = XOR256(Bgu, ANDnu256(Bga, Bge));
    t256 = #VPANDN_256(Bga, Bge);
    t256 ^= Bgu;
    E_4x[gu] = t256;

    return A_4x, E_4x;
}

inline fn __fourth_last(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Bka, Bke, Bki, Bko, Bku;
    reg u256 t256;

    t256 = A_4x[be];
    t256 ^= De;
    A_4x[be] = t256;
    Bka = __rol_4u64(t256, 1);

    t256 = A_4x[gi];
    t256 ^= Di;
    A_4x[gi] = t256;
    Bke = __rol_4u64(t256, 6);

    t256 = A_4x[ko];
    t256 ^= Do;
    A_4x[ko] = t256;
    Bki = __rol_4u64(t256, 25);

    // E##ka = XOR256(Bka, ANDnu256(Bke, Bki));
    t256 = #VPANDN_256(Bke, Bki);
    t256 ^= Bka;
    E_4x[ka] = t256;
    
    t256 = A_4x[mu];
    t256 ^= Du;
    A_4x[mu] = t256;
    Bko = __rol_4u64_rho8(t256);

    // E##ke = XOR256(Bke, ANDnu256(Bki, Bko));
    t256 = #VPANDN_256(Bki, Bko);
    t256 ^= Bke;
    E_4x[ke] = t256;
    
    t256 = A_4x[sa];
    t256 ^= Da;
    A_4x[sa] = t256;
    Bku = __rol_4u64(t256, 18);

    // E##ki = XOR256(Bki, ANDnu256(Bko, Bku))
    t256 = #VPANDN_256(Bko, Bku);
    t256 ^= Bki;
    E_4x[ki] = t256;

    //  E##ko = XOR256(Bko, ANDnu256(Bku, Bka));
    t256 = #VPANDN_256(Bku, Bka);
    t256 ^= Bko;
    E_4x[ko] = t256;

    //  E##ku = XOR256(Bku, ANDnu256(Bka, Bke));
    t256 = #VPANDN_256(Bka, Bke);
    t256 ^= Bku;
    E_4x[ku] = t256;

    return A_4x, E_4x;
}

inline fn __fifth_last(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Bma, Bme, Bmi, Bmo, Bmu;
    reg u256 t256;

    t256 = A_4x[bu];
    t256 ^= Du;
    A_4x[bu] = t256;
    Bma = __rol_4u64(t256, 27);

    t256 = A_4x[ga];
    t256 ^= Da;
    A_4x[ga] = t256;
    Bme = __rol_4u64(t256, 36);

    t256 = A_4x[ke];
    t256 ^= De;
    A_4x[ke] = t256;
    Bmi = __rol_4u64(t256, 10);

    // E##ma = XOR256(Bma, ANDnu256(Bme, Bmi));
    t256 = #VPANDN_256(Bme, Bmi);
    t256 ^= Bma;
    E_4x[ma] = t256;

    t256 = A_4x[mi];
    t256 ^= Di;
    A_4x[mi] = t256;
    Bmo = __rol_4u64(t256, 15);

    // E##me = XOR256(Bme, ANDnu256(Bmi, Bmo));
    t256 = #VPANDN_256(Bmi, Bmo);
    t256 ^= Bme;
    E_4x[me] = t256;

    t256 = A_4x[so];
    t256 ^= Do;
    A_4x[so] = t256;
    Bmu = __rol_4u64_rho56(t256);

    // E##mi = XOR256(Bmi, ANDnu256(Bmo, Bmu));
    t256 = #VPANDN_256(Bmo, Bmu);
    t256 ^= Bmi;
    E_4x[mi] = t256;

    // E##mo = XOR256(Bmo, ANDnu256(Bmu, Bma));
    t256 = #VPANDN_256(Bmu, Bma);
    t256 ^= Bmo;
    E_4x[mo] = t256;

    // E##mu = XOR256(Bmu, ANDnu256(Bma, Bme));
    t256 = #VPANDN_256(Bma, Bme);
    t256 ^= Bmu;
    E_4x[mu] = t256;

    return A_4x, E_4x;
}

inline fn __sixth_last(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Bsa, Bse, Bsi, Bso, Bsu;
    reg u256 t256;

    t256 = A_4x[bi];
    t256 ^= Di;
    A_4x[bi] = t256;
    Bsa = __rol_4u64(t256, 62);

    t256 = A_4x[go];
    t256 ^= Do;
    A_4x[go] = t256;
    Bse = __rol_4u64(t256, 55);

    t256 = A_4x[ku];
    t256 ^= Du;
    A_4x[ku] = t256;
    Bsi = __rol_4u64(t256, 39);

    // E##sa = XOR256(Bsa, ANDnu256(Bse, Bsi));
    t256 = #VPANDN_256(Bse, Bsi);
    t256 ^= Bsa;
    E_4x[sa] = t256;

    t256 = A_4x[ma];
    t256 ^= Da;
    A_4x[ma] = t256;
    Bso = __rol_4u64(t256, 41);

    // E##se = XOR256(Bse, ANDnu256(Bsi, Bso))
    t256 = #VPANDN_256(Bsi, Bso);
    t256 ^= Bse;
    E_4x[se] = t256;

    t256 = A_4x[se];
    t256 ^= De;
    A_4x[se] = t256;
    Bsu = __rol_4u64(t256, 2);

    // E##si = XOR256(Bsi, ANDnu256(Bso, Bsu)); 
    t256 = #VPANDN_256(Bso, Bsu);
    t256 ^= Bsi;
    E_4x[si] = t256;

    // E##so = XOR256(Bso, ANDnu256(Bsu, Bsa));
    t256 = #VPANDN_256(Bsu, Bsa);
    t256 ^= Bso;
    E_4x[so] = t256;

    // E##su = XOR256(Bsu, ANDnu256(Bsa, Bse));
    t256 = #VPANDN_256(Bsa, Bse);
    t256 ^= Bsu;
    E_4x[su] = t256;

    return A_4x, E_4x;
}

inline fn __theta_rho_pi_chi_iota_prepare_theta_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{   
    reg u256 Da, De, Di, Do, Du;

    Da, De, Di, Do, Du = __first(Ca, Ce, Ci, Co, Cu);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __second_even(A_4x, E_4x, index, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __third_even(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);
  
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __fourth_even(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __fifth_even(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __sixth_even(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __theta_rho_pi_chi_iota_prepare_theta_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{   
    reg u256 Da, De, Di, Do, Du;

    Da, De, Di, Do, Du = __first(Ca, Ce, Ci, Co, Cu);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __second_odd(A_4x, E_4x, index, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __third_odd(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);
  
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __fourth_odd(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __fifth_odd(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __sixth_odd(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __theta_rho_pi_chi_iota(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Da, De, Di, Do, Du;

    Da, De, Di, Do, Du = __first(Ca, Ce, Ci, Co, Cu);

    A_4x, E_4x = __second_last(A_4x, E_4x, index, Da, De, Di, Do, Du);

    A_4x, E_4x = __third_last(A_4x, E_4x, Da, De, Di, Do, Du);

    A_4x, E_4x = __fourth_last(A_4x, E_4x, Da, De, Di, Do, Du);

    A_4x, E_4x  = __fifth_last(A_4x, E_4x, Da, De, Di, Do, Du);
 
    A_4x, E_4x  = __sixth_last(A_4x, E_4x, Da, De, Di, Do, Du);

    return A_4x, E_4x;
}

fn _KeccakF1600_StatePermute4x(reg ptr u256[25] A_4x) -> reg ptr u256[25]
{
    reg u256 Ca, Ce, Ci, Co, Cu;

    stack u256[25] E_4x;

    /** Rounds24 **/
    Ca, Ce, Ci, Co, Cu = __prepare_theta(A_4x);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 0, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 1, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 2, Ca, Ce, Ci, Co, Cu); 
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 3, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 4, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 5, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 6, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 7, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 8, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 9, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 10, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 11, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 12, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 13, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 14, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 15, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 16, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 17, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 18, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 19, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 20, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 21, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 22, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x = __theta_rho_pi_chi_iota(E_4x, A_4x, 23, Ca, Ce, Ci, Co, Cu);


    return A_4x;
}

fn _shake128_absorb4x_34(reg ptr u256[25] s, reg ptr u8[34] m0 m1 m2 m3) -> reg ptr u256[25]
{
	inline int i;
  reg u256 t0 t1;
  reg u16 t16;
	reg u64 t64;

  for i = 0 to 25
  {
    t0 = #set0_256();
    s[i] = t0;
  }

	for i = 0 to 4
  {
    t64 = m0[u64 i];
    s[u64 4 * i] ^= t64;
    t64 = m1[u64 i];
    s[u64 4 * i + 1] ^= t64;
    t64 = m2[u64 i];
    s[u64 4 * i + 2] ^= t64;
    t64 = m3[u64 i];
    s[u64 4 * i + 3] ^= t64;
  }

  t16 = m0.[u16 32];
  s[u16 64] ^= t16;
  s[u8 130] ^= 0x1F;

  t16 = m1.[u16 32];
  s[u16 68] ^= t16;
  s[u8 138] ^= 0x1F;

  t16 = m2.[u16 32];
  s[u16 72] ^= t16;
  s[u8 146] ^= 0x1F;

  t16 = m3.[u16 32];
  s[u16 76] ^= t16;
  s[u8 154] ^= 0x1F;

  t0 = shake_sep[u256 0];
  t1 = s[SHAKE128_RATE / 8 - 1];
  t0 = t0 ^ t1;
  s[SHAKE128_RATE / 8 - 1] = t0;

	return s;
}


fn _shake256_absorb4x_33(reg ptr u256[25] s, reg ptr u8[33] m0 m1 m2 m3) -> reg ptr u256[25]
{
	inline int i;
  reg u256 t0 t1;
	reg u64 t64;
  reg u8 t8;

  for i = 0 to 25
  {
    t0 = #set0_256();
    s[i] = t0;
  }

	for i = 0 to 4
  {
    t64 = m0[u64 i];
    s[u64 4 * i] ^= t64;
    t64 = m1[u64 i];
    s[u64 4 * i + 1] ^= t64;
    t64 = m2[u64 i];
    s[u64 4 * i + 2] ^= t64;
    t64 = m3[u64 i];
    s[u64 4 * i + 3] ^= t64;
  }

  t8 = m0[32];
  s[u8 128] ^= t8;
  s[u8 129] ^= 0x1F;

  t8 = m1[32];
  s[u8 136] ^= t8;
  s[u8 137] ^= 0x1F;

  t8 = m2[32];
  s[u8 144] ^= t8;
  s[u8 145] ^= 0x1F;

  t8 = m3[32];
  s[u8 152] ^= t8;
  s[u8 153] ^= 0x1F;

  t0 = shake_sep[u256 0];
  t1 = s[SHAKE256_RATE / 8 - 1];
  t0 = t0 ^ t1;
  s[SHAKE256_RATE / 8 - 1] = t0;

	return s;
}

inline
fn __shake256_squeezeblock4x(reg ptr u256[25] state, reg ptr u8[SHAKE256_RATE] h0 h1 h2 h3) -> reg ptr u256[25], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE]
{
  reg u256 t256;
  reg u128 t128;
  inline int i;

  state = _KeccakF1600_StatePermute4x(state);

	for i = 0 to (SHAKE256_RATE / 8) {
    t256 = state[i];
    t128 = (128u)t256;
		h0[u64 i] = #VMOVLPD(t128);
		h1[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI128(t256, 1);
		h2[u64 i] = #VMOVLPD(t128);
		h3[u64 i] = #VMOVHPD(t128);
	}

  return state, h0, h1, h2, h3;
}
inline 
fn __shuffle8(reg u256 a b) -> reg u256, reg u256
{
  reg u256 r0 r1; 
  r0 = #VPERM2I128(a,b,0x20);
  r1 = #VPERM2I128(a,b,0x31);
  return r0, r1;
}

inline 
fn __shuffle4(reg u256 a b) -> reg u256, reg u256
{
  reg u256 r0 r1; 
  r0 = #VPUNPCKL_4u64(a,b);
  r1 = #VPUNPCKH_4u64(a,b);
  return r0, r1;
}

inline 
fn __shuffle2(reg u256 a b) -> reg u256, reg u256
{
  reg u256 t0 t1;
  t0 = #VMOVSLDUP_8u32(b);
  t0 = #VPBLEND_8u32(a, t0, 0xAA);
  a = #VPSRL_4u64(a,32);
  t1 = #VPBLEND_8u32(a, b, 0xAA);
  return t0, t1;
}


inline 
fn __shuffle1(reg u256 a b) -> reg u256, reg u256
{
  reg u256 r0 r1 t0 t1; 
  t0 = #VPSLL_8u32(b,16);
  r0 = #VPBLEND_16u16(a,t0,0xAA);
  t1 = #VPSRL_8u32(a,16);
  r1 = #VPBLEND_16u16(t1,b,0xAA);
  return r0, r1;
}


// Transform from AVX order to bitreversed order
inline 
fn __nttpack128(reg u256 r0 r1 r2 r3 r4 r5 r6 r7)
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  r0, r1 = __shuffle1(r0, r1);
  r2, r3 = __shuffle1(r2, r3);
  r4, r5 = __shuffle1(r4, r5);
  r6, r7 = __shuffle1(r6, r7);

  r0, r2 = __shuffle2(r0, r2);
  r4, r6 = __shuffle2(r4, r6);
  r1, r3 = __shuffle2(r1, r3);
  r5, r7 = __shuffle2(r5, r7);

  r0, r4 = __shuffle4(r0, r4);
  r1, r5 = __shuffle4(r1, r5);
  r2, r6 = __shuffle4(r2, r6);
  r3, r7 = __shuffle4(r3, r7);

  r0, r1 = __shuffle8(r0, r1);
  r2, r3 = __shuffle8(r2, r3);
  r4, r5 = __shuffle8(r4, r5);
  r6, r7 = __shuffle8(r6, r7);

  return r0, r2, r4, r6, r1, r3, r5, r7;
}


// Transform from bitreversed order to AVX order
inline
fn __nttunpack128(reg u256 r0 r1 r2 r3 r4 r5 r6 r7)
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  r0, r4 = __shuffle8(r0, r4);
  r1, r5 = __shuffle8(r1, r5);
  r2, r6 = __shuffle8(r2, r6);
  r3, r7 = __shuffle8(r3, r7);

  r0, r2 = __shuffle4(r0, r2);
  r4, r6 = __shuffle4(r4, r6);
  r1, r3 = __shuffle4(r1, r3);
  r5, r7 = __shuffle4(r5, r7);

  r0, r1 = __shuffle2(r0, r1);
  r2, r3 = __shuffle2(r2, r3);
  r4, r5 = __shuffle2(r4, r5);
  r6, r7 = __shuffle2(r6, r7);

  r0, r4 = __shuffle1(r0, r4);
  r1, r5 = __shuffle1(r1, r5);
  r2, r6 = __shuffle1(r2, r6);
  r3, r7 = __shuffle1(r3, r7);

  return r0, r4, r1, r5, r2, r6, r3, r7;
}

fn _nttpack(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 r0 r1 r2 r3 r4 r5 r6 r7;

  r0 = rp.[u256 32*0];
  r1 = rp.[u256 32*1];
  r2 = rp.[u256 32*2];
  r3 = rp.[u256 32*3];
  r4 = rp.[u256 32*4];
  r5 = rp.[u256 32*5];
  r6 = rp.[u256 32*6];
  r7 = rp.[u256 32*7];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*0] = r0;
  rp.[u256 32*1] = r1;
  rp.[u256 32*2] = r2;
  rp.[u256 32*3] = r3;
  rp.[u256 32*4] = r4;
  rp.[u256 32*5] = r5;
  rp.[u256 32*6] = r6;
  rp.[u256 32*7] = r7;

  r0 = rp.[u256 32*8];
  r1 = rp.[u256 32*9];
  r2 = rp.[u256 32*10];
  r3 = rp.[u256 32*11];
  r4 = rp.[u256 32*12];
  r5 = rp.[u256 32*13];
  r6 = rp.[u256 32*14];
  r7 = rp.[u256 32*15];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*8] = r0;
  rp.[u256 32*9] = r1;
  rp.[u256 32*10] = r2;
  rp.[u256 32*11] = r3;
  rp.[u256 32*12] = r4;
  rp.[u256 32*13] = r5;
  rp.[u256 32*14] = r6;
  rp.[u256 32*15] = r7;

  return rp;
}

fn _nttunpack(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 r0 r1 r2 r3 r4 r5 r6 r7;

  r0 = rp.[u256 32*0];
  r1 = rp.[u256 32*1];
  r2 = rp.[u256 32*2];
  r3 = rp.[u256 32*3];
  r4 = rp.[u256 32*4];
  r5 = rp.[u256 32*5];
  r6 = rp.[u256 32*6];
  r7 = rp.[u256 32*7];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttunpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*0] = r0;
  rp.[u256 32*1] = r1;
  rp.[u256 32*2] = r2;
  rp.[u256 32*3] = r3;
  rp.[u256 32*4] = r4;
  rp.[u256 32*5] = r5;
  rp.[u256 32*6] = r6;
  rp.[u256 32*7] = r7;

  r0 = rp.[u256 32*8];
  r1 = rp.[u256 32*9];
  r2 = rp.[u256 32*10];
  r3 = rp.[u256 32*11];
  r4 = rp.[u256 32*12];
  r5 = rp.[u256 32*13];
  r6 = rp.[u256 32*14];
  r7 = rp.[u256 32*15];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttunpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*8] = r0;
  rp.[u256 32*9] = r1;
  rp.[u256 32*10] = r2;
  rp.[u256 32*11] = r3;
  rp.[u256 32*12] = r4;
  rp.[u256 32*13] = r5;
  rp.[u256 32*14] = r6;
  rp.[u256 32*15] = r7;

  return rp;
}
u16[128] jzetas = {2285, 2571, 2970, 1812, 1493, 1422, 287, 202, 3158, 622, 1577, 182, 962, 2127, 1855, 1468, 
                  573, 2004, 264, 383, 2500, 1458, 1727, 3199, 2648, 1017, 732, 608, 1787, 411, 3124, 1758, 
                  1223, 652, 2777, 1015, 2036, 1491, 3047, 1785, 516, 3321, 3009, 2663, 1711, 2167, 126, 1469, 
                  2476, 3239, 3058, 830, 107, 1908, 3082, 2378, 2931, 961, 1821, 2604, 448, 2264, 677, 2054, 
                  2226, 430, 555, 843, 2078, 871, 1550, 105, 422, 587, 177, 3094, 3038, 2869, 1574, 1653, 
                  3083, 778, 1159, 3182, 2552, 1483, 2727, 1119, 1739, 644, 2457, 349, 418, 329, 3173, 3254, 
                  817, 1097, 603, 610, 1322, 2044, 1864, 384, 2114, 3193, 1218, 1994, 2455, 220, 2142, 1670, 
                  2144, 1799, 2051, 794, 1819, 2475, 2459, 478, 3221, 3021, 996, 991, 958, 1869, 1522, 1628};


u16[128] jzetas_inv = {1701, 1807, 1460, 2371, 2338, 2333, 308, 108, 2851, 870, 854, 1510, 2535, 1278, 1530, 1185, 
                       1659, 1187, 3109, 874, 1335, 2111, 136, 1215, 2945, 1465, 1285, 2007, 2719, 2726, 2232, 2512, 
                       75, 156, 3000, 2911, 2980, 872, 2685, 1590, 2210, 602, 1846, 777, 147, 2170, 2551, 246, 
                       1676, 1755, 460, 291, 235, 3152, 2742, 2907, 3224, 1779, 2458, 1251, 2486, 2774, 2899, 1103, 
                       1275, 2652, 1065, 2881, 725, 1508, 2368, 398, 951, 247, 1421, 3222, 2499, 271, 90, 853, 
                       1860, 3203, 1162, 1618, 666, 320, 8, 2813, 1544, 282, 1838, 1293, 2314, 552, 2677, 2106, 
                       1571, 205, 2918, 1542, 2721, 2597, 2312, 681, 130, 1602, 1871, 829, 2946, 3065, 1325, 2756, 
                       1861, 1474, 1202, 2367, 3147, 1752, 2707, 171, 3127, 3042, 1907, 1836, 1517, 359, 758, 1441};

u16[400] jzetas_exp = {31499, 31499,  2571,  2571, 14746, 14746,  2970,  2970, 13525, 13525, 13525, 13525, 13525, 13525, 13525, 13525,
                       53134, 53134, 53134, 53134, 53134, 53134, 53134, 53134, 1493,  1493,  1493,  1493,  1493,  1493,  1493,  1493,
                        1422,  1422,  1422,  1422,  1422,  1422,  1422,  1422, 44630, 44630, 44630, 44630, 27758, 27758, 27758, 27758,
                       61737, 61737, 61737, 61737, 49846, 49846, 49846, 49846, 3158,  3158,  3158,  3158,   622,   622,   622,   622,
                        1577,  1577,  1577,  1577,   182,   182,   182,   182, 59709, 59709, 17364, 17364, 39176, 39176, 36479, 36479,
                        5572,  5572, 64434, 64434, 21439, 21439, 39295, 39295, 573,   573,  2004,  2004,   264,   264,   383,   383,
                        2500,  2500,  1458,  1458,  1727,  1727,  3199,  3199, 59847, 59020,  1497, 30967, 41972, 20179, 20711, 25081,
                       52740, 26617, 16065, 53095,  9135, 64887, 39550, 27837, 1223,   652,  2777,  1015,  2036,  1491,  3047,  1785,
                         516,  3321,  3009,  2663,  1711,  2167,   126,  1469, 65202, 54059, 33310, 20494, 37798,   945, 50654,  6182,
                       32011, 10631, 29176, 36775, 47051, 17561, 51106, 60261, 2226,   555,  2078,  1550,   422,   177,  3038,  1574,
                        3083,  1159,  2552,  2727,  1739,  2457,   418,  3173, 11182, 13387, 51303, 43881, 13131, 60950, 23093,  5493,
                       33034, 30318, 46795, 12639, 20100, 18525, 19529, 52918, 430,   843,   871,   105,   587,  3094,  2869,  1653,
                         778,  3182,  1483,  1119,   644,   349,   329,  3254, 788,   788,  1812,  1812, 28191, 28191, 28191, 28191,
                       28191, 28191, 28191, 28191, 48842, 48842, 48842, 48842, 48842, 48842, 48842, 48842,   287,   287,   287,   287,
                         287,   287,   287,   287,   202,   202,   202,   202, 202,   202,   202,   202, 10690, 10690, 10690, 10690,
                        1359,  1359,  1359,  1359, 54335, 54335, 54335, 54335, 31164, 31164, 31164, 31164,   962,   962,   962,   962,
                        2127,  2127,  2127,  2127,  1855,  1855,  1855,  1855, 1468,  1468,  1468,  1468, 37464, 37464, 24313, 24313,
                       55004, 55004,  8800,  8800, 18427, 18427,  8859,  8859, 26676, 26676, 49374, 49374,  2648,  2648,  1017,  1017,
                         732,   732,   608,   608,  1787,  1787,   411,   411, 3124,  3124,  1758,  1758, 19884, 37287, 49650, 56638,
                       37227,  9076, 35338, 18250, 13427, 14017, 36381, 52780, 16832,  4312, 41381, 47622,  2476,  3239,  3058,   830,
                         107,  1908,  3082,  2378,  2931,   961,  1821,  2604, 448,  2264,   677,  2054, 34353, 25435, 58154, 24392,
                       44610, 10946, 24215, 16990, 10336, 57603, 43035, 10907, 31637, 28644, 23998, 48114,   817,   603,  1322,  1864,
                        2114,  1218,  2455,  2142,  2144,  2051,  1819,  2459, 3221,   996,   958,  1522, 20297,  2146, 15356, 33152,
                       59257, 50634, 54492, 14470, 44039, 45338, 23211, 48094, 41677, 45279,  7757, 23132,  1097,   610,  2044,   384,
                        3193,  1994,   220,  1670,  1799,   794,  2475,   478, 3021,   991,  1869,  1628,     0,     0,     0,     0};

u16[400] jzetas_inv_exp = {42405, 57780, 20258, 23860, 17443, 42326, 20199, 21498, 51067, 11045, 14903,  6280, 32385, 50181, 63391, 45240,
                            1701,  1460,  2338,   308,  2851,   854,  2535,  1530, 1659,  3109,  1335,   136,  2945,  1285,  2719,  2232,
                           17423, 41539, 36893, 33900, 54630, 22502,  7934, 55201, 48547, 41322, 54591, 20927, 41145,  7383, 40102, 31184,
                            1807,  2371,  2333,   108,   870,  1510,  1278,  1185, 1187,   874,  2111,  1215,  1465,  2007,  2726,  2512,
                           17915, 24156, 61225, 48705, 12757, 29156, 51520, 52110, 47287, 30199, 56461, 28310,  8899, 15887, 28250, 45653,
                            1275,  2652,  1065,  2881,   725,  1508,  2368,   398, 951,   247,  1421,  3222,  2499,   271,    90,   853,
                           16163, 16163, 38861, 38861, 56678, 56678, 47110, 47110, 56737, 56737, 10533, 10533, 41224, 41224, 28073, 28073,
                            1571,  1571,   205,   205,  2918,  2918,  1542,  1542, 2721,  2721,  2597,  2597,  2312,  2312,   681,   681,
                           34373, 34373, 34373, 34373, 11202, 11202, 11202, 11202, 64178, 64178, 64178, 64178, 54847, 54847, 54847, 54847,
                            1861,  1861,  1861,  1861,  1474,  1474,  1474,  1474, 1202,  1202,  1202,  1202,  2367,  2367,  2367,  2367,
                           16695, 16695, 16695, 16695, 16695, 16695, 16695, 16695, 37346, 37346, 37346, 37346, 37346, 37346, 37346, 37346,
                            3127,  3127,  3127,  3127,  3127,  3127,  3127,  3127, 3042,  3042,  3042,  3042,  3042,  3042,  3042,  3042,
                           64749, 64749,  1517,  1517, 12619, 46008, 47012, 45437, 52898, 18742, 35219, 32503, 60044, 42444,  4587, 52406,
                           21656, 14234, 52150, 54355,    75,  3000,  2980,  2685, 2210,  1846,   147,  2551,  1676,   460,   235,  2742,
                            3224,  2458,  2486,  2899,  5276, 14431, 47976, 18486, 28762, 36361, 54906, 33526, 59355, 14883, 64592, 27739,
                           45043, 32227, 11478,   335,   156,  2911,   872,  1590, 602,   777,  2170,   246,  1755,   291,  3152,  2907,
                            1779,  1251,  2774,  1103, 37700, 25987,   650, 56402, 12442, 49472, 38920, 12797, 40456, 44826, 45358, 23565,
                           34570, 64040,  6517,  5690,  1860,  3203,  1162,  1618, 666,   320,     8,  2813,  1544,   282,  1838,  1293,
                            2314,   552,  2677,  2106, 26242, 26242, 44098, 44098, 1103,  1103, 59965, 59965, 29058, 29058, 26361, 26361,
                           48173, 48173,  5828,  5828,   130,   130,  1602,  1602, 1871,  1871,   829,   829,  2946,  2946,  3065,  3065,
                            1325,  1325,  2756,  2756, 15691, 15691, 15691, 15691, 3800,  3800,  3800,  3800, 37779, 37779, 37779, 37779,
                           20907, 20907, 20907, 20907,  3147,  3147,  3147,  3147, 1752,  1752,  1752,  1752,  2707,  2707,  2707,  2707,
                             171,   171,   171,   171, 12403, 12403, 12403, 12403, 12403, 12403, 12403, 12403, 52012, 52012, 52012, 52012,
                           52012, 52012, 52012, 52012,  1907,  1907,  1907,  1907, 1907,  1907,  1907,  1907,  1836,  1836,  1836,  1836,
                            1836,  1836,  1836,  1836, 50791, 50791,   359,   359, 60300, 60300,  1932,  1932,     0,     0,     0,     0
};

u16[16] jqx16 = {KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q,
                 KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q, KYBER_Q};

u16[16] jqinvx16 = {62209, 62209, 62209, 62209, 62209, 62209, 62209, 62209, 
                    62209, 62209, 62209, 62209, 62209, 62209, 62209, 62209};

u16[16] jvx16 = {20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159,
                 20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159};

u16[16] jfhix16 = {1441, 1441, 1441, 1441, 1441, 1441, 1441, 1441,
                   1441, 1441, 1441, 1441, 1441, 1441, 1441, 1441};

u16[16] jflox16 = {55457, 55457, 55457, 55457, 55457, 55457, 55457, 55457,
                   55457, 55457, 55457, 55457, 55457, 55457, 55457, 55457};

u16[16] maskx16 = {4095, 4095, 4095, 4095, 4095, 4095, 4095, 4095,
                   4095, 4095, 4095, 4095, 4095, 4095, 4095, 4095};

u16[16] hqx16_p1 = {1665, 1665, 1665, 1665, 1665, 1665, 1665, 1665,
                 1665, 1665, 1665, 1665, 1665, 1665, 1665, 1665};

u16[16] hqx16_m1 =  {1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664,
                 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664};

u16[16] hhqx16 = {832, 832, 832, 832, 832, 832, 832, 832,
                  832, 832, 832, 832, 832, 832, 832, 832};

u16[16] mqinvx16 = {80635, 80635, 80635, 80635, 80635, 80635, 80635, 80635,
                    80635, 80635, 80635, 80635, 80635, 80635, 80635, 80635};

u16[16] jdmontx16 = {1353, 1353, 1353, 1353, 1353, 1353, 1353, 1353,
                    1353, 1353, 1353, 1353, 1353, 1353, 1353, 1353};
param int QINV    = 62209;     /* q^(-1) mod 2^16 */
param int MONT    = 2285;      /* 2^16 % Q */
param int BARR    = 20159;     /* (1U << 26)/KYBER_Q + 1 */

inline 
fn __csubq(reg u256 r qx16) -> reg u256
{
  reg u256 t;
  r = #VPSUB_16u16(r, qx16);
  t = #VPSRA_16u16(r, 15);
  t = #VPAND_256(t, qx16);
  r = #VPADD_16u16(t, r);
  return r;
}

inline 
fn __red16x(reg u256 r qx16 vx16) -> reg u256
{
  reg u256 x;
  x = #VPMULH_16u16(r, vx16);
  x = #VPSRA_16u16(x, 10);
  x = #VPMULL_16u16(x, qx16);
  r = #VPSUB_16u16(r, x);
  return r;
}

inline 
fn __fqmulprecomp16x(reg u256 b al ah qx16) -> reg u256
{
  reg u256 x;
  x = #VPMULL_16u16(al, b);
  b = #VPMULH_16u16(ah, b);
  x = #VPMULH_16u16(x, qx16);
  b = #VPSUB_16u16(b, x);
  return b;
}

inline
fn __fqmulx16(reg u256 a b qx16 qinvx16) -> reg u256
{
  reg u256 rd rhi rlo;
  rhi = #VPMULH_16u16(a, b);
  rlo = #VPMULL_16u16(a, b);

  rlo = #VPMULL_16u16(rlo, qinvx16);
  rlo = #VPMULH_16u16(rlo, qx16);
  rd = #VPSUB_16u16(rhi, rlo);

  return rd;
}

inline
fn __fqmul(reg u16 a, reg u16 b) -> reg u16
{
  reg u32 ad;
  reg u32 bd;
  reg u32 c;
  reg u32 t;
  reg u16 r;
  reg u32 u;

  ad = (32s)a;
  bd = (32s)b;

  c = ad * bd;

  u = c * QINV;
  u <<= 16;
  //u = #SAR_32(u, 16);
  u >>s= 16;
  t = u * KYBER_Q;
  t = c - t;
  //t = #SAR_32(t, 16);
  t >>s= 16;
  r = t;
  return r;
}

inline
fn __barrett_reduce(reg u16 a) -> reg u16
{
  reg u32 t;
  reg u16 r;
  t = (32s)a;
  t = t * BARR;
  //t = #SAR_32(t, 26);
  t >>s= 26;
  t *= KYBER_Q;
  r = t;
  r = a;
  r -= t;
  return r;
}

fn _poly_add2(reg ptr u16[KYBER_N] rp bp) -> stack u16[KYBER_N]
{
  inline int i;
  reg u256 a;
  reg u256 b;
  reg u256 r;

  for i = 0 to 16 {
    a = rp.[u256 32*i];
    b = bp.[u256 32*i];
    r = #VPADD_16u16(a, b);
    rp.[u256 32*i] = r;
  }

  return rp;
}

fn _poly_csubq(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 r qx16;
  inline int i;
  
  qx16 = jqx16[u256 0];

  for i=0 to 16 {
    r = rp.[u256 32*i];
    r = __csubq(r, qx16);
    rp.[u256 32*i] = r;
  }

  return rp;
}

inline
fn __w256_interleave_u16(reg u256 al ah) -> reg u256, reg u256 {
 reg u256 a0 a1;

 a0  = #VPUNPCKL_16u16(al, ah);
 a1  = #VPUNPCKH_16u16(al, ah);

 return a0, a1;
}

inline
fn __w256_deintereleave_u16(reg u256 _zero a0 a1) -> reg u256, reg u256 {
  reg u256 al ah;

  al = #VPBLEND_16u16(a0,_zero,0xAA);
  ah = #VPBLEND_16u16(a1,_zero,0xAA);
  al = #VPACKUS_8u32(al, ah);
  a0 = #VPSRL_8u32(a0,16);
  a1 = #VPSRL_8u32(a1,16);
  ah = #VPACKUS_8u32(a0, a1);

  return al, ah;
}

inline
fn __mont_red(reg u256 lo hi qx16 qinvx16) -> reg u256 {
  reg u256 m;

  m  = #VPMULL_16u16(lo, qinvx16);
  m  = #VPMULH_16u16(m, qx16);
  lo = #VPSUB_16u16(hi, m);

  return lo;
}

inline
fn __wmul_16u16(reg u256 x y) -> reg u256, reg u256 {
 reg u256 xyL xyH xy0 xy1;
 xyL = #VPMULL_16u16(x, y);
 xyH = #VPMULH_16u16(x, y);
 xy0, xy1 = __w256_interleave_u16(xyL, xyH);

 return xy0, xy1;
}

inline 
fn __schoolbook16x(reg u256 are aim bre bim zeta zetaqinv qx16 qinvx16, inline int sign) -> reg u256, reg u256
{ reg u256 zaim ac0 ac1 zbd0 zbd1 ad0 ad1 bc0 bc1 x0 x1 y0 y1 _zero;

  zaim = __fqmulprecomp16x(aim, zetaqinv, zeta, qx16);

  ac0, ac1 = __wmul_16u16(are, bre);
  ad0, ad1 = __wmul_16u16(are, bim);
  bc0, bc1 = __wmul_16u16(aim, bre);
  zbd0, zbd1 = __wmul_16u16(zaim, bim);

  if (sign == 0) {
    x0 = #VPADD_8u32(ac0, zbd0);
    x1 = #VPADD_8u32(ac1, zbd1);
  } else {
    x0 = #VPSUB_8u32(ac0, zbd0);
    x1 = #VPSUB_8u32(ac1, zbd1);
  }
  y0 = #VPADD_8u32(bc0, ad0);
  y1 = #VPADD_8u32(bc1, ad1);

  _zero = #set0_256();
  x0, x1 = __w256_deintereleave_u16(_zero, x0, x1);
  y0, y1 = __w256_deintereleave_u16(_zero, y0, y1);
  x0 = __mont_red(x0, x1, qx16, qinvx16);
  y0 = __mont_red(y0, y1, qx16, qinvx16);
  return x0, y0;
}

fn _poly_basemul(reg ptr u16[KYBER_N] rp ap bp) -> reg ptr u16[KYBER_N]
{
  reg u256 zeta zetaqinv qx16 qinvx16 are aim bre bim;
  
  qx16    = jqx16.[u256 0];
  qinvx16 = jqinvx16.[u256 0];
  
  zetaqinv = jzetas_exp.[u256 272];
  zeta = jzetas_exp.[u256 304];

  are = ap.[u256 32*0];
  aim = ap.[u256 32*1];
  bre = bp.[u256 32*0];
  bim = bp.[u256 32*1];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*0] = are;
  rp.[u256 32*1] = aim;

  are = ap.[u256 32*2];
  aim = ap.[u256 32*3];
  bre = bp.[u256 32*2];
  bim = bp.[u256 32*3];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*2] = are;
  rp.[u256 32*3] = aim;

  zetaqinv = jzetas_exp.[u256 336];
  zeta = jzetas_exp.[u256 368];

  are = ap.[u256 32*4];
  aim = ap.[u256 32*5];
  bre = bp.[u256 32*4];
  bim = bp.[u256 32*5];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*4] = are;
  rp.[u256 32*5] = aim;

  are = ap.[u256 32*6];
  aim = ap.[u256 32*7];
  bre = bp.[u256 32*6];
  bim = bp.[u256 32*7];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*6] = are;
  rp.[u256 32*7] = aim;

  zetaqinv = jzetas_exp.[u256 664];
  zeta = jzetas_exp.[u256 696];

  are = ap.[u256 32*8];
  aim = ap.[u256 32*9];
  bre = bp.[u256 32*8];
  bim = bp.[u256 32*9];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*8] = are;
  rp.[u256 32*9] = aim;

  are = ap.[u256 32*10];
  aim = ap.[u256 32*11];
  bre = bp.[u256 32*10];
  bim = bp.[u256 32*11];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*10] = are;
  rp.[u256 32*11] = aim;

  zetaqinv = jzetas_exp.[u256 728];
  zeta = jzetas_exp.[u256 760];

  are = ap.[u256 32*12];
  aim = ap.[u256 32*13];
  bre = bp.[u256 32*12];
  bim = bp.[u256 32*13];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*12] = are;
  rp.[u256 32*13] = aim;

  are = ap.[u256 32*14];
  aim = ap.[u256 32*15];
  bre = bp.[u256 32*14];
  bim = bp.[u256 32*15];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*14] = are;
  rp.[u256 32*15] = aim;

  return rp;
}


u16 pc_shift1_s = 0x200;
u16 pc_mask_s = 0x0F;
u16 pc_shift2_s = 0x1001;
u32[8] pc_permidx_s = {0,4,1,5,2,6,3,7};

fn _poly_compress(reg u64 rp, reg ptr u16[KYBER_N] a) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3 v shift1 mask shift2 permidx;
  reg u128 t0 t1 t3;
  reg ptr u16[16] x16p;
  reg u64 t64;
  reg u32 t32;
  reg u16 t16;

  a = _poly_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  shift1 = #VPBROADCAST_16u16(pc_shift1_s);
  mask = #VPBROADCAST_16u16(pc_mask_s);
  shift2 = #VPBROADCAST_16u16(pc_shift2_s);
  permidx = pc_permidx_s[u256 0];

  for i=0 to KYBER_N/64
  {
    f0 = a[u256 4*i];
    f1 = a[u256 4*i + 1];
    f2 = a[u256 4*i + 2];
    f3 = a[u256 4*i + 3];
    f0 = #VPMULH_16u16(f0, v);
    f1 = #VPMULH_16u16(f1, v);
    f2 = #VPMULH_16u16(f2, v);
    f3 = #VPMULH_16u16(f3, v);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f1 = #VPMULHRS_16u16(f1, shift1);
    f2 = #VPMULHRS_16u16(f2, shift1);
    f3 = #VPMULHRS_16u16(f3, shift1);
    f0 = #VPAND_256(f0, mask);
    f1 = #VPAND_256(f1, mask);
    f2 = #VPAND_256(f2, mask);
    f3 = #VPAND_256(f3, mask);
    f0 = #VPACKUS_16u16(f0, f1);
    f2 = #VPACKUS_16u16(f2, f3);
    f0 = #VPMADDUBSW_256(f0, shift2);
    f2 = #VPMADDUBSW_256(f2, shift2);
    f0 = #VPACKUS_16u16(f0, f2);
    f0 = #VPERMD(permidx, f0);
    (u256)[rp + 32*i] = f0;
  }

  return a;
}

// FIXME: E_EPTR
fn _poly_compress_1(reg ptr u8[KYBER_POLYCOMPRESSEDBYTES] rp, reg ptr u16[KYBER_N] a) -> reg ptr u8[KYBER_POLYCOMPRESSEDBYTES], reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3 v shift1 mask shift2 permidx;
  reg u128 t0 t1 t3;
  reg ptr u16[16] x16p;
  reg u64 t64;
  reg u32 t32;
  reg u16 t16;

  a = _poly_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  shift1 = #VPBROADCAST_16u16(pc_shift1_s);
  mask = #VPBROADCAST_16u16(pc_mask_s);
  shift2 = #VPBROADCAST_16u16(pc_shift2_s);
  permidx = pc_permidx_s[u256 0];

  for i=0 to KYBER_N/64
  {
    f0 = a[u256 4*i];
    f1 = a[u256 4*i + 1];
    f2 = a[u256 4*i + 2];
    f3 = a[u256 4*i + 3];
    f0 = #VPMULH_16u16(f0, v);
    f1 = #VPMULH_16u16(f1, v);
    f2 = #VPMULH_16u16(f2, v);
    f3 = #VPMULH_16u16(f3, v);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f1 = #VPMULHRS_16u16(f1, shift1);
    f2 = #VPMULHRS_16u16(f2, shift1);
    f3 = #VPMULHRS_16u16(f3, shift1);
    f0 = #VPAND_256(f0, mask);
    f1 = #VPAND_256(f1, mask);
    f2 = #VPAND_256(f2, mask);
    f3 = #VPAND_256(f3, mask);
    f0 = #VPACKUS_16u16(f0, f1);
    f2 = #VPACKUS_16u16(f2, f3);
    f0 = #VPMADDUBSW_256(f0, shift2);
    f2 = #VPMADDUBSW_256(f2, shift2);
    f0 = #VPACKUS_16u16(f0, f2);
    f0 = #VPERMD(permidx, f0);
    rp.[u256 32*i] = f0;
  }

  return rp, a;
}



u8[32] pd_jshufbidx = {0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,
                       4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7};
u32 pd_mask_s = 0x00F0000F;
u32 pd_shift_s = 0x800800;

fn _poly_decompress(reg ptr u16[KYBER_N] rp, reg u64 ap) -> stack u16[KYBER_N]
{
  inline int i;
  reg u256 f q shufbidx mask shift;
  reg ptr u16[16] x16p;
  reg ptr u8[32] x32p;

  x16p = jqx16;
  q = x16p[u256 0];
  x32p = pd_jshufbidx;
  shufbidx = x32p[u256 0];
  mask = #VPBROADCAST_8u32(pd_mask_s);
  shift = #VPBROADCAST_8u32(pd_shift_s);

  f = #set0_256();

  for i=0 to KYBER_N/16
  {
    f = #VPBROADCAST_2u128((u128)[ap + 8*i]);
    f = #VPSHUFB_256(f, shufbidx);
    f = #VPAND_256(f, mask);
    f = #VPMULL_16u16(f, shift);
    f = #VPMULHRS_16u16(f, q);
    rp[u256 i] = f;
  }

  return rp;
}


fn _poly_frombytes(reg ptr u16[KYBER_N] rp, reg u64 ap) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 tt mask;
  reg ptr u16[16] maskp;

  maskp = maskx16;
  mask = maskp[u256 0];

  for i=0 to 2
  {
    t0 = (u256)[ap + 192*i];
    t1 = (u256)[ap + 192*i + 32];
    t2 = (u256)[ap + 192*i + 64];
    t3 = (u256)[ap + 192*i + 96];
    t4 = (u256)[ap + 192*i + 128];
    t5 = (u256)[ap + 192*i + 160];

    tt, t3 = __shuffle8(t0, t3);
    t0, t4 = __shuffle8(t1, t4);
    t1, t5 = __shuffle8(t2, t5);

    t2, t4 = __shuffle4(tt, t4);
    tt, t1 = __shuffle4(t3, t1);
    t3, t5 = __shuffle4(t0, t5);

    t0, t1 = __shuffle2(t2, t1);
    t2, t3 = __shuffle2(t4, t3);
    t4, t5 = __shuffle2(tt, t5);

    t6, t3 = __shuffle1(t0, t3);
    t0, t4 = __shuffle1(t1, t4);
    t1, t5 = __shuffle1(t2, t5);

    t7 = #VPSRL_16u16(t6, 12);
    t8 = #VPSLL_16u16(t3, 4);
    t7 = #VPOR_256(t7, t8);
    t6 = #VPAND_256(mask, t6);
    t7 = #VPAND_256(mask, t7);

    t8 = #VPSRL_16u16(t3, 8);
    t9 = #VPSLL_16u16(t0, 8);
    t8 = #VPOR_256(t8,t9);
    t8 = #VPAND_256(mask,t8);

    t9 = #VPSRL_16u16(t0, 4);
    t9 = #VPAND_256(mask, t9);

    t10 = #VPSRL_16u16(t4, 12);
    t11 = #VPSLL_16u16(t1, 4);
    t10 = #VPOR_256(t10, t11);
    t4 = #VPAND_256(mask,t4);
    t10 = #VPAND_256(mask, t10);

    t11 = #VPSRL_16u16(t1, 8);
    tt = #VPSLL_16u16(t5, 8);
    t11 = #VPOR_256(t11, tt);
    t11 = #VPAND_256(mask, t11);

    tt = #VPSRL_16u16(t5, 4);
    tt = #VPAND_256(mask, tt);

    rp[u256 8*i] = t6;
    rp[u256 8*i + 1] = t7;
    rp[u256 8*i + 2] = t8;
    rp[u256 8*i + 3] = t9;
    rp[u256 8*i + 4] = t4;
    rp[u256 8*i + 5] = t10;
    rp[u256 8*i + 6] = t11;
    rp[u256 8*i + 7] = tt;
  }

  return rp;
}

param int DMONT   = 1353;      /* (1ULL << 32) % KYBER_Q */

fn _poly_frommont(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 t qx16 qinvx16 dmontx16;
  inline int i;
  reg ptr u16[16] x16p;

  x16p = jqx16;
  qx16 = x16p[u256 0];
  x16p = jqinvx16;
  qinvx16 = x16p[u256 0];
  x16p = jdmontx16;
  dmontx16 = x16p[u256 0];

  for i=0 to KYBER_N/16
  {
    t = rp[u256 i];
    t = __fqmulx16(t, dmontx16, qx16, qinvx16);
    rp[u256 i] = t;
  }

  return rp; 
}

u32[4] pfm_shift_s = {3, 2, 1, 0};
u8[16] pfm_idx_s = {0, 1, 4, 5, 8, 9, 12, 13,
                2, 3, 6, 7, 10, 11, 14, 15};

fn _poly_frommsg(reg ptr u16[KYBER_N] rp, reg u64 ap) -> stack u16[KYBER_N]
{
  inline int i;
  reg u256 f g0 g1 g2 g3 g4 h0 h1 h2 h3;
  reg u256 shift idx hqs;
  reg ptr u16[16] x16p;

  x16p = hqx16_p1;
  hqs = x16p[u256 0];
  shift = #VPBROADCAST_2u128(pfm_shift_s[u128 0]);
  idx = #VPBROADCAST_2u128(pfm_idx_s[u128 0]);

  f = (u256)[ap];

  for i=0 to 4
  {
    g3 =  #VPSHUFD_256(f, 0x55*i);
    g3 = #VPSLLV_8u32(g3, shift);
    g3 = #VPSHUFB_256(g3, idx);
    g0 = #VPSLL_16u16(g3,12);
    g1 = #VPSLL_16u16(g3,8);
    g2 = #VPSLL_16u16(g3,4);
    g0 = #VPSRA_16u16(g0,15);
    g1 = #VPSRA_16u16(g1,15);
    g2 = #VPSRA_16u16(g2,15);
    g3 = #VPSRA_16u16(g3,15);
    g0 = #VPAND_256(g0,hqs);
    g1 = #VPAND_256(g1,hqs);
    g2 = #VPAND_256(g2,hqs);
    g3 = #VPAND_256(g3,hqs);
    h0 = #VPUNPCKL_4u64(g0,g1);
    h2 = #VPUNPCKH_4u64(g0,g1);
    h1 = #VPUNPCKL_4u64(g2,g3);
    h3 = #VPUNPCKH_4u64(g2,g3);
    g0 = #VPERM2I128(h0,h1,0x20);
    g2 = #VPERM2I128(h0,h1,0x31);
    g1 = #VPERM2I128(h2,h3,0x20);
    g3 = #VPERM2I128(h2,h3,0x31);
    rp[u256 2*i] = g0;
    rp[u256 2*i + 1] = g1;
    rp[u256 2*i + 8] = g2;
    rp[u256 2*i + 8 + 1] = g3;
  }

  return rp;
}

// FIXME: E_EPTR
fn _poly_frommsg_1(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_INDCPA_MSGBYTES] ap) -> stack u16[KYBER_N]
{
  inline int i;
  reg u256 f g0 g1 g2 g3 g4 h0 h1 h2 h3;
  reg u256 shift idx hqs;
  reg ptr u16[16] x16p;

  x16p = hqx16_p1;
  hqs = x16p[u256 0];
  shift = #VPBROADCAST_2u128(pfm_shift_s[u128 0]);
  idx = #VPBROADCAST_2u128(pfm_idx_s[u128 0]);

  f = ap[u256 0];

  for i=0 to 4
  {
    g3 = #VPSHUFD_256(f, 0x55*i);
    g3 = #VPSLLV_8u32(g3, shift);
    g3 = #VPSHUFB_256(g3, idx);
    g0 = #VPSLL_16u16(g3,12);
    g1 = #VPSLL_16u16(g3,8);
    g2 = #VPSLL_16u16(g3,4);
    g0 = #VPSRA_16u16(g0,15);
    g1 = #VPSRA_16u16(g1,15);
    g2 = #VPSRA_16u16(g2,15);
    g3 = #VPSRA_16u16(g3,15);
    g0 = #VPAND_256(g0,hqs);
    g1 = #VPAND_256(g1,hqs);
    g2 = #VPAND_256(g2,hqs);
    g3 = #VPAND_256(g3,hqs);
    h0 = #VPUNPCKL_4u64(g0,g1);
    h2 = #VPUNPCKH_4u64(g0,g1);
    h1 = #VPUNPCKL_4u64(g2,g3);
    h3 = #VPUNPCKH_4u64(g2,g3);
    g0 = #VPERM2I128(h0,h1,0x20);
    g2 = #VPERM2I128(h0,h1,0x31);
    g1 = #VPERM2I128(h2,h3,0x20);
    g3 = #VPERM2I128(h2,h3,0x31);
    rp[u256 2*i] = g0;
    rp[u256 2*i + 1] = g1;
    rp[u256 2*i + 8] = g2;
    rp[u256 2*i + 8 + 1] = g3;
  }

  return rp;
}

param int NOISE_NBLOCKS = (KYBER_ETA1 * KYBER_N/4 + SHAKE256_RATE - 1)/SHAKE256_RATE;

u8[32] cbd_jshufbidx = {0, 1, 2, -1, 3, 4, 5, -1, 6, 7, 8, -1, 9, 10, 11, -1,
                        4, 5, 6, -1, 7, 8, 9, -1, 10, 11, 12, -1, 13, 14, 15, -1};

inline
fn __cbd3(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8] buf) -> reg ptr u16[KYBER_N]{
  inline int i;
  reg u256 f0 f1 f2 f3;
  reg u256 mask249 mask6DB mask07 mask70 mask3 shufbidx;
  stack u32 mask249_s mask6DB_s mask07_s mask70_s;
  stack u16 mask3_s;

  mask249_s = 0x249249;
  mask6DB_s = 0x6DB6DB;
  mask07_s = 7;
  mask70_s = (7 << 16);
  mask3_s = 3;

  mask249 = #VPBROADCAST_8u32(mask249_s);
  mask6DB = #VPBROADCAST_8u32(mask6DB_s);
  mask07  = #VPBROADCAST_8u32(mask07_s);
  mask70  = #VPBROADCAST_8u32(mask70_s);
  mask3   = #VPBROADCAST_16u16(mask3_s);
  shufbidx = cbd_jshufbidx[u256 0];

  for i=0 to KYBER_N/32
  {
    f0 = buf.[u256 24*i];
    f0 = #VPERMQ(f0, 0x94);
    f0 = #VPSHUFB_256(f0, shufbidx);

    f1 = #VPSRL_8u32(f0, 1);
    f2 = #VPSRL_8u32(f0, 2);
    f0 = #VPAND_256(mask249, f0);
    f1 = #VPAND_256(mask249, f1);
    f2 = #VPAND_256(mask249, f2);
    f0 = #VPADD_8u32(f0, f1);
    f0 = #VPADD_8u32(f0, f2);

    f1 = #VPSRL_8u32(f0, 3);
    f0 = #VPADD_8u32(f0, mask6DB);
    f0 = #VPSUB_8u32(f0, f1);

    f1 = #VPSLL_8u32(f0, 10);
    f2 = #VPSRL_8u32(f0, 12);
    f3 = #VPSRL_8u32(f0, 2);
    f0 = #VPAND_256(f0, mask07);
    f1 = #VPAND_256(f1, mask70);
    f2 = #VPAND_256(f2, mask07);
    f3 = #VPAND_256(f3, mask70);
    f0 = #VPADD_16u16(f0, f1);
    f1 = #VPADD_16u16(f2, f3);
    f0 = #VPSUB_16u16(f0, mask3);
    f1 = #VPSUB_16u16(f1, mask3);

    f2 = #VPUNPCKL_8u32(f0, f1);
    f3 = #VPUNPCKH_8u32(f0, f1);

    f0 = #VPERM2I128(f2, f3, 0x20);
    f1 = #VPERM2I128(f2, f3, 0x31);

    rp[u256 2*i] = f0;
    rp[u256 2*i + 1] = f1;
  }

  return rp;
}


inline
fn __cbd2(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_ETA2*KYBER_N/4] buf) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3;
  reg u256 mask55 mask33 mask03 mask0F;
  reg u128 t;
  stack u32 mask55_s mask33_s mask03_s mask0F_s;

  mask55_s = 0x55555555;
  mask33_s = 0x33333333;
  mask03_s = 0x03030303;
  mask0F_s = 0x0F0F0F0F;

  mask55 = #VPBROADCAST_8u32(mask55_s);
  mask33 = #VPBROADCAST_8u32(mask33_s);
  mask03 = #VPBROADCAST_8u32(mask03_s);
  mask0F = #VPBROADCAST_8u32(mask0F_s);

  for i=0 to KYBER_N/64
  {
    f0 = buf[u256 i];

    f1 = #VPSRL_16u16(f0, 1);
    f0 = #VPAND_256(mask55, f0);
    f1 = #VPAND_256(mask55, f1);
    f0 = #VPADD_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 2);
    f0 = #VPAND_256(mask33, f0);
    f1 = #VPAND_256(mask33, f1);
    f0 = #VPADD_32u8(f0, mask33);
    f0 = #VPSUB_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 4);
    f0 = #VPAND_256(mask0F, f0);
    f1 = #VPAND_256(mask0F, f1);
    f0 = #VPSUB_32u8(f0, mask03);
    f1 = #VPSUB_32u8(f1, mask03);

    f2 = #VPUNPCKL_32u8(f0, f1);
    f3 = #VPUNPCKH_32u8(f0, f1);

    t = (128u)f2;
    f0 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f2, 1);
    f1 = #VPMOVSX_16u8_16u16(t);
    t = (128u)f3;
    f2 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f3, 1);
    f3 = #VPMOVSX_16u8_16u16(t);
    rp[u256 4*i] = f0;
    rp[u256 4*i + 1] = f2;
    rp[u256 4*i + 2] = f1;
    rp[u256 4*i + 3] = f3;
  }

  return rp;
}

/* buf 32 bytes longer for cbd3 (KYBER_ETA1 == 3) */
inline
fn __poly_cbd_eta1(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8] buf) -> reg ptr u16[KYBER_N]
{
  if(KYBER_ETA1 == 2) { // resolved at compile-time
    rp = __cbd2(rp, buf[0:KYBER_ETA2*KYBER_N/4]);
  } else {
    rp = __cbd3(rp, buf);
  }

  return rp;
}

inline
fn __poly_cbd_eta2(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_ETA2*KYBER_N/4] buf) -> reg ptr u16[KYBER_N]
{
  if(KYBER_ETA2 == 2) {
    rp = __cbd2(rp, buf);
  }
  return rp;
}


#[returnaddress="stack"]
fn _poly_getnoise(reg ptr u16[KYBER_N] rp, reg ptr u8[KYBER_SYMBYTES] seed, reg u8 nonce) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3;
  reg u256 mask55 mask33 mask03 mask0F;
  reg u128 t;
  reg u64 t64;
  stack ptr u16[KYBER_N] srp;
  stack u8[128] buf;
  stack u8[33] extseed;
  stack u32 mask55_s mask33_s mask03_s mask0F_s;

  mask55_s = 0x55555555;
  mask33_s = 0x33333333;
  mask03_s = 0x03030303;
  mask0F_s = 0x0F0F0F0F;

  srp = rp;

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = seed[u64 i];
    extseed[u64 i] = t64;
  }
  extseed[KYBER_SYMBYTES] = nonce;

  buf = _ishake256_128_33(buf, extseed);

  mask55 = #VPBROADCAST_8u32(mask55_s);
  mask33 = #VPBROADCAST_8u32(mask33_s);
  mask03 = #VPBROADCAST_8u32(mask03_s);
  mask0F = #VPBROADCAST_8u32(mask0F_s);

  rp = srp;

  for i=0 to KYBER_N/64
  {
    f0 = buf[u256 i];

    f1 = #VPSRL_16u16(f0, 1);
    f0 = #VPAND_256(mask55, f0);
    f1 = #VPAND_256(mask55, f1);
    f0 = #VPADD_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 2);
    f0 = #VPAND_256(mask33, f0);
    f1 = #VPAND_256(mask33, f1);
    f0 = #VPADD_32u8(f0, mask33);
    f0 = #VPSUB_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 4);
    f0 = #VPAND_256(mask0F, f0);
    f1 = #VPAND_256(mask0F, f1);
    f0 = #VPSUB_32u8(f0, mask03);
    f1 = #VPSUB_32u8(f1, mask03);

    f2 = #VPUNPCKL_32u8(f0, f1);
    f3 = #VPUNPCKH_32u8(f0, f1);

    t = (128u)f2;
    f0 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f2, 1);
    f1 = #VPMOVSX_16u8_16u16(t);
    t = (128u)f3;
    f2 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f3, 1);
    f3 = #VPMOVSX_16u8_16u16(t);
    rp[u256 4*i] = f0;
    rp[u256 4*i + 1] = f2;
    rp[u256 4*i + 2] = f1;
    rp[u256 4*i + 3] = f3;
  }

  return rp;
}

inline
fn __shake256_squeezenblocks4x(reg ptr u256[25] state, reg ptr u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3) -> reg ptr u256[25], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE]
{
  inline int i;

  for i = 0 to NOISE_NBLOCKS
  {
    state, buf0[i*SHAKE256_RATE:SHAKE256_RATE], buf1[i*SHAKE256_RATE:SHAKE256_RATE], buf2[i*SHAKE256_RATE:SHAKE256_RATE], buf3[i*SHAKE256_RATE:SHAKE256_RATE] = __shake256_squeezeblock4x(state, buf0[i*SHAKE256_RATE:SHAKE256_RATE], buf1[i*SHAKE256_RATE:SHAKE256_RATE], buf2[i*SHAKE256_RATE:SHAKE256_RATE], buf3[i*SHAKE256_RATE:SHAKE256_RATE]);
  }

  return state, buf0, buf1, buf2, buf3;
}

#[returnaddress="stack"]
fn _poly_getnoise_eta1_4x(reg ptr u16[KYBER_N] r0 r1 r2 r3, reg ptr u8[KYBER_SYMBYTES] seed, reg u8 nonce) -> reg ptr u16[KYBER_N], reg ptr u16[KYBER_N], reg ptr u16[KYBER_N], reg ptr u16[KYBER_N]
{
  reg u256 f;
  stack u256[25] state;
  stack u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3;

  f = seed[u256 0];
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;

  buf0.[32] = nonce;
  nonce += 1;
  buf1.[32] = nonce;
  nonce += 1;
  buf2.[32] = nonce;
  nonce += 1;
  buf3.[32] = nonce;

  state = _shake256_absorb4x_33(state, buf0[0:33], buf1[0:33], buf2[0:33], buf3[0:33]);
  state, buf0, buf1, buf2, buf3 = __shake256_squeezenblocks4x(state, buf0, buf1, buf2, buf3);
  
  r0 = __poly_cbd_eta1(r0, buf0[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);
  r1 = __poly_cbd_eta1(r1, buf1[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);
  r2 = __poly_cbd_eta1(r2, buf2[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);
  r3 = __poly_cbd_eta1(r3, buf3[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);

  return r0, r1, r2, r3;
}

#[returnaddress="stack"]
fn _poly_getnoise_eta1122_4x(reg ptr u16[KYBER_N] r0 r1 r2 r3, reg ptr u8[KYBER_SYMBYTES] seed, reg u8 nonce) -> reg ptr u16[KYBER_N], reg ptr u16[KYBER_N], reg ptr u16[KYBER_N], reg ptr u16[KYBER_N]
{
  reg u256 f;
  stack u256[25] state;
  stack u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3;

  f = seed[u256 0];
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;

  buf0.[32] = nonce;
  nonce += 1;
  buf1.[32] = nonce;
  nonce += 1;
  buf2.[32] = nonce;
  nonce += 1;
  buf3.[32] = nonce;

  state = _shake256_absorb4x_33(state, buf0[0:33], buf1[0:33], buf2[0:33], buf3[0:33]);
  state, buf0, buf1, buf2, buf3 = __shake256_squeezenblocks4x(state, buf0, buf1, buf2, buf3);

  r0 = __poly_cbd_eta1(r0, buf0[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);
  r1 = __poly_cbd_eta1(r1, buf1[0:KYBER_ETA1*KYBER_N/4+(KYBER_ETA1 - 2)*8]);
  r2 = __poly_cbd_eta2(r2, buf2[0:KYBER_ETA2*KYBER_N/4]);
  r3 = __poly_cbd_eta2(r3, buf3[0:KYBER_ETA2*KYBER_N/4]);

  return r0, r1, r2, r3;
}


inline
fn __invntt___butterfly64x(reg u256 rl0 rl1 rl2 rl3 rh0 rh1 rh2 rh3 zl0 zl1 zh0 zh1 qx16) 
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7;

  t0  = #VPSUB_16u16(rl0, rh0);
  t1  = #VPSUB_16u16(rl1, rh1);
  t2  = #VPSUB_16u16(rl2, rh2);

  rl0 = #VPADD_16u16(rh0, rl0);
  rl1 = #VPADD_16u16(rh1, rl1);
  rh0 = #VPMULL_16u16(zl0, t0);

  rl2 = #VPADD_16u16(rh2, rl2);
  rh1 = #VPMULL_16u16(zl0, t1);
  t3  = #VPSUB_16u16(rl3, rh3);

  rl3 = #VPADD_16u16(rh3, rl3);
  rh2 = #VPMULL_16u16(zl1, t2);
  rh3 = #VPMULL_16u16(zl1, t3);
  
  t0  = #VPMULH_16u16(zh0, t0);
  t1  = #VPMULH_16u16(zh0, t1);

  t2  = #VPMULH_16u16(zh1, t2);
  t3  = #VPMULH_16u16(zh1, t3);

  // Reduce
  rh0  = #VPMULH_16u16(qx16, rh0);
  rh1  = #VPMULH_16u16(qx16, rh1);
  rh2  = #VPMULH_16u16(qx16, rh2);
  rh3  = #VPMULH_16u16(qx16, rh3);
  
  rh0  = #VPSUB_16u16(t0, rh0);
  rh1  = #VPSUB_16u16(t1, rh1);
  rh2  = #VPSUB_16u16(t2, rh2);
  rh3  = #VPSUB_16u16(t3, rh3);

  return rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3;
}

fn _poly_invntt(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 zeta0 zeta1 zeta2 zeta3 r0 r1 r2 r3 r4 r5 r6 r7 qx16 vx16 flox16 fhix16;
  reg ptr u16[400] zetasp;
  reg ptr u16[16] qx16p;
  inline int i;

  zetasp = jzetas_inv_exp;
  qx16 = jqx16[u256 0];

  for i=0 to 2 
  {
    // level 0:
    zeta0 = zetasp.[u256 0+392*i];
    zeta1 = zetasp.[u256 64+392*i];
    zeta2 = zetasp.[u256 32+392*i];
    zeta3 = zetasp.[u256 96+392*i];

    r0 = rp.[u256 32*0+256*i];
    r1 = rp.[u256 32*1+256*i];
    r2 = rp.[u256 32*2+256*i];
    r3 = rp.[u256 32*3+256*i];
    r4 = rp.[u256 32*4+256*i];
    r5 = rp.[u256 32*5+256*i];
    r6 = rp.[u256 32*6+256*i];
    r7 = rp.[u256 32*7+256*i];

    r0, r1, r4, r5, r2, r3, r6, r7 = __invntt___butterfly64x(r0, r1, r4, r5, r2, r3, r6, r7, zeta0, zeta1, zeta2, zeta3, qx16);

    // level 1:
    vx16 = jvx16[u256 0];
    zeta0 = zetasp.[u256 128+392*i];
    zeta1 = zetasp.[u256 160+392*i];
    r0 = __red16x(r0, qx16, vx16);
    r1 = __red16x(r1, qx16, vx16);
    r4 = __red16x(r4, qx16, vx16);
    r5 = __red16x(r5, qx16, vx16);

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);
    
    r0, r1 = __shuffle1(r0, r1);
    r2, r3 = __shuffle1(r2, r3);
    r4, r5 = __shuffle1(r4, r5);
    r6, r7 = __shuffle1(r6, r7);

    // level 2:
    zeta0 = zetasp.[u256 192+392*i];
    zeta1 = zetasp.[u256 224+392*i];


    r0, r2, r4, r6, r1, r3, r5, r7 = __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r2 = __shuffle2(r0, r2);
    r4, r6 = __shuffle2(r4, r6);
    r1, r3 = __shuffle2(r1, r3);
    r5, r7 = __shuffle2(r5, r7);

    // level 3:
    zeta0 = zetasp.[u256 256+392*i];
    zeta1 = zetasp.[u256 288+392*i];

    r0, r4, r1, r5, r2, r6, r3, r7 = __invntt___butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r4 = __shuffle4(r0, r4);
    r1, r5 = __shuffle4(r1, r5);
    r2, r6 = __shuffle4(r2, r6);
    r3, r7 = __shuffle4(r3, r7);

    // level 4:
    zeta0 = zetasp.[u256 320+392*i];
    zeta1 = zetasp.[u256 352+392*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r1 = __shuffle8(r0, r1);
    r2, r3 = __shuffle8(r2, r3);
    r4, r5 = __shuffle8(r4, r5);
    r6, r7 = __shuffle8(r6, r7);

    // level 5:
    zeta0 = #VPBROADCAST_8u32(zetasp.[u32 384+392*i]);
    zeta1 = #VPBROADCAST_8u32(zetasp.[u32 388+392*i]);

    r0, r2, r4, r6, r1, r3, r5, r7 = __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    if (i==0) {
     rp.[u256 32*0+256*i] = r0;
     rp.[u256 32*1+256*i] = r2;
     rp.[u256 32*2+256*i] = r4;
     rp.[u256 32*3+256*i] = r6;
    }
    rp.[u256 32*4+256*i] = r1;
    rp.[u256 32*5+256*i] = r3;
    rp.[u256 32*6+256*i] = r5;
    rp.[u256 32*7+256*i] = r7;
  }

  zeta0 = #VPBROADCAST_8u32(zetasp.[u32 784]);
  zeta1 = #VPBROADCAST_8u32(zetasp.[u32 788]);

  for i=0 to 2
  {
    if (i == 0) {
     r7 = r6;
     r6 = r4;
     r5 = r2;
     r4 = r0;
    } else {
     r4 = rp.[u256 32*8+128*i];
     r5 = rp.[u256 32*9+128*i];
     r6 = rp.[u256 32*10+128*i];
     r7 = rp.[u256 32*11+128*i];
    }
    r0 = rp.[u256 32*0+128*i];
    r1 = rp.[u256 32*1+128*i];
    r2 = rp.[u256 32*2+128*i];
    r3 = rp.[u256 32*3+128*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    flox16 = jflox16[u256 0];
    fhix16 = jfhix16[u256 0];

    rp.[u256 32*8+128*i]  = r4;
    rp.[u256 32*9+128*i]  = r5;
    rp.[u256 32*10+128*i] = r6;
    rp.[u256 32*11+128*i] = r7;

    r0 = __fqmulprecomp16x(r0, flox16, fhix16, qx16);
    r1 = __fqmulprecomp16x(r1, flox16, fhix16, qx16);
    r2 = __fqmulprecomp16x(r2, flox16, fhix16, qx16);
    r3 = __fqmulprecomp16x(r3, flox16, fhix16, qx16);

    rp.[u256 32*0+128*i] = r0;
    rp.[u256 32*1+128*i] = r1;
    rp.[u256 32*2+128*i] = r2;
    rp.[u256 32*3+128*i] = r3;
  }

  return rp;
}

inline
fn __butterfly64x(reg u256 rl0 rl1 rl2 rl3 rh0 rh1 rh2 rh3 zl0 zl1 zh0 zh1 qx16) 
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7;

  t0 = #VPMULL_16u16(zl0, rh0);
  t1 = #VPMULH_16u16(zh0, rh0);
  t2 = #VPMULL_16u16(zl0, rh1);
  t3 = #VPMULH_16u16(zh0, rh1);
  t4 = #VPMULL_16u16(zl1, rh2);
  t5 = #VPMULH_16u16(zh1, rh2);
  t6 = #VPMULL_16u16(zl1, rh3);
  t7 = #VPMULH_16u16(zh1, rh3);

  t0 = #VPMULH_16u16(t0, qx16);
  t2 = #VPMULH_16u16(t2, qx16);
  t4 = #VPMULH_16u16(t4, qx16);
  t6 = #VPMULH_16u16(t6, qx16);

  //rh1 = #VPSUB_16u16(t3, rl1);
  rh1 = #VPSUB_16u16(rl1, t3);
  rl1 = #VPADD_16u16(t3, rl1);
  //rh0 = #VPSUB_16u16(t1, rl0);
  rh0 = #VPSUB_16u16(rl0, t1);
  rl0 = #VPADD_16u16(t1, rl0);
  //rh3 = #VPSUB_16u16(t7, rl3);
  rh3 = #VPSUB_16u16(rl3, t7);
  rl3 = #VPADD_16u16(t7, rl3);
  //rh2 = #VPSUB_16u16(t5, rl2);
  rh2 = #VPSUB_16u16(rl2, t5);
  rl2 = #VPADD_16u16(t5, rl2);

  rh0 = #VPADD_16u16(t0, rh0);
  //rl0 = #VPSUB_16u16(t0, rl0);
  rl0 = #VPSUB_16u16(rl0, t0);
  rh1 = #VPADD_16u16(t2, rh1);
  //rl1 = #VPSUB_16u16(t2, rl1);
  rl1 = #VPSUB_16u16(rl1, t2);
  rh2 = #VPADD_16u16(t4, rh2);
  //rl2 = #VPSUB_16u16(t4, rl2);
  rl2 = #VPSUB_16u16(rl2, t4);
  rh3 = #VPADD_16u16(t6, rh3);
  //rl3 = #VPSUB_16u16(t6, rl3);
  rl3 = #VPSUB_16u16(rl3, t6);

  return rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3;
}

fn _poly_ntt(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 zeta0 zeta1 zeta2 zeta3 r0 r1 r2 r3 r4 r5 r6 r7 qx16 vx16;
  reg u32 t;
  reg u16 w;
  reg ptr u16[400] zetasp;
  inline int i;

  zetasp = jzetas_exp;
  qx16 = jqx16[u256 0];

  zeta0 = #VPBROADCAST_8u32(zetasp[u32 0]);
  zeta1 = #VPBROADCAST_8u32(zetasp[u32 1]);

  r0 = rp.[u256 32*0];
  r1 = rp.[u256 32*1];
  r2 = rp.[u256 32*2];
  r3 = rp.[u256 32*3];
  r4 = rp.[u256 32*8];
  r5 = rp.[u256 32*9];
  r6 = rp.[u256 32*10];
  r7 = rp.[u256 32*11];

  r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

  rp.[u256 32*0] = r0;
  rp.[u256 32*1] = r1;
  rp.[u256 32*2] = r2;
  rp.[u256 32*3] = r3;
  rp.[u256 32*8] = r4;
  rp.[u256 32*9] = r5;
  rp.[u256 32*10] = r6;
  rp.[u256 32*11] = r7;

  r0 = rp.[u256 32*4];
  r1 = rp.[u256 32*5];
  r2 = rp.[u256 32*6];
  r3 = rp.[u256 32*7];
  r4 = rp.[u256 32*12];
  r5 = rp.[u256 32*13];
  r6 = rp.[u256 32*14];
  r7 = rp.[u256 32*15];

  r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

  /*
   rp.[u256 32*4] = r0;
   rp.[u256 32*5] = r1;
   rp.[u256 32*6] = r2;
   rp.[u256 32*7] = r3;
  */
  rp.[u256 32*12] = r4;
  rp.[u256 32*13] = r5;
  rp.[u256 32*14] = r6;
  rp.[u256 32*15] = r7;

  for i=0 to 2 {

    // level 1
    zeta0 = #VPBROADCAST_8u32(zetasp.[u32 8 + 392*i]);
    zeta1 = #VPBROADCAST_8u32(zetasp.[u32 12 + 392*i]);

    if ( i == 0) {
     r4 = r0;
     r5 = r1;
     r6 = r2;
     r7 = r3;
    } else {
     r4 = rp.[u256 32*4+256*i];
     r5 = rp.[u256 32*5+256*i];
     r6 = rp.[u256 32*6+256*i];
     r7 = rp.[u256 32*7+256*i];
    }
    r0 = rp.[u256 32*0+256*i];
    r1 = rp.[u256 32*1+256*i];
    r2 = rp.[u256 32*2+256*i];
    r3 = rp.[u256 32*3+256*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 2
    zeta0 = zetasp.[u256 16 + 392*i];
    zeta1 = zetasp.[u256 48 + 392*i];

    r0, r4 = __shuffle8(r0, r4);
    r1, r5 = __shuffle8(r1, r5);
    r2, r6 = __shuffle8(r2, r6);
    r3, r7 = __shuffle8(r3, r7);

    r0, r4, r1, r5, r2, r6, r3, r7 = __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 3
    zeta0 = zetasp.[u256 80 + 392*i];
    zeta1 = zetasp.[u256 112 + 392*i];

    r0, r2 = __shuffle4(r0, r2);
    r4, r6 = __shuffle4(r4, r6);
    r1, r3 = __shuffle4(r1, r3);
    r5, r7 = __shuffle4(r5, r7);

    r0, r2, r4, r6, r1, r3, r5, r7 = __butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 4
    zeta0 = zetasp.[u256 144 + 392*i];
    zeta1 = zetasp.[u256 176 + 392*i];

    r0, r1 = __shuffle2(r0, r1);
    r2, r3 = __shuffle2(r2, r3);
    r4, r5 = __shuffle2(r4, r5);
    r6, r7 = __shuffle2(r6, r7);

    r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 5
    zeta0 = zetasp.[u256 208 + 392*i];
    zeta1 = zetasp.[u256 240 + 392*i];

    r0, r4 = __shuffle1(r0, r4);
    r1, r5 = __shuffle1(r1, r5);
    r2, r6 = __shuffle1(r2, r6);
    r3, r7 = __shuffle1(r3, r7);

    r0, r4, r1, r5, r2, r6, r3, r7 = __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 6
    zeta0 = zetasp.[u256 272 + 392*i];
    zeta2 = zetasp.[u256 304 + 392*i];
    zeta1 = zetasp.[u256 336 + 392*i];
    zeta3 = zetasp.[u256 368 + 392*i];

    r0, r4, r2, r6, r1, r5, r3, r7 = __butterfly64x(r0, r4, r2, r6, r1, r5, r3, r7, zeta0, zeta1, zeta2, zeta3, qx16);

    vx16 = jvx16[u256 0];

    r0 = __red16x(r0, qx16, vx16);
    r4 = __red16x(r4, qx16, vx16);
    r2 = __red16x(r2, qx16, vx16);
    r6 = __red16x(r6, qx16, vx16);
    r1 = __red16x(r1, qx16, vx16);
    r5 = __red16x(r5, qx16, vx16);
    r3 = __red16x(r3, qx16, vx16);
    r7 = __red16x(r7, qx16, vx16);

    rp.[u256 32*0+256*i] = r0;
    rp.[u256 32*1+256*i] = r4;
    rp.[u256 32*2+256*i] = r1;
    rp.[u256 32*3+256*i] = r5;
    rp.[u256 32*4+256*i] = r2;
    rp.[u256 32*5+256*i] = r6;
    rp.[u256 32*6+256*i] = r3;
    rp.[u256 32*7+256*i] = r7;
  }

  return rp;
}

inline
fn __poly_reduce(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 r qx16 vx16;
  
  qx16 = jqx16[u256 0];
  vx16 = jvx16[u256 0];

  for i=0 to 16 
  {
    r = rp.[u256 32*i];
    r = __red16x(r, qx16, vx16);
    rp.[u256 32*i] = r;
  }
  return rp;
}

fn _poly_sub(reg ptr u16[KYBER_N] rp ap bp) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 a;
  reg u256 b;
  reg u256 r;

  for i = 0 to 16 {
    a = ap.[u256 32*i];
    b = bp.[u256 32*i];
    r = #VPSUB_16u16(a, b);
    rp.[u256 32*i] = r;
  }

  return rp;
}

fn _poly_tobytes(reg u64 rp, reg ptr u16[KYBER_N] a) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7 qx16 tt ttt;
  reg ptr u16[16] jqx16_p;

  jqx16_p = jqx16;
  qx16 = jqx16_p[u256 0];

  for i = 0 to 2
  {
    t0 = a[u256 8*i];
    t1 = a[u256 8*i + 1];
    t2 = a[u256 8*i + 2];
    t3 = a[u256 8*i + 3];
    t4 = a[u256 8*i + 4];
    t5 = a[u256 8*i + 5];
    t6 = a[u256 8*i + 6];
    t7 = a[u256 8*i + 7];

    t0 = __csubq(t0, qx16);
    t1 = __csubq(t1, qx16);
    t2 = __csubq(t2, qx16);
    t3 = __csubq(t3, qx16);
    t4 = __csubq(t4, qx16);
    t5 = __csubq(t5, qx16);
    t6 = __csubq(t6, qx16);
    t7 = __csubq(t7, qx16);

    tt = #VPSLL_16u16(t1, 12);
    tt |= t0;

    t0 = #VPSRL_16u16(t1, 4);
    t1 = #VPSLL_16u16(t2, 8);
    t0 |= t1;

    t1 = #VPSRL_16u16(t2, 8);
    t2 = #VPSLL_16u16(t3, 4);
    t1 |= t2;

    t2 = #VPSLL_16u16(t5, 12);
    t2 |= t4;

    t3 = #VPSRL_16u16(t5, 4);
    t4 = #VPSLL_16u16(t6, 8);
    t3 |= t4;

    t4 = #VPSRL_16u16(t6, 8);
    t5 = #VPSLL_16u16(t7, 4);
    t4 |= t5;

    ttt, t0 = __shuffle1(tt, t0);
    tt, t2 = __shuffle1(t1, t2);
    t1, t4 = __shuffle1(t3, t4);

    t3, tt= __shuffle2(ttt, tt);
    ttt, t0 = __shuffle2(t1, t0);
    t1, t4 = __shuffle2(t2, t4);

    t2, ttt = __shuffle4(t3, ttt);
    t3, tt = __shuffle4(t1, tt);
    t1, t4 = __shuffle4(t0, t4);

    t0, t3 = __shuffle8(t2, t3);
    t2, ttt = __shuffle8(t1, ttt);
    t1, t4 = __shuffle8(tt, t4);

    (u256)[rp + 192*i] = t0;
    (u256)[rp + 192*i + 32] = t2;
    (u256)[rp + 192*i + 64] = t1;
    (u256)[rp + 192*i + 96] = t3;
    (u256)[rp + 192*i + 128] = ttt;
    (u256)[rp + 192*i + 160] = t4;
  }

  return a;
}

fn _poly_tomsg(reg u64 rp, reg ptr u16[KYBER_N] a) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 g0 g1 hq hhq;
  reg ptr u16[16] px16;
  reg u32 c;

  a = _poly_csubq(a);

  px16 = hqx16_m1;
  hq = px16[u256 0];

  px16 = hhqx16;
  hhq = px16[u256 0];

  for i=0 to KYBER_N/32
  {
    f0 = a[u256 2*i];
    f1 = a[u256 2*i + 1];
    f0 = #VPSUB_16u16(hq, f0);
    f1 = #VPSUB_16u16(hq, f1);
    g0 = #VPSRA_16u16(f0, 15);
    g1 = #VPSRA_16u16(f1, 15);
    f0 = #VPXOR_256(f0, g0);
    f1 = #VPXOR_256(f1, g1);
    f0 = #VPSUB_16u16(f0, hhq);
    f1 = #VPSUB_16u16(f1, hhq);
    f0 = #VPACKSS_16u16(f0, f1);
    f0 = #VPERMQ(f0, 0xD8);
    c = #VPMOVMSKB_u256u32(f0);
    (u32)[rp+4*i] = c;
  }
  return a;
}

// FIXME: E_EPTR
fn _poly_tomsg_1(reg ptr u8[KYBER_INDCPA_MSGBYTES] rp, reg ptr u16[KYBER_N] a) -> reg ptr u8[KYBER_INDCPA_MSGBYTES], reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 g0 g1 hq hhq;
  reg ptr u16[16] px16;
  reg u32 c;

  a = _poly_csubq(a);

  px16 = hqx16_m1;
  hq = px16[u256 0];

  px16 = hhqx16;
  hhq = px16[u256 0];

  for i=0 to KYBER_N/32
  {
    f0 = a[u256 2*i];
    f1 = a[u256 2*i + 1];
    f0 = #VPSUB_16u16(hq, f0);
    f1 = #VPSUB_16u16(hq, f1);
    g0 = #VPSRA_16u16(f0, 15);
    g1 = #VPSRA_16u16(f1, 15);
    f0 = #VPXOR_256(f0, g0);
    f1 = #VPXOR_256(f1, g1);
    f0 = #VPSUB_16u16(f0, hhq);
    f1 = #VPSUB_16u16(f1, hhq);
    f0 = #VPACKSS_16u16(f0, f1);
    f0 = #VPERMQ(f0, 0xD8);
    c = #VPMOVMSKB_u256u32(f0);
    rp[u32 i] = c;
  }
  return rp, a;
}

inline
fn __polyvec_add2(stack u16[KYBER_VECN] r, stack u16[KYBER_VECN] b) -> stack u16[KYBER_VECN]
{
  inline int i;

  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N]         = _poly_add2(r[i*KYBER_N:KYBER_N], b[i*KYBER_N:KYBER_N]);
  }

  return r;
}

inline
fn __polyvec_csubq(stack u16[KYBER_VECN] r) -> stack u16[KYBER_VECN]
{
  inline int i;

  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N] = _poly_csubq(r[i*KYBER_N:KYBER_N]);
  }

  return r;
}

u32 pvd_q_s = 0x0d013404;
u8[32] pvd_shufbdidx_s = {0, 1, 1, 2, 2, 3, 3, 4,
                     5, 6, 6, 7, 7, 8, 8, 9,
                     2, 3, 3, 4, 4, 5, 5, 6,
                     7, 8, 8, 9, 9, 10, 10, 11};
u64 pvd_sllvdidx_s = 0x04;
u32 pvd_mask_s = 0x7fe01ff8;

inline
fn __polyvec_decompress(reg u64 rp) -> stack u16[KYBER_VECN]
{
  inline int i k;
  reg u256 f q shufbidx sllvdidx mask;
  stack u16[KYBER_VECN] r;

  q = #VPBROADCAST_8u32(pvd_q_s);
  shufbidx = pvd_shufbdidx_s[u256 0];
  sllvdidx = #VPBROADCAST_4u64(pvd_sllvdidx_s);
  mask = #VPBROADCAST_8u32(pvd_mask_s);

  for k=0 to KYBER_K
  {
    for i=0 to KYBER_N/16
    {
      f = (u256)[rp + 320 * k + 20 * i];
      f = #VPERMQ(f, 0x94);
      f = #VPSHUFB_256(f, shufbidx);
      f = #VPSLLV_8u32(f, sllvdidx);
      f = #VPSRL_16u16(f, 1);
      f = #VPAND_256(f, mask);
      f = #VPMULHRS_16u16(f, q);
      r[u256 16*k + i] = f;
    }
  }

  return r;
}

u16 pvc_off_s = 0x0f;
u16 pvc_shift1_s = 0x1000;
u16 pvc_mask_s = 0x03ff;
u64 pvc_shift2_s = 0x0400000104000001;
u64 pvc_sllvdidx_s = 0x0C;
u8[32] pvc_shufbidx_s = {0, 1, 2, 3, 4, 8, 9, 10, 11, 12, -1, -1, -1, -1, -1, -1,
                         9, 10, 11, 12, -1, -1, -1, -1, -1, -1, 0, 1, 2, 3, 4, 8};

inline
fn __polyvec_compress(reg u64 rp, stack u16[KYBER_VECN] a)
{
  inline int i;
  reg u256 f0 f1 f2 v v8 off shift1 mask shift2 sllvdidx shufbidx;
  reg u128 t0 t1;
  reg ptr u16[16] x16p;
  reg ptr u8[32] x8p;

  a = __polyvec_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  v8 = #VPSLL_16u16(v, 3);
  off = #VPBROADCAST_16u16(pvc_off_s);
  shift1 = #VPBROADCAST_16u16(pvc_shift1_s);
  mask = #VPBROADCAST_16u16(pvc_mask_s);
  shift2 = #VPBROADCAST_4u64(pvc_shift2_s);
  sllvdidx = #VPBROADCAST_4u64(pvc_sllvdidx_s);
  shufbidx = pvc_shufbidx_s[u256 0];

  for i=0 to KYBER_VECN/16
  {
    f0 = a[u256 i];
    f1 = #VPMULL_16u16(f0, v8);
    f2 = #VPADD_16u16(f0, off);
    f0 = #VPSLL_16u16(f0, 3);
    f0 = #VPMULH_16u16(f0, v);
    f2 = #VPSUB_16u16(f1, f2);
    f1 = #VPANDN_256(f1, f2);
    f1 = #VPSRL_16u16(f1, 15);
    f0 = #VPSUB_16u16(f0, f1);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f0 = #VPAND_256(f0, mask);
    f0 = #VPMADDWD_256(f0, shift2);
    f0 = #VPSLLV_8u32(f0, sllvdidx);
    f0 = #VPSRL_4u64(f0, 12);
    f0 = #VPSHUFB_256(f0, shufbidx);
    t0 = (128u)f0;
    t1 = #VEXTRACTI128(f0, 1);
    t0 = #VPBLEND_8u16(t0, t1, 0xE0);
    (u128)[rp + 20*i] = t0;
    (u32)[rp + 20*i + 16] = #VPEXTR_32(t1, 0);
  }
}

// FIXME: E_PTR
inline
fn __polyvec_compress_1(reg ptr u8[KYBER_POLYVECCOMPRESSEDBYTES] rp, stack u16[KYBER_VECN] a) -> reg ptr u8[KYBER_POLYVECCOMPRESSEDBYTES]
{
  inline int i;
  reg u256 f0 f1 f2 v v8 off shift1 mask shift2 sllvdidx shufbidx;
  reg u128 t0 t1;
  reg ptr u16[16] x16p;
  reg ptr u8[32] x8p;

  a = __polyvec_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  v8 = #VPSLL_16u16(v, 3);
  off = #VPBROADCAST_16u16(pvc_off_s);
  shift1 = #VPBROADCAST_16u16(pvc_shift1_s);
  mask = #VPBROADCAST_16u16(pvc_mask_s);
  shift2 = #VPBROADCAST_4u64(pvc_shift2_s);
  sllvdidx = #VPBROADCAST_4u64(pvc_sllvdidx_s);
  shufbidx = pvc_shufbidx_s[u256 0];

  for i=0 to KYBER_VECN/16
  {
    f0 = a[u256 i];
    f1 = #VPMULL_16u16(f0, v8);
    f2 = #VPADD_16u16(f0, off);
    f0 = #VPSLL_16u16(f0, 3);
    f0 = #VPMULH_16u16(f0, v);
    f2 = #VPSUB_16u16(f1, f2);
    f1 = #VPANDN_256(f1, f2);
    f1 = #VPSRL_16u16(f1, 15);
    f0 = #VPSUB_16u16(f0, f1);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f0 = #VPAND_256(f0, mask);
    f0 = #VPMADDWD_256(f0, shift2);
    f0 = #VPSLLV_8u32(f0, sllvdidx);
    f0 = #VPSRL_4u64(f0, 12);
    f0 = #VPSHUFB_256(f0, shufbidx);
    t0 = (128u)f0;
    t1 = #VEXTRACTI128(f0, 1);
    t0 = #VPBLEND_8u16(t0, t1, 0xE0);
    rp.[u128 20*i] = t0;
    rp.[u32 20*i + 16] = #VPEXTR_32(t1, 0);
  }

  return rp;
}

inline
fn __polyvec_frombytes(reg u64 ap) -> stack u16[KYBER_VECN]
{
  stack u16[KYBER_VECN] r;
  reg u64 pp;
  inline int i;

  pp = ap;
  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N] = _poly_frombytes(r[i*KYBER_N:KYBER_N], pp);
    pp += KYBER_POLYBYTES;
  }

  return r;
}


inline
fn __polyvec_invntt(stack u16[KYBER_VECN] r) -> stack u16[KYBER_VECN]
{
  inline int i;

  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N] = _poly_invntt(r[i*KYBER_N:KYBER_N]);
  }

  return r;
}


inline
fn __polyvec_ntt(stack u16[KYBER_VECN] r) -> stack u16[KYBER_VECN]
{
  inline int i;
  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N] = _poly_ntt(r[i*KYBER_N:KYBER_N]);
  }

  return r;
}


inline
fn __polyvec_reduce(stack u16[KYBER_VECN] r) -> stack u16[KYBER_VECN]
{
  inline int i;

  for i=0 to KYBER_K
  {
    r[i*KYBER_N:KYBER_N] = __poly_reduce(r[i*KYBER_N:KYBER_N]);
  }

  return r;
}


inline
fn __polyvec_pointwise_acc(stack u16[KYBER_N] r, stack u16[KYBER_VECN] a b) -> stack u16[KYBER_N]
{
  stack u16[KYBER_N] t;
  inline int i;

  r = _poly_basemul(r, a[0:KYBER_N], b[0:KYBER_N]);
  for i=1 to KYBER_K
  {
    t = _poly_basemul(t, a[i*KYBER_N:KYBER_N], b[i*KYBER_N:KYBER_N]);
    r = _poly_add2(r, t);
  }

  // r = __poly_reduce(r);
  
  return r;
}


inline
fn __polyvec_tobytes(reg u64 rp, stack u16[KYBER_VECN] a)
{
  reg u64 pp;
  inline int i;

  pp = rp;
  for i=0 to KYBER_K
  {
    a[i*KYBER_N:KYBER_N] = _poly_tobytes(pp, a[i*KYBER_N:KYBER_N]);
    pp += KYBER_POLYBYTES;
  }
}

inline
fn __shake128_squeezeblock4x(reg ptr u256[25] state, reg ptr u8[SHAKE128_RATE] h0 h1 h2 h3) -> reg ptr u256[25], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE]
{
  reg u256 t256;
  reg u128 t128;
  inline int i;

  state = _KeccakF1600_StatePermute4x(state);

	for i = 0 to (SHAKE128_RATE / 8) {
    t256 = state[i];
    t128 = (128u)t256;
		h0[u64 i] = #VMOVLPD(t128);
		h1[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI128(t256, 1);
		h2[u64 i] = #VMOVLPD(t128);
		h3[u64 i] = #VMOVHPD(t128);
	}

  return state, h0, h1, h2, h3;
}

param int GENMATRIX_NBLOCKS = ((12*KYBER_N/8*4096/KYBER_Q + SHAKE128_RATE)/SHAKE128_RATE);
param int REJ_UNIFORM_AVX_BUFLEN = GENMATRIX_NBLOCKS * SHAKE128_RATE;

u8[2048] ru_idx = {-1, -1, -1, -1, -1, -1, -1, -1,
                0, -1, -1, -1, -1, -1, -1, -1,
                2, -1, -1, -1, -1, -1, -1, -1,
                0,  2, -1, -1, -1, -1, -1, -1,
                4, -1, -1, -1, -1, -1, -1, -1,
                0,  4, -1, -1, -1, -1, -1, -1,
                2,  4, -1, -1, -1, -1, -1, -1,
                0,  2,  4, -1, -1, -1, -1, -1,
                6, -1, -1, -1, -1, -1, -1, -1,
                0,  6, -1, -1, -1, -1, -1, -1,
                2,  6, -1, -1, -1, -1, -1, -1,
                0,  2,  6, -1, -1, -1, -1, -1,
                4,  6, -1, -1, -1, -1, -1, -1,
                0,  4,  6, -1, -1, -1, -1, -1,
                2,  4,  6, -1, -1, -1, -1, -1,
                0,  2,  4,  6, -1, -1, -1, -1,
                8, -1, -1, -1, -1, -1, -1, -1,
                0,  8, -1, -1, -1, -1, -1, -1,
                2,  8, -1, -1, -1, -1, -1, -1,
                0,  2,  8, -1, -1, -1, -1, -1,
                4,  8, -1, -1, -1, -1, -1, -1,
                0,  4,  8, -1, -1, -1, -1, -1,
                2,  4,  8, -1, -1, -1, -1, -1,
                0,  2,  4,  8, -1, -1, -1, -1,
                6,  8, -1, -1, -1, -1, -1, -1,
                0,  6,  8, -1, -1, -1, -1, -1,
                2,  6,  8, -1, -1, -1, -1, -1,
                0,  2,  6,  8, -1, -1, -1, -1,
                4,  6,  8, -1, -1, -1, -1, -1,
                0,  4,  6,  8, -1, -1, -1, -1,
                2,  4,  6,  8, -1, -1, -1, -1,
                0,  2,  4,  6,  8, -1, -1, -1,
                10, -1, -1, -1, -1, -1, -1, -1,
                0, 10, -1, -1, -1, -1, -1, -1,
                2, 10, -1, -1, -1, -1, -1, -1,
                0,  2, 10, -1, -1, -1, -1, -1,
                4, 10, -1, -1, -1, -1, -1, -1,
                0,  4, 10, -1, -1, -1, -1, -1,
                2,  4, 10, -1, -1, -1, -1, -1,
                0,  2,  4, 10, -1, -1, -1, -1,
                6, 10, -1, -1, -1, -1, -1, -1,
                0,  6, 10, -1, -1, -1, -1, -1,
                2,  6, 10, -1, -1, -1, -1, -1,
                0,  2,  6, 10, -1, -1, -1, -1,
                4,  6, 10, -1, -1, -1, -1, -1,
                0,  4,  6, 10, -1, -1, -1, -1,
                2,  4,  6, 10, -1, -1, -1, -1,
                0,  2,  4,  6, 10, -1, -1, -1,
                8, 10, -1, -1, -1, -1, -1, -1,
                0,  8, 10, -1, -1, -1, -1, -1,
                2,  8, 10, -1, -1, -1, -1, -1,
                0,  2,  8, 10, -1, -1, -1, -1,
                4,  8, 10, -1, -1, -1, -1, -1,
                0,  4,  8, 10, -1, -1, -1, -1,
                2,  4,  8, 10, -1, -1, -1, -1,
                0,  2,  4,  8, 10, -1, -1, -1,
                6,  8, 10, -1, -1, -1, -1, -1,
                0,  6,  8, 10, -1, -1, -1, -1,
                2,  6,  8, 10, -1, -1, -1, -1,
                0,  2,  6,  8, 10, -1, -1, -1,
                4,  6,  8, 10, -1, -1, -1, -1,
                0,  4,  6,  8, 10, -1, -1, -1,
                2,  4,  6,  8, 10, -1, -1, -1,
                0,  2,  4,  6,  8, 10, -1, -1,
                12, -1, -1, -1, -1, -1, -1, -1,
                0, 12, -1, -1, -1, -1, -1, -1,
                2, 12, -1, -1, -1, -1, -1, -1,
                0,  2, 12, -1, -1, -1, -1, -1,
                4, 12, -1, -1, -1, -1, -1, -1,
                0,  4, 12, -1, -1, -1, -1, -1,
                2,  4, 12, -1, -1, -1, -1, -1,
                0,  2,  4, 12, -1, -1, -1, -1,
                6, 12, -1, -1, -1, -1, -1, -1,
                0,  6, 12, -1, -1, -1, -1, -1,
                2,  6, 12, -1, -1, -1, -1, -1,
                0,  2,  6, 12, -1, -1, -1, -1,
                4,  6, 12, -1, -1, -1, -1, -1,
                0,  4,  6, 12, -1, -1, -1, -1,
                2,  4,  6, 12, -1, -1, -1, -1,
                0,  2,  4,  6, 12, -1, -1, -1,
                8, 12, -1, -1, -1, -1, -1, -1,
                0,  8, 12, -1, -1, -1, -1, -1,
                2,  8, 12, -1, -1, -1, -1, -1,
                0,  2,  8, 12, -1, -1, -1, -1,
                4,  8, 12, -1, -1, -1, -1, -1,
                0,  4,  8, 12, -1, -1, -1, -1,
                2,  4,  8, 12, -1, -1, -1, -1,
                0,  2,  4,  8, 12, -1, -1, -1,
                6,  8, 12, -1, -1, -1, -1, -1,
                0,  6,  8, 12, -1, -1, -1, -1,
                2,  6,  8, 12, -1, -1, -1, -1,
                0,  2,  6,  8, 12, -1, -1, -1,
                4,  6,  8, 12, -1, -1, -1, -1,
                0,  4,  6,  8, 12, -1, -1, -1,
                2,  4,  6,  8, 12, -1, -1, -1,
                0,  2,  4,  6,  8, 12, -1, -1,
                10, 12, -1, -1, -1, -1, -1, -1,
                0, 10, 12, -1, -1, -1, -1, -1,
                2, 10, 12, -1, -1, -1, -1, -1,
                0,  2, 10, 12, -1, -1, -1, -1,
                4, 10, 12, -1, -1, -1, -1, -1,
                0,  4, 10, 12, -1, -1, -1, -1,
                2,  4, 10, 12, -1, -1, -1, -1,
                0,  2,  4, 10, 12, -1, -1, -1,
                6, 10, 12, -1, -1, -1, -1, -1,
                0,  6, 10, 12, -1, -1, -1, -1,
                2,  6, 10, 12, -1, -1, -1, -1,
                0,  2,  6, 10, 12, -1, -1, -1,
                4,  6, 10, 12, -1, -1, -1, -1,
                0,  4,  6, 10, 12, -1, -1, -1,
                2,  4,  6, 10, 12, -1, -1, -1,
                0,  2,  4,  6, 10, 12, -1, -1,
                8, 10, 12, -1, -1, -1, -1, -1,
                0,  8, 10, 12, -1, -1, -1, -1,
                2,  8, 10, 12, -1, -1, -1, -1,
                0,  2,  8, 10, 12, -1, -1, -1,
                4,  8, 10, 12, -1, -1, -1, -1,
                0,  4,  8, 10, 12, -1, -1, -1,
                2,  4,  8, 10, 12, -1, -1, -1,
                0,  2,  4,  8, 10, 12, -1, -1,
                6,  8, 10, 12, -1, -1, -1, -1,
                0,  6,  8, 10, 12, -1, -1, -1,
                2,  6,  8, 10, 12, -1, -1, -1,
                0,  2,  6,  8, 10, 12, -1, -1,
                4,  6,  8, 10, 12, -1, -1, -1,
                0,  4,  6,  8, 10, 12, -1, -1,
                2,  4,  6,  8, 10, 12, -1, -1,
                0,  2,  4,  6,  8, 10, 12, -1,
                14, -1, -1, -1, -1, -1, -1, -1,
                0, 14, -1, -1, -1, -1, -1, -1,
                2, 14, -1, -1, -1, -1, -1, -1,
                0,  2, 14, -1, -1, -1, -1, -1,
                4, 14, -1, -1, -1, -1, -1, -1,
                0,  4, 14, -1, -1, -1, -1, -1,
                2,  4, 14, -1, -1, -1, -1, -1,
                0,  2,  4, 14, -1, -1, -1, -1,
                6, 14, -1, -1, -1, -1, -1, -1,
                0,  6, 14, -1, -1, -1, -1, -1,
                2,  6, 14, -1, -1, -1, -1, -1,
                0,  2,  6, 14, -1, -1, -1, -1,
                4,  6, 14, -1, -1, -1, -1, -1,
                0,  4,  6, 14, -1, -1, -1, -1,
                2,  4,  6, 14, -1, -1, -1, -1,
                0,  2,  4,  6, 14, -1, -1, -1,
                8, 14, -1, -1, -1, -1, -1, -1,
                0,  8, 14, -1, -1, -1, -1, -1,
                2,  8, 14, -1, -1, -1, -1, -1,
                0,  2,  8, 14, -1, -1, -1, -1,
                4,  8, 14, -1, -1, -1, -1, -1,
                0,  4,  8, 14, -1, -1, -1, -1,
                2,  4,  8, 14, -1, -1, -1, -1,
                0,  2,  4,  8, 14, -1, -1, -1,
                6,  8, 14, -1, -1, -1, -1, -1,
                0,  6,  8, 14, -1, -1, -1, -1,
                2,  6,  8, 14, -1, -1, -1, -1,
                0,  2,  6,  8, 14, -1, -1, -1,
                4,  6,  8, 14, -1, -1, -1, -1,
                0,  4,  6,  8, 14, -1, -1, -1,
                2,  4,  6,  8, 14, -1, -1, -1,
                0,  2,  4,  6,  8, 14, -1, -1,
                10, 14, -1, -1, -1, -1, -1, -1,
                0, 10, 14, -1, -1, -1, -1, -1,
                2, 10, 14, -1, -1, -1, -1, -1,
                0,  2, 10, 14, -1, -1, -1, -1,
                4, 10, 14, -1, -1, -1, -1, -1,
                0,  4, 10, 14, -1, -1, -1, -1,
                2,  4, 10, 14, -1, -1, -1, -1,
                0,  2,  4, 10, 14, -1, -1, -1,
                6, 10, 14, -1, -1, -1, -1, -1,
                0,  6, 10, 14, -1, -1, -1, -1,
                2,  6, 10, 14, -1, -1, -1, -1,
                0,  2,  6, 10, 14, -1, -1, -1,
                4,  6, 10, 14, -1, -1, -1, -1,
                0,  4,  6, 10, 14, -1, -1, -1,
                2,  4,  6, 10, 14, -1, -1, -1,
                0,  2,  4,  6, 10, 14, -1, -1,
                8, 10, 14, -1, -1, -1, -1, -1,
                0,  8, 10, 14, -1, -1, -1, -1,
                2,  8, 10, 14, -1, -1, -1, -1,
                0,  2,  8, 10, 14, -1, -1, -1,
                4,  8, 10, 14, -1, -1, -1, -1,
                0,  4,  8, 10, 14, -1, -1, -1,
                2,  4,  8, 10, 14, -1, -1, -1,
                0,  2,  4,  8, 10, 14, -1, -1,
                6,  8, 10, 14, -1, -1, -1, -1,
                0,  6,  8, 10, 14, -1, -1, -1,
                2,  6,  8, 10, 14, -1, -1, -1,
                0,  2,  6,  8, 10, 14, -1, -1,
                4,  6,  8, 10, 14, -1, -1, -1,
                0,  4,  6,  8, 10, 14, -1, -1,
                2,  4,  6,  8, 10, 14, -1, -1,
                0,  2,  4,  6,  8, 10, 14, -1,
                12, 14, -1, -1, -1, -1, -1, -1,
                0, 12, 14, -1, -1, -1, -1, -1,
                2, 12, 14, -1, -1, -1, -1, -1,
                0,  2, 12, 14, -1, -1, -1, -1,
                4, 12, 14, -1, -1, -1, -1, -1,
                0,  4, 12, 14, -1, -1, -1, -1,
                2,  4, 12, 14, -1, -1, -1, -1,
                0,  2,  4, 12, 14, -1, -1, -1,
                6, 12, 14, -1, -1, -1, -1, -1,
                0,  6, 12, 14, -1, -1, -1, -1,
                2,  6, 12, 14, -1, -1, -1, -1,
                0,  2,  6, 12, 14, -1, -1, -1,
                4,  6, 12, 14, -1, -1, -1, -1,
                0,  4,  6, 12, 14, -1, -1, -1,
                2,  4,  6, 12, 14, -1, -1, -1,
                0,  2,  4,  6, 12, 14, -1, -1,
                8, 12, 14, -1, -1, -1, -1, -1,
                0,  8, 12, 14, -1, -1, -1, -1,
                2,  8, 12, 14, -1, -1, -1, -1,
                0,  2,  8, 12, 14, -1, -1, -1,
                4,  8, 12, 14, -1, -1, -1, -1,
                0,  4,  8, 12, 14, -1, -1, -1,
                2,  4,  8, 12, 14, -1, -1, -1,
                0,  2,  4,  8, 12, 14, -1, -1,
                6,  8, 12, 14, -1, -1, -1, -1,
                0,  6,  8, 12, 14, -1, -1, -1,
                2,  6,  8, 12, 14, -1, -1, -1,
                0,  2,  6,  8, 12, 14, -1, -1,
                4,  6,  8, 12, 14, -1, -1, -1,
                0,  4,  6,  8, 12, 14, -1, -1,
                2,  4,  6,  8, 12, 14, -1, -1,
                0,  2,  4,  6,  8, 12, 14, -1,
                10, 12, 14, -1, -1, -1, -1, -1,
                0, 10, 12, 14, -1, -1, -1, -1,
                2, 10, 12, 14, -1, -1, -1, -1,
                0,  2, 10, 12, 14, -1, -1, -1,
                4, 10, 12, 14, -1, -1, -1, -1,
                0,  4, 10, 12, 14, -1, -1, -1,
                2,  4, 10, 12, 14, -1, -1, -1,
                0,  2,  4, 10, 12, 14, -1, -1,
                6, 10, 12, 14, -1, -1, -1, -1,
                0,  6, 10, 12, 14, -1, -1, -1,
                2,  6, 10, 12, 14, -1, -1, -1,
                0,  2,  6, 10, 12, 14, -1, -1,
                4,  6, 10, 12, 14, -1, -1, -1,
                0,  4,  6, 10, 12, 14, -1, -1,
                2,  4,  6, 10, 12, 14, -1, -1,
                0,  2,  4,  6, 10, 12, 14, -1,
                8, 10, 12, 14, -1, -1, -1, -1,
                0,  8, 10, 12, 14, -1, -1, -1,
                2,  8, 10, 12, 14, -1, -1, -1,
                0,  2,  8, 10, 12, 14, -1, -1,
                4,  8, 10, 12, 14, -1, -1, -1,
                0,  4,  8, 10, 12, 14, -1, -1,
                2,  4,  8, 10, 12, 14, -1, -1,
                0,  2,  4,  8, 10, 12, 14, -1,
                6,  8, 10, 12, 14, -1, -1, -1,
                0,  6,  8, 10, 12, 14, -1, -1,
                2,  6,  8, 10, 12, 14, -1, -1,
                0,  2,  6,  8, 10, 12, 14, -1,
                4,  6,  8, 10, 12, 14, -1, -1,
                0,  4,  6,  8, 10, 12, 14, -1,
                2,  4,  6,  8, 10, 12, 14, -1,
                0,  2,  4,  6,  8, 10, 12, 14};

inline
fn __shake128_squeezenblocks(reg u256[7] state, stack u8[REJ_UNIFORM_AVX_BUFLEN] out)
      -> reg u256[7], stack u8[REJ_UNIFORM_AVX_BUFLEN]
{
  inline int i;

  for i = 0 to GENMATRIX_NBLOCKS 
  {
      state, out[i*SHAKE128_RATE:SHAKE128_RATE] = __shake128_squeezeblock(state, out[i*SHAKE128_RATE:SHAKE128_RATE]);
  }
  return state, out;
}

inline
fn __shake128_squeezenblocks4x(reg ptr u256[25] state, reg ptr u8[REJ_UNIFORM_AVX_BUFLEN] h0 h1 h2 h3)
  -> reg ptr u256[25], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN], reg ptr u8[REJ_UNIFORM_AVX_BUFLEN]
{
  inline int i;

  for i = 0 to GENMATRIX_NBLOCKS
  {
    state, h0[i*SHAKE128_RATE:SHAKE128_RATE], h1[i*SHAKE128_RATE:SHAKE128_RATE], h2[i*SHAKE128_RATE:SHAKE128_RATE], h3[i*SHAKE128_RATE:SHAKE128_RATE] = __shake128_squeezeblock4x(state, h0[i*SHAKE128_RATE:SHAKE128_RATE], h1[i*SHAKE128_RATE:SHAKE128_RATE], h2[i*SHAKE128_RATE:SHAKE128_RATE], h3[i*SHAKE128_RATE:SHAKE128_RATE]);
  }

  return state, h0, h1, h2, h3;
}

inline
fn __rej_uniform(reg ptr u16[KYBER_N] rp, reg u64 offset, reg ptr u8[SHAKE128_RATE] buf, inline int buflen) ->  reg u64, stack u16[KYBER_N]
{
  reg u16 val0 val1;
  reg u16 t;
  reg u64 pos ctr;
  reg u8 fl1 fl2;
  reg bool b;

  ctr = offset;
  pos = 0;

  ?{ "<=u" = b }= #CMP_64(ctr, KYBER_N - 1);
  fl1 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(pos, buflen - 3);
  fl2 = #SETcc(b);

  _, _, _, _, b = #TEST_8(fl1, fl2);

  while(!b)
  {
    val0 = (16u)buf[(int)pos];
    pos += 1;

    t   = (16u)buf[(int)pos];
    val1 = t;
    val1 >>= 4;

    t &= 0x0F;
    t <<= 8;
    val0 |= t;
    pos += 1;

    t   = (16u)buf[(int)pos];
    t <<= 4;
    val1 |= t;
    pos += 1;

    if(val0 < KYBER_Q)
    {
      rp[(int)ctr] = val0;
      ctr += 1;
    }

    if(ctr < KYBER_N)
    {
      if(val1 < KYBER_Q)
      {
        rp[(int)ctr] = val1;
        ctr += 1;
      }
    }

    ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 1);
    fl1 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(pos, buflen - 3);
    fl2 = #SETcc(b);

    _, _, _, _, b = #TEST_8(fl1, fl2);
  }

  return ctr, rp;
}

u8 ru_ones_s = 1;
u16 ru_mask_s = 0x0FFF;
u8[32] ru_idx8_s = {0, 1, 1, 2, 3, 4, 4, 5,
                 6, 7, 7, 8, 9, 10, 10, 11,
                 4, 5, 5, 6, 7, 8, 8, 9,
                 10, 11, 11, 12, 13, 14, 14, 15};

fn _rej_uniform_avx(reg ptr u16[KYBER_N] rp, reg ptr u8[REJ_UNIFORM_AVX_BUFLEN] buf) -> reg u64, reg ptr u16[KYBER_N]
{
  reg u256 f0 f1 g0 g1 g2 g3;
  reg u256 bound ones mask idx8;
  reg u128 f t l h;
  reg u64 pos ctr t64 t64_1 t64_2 t64_3;
  reg u64 good;
  reg u16 val0 val1 t16;
  reg ptr u8[2048] idxp;
  reg u8 fl1 fl2;
  reg bool b;

  idxp = ru_idx;

  bound = jqx16[u256 0];
  ctr = 0;
  pos = 0;
  ones = #VPBROADCAST_32u8(ru_ones_s);
  mask = #VPBROADCAST_16u16(ru_mask_s);
  idx8 = ru_idx8_s[u256 0];

  ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 32);
  fl1 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 48);
  fl2 = #SETcc(b);

   _, _, _, _, b = #TEST_8(fl1, fl2);

  while(!b)
  {
    f0 = #VPERMQ(buf.[u256 (int)pos], 0x94);
    f1 = #VPERMQ(buf.[u256 24 + (int)pos], 0x94);
    f0 = #VPSHUFB_256(f0, idx8);
    f1 = #VPSHUFB_256(f1, idx8);
    g0 = #VPSRL_16u16(f0, 4);
    g1 = #VPSRL_16u16(f1, 4);
    f0 = #VPBLEND_16u16(f0, g0, 0xAA);
    f1 = #VPBLEND_16u16(f1, g1, 0xAA);
    f0 = #VPAND_256(f0, mask);
    f1 = #VPAND_256(f1, mask);

    g0 = #VPCMPGT_16u16(bound, f0);
    g1 = #VPCMPGT_16u16(bound, f1);

    g0 = #VPACKSS_16u16(g0, g1);
    good = #VPMOVMSKB_u256u64(g0);

    t64 = good;
    t64 &= 0xFF;
    g0 = (256u) #VMOV(idxp[u64 (int)t64]);

    t64_1 = good;
    t64_1 >>= 16;
    t64_1 &= 0xFF;
    l = #VMOV(idxp[u64 (int)t64_1]);

    t64_2 = good;
    t64_2 >>= 8;
    t64_2 &= 0xFF;
    g1 = (256u) #VMOV(idxp[u64 (int)t64_2]);

    t64_3 = good;
    t64_3 >>= 24;
    t64_3 &= 0xFF;
    h = #VMOV(idxp[u64 (int)t64_3]);

    g0 = #VINSERTI128(g0, l, 1);

    _, _, _, _, _, t64 = #POPCNT_64(t64);
    _, _, _, _, _, t64_1 = #POPCNT_64(t64_1);
    t64 += ctr;

    g1 = #VINSERTI128(g1, h, 1);

    t64_1 += t64;
    _, _, _, _, _, t64_2 = #POPCNT_64(t64_2);
    t64_2 += t64_1;
    _, _, _, _, _, t64_3 = #POPCNT_64(t64_3);
    t64_3 += t64_2;

    g2 = #VPADD_32u8(g0, ones);
    g0 = #VPUNPCKL_32u8(g0, g2);
    g3 = #VPADD_32u8(g1, ones);
    g1 = #VPUNPCKL_32u8(g1, g3);

    f0 = #VPSHUFB_256(f0, g0);
    f1 = #VPSHUFB_256(f1, g1);

    rp.[u128 2*(int)ctr] = (128u)f0;
    rp.[u128 2*(int)t64] = #VEXTRACTI128(f0, 1);
    rp.[u128 2*(int)t64_1] = (128u)f1;
    rp.[u128 2*(int)t64_2] = #VEXTRACTI128(f1, 1);

    ctr = t64_3;

    ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 32);
    fl1 = #SETcc(b);

    pos += 48;
    ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 48);
    fl2 = #SETcc(b);

     _, _, _, _, b = #TEST_8(fl1, fl2);
  }

  ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 8);
  fl1 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 12);
  fl2 = #SETcc(b);

   _, _, _, _, b = #TEST_8(fl1, fl2);

  t64 = 0x5555;
  while(!b)
  {
    f = buf.[u128 (int)pos];
    f = #VPSHUFB_128(f, idx8);
    t = #VPSRL_8u16(f, 4);
    f = #VPBLEND_8u16(f, t, 0xAA);
    f = #VPAND_128(f, mask);

    t = #VPCMPGT_8u16(bound, f);
    good = #VPMOVMSKB_u128u64(t);

    good = #PEXT_64(good, t64);
    l = #VMOV(idxp[u64 (int)good]);
    _, _, _, _, _, good =  #POPCNT_64(good);

    h = #VPADD_16u8(l, ones);
    l = #VPUNPCKL_16u8(l, h);
    f = #VPSHUFB_128(f, l);

    rp.[u128 2*(int)ctr] = f;
    ctr += good;

    pos += 12;
    ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 8);
    fl1 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 12);
    fl2 = #SETcc(b);

     _, _, _, _, b = #TEST_8(fl1, fl2);
  }

  ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 1);
  fl1 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 3);
  fl2 = #SETcc(b);

   _, _, _, _, b = #TEST_8(fl1, fl2);

  while(!b)
  {
    val0 = (16u)buf[(int)pos];
    pos += 1;
    t16 = (16u)buf[(int)pos];
    pos += 1;
    val1 = t16;

    t16 <<= 8;
    val0 |= t16;
    val0 &= 0xFFF;

    val1 >>= 4;
    t16 = (16u)buf[(int)pos];
    pos += 1;
    t16 <<= 4;
    val1 |= t16;

    if(val0 < KYBER_Q)
    {
      rp[(int)ctr] = val0;
      ctr += 1;
    }
    if(val1 < KYBER_Q)
    {
      if(ctr < KYBER_N)
      {
        rp[(int)ctr] = val1;
        ctr += 1;
      }
    }

    ?{ "<=u" = b } = #CMP_64(ctr, KYBER_N - 1);
    fl1 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(pos, REJ_UNIFORM_AVX_BUFLEN - 3);
    fl2 = #SETcc(b);

    _, _, _, _, b = #TEST_8(fl1, fl2);
  }

  return ctr, rp;
}

inline
fn __gen_matrix(stack u8[KYBER_SYMBYTES] seed, inline int transposed) -> stack u16[KYBER_K*KYBER_VECN]
{
  stack u8[REJ_UNIFORM_AVX_BUFLEN] buf0;
  stack u8[REJ_UNIFORM_AVX_BUFLEN] buf1;
  stack u8[REJ_UNIFORM_AVX_BUFLEN] buf2;
  stack u8[REJ_UNIFORM_AVX_BUFLEN] buf3;
  reg u256[7] statev;
  stack u64[28] s_state;
  stack u256[25] state;
  stack u16[KYBER_K*KYBER_VECN] rr;
  stack u256 fs;
  reg u256 f;
  reg u64 ctr0 ctr1 ctr2 ctr3 tmp;
  stack u64 ctr0_s;
  reg u8 flg0 flg1 bflg;
  reg bool b;
  reg bool zf;

  inline int i, j;

  f = seed[u256 0];
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;
  fs = f;

  if(transposed == 1)
  {
    buf0[KYBER_SYMBYTES]   = 0;
    buf0[KYBER_SYMBYTES+1] = 0;
    buf1[KYBER_SYMBYTES]   = 0;
    buf1[KYBER_SYMBYTES+1] = 1;
    buf2[KYBER_SYMBYTES]   = 0;
    buf2[KYBER_SYMBYTES+1] = 2;
    buf3[KYBER_SYMBYTES]   = 1;
    buf3[KYBER_SYMBYTES+1] = 0;
  }
  else
  {
    buf0[KYBER_SYMBYTES]   = 0;
    buf0[KYBER_SYMBYTES+1] = 0;
    buf1[KYBER_SYMBYTES]   = 1;
    buf1[KYBER_SYMBYTES+1] = 0;
    buf2[KYBER_SYMBYTES]   = 2;
    buf2[KYBER_SYMBYTES+1] = 0;
    buf3[KYBER_SYMBYTES]   = 0;
    buf3[KYBER_SYMBYTES+1] = 1;
  }

  state = _shake128_absorb4x_34(state, buf0[0:34], buf1[0:34], buf2[0:34], buf3[0:34]);
  state, buf0, buf1, buf2, buf3 = __shake128_squeezenblocks4x(state, buf0, buf1, buf2, buf3);

  tmp, rr[0*KYBER_VECN+0*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[0*KYBER_VECN+0*KYBER_N:KYBER_N], buf0);
  ctr0 = tmp;
  tmp, rr[0*KYBER_VECN+1*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[0*KYBER_VECN+1*KYBER_N:KYBER_N], buf1);
  ctr1 = tmp;
  tmp, rr[0*KYBER_VECN+2*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[0*KYBER_VECN+2*KYBER_N:KYBER_N], buf2);
  ctr2 = tmp;
  ctr3, rr[1*KYBER_VECN+0*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[1*KYBER_VECN+0*KYBER_N:KYBER_N], buf3);

  ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
  flg0 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(ctr1, KYBER_N - 1);
  flg1 = #SETcc(b);

  _, _, _, _, _, bflg = #OR_8(flg0, flg1);

  ?{ "<=u" = b } = #CMP_64(ctr2, KYBER_N - 1);
  flg0 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(ctr3, KYBER_N - 1);
  flg1 = #SETcc(b);

  _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
  _, _, _, _, zf, _ = #OR_8(flg0, bflg);

  while (!zf) {
    state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE] = __shake128_squeezeblock4x(state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE]);

    ctr0, rr[0*KYBER_VECN+0*KYBER_N:KYBER_N] = __rej_uniform(rr[0*KYBER_VECN+0*KYBER_N:KYBER_N], ctr0, buf0[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr1, rr[0*KYBER_VECN+1*KYBER_N:KYBER_N] = __rej_uniform(rr[0*KYBER_VECN+1*KYBER_N:KYBER_N], ctr1, buf1[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr2, rr[0*KYBER_VECN+2*KYBER_N:KYBER_N] = __rej_uniform(rr[0*KYBER_VECN+2*KYBER_N:KYBER_N], ctr2, buf2[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr3, rr[1*KYBER_VECN+0*KYBER_N:KYBER_N] = __rej_uniform(rr[1*KYBER_VECN+0*KYBER_N:KYBER_N], ctr3, buf3[0:SHAKE128_RATE], SHAKE128_RATE);

    ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
    flg0 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(ctr1, KYBER_N - 1);
    flg1 = #SETcc(b);

    _, _, _, _, _, bflg = #OR_8(flg0, flg1);

    ?{ "<=u" = b } = #CMP_64(ctr2, KYBER_N - 1);
    flg0 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(ctr3, KYBER_N - 1);
    flg1 = #SETcc(b);

    _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
    _, _, _, _, zf, _ = #OR_8(flg0, bflg);
  }
  
  f = fs;
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;
  fs = f;

  if(transposed == 1)
  {
    buf0[KYBER_SYMBYTES]   = 1;
    buf0[KYBER_SYMBYTES+1] = 1;
    buf1[KYBER_SYMBYTES]   = 1;
    buf1[KYBER_SYMBYTES+1] = 2;
    buf2[KYBER_SYMBYTES]   = 2;
    buf2[KYBER_SYMBYTES+1] = 0;
    buf3[KYBER_SYMBYTES]   = 2;
    buf3[KYBER_SYMBYTES+1] = 1;
  }
  else
  {
    buf0[KYBER_SYMBYTES]   = 1;
    buf0[KYBER_SYMBYTES+1] = 1;
    buf1[KYBER_SYMBYTES]   = 2;
    buf1[KYBER_SYMBYTES+1] = 1;
    buf2[KYBER_SYMBYTES]   = 0;
    buf2[KYBER_SYMBYTES+1] = 2;
    buf3[KYBER_SYMBYTES]   = 1;
    buf3[KYBER_SYMBYTES+1] = 2;
  }

  state = _shake128_absorb4x_34(state, buf0[0:34], buf1[0:34], buf2[0:34], buf3[0:34]);
  state, buf0, buf1, buf2, buf3 = __shake128_squeezenblocks4x(state, buf0, buf1, buf2, buf3);

  tmp, rr[1*KYBER_VECN+1*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[1*KYBER_VECN+1*KYBER_N:KYBER_N], buf0);
  ctr0 = tmp;
  tmp, rr[1*KYBER_VECN+2*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[1*KYBER_VECN+2*KYBER_N:KYBER_N], buf1);
  ctr1 = tmp;
  tmp, rr[2*KYBER_VECN+0*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[2*KYBER_VECN+0*KYBER_N:KYBER_N], buf2);
  ctr2 = tmp;
  ctr3, rr[2*KYBER_VECN+1*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[2*KYBER_VECN+1*KYBER_N:KYBER_N], buf3);

  ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
  flg0 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(ctr1, KYBER_N - 1);
  flg1 = #SETcc(b);

  _, _, _, _, _, bflg = #OR_8(flg0, flg1);

  ?{ "<=u" = b } = #CMP_64(ctr2, KYBER_N - 1);
  flg0 = #SETcc(b);

  ?{ "<=u" = b } = #CMP_64(ctr3, KYBER_N - 1);
  flg1 = #SETcc(b);

  _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
  _, _, _, _, zf, _ = #OR_8(flg0, bflg);


  while(!zf) {
    state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE] = __shake128_squeezeblock4x(state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE]);

    ctr0, rr[1*KYBER_VECN+1*KYBER_N:KYBER_N] = __rej_uniform(rr[1*KYBER_VECN+1*KYBER_N:KYBER_N], ctr0, buf0[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr1, rr[1*KYBER_VECN+2*KYBER_N:KYBER_N] = __rej_uniform(rr[1*KYBER_VECN+2*KYBER_N:KYBER_N], ctr1, buf1[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr2, rr[2*KYBER_VECN+0*KYBER_N:KYBER_N] = __rej_uniform(rr[2*KYBER_VECN+0*KYBER_N:KYBER_N], ctr2, buf2[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr3, rr[2*KYBER_VECN+1*KYBER_N:KYBER_N] = __rej_uniform(rr[2*KYBER_VECN+1*KYBER_N:KYBER_N], ctr3, buf3[0:SHAKE128_RATE], SHAKE128_RATE);

    ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
    flg0 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(ctr1, KYBER_N - 1);
    flg1 = #SETcc(b);

    _, _, _, _, _, bflg = #OR_8(flg0, flg1);

    ?{ "<=u" = b } = #CMP_64(ctr2, KYBER_N - 1);
    flg0 = #SETcc(b);

    ?{ "<=u" = b } = #CMP_64(ctr3, KYBER_N - 1);
    flg1 = #SETcc(b);

    _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
    _, _, _, _, zf, _ = #OR_8(flg0, bflg);
  }

  f = fs;
  buf0[u256 0] = f;
  buf0[KYBER_SYMBYTES]   = 2;
  buf0[KYBER_SYMBYTES+1] = 2;

  statev = __shake128_absorb34(statev, buf0[0:34]);
  statev, buf0 = __shake128_squeezenblocks(statev, buf0);

  // spill state to free registers for rejection sampling
  for i=0 to 7 { s_state[u256 i] = statev[i]; } 

  ctr0, rr[2*KYBER_VECN+2*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[2*KYBER_VECN+2*KYBER_N:KYBER_N], buf0);

  ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
  bflg = #SETcc(b);

  for i=0 to 7 { statev[i] = s_state[u256 i]; }
  
  while(bflg != 0) {
    statev, buf0[0:SHAKE128_RATE] = __shake128_squeezeblock(statev, buf0[0:SHAKE128_RATE]);

    ctr0, rr[2*KYBER_VECN+2*KYBER_N:KYBER_N] = __rej_uniform(rr[2*KYBER_VECN+2*KYBER_N:KYBER_N], ctr0, buf0[0:SHAKE128_RATE], SHAKE128_RATE);

    ?{ "<=u" = b } = #CMP_64(ctr0, KYBER_N - 1);
    bflg = #SETcc(b);
  }

  for i = 0 to KYBER_K
  {
    for j = 0 to KYBER_K
    {
      rr[i*KYBER_VECN+j*KYBER_N:KYBER_N] = _nttunpack(rr[i*KYBER_VECN+j*KYBER_N:KYBER_N]);
    }
  }

  return rr;
}

inline
fn __indcpa_keypair(reg u64 pkp, reg u64 skp)
{
  stack u64 spkp sskp;
  stack u8[KYBER_SYMBYTES] rb;
  stack u16[KYBER_K*KYBER_VECN] aa;
  stack u16[KYBER_VECN] e pkpv skpv;
  stack u8[64] buf;
  stack u8[KYBER_SYMBYTES] publicseed noiseseed;
  stack u8[32] inbuf;
  reg u64 t64;
  reg u8 nonce;
  inline int i;

  spkp = pkp;
  sskp = skp;

  rb = #randombytes(rb);

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = rb[u64 i];
    inbuf[u64 i] = t64;
  }

  buf = _isha3_512_32(buf, inbuf);

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = buf[u64 i];
    publicseed[u64 i] = t64;
    t64 = buf[u64 i + KYBER_SYMBYTES/8];
    noiseseed[u64 i] = t64;
  }

  aa = __gen_matrix(publicseed, 0);

  nonce = 0;
  skpv[0:KYBER_N], skpv[KYBER_N:KYBER_N], skpv[2*KYBER_N:KYBER_N], e[0:KYBER_N] = _poly_getnoise_eta1_4x(skpv[0:KYBER_N], skpv[KYBER_N:KYBER_N], skpv[2*KYBER_N:KYBER_N], e[0:KYBER_N], noiseseed, nonce);

  nonce = 4;
  e[KYBER_N:KYBER_N], e[2*KYBER_N:KYBER_N], pkpv[0:KYBER_N], pkpv[KYBER_N:KYBER_N] = _poly_getnoise_eta1_4x(e[KYBER_N:KYBER_N], e[2*KYBER_N:KYBER_N], pkpv[0:KYBER_N], pkpv[KYBER_N:KYBER_N], noiseseed, nonce);

  skpv = __polyvec_ntt(skpv);
  e    = __polyvec_ntt(e);

  for i=0 to KYBER_K
  {
    pkpv[i*KYBER_N:KYBER_N] = __polyvec_pointwise_acc(pkpv[i*KYBER_N:KYBER_N], aa[i*KYBER_VECN:KYBER_VECN], skpv);
    pkpv[i*KYBER_N:KYBER_N] = _poly_frommont(pkpv[i*KYBER_N:KYBER_N]);
  }

  pkpv = __polyvec_add2(pkpv, e);
  pkpv = __polyvec_reduce(pkpv);

  pkp = spkp;
  skp = sskp;

  __polyvec_tobytes(skp, skpv);
  __polyvec_tobytes(pkp, pkpv);

  pkp += KYBER_POLYVECBYTES;

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = publicseed[u64 i];
    (u64)[pkp] = t64;
    pkp += 8;
  }
}

// FIXME: E_EPTR
inline
fn __indcpa_enc_0(stack u64 sctp, reg ptr u8[KYBER_INDCPA_MSGBYTES] msgp, reg u64 pkp, reg ptr u8[KYBER_SYMBYTES] noiseseed)
{
  stack u16[KYBER_VECN] pkpv sp ep bp;
  stack u16[KYBER_K*KYBER_VECN] aat;
  stack u16[KYBER_N] k epp v;
  stack u8[KYBER_SYMBYTES] publicseed;
  stack ptr u8[KYBER_SYMBYTES] s_noiseseed;
  reg ptr u8[KYBER_SYMBYTES] lnoiseseed;
  reg u64 i ctp t64;
  reg u8 nonce;
  inline int w;

  pkpv = __polyvec_frombytes(pkp);

  i = 0;
  pkp += KYBER_POLYVECBYTES;
  while (i < KYBER_SYMBYTES/8)
  {
    t64 = (u64)[pkp];
    publicseed[u64 (int)i] = t64;
    pkp += 8;
    i += 1;
  }

  k = _poly_frommsg_1(k, msgp);

  s_noiseseed = noiseseed;
  aat = __gen_matrix(publicseed, 1);
  lnoiseseed = s_noiseseed;

  nonce = 0;
  sp[0:KYBER_N], sp[KYBER_N:KYBER_N], sp[2*KYBER_N:KYBER_N], ep[0:KYBER_N] = _poly_getnoise_eta1_4x(sp[0:KYBER_N], sp[KYBER_N:KYBER_N], sp[2*KYBER_N:KYBER_N], ep[0:KYBER_N], lnoiseseed, nonce);

  nonce = 4;
  ep[KYBER_N:KYBER_N], ep[2*KYBER_N:KYBER_N], epp, bp[0:KYBER_N] = _poly_getnoise_eta1_4x(ep[KYBER_N:KYBER_N], ep[2*KYBER_N:KYBER_N], epp, bp[0:KYBER_N], lnoiseseed, nonce);

  sp = __polyvec_ntt(sp);

  for w=0 to KYBER_K
  {
    bp[w*KYBER_N:KYBER_N] = __polyvec_pointwise_acc(bp[w*KYBER_N:KYBER_N], aat[w*KYBER_VECN:KYBER_VECN], sp);
  }

  v = __polyvec_pointwise_acc(v, pkpv, sp);

  bp = __polyvec_invntt(bp);
  v = _poly_invntt(v);

  bp = __polyvec_add2(bp, ep);
  v = _poly_add2(v, epp);
  v = _poly_add2(v, k);
  bp = __polyvec_reduce(bp);
  v  = __poly_reduce(v);

  ctp = sctp;
  __polyvec_compress(ctp, bp);
  ctp += KYBER_POLYVECCOMPRESSEDBYTES;
  v = _poly_compress(ctp, v);
}

// FIXME: E_EPTR
inline
fn __indcpa_enc_1(reg ptr u8[KYBER_INDCPA_BYTES] ctp, reg ptr u8[KYBER_INDCPA_MSGBYTES] msgp, reg u64 pkp, reg ptr u8[KYBER_SYMBYTES] noiseseed) -> reg ptr u8[KYBER_INDCPA_BYTES]
{
  stack u16[KYBER_VECN] pkpv sp ep bp;
  stack u16[KYBER_K*KYBER_VECN] aat;
  stack u16[KYBER_N] k epp v;
  stack u8[KYBER_SYMBYTES] publicseed;
  stack ptr u8[KYBER_SYMBYTES] s_noiseseed;
  reg ptr u8[KYBER_SYMBYTES] lnoiseseed;
  stack ptr u8[KYBER_INDCPA_BYTES] sctp;
  reg u64 i t64;
  reg u8 nonce;
  inline int w;

  sctp = ctp;

  pkpv = __polyvec_frombytes(pkp);

  i = 0;
  pkp += KYBER_POLYVECBYTES;
  while (i < KYBER_SYMBYTES/8)
  {
    t64 = (u64)[pkp];
    publicseed[u64 (int)i] = t64;
    pkp += 8;
    i += 1;
  }

  k = _poly_frommsg_1(k, msgp);

  s_noiseseed = noiseseed;
  aat = __gen_matrix(publicseed, 1);
  lnoiseseed = s_noiseseed;

  nonce = 0;
  sp[0:KYBER_N], sp[KYBER_N:KYBER_N], sp[2*KYBER_N:KYBER_N], ep[0:KYBER_N] = _poly_getnoise_eta1_4x(sp[0:KYBER_N], sp[KYBER_N:KYBER_N], sp[2*KYBER_N:KYBER_N], ep[0:KYBER_N], lnoiseseed, nonce);

  nonce = 4;
  ep[KYBER_N:KYBER_N], ep[2*KYBER_N:KYBER_N], epp, bp[0:KYBER_N] = _poly_getnoise_eta1_4x(ep[KYBER_N:KYBER_N], ep[2*KYBER_N:KYBER_N], epp, bp[0:KYBER_N], lnoiseseed, nonce);

  sp = __polyvec_ntt(sp);
    
  for w=0 to KYBER_K
  {
    bp[w*KYBER_N:KYBER_N] = __polyvec_pointwise_acc(bp[w*KYBER_N:KYBER_N], aat[w*KYBER_VECN:KYBER_VECN], sp);
  }

  v = __polyvec_pointwise_acc(v, pkpv, sp);

  bp = __polyvec_invntt(bp);
  v = _poly_invntt(v);

  bp = __polyvec_add2(bp, ep);
  v = _poly_add2(v, epp);
  v = _poly_add2(v, k);
  bp = __polyvec_reduce(bp);
  v  = __poly_reduce(v);

  ctp = sctp;
  ctp[0:KYBER_POLYVECCOMPRESSEDBYTES] = __polyvec_compress_1(ctp[0:KYBER_POLYVECCOMPRESSEDBYTES], bp);
  ctp[KYBER_POLYVECCOMPRESSEDBYTES:KYBER_POLYCOMPRESSEDBYTES], v = _poly_compress_1(ctp[KYBER_POLYVECCOMPRESSEDBYTES:KYBER_POLYCOMPRESSEDBYTES], v);

  return ctp;
}


inline
fn __indcpa_dec(reg ptr u8[KYBER_INDCPA_MSGBYTES] msgp, reg u64 ctp, reg u64 skp) -> reg ptr u8[KYBER_INDCPA_MSGBYTES]
{
  stack u16[KYBER_N] t v mp;
  stack u16[KYBER_VECN] bp skpv;

  bp = __polyvec_decompress(ctp);
  ctp += KYBER_POLYVECCOMPRESSEDBYTES;
  v = _poly_decompress(v, ctp);

  skpv = __polyvec_frombytes(skp);
  
  bp = __polyvec_ntt(bp);
  t = __polyvec_pointwise_acc(t, skpv, bp);
  t = _poly_invntt(t);

  mp = _poly_sub(mp, v, t);
  mp = __poly_reduce(mp);
  
  msgp, mp = _poly_tomsg_1(msgp, mp);

  return msgp;
}
inline 
fn __verify(reg u64 ctp, reg ptr u8[KYBER_INDCPA_BYTES] ctpc) -> reg u64
{
  reg u256 f g h;
  reg u64 cnd t64;
  reg u8 t1 t2;
  reg bool zf;
  inline int i off;

  cnd = 0;
  t64 = 1;
  h = #set0_256();

  for i=0 to KYBER_INDCPA_BYTES/32
  {
    f = ctpc.[u256 32*i];
    g = (u256)[ctp + 32*i];
    f = #VPXOR_256(f, g);
    h = #VPOR_256(h, f);
  }

  _, _, _, _, zf = #VPTEST_256(h, h);

  cnd = t64 if !zf;

  off = KYBER_CIPHERTEXTBYTES/32 * 32;

  for i=off to KYBER_CIPHERTEXTBYTES
  {
    t1 = ctpc.[i];
    t2 = (u8)[ctp + i];
    t1 ^= t2;
    t64 = (64u)t1;
    cnd |= t64;
  }

  cnd = -cnd;
  cnd >>= 63;

  return cnd;
}

inline
fn __cmov(reg ptr u8[KYBER_SYMBYTES] dst, reg u64 src cnd) -> reg ptr u8[KYBER_SYMBYTES]
{
  reg u256 f g m;
  stack u64 scnd;
  reg u8 t1 t2 bcond;
  inline int i off;

  cnd = -cnd;
  scnd = cnd;

  m = #VPBROADCAST_4u64(scnd);

  for i=0 to KYBER_SYMBYTES/32
  {
    f = dst.[u256 32*i];
    g = (u256)[src + 32*i];
    f = #VPBLENDVB_256(f, g, m);
    dst.[u256 32*i] = f;
  }

  off = KYBER_SYMBYTES/32 * 32;

  bcond = (8u)cnd;
  for i=off to KYBER_SYMBYTES
  {
    t1 = dst.[i];
    t2 = (u8)[src + i];
    t2 = t2 ^ t1;
    t2 = t2 & cnd;
    t1 ^= t2;
    dst.[u8 i] = t1;
  }

  return dst;
}

inline
fn __crypto_kem_keypair_jazz(reg u64 pkp, reg u64 skp)
{
  stack u8[32] h_pk;
  stack u8[KYBER_SYMBYTES] rb;
  stack u64 s_skp s_pkp;
  reg u64 t64;
  inline int i;

  s_pkp = pkp;
  s_skp = skp;

  __indcpa_keypair(pkp, skp);

  skp = s_skp;
  skp += KYBER_POLYVECBYTES;
  pkp = s_pkp;

  for i=0 to KYBER_INDCPA_PUBLICKEYBYTES/8
  {
    t64 = (u64)[pkp + 8*i];
    (u64)[skp] = t64;
    skp += 8;
  }

  s_skp = skp;
  pkp = s_pkp;
  t64 = KYBER_INDCPA_PUBLICKEYBYTES;
  h_pk = _isha3_256(h_pk, pkp, t64);
  skp = s_skp;

  for i=0 to 4
  {
    t64 = h_pk[u64 i];
    (u64)[skp] = t64;
    skp += 8;
  }

  rb = #randombytes(rb);

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = rb[u64 i];
    (u64)[skp] = t64;
    skp += 8;
  }
}

inline
fn __crypto_kem_enc_jazz(reg u64 ctp, reg u64 shkp, reg u64 pkp)
{
  stack u8[KYBER_SYMBYTES * 2] buf kr;
  stack u8[KYBER_SYMBYTES] rb;
  stack u64 s_pkp s_ctp s_shkp;
  reg u64 t64;
  inline int i;

  s_pkp = pkp;
  s_ctp = ctp;
  s_shkp = shkp;

  rb = #randombytes(rb);

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = rb[u64 i];
    kr[u64 i] = t64;
  }
  
  buf[0:32] = _isha3_256_32(buf[0:32], kr[0:KYBER_SYMBYTES]);

  pkp = s_pkp;

  t64 = KYBER_PUBLICKEYBYTES;
  buf[KYBER_SYMBYTES:32] = _isha3_256(buf[KYBER_SYMBYTES:32], pkp, t64);

  kr = _isha3_512_64(kr, buf);

  pkp = s_pkp;

  __indcpa_enc_0(s_ctp, buf[0:KYBER_INDCPA_MSGBYTES], pkp, kr[KYBER_SYMBYTES:KYBER_SYMBYTES]);

  ctp = s_ctp;
  t64 = KYBER_INDCPA_BYTES;
  kr[KYBER_SYMBYTES:32] = _isha3_256(kr[KYBER_SYMBYTES:32], ctp, t64);

  shkp = s_shkp;
  t64 = KYBER_SSBYTES;
  _shake256_64(shkp, t64, kr);
}

inline
fn __crypto_kem_dec_jazz(reg u64 shkp, reg u64 ctp, reg u64 skp)
{
  stack u8[KYBER_INDCPA_BYTES] ctpc;
  stack u8[2*KYBER_SYMBYTES] kr buf;
  stack u64 s_skp s_ctp s_shkp;
  reg u64 pkp hp zp t64 cnd;
  inline int i;

  s_shkp = shkp;
  s_ctp = ctp;

  buf[0:KYBER_INDCPA_MSGBYTES] = __indcpa_dec(buf[0:KYBER_INDCPA_MSGBYTES], ctp, skp);

  hp = #LEA(skp + 32); //hp = skp + 32;
  hp += 24 * KYBER_K * KYBER_N>>3;

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = (u64)[hp + 8*i];
    buf.[u64 KYBER_SYMBYTES + 8*i] = t64;
  }

  s_skp = skp;

  kr = _isha3_512_64(kr, buf);

  pkp = s_skp;
  pkp += 12 * KYBER_K * KYBER_N>>3;

  ctpc = __indcpa_enc_1(ctpc, buf[0:KYBER_INDCPA_MSGBYTES], pkp, kr[KYBER_SYMBYTES:KYBER_SYMBYTES]);

  ctp = s_ctp;
  cnd = __verify(ctp, ctpc);

  zp = s_skp;
  zp += 64;
  zp += 24 * KYBER_K * KYBER_N>>3;
  kr[0:KYBER_SYMBYTES] = __cmov(kr[0:KYBER_SYMBYTES], zp, cnd);

  t64 = KYBER_INDCPA_BYTES;
  kr[KYBER_SYMBYTES:32] = _isha3_256(kr[KYBER_SYMBYTES:32], ctp, t64);

  shkp = s_shkp;
  t64 = KYBER_SSBYTES;
  _shake256_64(shkp, t64, kr);
}

export fn jade_kem_kyber_kyber768_amd64_avx2_keypair(reg u64 public_key secret_key) -> reg u64
{
  reg u64 r;
  __crypto_kem_keypair_jazz(public_key, secret_key);
  ?{}, r = #set0();
  return r;
}

export fn jade_kem_kyber_kyber768_amd64_avx2_enc(reg u64 ciphertext shared_secret public_key) -> reg u64
{
  reg u64 r;
  __crypto_kem_enc_jazz(ciphertext, shared_secret, public_key);
  ?{}, r = #set0();
  return r;
}

export fn jade_kem_kyber_kyber768_amd64_avx2_dec(reg u64 shared_secret ciphertext secret_key) -> reg u64
{
  reg u64 r;
  __crypto_kem_dec_jazz(shared_secret, ciphertext, secret_key);
  ?{}, r = #set0();
  return r;
}
