require "fips202_common.jinc"

require "fips202_4x.jinc"

param int KECCAK_ROUNDS=24;

u256[24] KECCAK_IOTAS =
{  (4u64)[0x0000000000000001, 0x0000000000000001, 0x0000000000000001, 0x0000000000000001]
  ,(4u64)[0x0000000000008082, 0x0000000000008082, 0x0000000000008082, 0x0000000000008082]
  ,(4u64)[0x800000000000808a, 0x800000000000808a, 0x800000000000808a, 0x800000000000808a]
  ,(4u64)[0x8000000080008000, 0x8000000080008000, 0x8000000080008000, 0x8000000080008000]
  ,(4u64)[0x000000000000808b, 0x000000000000808b, 0x000000000000808b, 0x000000000000808b]
  ,(4u64)[0x0000000080000001, 0x0000000080000001, 0x0000000080000001, 0x0000000080000001]
  ,(4u64)[0x8000000080008081, 0x8000000080008081, 0x8000000080008081, 0x8000000080008081]
  ,(4u64)[0x8000000000008009, 0x8000000000008009, 0x8000000000008009, 0x8000000000008009]
  ,(4u64)[0x000000000000008a, 0x000000000000008a, 0x000000000000008a, 0x000000000000008a]
  ,(4u64)[0x0000000000000088, 0x0000000000000088, 0x0000000000000088, 0x0000000000000088]
  ,(4u64)[0x0000000080008009, 0x0000000080008009, 0x0000000080008009, 0x0000000080008009]
  ,(4u64)[0x000000008000000a, 0x000000008000000a, 0x000000008000000a, 0x000000008000000a]
  ,(4u64)[0x000000008000808b, 0x000000008000808b, 0x000000008000808b, 0x000000008000808b]
  ,(4u64)[0x800000000000008b, 0x800000000000008b, 0x800000000000008b, 0x800000000000008b]
  ,(4u64)[0x8000000000008089, 0x8000000000008089, 0x8000000000008089, 0x8000000000008089]
  ,(4u64)[0x8000000000008003, 0x8000000000008003, 0x8000000000008003, 0x8000000000008003]
  ,(4u64)[0x8000000000008002, 0x8000000000008002, 0x8000000000008002, 0x8000000000008002]
  ,(4u64)[0x8000000000000080, 0x8000000000000080, 0x8000000000000080, 0x8000000000000080]
  ,(4u64)[0x000000000000800a, 0x000000000000800a, 0x000000000000800a, 0x000000000000800a]
  ,(4u64)[0x800000008000000a, 0x800000008000000a, 0x800000008000000a, 0x800000008000000a]
  ,(4u64)[0x8000000080008081, 0x8000000080008081, 0x8000000080008081, 0x8000000080008081]
  ,(4u64)[0x8000000000008080, 0x8000000000008080, 0x8000000000008080, 0x8000000000008080]
  ,(4u64)[0x0000000080000001, 0x0000000080000001, 0x0000000080000001, 0x0000000080000001]
  ,(4u64)[0x8000000080008008, 0x8000000080008008, 0x8000000080008008, 0x8000000080008008]
};


u256[6] KECCAK_RHOTATES_LEFT = 
{
  (4u64)[41, 36, 18,  3],
  (4u64)[27, 28, 62,  1],
  (4u64)[39, 56,  6, 45],
  (4u64)[ 8, 55, 61, 10],
  (4u64)[20, 25, 15,  2],
  (4u64)[14, 21, 43, 44]
};


u256[6] KECCAK_RHOTATES_RIGHT =
{
  (4u64)[64-41, 64-36, 64-18, 64- 3],
  (4u64)[64-27, 64-28, 64-62, 64- 1],
  (4u64)[64-39, 64-56, 64- 6, 64-45],
  (4u64)[64- 8, 64-55, 64-61, 64-10],
  (4u64)[64-20, 64-25, 64-15, 64- 2],
  (4u64)[64-14, 64-21, 64-43, 64-44]
};


u64[25] KECCAK_A_JAGGED = 
{
   0,  4,  5,  6,  7,
  10, 24, 13, 18, 23,
   8, 16, 25, 22, 15,
  11, 12, 21, 26, 19,
   9, 20, 17, 14, 27
};

// INVERSE PERMUTATION
u64[24] KECCAK_ST_PINV = 
{ 1*32,  2*32,    3*32,  4*32,
 10*32, 20*32,    5*32, 15*32,
 16*32,  7*32,   23*32, 14*32,
 11*32, 22*32,    8*32, 19*32,
 21*32, 17*32,   13*32,  9*32,
  6*32, 12*32,   18*32, 24*32
};

// INVERSE PERMUTATION as a function
inline
fn __INV_P(inline int i) -> inline int {
 inline int x;
 if (i < 4) {
   x = i+1;
 } else if (i == 4) {
   x = 10;
 } else if (i == 5) {
   x = 20;
 } else if (i == 6) {
   x = 5;
 } else if (i == 7) {
   x = 15;
 } else if (i == 8) {
   x = 16;
 } else if (i == 9) {
   x = 7;
 } else if (i == 10) {
   x = 23;
 } else if (i == 11) {
   x = 14;
 } else if (i == 12) {
   x = 11;
 } else if (i == 13) {
   x = 22;
 } else if (i == 14) {
   x = 8;
 } else if (i == 15) {
   x = 19;
 } else if (i == 16) {
   x = 21;
 } else if (i == 17) {
   x = 17;
 } else if (i == 18) {
   x = 13;
 } else if (i == 19) {
   x = 9;
 } else if (i == 20) {
   x = 6;
 } else if (i == 21) {
   x = 12;
 } else if (i == 22) {
   x = 18;
 } else {
   x = 24;
 }

 return x;
}

// TO_FROM reg array
inline fn __7u256_array(reg u256 st0r st1r st2r st3r st4r st5r st6r) -> reg u256[7] {
  reg u256[7] state;
  state[0] = st0r;
  state[1] = st1r;
  state[2] = st2r;
  state[3] = st3r;
  state[4] = st4r;
  state[5] = st5r;
  state[6] = st6r;  
  return state;
}

inline fn __array_7u256(reg u256[7] state) -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256 {
  reg u256 st0r st1r st2r st3r st4r st5r st6r;
  st0r = state[0];
  st1r = state[1];
  st2r = state[2];
  st3r = state[3];
  st4r = state[4];
  st5r = state[5];
  st6r = state[6];
  return st0r, st1r, st2r, st3r, st4r, st5r, st6r;
}

fn _keccakf1600_avx2(reg u256 st0r st1r st2r st3r st4r st5r st6r) -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  reg u256[7] state;
  reg u256[9] t;
  reg u256 c00 c14 d00 d14;

  reg bool zf;
  reg u64 r iotas_o;

  reg ptr u256[24] iotas_p;
  reg ptr u256[6] rhotates_left_p;
  reg ptr u256[6] rhotates_right_p;

  state = __7u256_array(st0r, st1r, st2r, st3r, st4r, st5r, st6r);

  iotas_p = KECCAK_IOTAS;
  iotas_o = 0;
  rhotates_left_p = KECCAK_RHOTATES_LEFT;
  rhotates_right_p = KECCAK_RHOTATES_RIGHT;

  r = KECCAK_ROUNDS;
  while
  {
    //######################################## Theta
    c00 = #VPSHUFD_256(state[2], (4u2)[1,0,3,2]);
    c14 = state[5] ^ state[3];
    t[2] = state[4] ^ state[6];
    c14 = c14 ^ state[1];
    c14 = c14 ^ t[2];
    t[4] = #VPERMQ(c14, (4u2)[2,1,0,3]);
    c00 = c00 ^ state[2];
    t[0] = #VPERMQ(c00, (4u2)[1,0,3,2]);
    t[1] = c14 >>4u64 63;
    t[2] = c14 +4u64 c14;
    t[1] = t[1] | t[2];
    d14 = #VPERMQ(t[1], (4u2)[0,3,2,1]);
    d00 = t[1] ^ t[4];
    d00 = #VPERMQ(d00, (4u2)[0,0,0,0]);
    c00 = c00 ^ state[0];
    c00 = c00 ^ t[0];
    t[0] = c00 >>4u64 63;
    t[1] = c00 +4u64 c00;
    t[1] = t[1] | t[0];
    state[2] = state[2] ^ d00;
    state[0] = state[0] ^ d00;
    d14 = #VPBLEND_8u32(d14, t[1], (8u1)[1,1,0,0,0,0,0,0]);
    t[4] = #VPBLEND_8u32(t[4], c00, (8u1)[0,0,0,0,0,0,1,1]);
    d14 = d14 ^ t[4];
    
    //######################################## Rho + Pi + pre-Chi shuffle
    t[3] = #VPSLLV_4u64(state[2], rhotates_left_p[0] );
    state[2] = #VPSRLV_4u64(state[2], rhotates_right_p[0] );
    state[2] = state[2] | t[3];
    state[3] = state[3] ^ d14;
    t[4] = #VPSLLV_4u64(state[3], rhotates_left_p[2] );
    state[3] = #VPSRLV_4u64(state[3], rhotates_right_p[2] );
    state[3] = state[3] | t[4];
    state[4] = state[4] ^ d14;
    t[5] = #VPSLLV_4u64(state[4], rhotates_left_p[3] );
    state[4] = #VPSRLV_4u64(state[4], rhotates_right_p[3] );
    state[4] = state[4] | t[5];
    state[5] = state[5] ^ d14;
    t[6] = #VPSLLV_4u64(state[5], rhotates_left_p[4] );
    state[5] = #VPSRLV_4u64(state[5], rhotates_right_p[4] );
    state[5] = state[5] | t[6];
    state[6] = state[6] ^ d14;
    t[3] = #VPERMQ(state[2], (4u2)[2,0,3,1]);
    t[4] = #VPERMQ(state[3], (4u2)[2,0,3,1]);
    t[7] = #VPSLLV_4u64(state[6], rhotates_left_p[5] );
    t[1] = #VPSRLV_4u64(state[6], rhotates_right_p[5] );
    t[1] = t[1] | t[7];
    state[1] = state[1] ^ d14;
    t[5] = #VPERMQ(state[4], (4u2)[0,1,2,3]);
    t[6] = #VPERMQ(state[5], (4u2)[1,3,0,2]);
    t[8] = #VPSLLV_4u64(state[1], rhotates_left_p[1] );
    t[2] = #VPSRLV_4u64(state[1], rhotates_right_p[1] );
    t[2] = t[2] | t[8];

    //######################################## Chi
    t[7] = #VPSRLDQ_256(t[1], 8);
    t[0] = !t[1] & t[7];
    state[3] = #VPBLEND_8u32(t[2], t[6], (8u1)[0,0,0,0,1,1,0,0]);
    t[8] = #VPBLEND_8u32(t[4], t[2], (8u1)[0,0,0,0,1,1,0,0]);
    state[5] = #VPBLEND_8u32(t[3], t[4], (8u1)[0,0,0,0,1,1,0,0]);
    t[7] = #VPBLEND_8u32(t[2], t[3], (8u1)[0,0,0,0,1,1,0,0]);
    state[3] = #VPBLEND_8u32(state[3], t[4], (8u1)[0,0,1,1,0,0,0,0]);
    t[8] = #VPBLEND_8u32(t[8], t[5], (8u1)[0,0,1,1,0,0,0,0]);
    state[5] = #VPBLEND_8u32(state[5], t[2], (8u1)[0,0,1,1,0,0,0,0]);
    t[7] = #VPBLEND_8u32(t[7], t[6], (8u1)[0,0,1,1,0,0,0,0]);
    state[3] = #VPBLEND_8u32(state[3], t[5], (8u1)[1,1,0,0,0,0,0,0]);
    t[8] = #VPBLEND_8u32(t[8], t[6], (8u1)[1,1,0,0,0,0,0,0]);
    state[5] = #VPBLEND_8u32(state[5], t[6], (8u1)[1,1,0,0,0,0,0,0]);
    t[7] = #VPBLEND_8u32(t[7], t[4], (8u1)[1,1,0,0,0,0,0,0]);
    state[3] = !state[3] & t[8];
    state[5] = !state[5] & t[7];
    state[6] = #VPBLEND_8u32(t[5], t[2], (8u1)[0,0,0,0,1,1,0,0]);
    t[8] = #VPBLEND_8u32(t[3], t[5], (8u1)[0,0,0,0,1,1,0,0]);
    state[3] = state[3] ^ t[3];
    state[6] = #VPBLEND_8u32(state[6], t[3], (8u1)[0,0,1,1,0,0,0,0]);
    t[8] = #VPBLEND_8u32(t[8], t[4], (8u1)[0,0,1,1,0,0,0,0]);
    state[5] = state[5] ^ t[5];
    state[6] = #VPBLEND_8u32(state[6], t[4], (8u1)[1,1,0,0,0,0,0,0]);
    t[8] = #VPBLEND_8u32(t[8], t[2], (8u1)[1,1,0,0,0,0,0,0]);
    state[6] = !state[6] & t[8];
    state[6] = state[6] ^ t[6];
    state[4] = #VPERMQ(t[1], (4u2)[0,1,3,2]);
    t[8] = #VPBLEND_8u32(state[4], state[0], (8u1)[0,0,1,1,0,0,0,0]);
    state[1] = #VPERMQ(t[1], (4u2)[0,3,2,1]);
    state[1] = #VPBLEND_8u32(state[1], state[0], (8u1)[1,1,0,0,0,0,0,0]);
    state[1] = !state[1] & t[8];
    state[2] = #VPBLEND_8u32(t[4], t[5], (8u1)[0,0,0,0,1,1,0,0]);
    t[7] = #VPBLEND_8u32(t[6], t[4], (8u1)[0,0,0,0,1,1,0,0]);
    state[2] = #VPBLEND_8u32(state[2], t[6], (8u1)[0,0,1,1,0,0,0,0]);
    t[7] = #VPBLEND_8u32(t[7], t[3], (8u1)[0,0,1,1,0,0,0,0]);
    state[2] = #VPBLEND_8u32(state[2], t[3], (8u1)[1,1,0,0,0,0,0,0]);
    t[7] = #VPBLEND_8u32(t[7], t[5], (8u1)[1,1,0,0,0,0,0,0]);
    state[2] = !state[2] & t[7];
    state[2] = state[2] ^ t[2];
    t[0] = #VPERMQ(t[0], (4u2)[0,0,0,0]);
    state[3] = #VPERMQ(state[3], (4u2)[0,1,2,3]);
    state[5] = #VPERMQ(state[5], (4u2)[2,0,3,1]);
    state[6] = #VPERMQ(state[6], (4u2)[1,3,0,2]);
    state[4] = #VPBLEND_8u32(t[6], t[3], (8u1)[0,0,0,0,1,1,0,0]);
    t[7] = #VPBLEND_8u32(t[5], t[6], (8u1)[0,0,0,0,1,1,0,0]);
    state[4] = #VPBLEND_8u32(state[4], t[5], (8u1)[0,0,1,1,0,0,0,0]);
    t[7] = #VPBLEND_8u32(t[7], t[2], (8u1)[0,0,1,1,0,0,0,0]);
    state[4] = #VPBLEND_8u32(state[4], t[2], (8u1)[1,1,0,0,0,0,0,0]);
    t[7] = #VPBLEND_8u32(t[7], t[3], (8u1)[1,1,0,0,0,0,0,0]);
    state[4] = !state[4] & t[7];
    state[0] = state[0] ^ t[0];
    state[1] = state[1] ^ t[1];
    state[4] = state[4] ^ t[4];

    //######################################## Iota
    state[0] = state[0] ^ iotas_p.[(int) iotas_o];
    iotas_o += 32;

    _,_,_,zf,r = #DEC_64(r);
  }(!zf)

  st0r, st1r, st2r, st3r, st4r, st5r, st6r = __array_7u256(state);

  return st0r, st1r, st2r, st3r, st4r, st5r, st6r;
}


inline 
fn __keccakf1600_avx2(reg u256[7] state) -> reg u256[7]
{
  reg u256 st0r st1r st2r st3r st4r st5r st6r;

  st0r, st1r, st2r, st3r, st4r, st5r, st6r = __array_7u256(state);

  st0r, st1r, st2r, st3r, st4r, st5r, st6r
  = _keccakf1600_avx2(st0r, st1r, st2r, st3r, st4r, st5r, st6r);

  state = __7u256_array(st0r, st1r, st2r, st3r, st4r, st5r, st6r);

  return state;
}







inline
fn __u128x2_u256(reg u128 x0 x1) -> reg u256 {
  reg u256 y;
  //y = (256u) x0;
  //y = #VINSERTI128(y, x1, 1);
  y = (2u128)[x1, x0];
  return y;
}

inline
fn __u64_u256(reg u64 y, inline int pos) -> reg u256 {
  reg u256 x;
  reg u128 t;
  if ((pos%2) == 0) {
    t = (128u) y;
  } else {
    t = #set0_128();
    t = #VPINSR_2u64(t, y, 1);
  }
  x = #set0_256();
  x = #VINSERTI128(x, t, pos/2);
  return x;
}

inline
fn __u128_u256(reg u128 y, inline int pos) -> reg u256 {
  reg u256 x;
  x = #set0_256();
  x = #VINSERTI128(x, y, pos);
  return x;
}

/*
inline
fn __u64x2_u128(reg u64 x0 x1) -> reg u128 {
  reg u128 y;
  y = (128u) x0;
  y = #VPINSR_2u64(y, x1, 1);
  return y;
}

inline
fn __u64x4_u256(reg u64 x0 x1 x2 x3) -> reg u256 {
  reg u128 h l;
  reg u256 y;
  l = __u64x2_u128(x0, x1);
  h = __u64x2_u128(x2, x3);
  y = (2u128)[h, l];
  return y;
}

*/

/*
 * GENERIC ABSORB
 */

inline
fn __read_below_rate(reg u64 in, inline int j rate) -> reg u64 {
  inline int i;
  reg u64 t;
  i = __INV_P(j);
  if (8*i < rate) { // static check
    t = [in + 8*i];
  } else {
    t = #set0();
  }
  return t;
}

inline
fn __add_full_block_avx2(
  reg u256[7] state,
  reg u64 in inlen,
  inline int rate //in bytes
) -> reg u256[7], reg u64, reg u64
{
  inline int j;
  reg u256 t256;
  reg u64 l t0 t1 t2 t3;

  t256 = #VPBROADCAST_4u64([in + 0]);
  state[0] ^= t256;
  for j = 0 to 6 {
    t0 = __read_below_rate(in, 4*j+0, rate);
    t1 = __read_below_rate(in, 4*j+1, rate);
    t2 = __read_below_rate(in, 4*j+2, rate);
    t3 = __read_below_rate(in, 4*j+3, rate);
    t256 = __u64x4_u256(t0, t1, t2, t3);
    state[j+1] ^= t256;
  }

  in += rate;
  inlen -= rate;
  return state, in, inlen;
}

inline
fn __read_below_inlen( reg u64 in
		     , reg u64 inlen //@pre: inlen < rate
		     , reg u64 trail
		     , inline int j
		     , inline int rate
		     ) -> reg u64
{
  inline int i ppos;
  reg u64 x y;
  reg u8 sh;

  i = __INV_P(j);

  x = 0;

  i *= 8; // ERROR???? (check this!!!)

  if (i < rate) { // static test
    if (inlen >= i) {
      if (inlen >= i+8) {
        x = [in + i];
      } else {
	sh = 0;
        if (inlen >= i+4) {
	  x = (64u) (u32) [in + i];
	  i += 4;
	  trail <<= 32;
	  sh = 32;
	}
        if (inlen >= i+2) {
	  y = (64u) (u16) [in + i];
	  y <<= sh;
	  x |= y;
	  i += 2;
	  trail <<= 16;
	  sh += 16;
	}
        if (inlen >= i+1) {
	  y = (64u) (u8) [in + i];
	  y <<= sh;
	  x |= y;
	  i += 1;
	  trail <<= 8;
	  sh += 8;
	}
	x |= trail;
       	}
      }
      ppos = __INV_P(rate/8-1);
      if (i == 8*ppos) {
        y = 0x8000000000000000;
        x |= y;
      }
    }
  return x;
}
    

inline
fn __add_final_block_avx2_NEW(
  reg  u256[7] state,
  reg   u64 in inlen,
  reg   u8  trail_byte,
  inline int rate
) -> reg u256[7]
{
  inline int j;
  reg u256 t256;
  reg u64 t0 t1 t2 t3;

  t256 = #VPBROADCAST_4u64([in + 0]);
  state[0] ^= t256;
  for j = 0 to 6 {
    t0 = __read_below_inlen(in, inlen, (64u) trail_byte, 4*j+0, rate);
    t1 = __read_below_inlen(in, inlen, (64u) trail_byte, 4*j+1, rate);
    t2 = __read_below_inlen(in, inlen, (64u) trail_byte, 4*j+2, rate);
    t3 = __read_below_inlen(in, inlen, (64u) trail_byte, 4*j+3, rate);
    t256 = __u64x4_u256(t0, t1, t2, t3);
    state[j+1] ^= t256;
  }

  return state;
}


inline fn __add_final_block_avx2(
  reg  u256[7] state,
  reg   u64 in inlen,
  reg   u8  trail_byte,
  inline int rate
) -> reg u256[7]
{
  inline int i;
  stack u64[28] s_state;
  reg ptr u64[25] a_jagged_p;
  reg u256 t256;
  reg u64 j l t inlen8;
  reg u8 c;

  a_jagged_p = KECCAK_A_JAGGED;

  t256 = #set0_256();
  for i=0 to 7
  { s_state[u256 i] = t256; }

  inlen8 = inlen;
  inlen8 >>= 3;
  j = 0;
  while ( j < inlen8 )
  {
    t = [in + 8*j];
    l = a_jagged_p[(int) j];
    s_state[(int) l] = t;
    j += 1;
  }
  l = a_jagged_p[(int) j];
  l <<= 3;
  j <<= 3;

  while ( j < inlen )
  {
    c = (u8)[in + j];
    s_state[u8 (int) l] = c;
    j += 1;
    l += 1;
  }

  s_state[u8 (int) l] = trail_byte;

  // i  = (rate-1) >> 3;
  i = rate; i -= 1; i = i/8;
  l  = a_jagged_p[i];
  l <<= 3;
  // l += ((rate-1) & 0x7)
  i = rate; i -= 1; i &= 0x7;
  l += i;

  s_state[u8 (int) l] ^= 0x80;

  t256 = #VPBROADCAST_4u64(s_state[0]);
  state[0] ^= t256;

  for i = 1 to 7
  { state[i] ^= s_state[u256 i]; }

  return state;
}


inline fn __absorb_avx2(
  reg u256[7] state,
  reg u64 in inlen,
  reg u8  trail_byte,
  inline int rate
) -> reg u256[7]
{
  // intermediate blocks
  while ( inlen >= rate )
  {
    state, in, inlen = __add_full_block_avx2(state, in, inlen, rate);
    state = __keccakf1600_avx2(state);
  }

  // final block
  state = __add_final_block_avx2(state, in, inlen, trail_byte, rate);

  return state;
}


/*
 * GENERIC SQUEEZE
 */









/*
 *  Specialized ABSORB functions
 */

// SHA3-256
// rate = 136; @16=(3,0)
// trail = 0x06

inline
fn __sha256_absorb32_avx2(reg const ptr u8[32] in) -> reg u256[7] {
  reg u256[7] state;
  reg u64 t0 t1 t2 t3 pad;

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  t0 = in[u64 1];
  t1 = in[u64 2];
  t2 = in[u64 3];
  t3 = 0x06;
  state[1] = __u64x4_u256(t0, t1, t2, t3);
  state[2] = #set0_256();
  pad = 0x8000000000000000;
  state[3] = __u64_u256(pad, 0);
  state[4] = #set0_256();
  state[5] = #set0_256();
  state[6] = #set0_256();

  return state;
}



// SHA3-512
// rate = 72; @8=(4,2)
// trail = 0x06

inline
fn __sha512_absorb32_avx2(reg const ptr u8[32] in) -> reg u256[7] {
  reg u256[7] state;
  reg u64 t0 t1 t2 t3 pad;

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  t0 = in[u64 1];
  t1 = in[u64 2];
  t2 = in[u64 3];
  t3 = 0x06;
  state[1] = __u64x4_u256(t0, t1, t2, t3);
  state[2] = #set0_256();
  state[3] = #set0_256();
  pad = 0x8000000000000000;
  state[4] = __u64_u256(pad, 2);
  state[5] = #set0_256();
  state[6] = #set0_256();

  return state;
}

inline
fn __sha512_absorb64_avx2(reg const ptr u8[64] in) -> reg u256[7] {
  reg u256[7] state;
  reg u64 t0 t1 t2 t3 pad;

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  t0 = in[u64 1];
  t1 = in[u64 2];
  t2 = in[u64 3];
  t3 = in[u64 4];
  state[1] = __u64x4_u256(t0, t1, t2, t3);
  t0 = in[u64 5];
  t1 = in[u64 6];
  t2 = in[u64 7];
  state[2] = __u64_u256(t0, 2);
  state[3] = __u64_u256(t2, 1);
  pad = 0x8000000000000006; // pad ^ trail_byte
  state[4] = __u64_u256(pad, 2);
  state[5] = #set0_256();
  state[6] = __u64_u256(t1, 0);
  
  return state;
}

// SHAKE128
// rate = 168; @20=(2,1)
// trail = 0x1F

inline
fn __shake128_absorb34_avx2(reg const ptr u8[34] in) -> reg u256[7] {
  reg u256[7] state;
  reg u64 t0 t1 t2 t3 pad;

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  t0 = in[u64 1];
  t1 = in[u64 2];
  t2 = in[u64 3];
  t3 = (64u) in[u16 16];
  t3 ^= 0x1F0000;
  state[1] = __u64x4_u256(t0, t1, t2, t3);
  pad = 0x8000000000000000;
  state[2] = __u64_u256(pad, 1);
  state[3] = #set0_256();
  state[4] = #set0_256();
  state[5] = #set0_256();
  state[6] = #set0_256();

  return state;
}

inline
fn __shake128_absorb_genmat_avx2(reg const ptr u8[32] seed, reg u64 pos) -> reg u256[7] {
  reg u256[7] state;
  reg u64 t0 t1 t2 pad;

  state[0] = #VPBROADCAST_4u64(seed[u64 0]);
  t0 = seed[u64 1];
  t1 = seed[u64 2];
  t2 = seed[u64 3];
  pos ^= 0x1F0000; // obs: pos == pos & 0xFFFF
  state[1] = __u64x4_u256(t0, t1, t2, pos);
  pad = 0x8000000000000000;
  state[2] = __u64_u256(pad, 1);
  state[3] = #set0_256();
  state[4] = #set0_256();
  state[5] = #set0_256();
  state[6] = #set0_256();

  return state;
}


// SHAKE256
// rate = 136; @16=(3,0)
// trail = 0x1F

inline
fn __shake256_absorb32_avx2(reg const ptr u8[32] in) -> reg u256[7] {
  reg u256[7] state;
  reg u64 t0 t1 t2 t3 pad;

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  t0 = in[u64 1];
  t1 = in[u64 2];
  t2 = in[u64 3];
  t3 = 0x1F;
  state[1] = __u64x4_u256(t0, t1, t2, t3);
  state[2] = #set0_256();
  pad = 0x8000000000000000;
  state[3] = __u64_u256(pad, 0);
  state[4] = #set0_256();
  state[5] = #set0_256();
  state[6] = #set0_256();

  return state;
}

inline
fn __shake256_absorb33_avx2(reg const ptr u8[33] in) -> reg u256[7] {
  reg u256[7] state;
  reg u64 t0 t1 t2 t3 pad;

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  t0 = in[u64 1];
  t1 = in[u64 2];
  t2 = in[u64 3];
  t3 = (64u) in[u8 32];
  t3 ^= 0x1F00;
  state[1] = __u64x4_u256(t0, t1, t2, t3);
  state[2] = #set0_256();
  pad = 0x8000000000000000;
  state[3] = __u64_u256(pad, 0);
  state[4] = #set0_256();
  state[5] = #set0_256();
  state[6] = #set0_256();

  return state;
}

inline
fn __shake256_absorb64_avx2(reg const ptr u8[64] in) -> reg u256[7] {
  reg u256[7] state;
  reg u128 t128;
  reg u64 t0 t1 t2 t3 t4 pad;

  state[0] = #VPBROADCAST_4u64(in[u64 0]);
  t0 = in[u64 1];
  t1 = in[u64 2];
  t2 = in[u64 3];
  t3 = in[u64 4];
  state[1] = __u64x4_u256(t0, t1, t2, t3);
  t0 = in[u64 5];
  t1 = in[u64 6];
  t2 = in[u64 7];
  state[2] = __u64_u256(t0, 2);
  pad = 0x8000000000000000;
  t0 = 0x1F;
  t128 = __u64x2_u128(pad, t2);
  state[3] = __u128_u256(t128, 0);
  state[4] = __u64_u256(t0, 2);
  state[5] = #set0_256();
  state[6] = __u64_u256(t1, 0);

  return state;
}


/*
 * SPECIALIZED SQUEEZE functions
 */


inline
fn __squeeze168_avx2(reg u256[7] state, reg ptr u8[168] buf) -> reg u256[7], reg ptr u8[168] {
  inline int i, k, bnd;
  //reg u256 t256;
  reg u128 r_l, r_h;

  bnd = 168/8;

  state = __keccakf1600_avx2(state);

  r_l = (128u) state[0];
  buf[u64 0] = #VMOVLPD(r_l);
  for i = 0 to 6 {
    //t256 = state[i+1];
    r_l = (128u) state[i+1];
    r_h = #VEXTRACTI128(state[i+1], 1);
    k = __INV_P(4*i+0);
    if (k < bnd) { buf[u64 k] = #VMOVLPD(r_l); }
    k = __INV_P(4*i+1);
    if (k < bnd) { buf[u64 k] = #VMOVHPD(r_l); }
    k = __INV_P(4*i+2);
    if (k < bnd) { buf[u64 k] = #VMOVLPD(r_h); }
    k = __INV_P(4*i+3);
    if (k < bnd) { buf[u64 k] = #VMOVHPD(r_h); }
  }

  return state, buf;
}

inline
fn __shake128_squeezeblock_avx2(reg u256[7] state, reg ptr u8[168] buf) -> reg u256[7], reg ptr u8[168] {
  state, buf = __squeeze168_avx2(state, buf);
  return state, buf;
}

inline
fn __shake128_squeeze3blocks_avx2(reg u256[7] state, stack u8[512] buf) -> reg u256[7], stack u8[512] {
  inline int i;
  reg ptr u8[168] buf0;

  buf0 = buf[0:168];
  state, buf0 = __shake128_squeezeblock_avx2(state, buf0);
  buf[0:168] = buf0;
  buf0 = buf[168:168];
  state, buf0 = __shake128_squeezeblock_avx2(state, buf0);
  buf[168:168] = buf0;
  buf0 = buf[336:168];
  state, buf0 = __shake128_squeezeblock_avx2(state, buf0);
  buf[336:168] = buf0;
  buf[u64 63] = 0;

  return state, buf;
}

inline
fn __squeeze128_avx2(reg u256[7] state, reg ptr u8[128] buf) -> reg u256[7], reg ptr u8[128] {
  inline int i, k, bnd;
  //reg u256 t256;
  reg u128 r_l, r_h;

  bnd = 128/8;

  state = __keccakf1600_avx2(state);

  r_l = (128u) state[0];
  buf[u64 0] = #VMOVLPD(r_l);
  for i = 0 to 6 {
    //t256 = state[i+1];
    r_l = (128u) state[i+1];
    r_h = #VEXTRACTI128(state[i+1], 1);
    k = __INV_P(4*i+0);
    if (k < bnd) { buf[u64 k] = #VMOVLPD(r_l); }
    k = __INV_P(4*i+1);
    if (k < bnd) { buf[u64 k] = #VMOVHPD(r_l); }
    k = __INV_P(4*i+2);
    if (k < bnd) { buf[u64 k] = #VMOVLPD(r_h); }
    k = __INV_P(4*i+3);
    if (k < bnd) { buf[u64 k] = #VMOVHPD(r_h); }
  }

  return state, buf;
}

inline
fn __shake256_128_33_avx2(reg ptr u8[128] buf, reg ptr u8[33] extseed) -> reg ptr u8[128] {
  reg u256[7] state;

  state = __shake256_absorb33_avx2(extseed);
  _, buf = __squeeze128_avx2(state, buf);

  return buf;
}

inline
fn __squeeze64_avx2(reg u256[7] state, reg ptr u8[64] buf) -> reg u256[7], reg ptr u8[64] {
  inline int i, k, bnd;
  reg u128 r_l, r_h;

  bnd = 64/8;

  state = __keccakf1600_avx2(state);

  r_l = (128u) state[0];
  buf[u64 0] = #VMOVLPD(r_l);
  for i = 0 to 6 {
    //t256 = state[i+1];
    r_l = (128u) state[i+1];
    r_h = #VEXTRACTI128(state[i+1], 1);
    k = __INV_P(4*i+0);
    if (k < bnd) { buf[u64 k] = #VMOVLPD(r_l); }
    k = __INV_P(4*i+1);
    if (k < bnd) { buf[u64 k] = #VMOVHPD(r_l); }
    k = __INV_P(4*i+2);
    if (k < bnd) { buf[u64 k] = #VMOVLPD(r_h); }
    k = __INV_P(4*i+3);
    if (k < bnd) { buf[u64 k] = #VMOVHPD(r_h); }
  }

  return state, buf;
}

inline
fn __squeeze32_avx2(reg u256[7] state, reg ptr u8[32] buf) -> reg u256[7], reg ptr u8[32] {
  reg u128 r_l, r_h;

  state = __keccakf1600_avx2(state);

  r_l = (128u) state[0];
  buf[u64 0] = #VMOVLPD(r_l);
  r_l = (128u) state[1];
  r_h = #VEXTRACTI128(state[1], 1);
  buf[u64 1] = #VMOVLPD(r_l);
  buf[u64 2] = #VMOVHPD(r_l);
  buf[u64 3] = #VMOVLPD(r_h);

  return state, buf;
}

inline
fn __squeeze32m_avx2(reg u256[7] state, reg u64 buf) -> reg u256[7] {
  reg u128 r_l, r_h;

  state = __keccakf1600_avx2(state);

  r_l = (128u) state[0];
  [buf] = #VMOVLPD(r_l);
  r_l = (128u) state[1];
  r_h = #VEXTRACTI128(state[1], 1);
  [buf + 1*8] = #VMOVLPD(r_l);
  [buf + 2*8] = #VMOVHPD(r_l);
  [buf + 3*8] = #VMOVLPD(r_h);

  return state;
}

fn _shake256_m32_64_avx2(reg u64 buf, reg ptr u8[64] in) {
  reg u256[7] state;
  state = __shake256_absorb64_avx2(in);
  _ = __squeeze32m_avx2(state, buf);
}

#[returnaddress="stack"]
fn _sha3_256_avx2(reg ptr u8[32] out, reg u64 in inlen) -> reg ptr u8[32]
{
  reg u256[7] state;
  reg u8 trail_byte;
  inline int i;

  for i=0 to 7
  { state[i] = #set0_256(); }

  trail_byte = 0x06;
  state = __absorb_avx2(state, in, inlen, trail_byte, SHA3_256_RATE);

  _, out = __squeeze32_avx2(state, out);

  return out;
}

inline
fn __sha3_512_32_avx2(reg ptr u8[64] buf, reg ptr u8[32] seed) -> reg ptr u8[64] {
  reg u256[7] state;

  state = __sha512_absorb32_avx2(seed);
  _, buf = __squeeze64_avx2(state, buf);

  return buf;
}

inline
fn __sha3_512_64_avx2(reg ptr u8[64] buf, reg ptr u8[64] data) -> reg ptr u8[64] {
  reg u256[7] state;

  state = __sha512_absorb64_avx2(data);
  _, buf = __squeeze64_avx2(state, buf);

  return buf;
}

fn _sha3_256_32_avx2(reg ptr u8[32] buf, reg ptr u8[32] data) -> reg ptr u8[32] {
  reg u256[7] state;

  state = __sha256_absorb32_avx2(data);
  _, buf = __squeeze32_avx2(state, buf);

  return buf;
}

/*
 * State conversion between other keccak implementations
 */

inline
fn __st_scalar_avx2(stack u64[25] st) -> reg u256[7] {
  inline int j i;
  reg u256[7] state;
  reg u256 t256;
  reg u64 t0 t1 t2 t3;

  t256 = #VPBROADCAST_4u64(st[u64 0]);
  state[0] = t256;
  for j = 0 to 6 {
    i = __INV_P(4*j+0);
    t0 = st[u64 i];
    i = __INV_P(4*j+1);
    t1 = st[u64 i];
    i = __INV_P(4*j+2);
    t2 = st[u64 i];
    i = __INV_P(4*j+3);
    t3 = st[u64 i];
    t256 = __u64x4_u256(t0, t1, t2, t3);
    state[j+1] = t256;
  }
  return state;
}


inline
fn __st4x_unpack_avx2( stack u256[25] state4x
		     ) -> stack u256[7]
                        , stack u256[7]
                        , stack u256[7]
                        , stack u256[7]
{
  inline int i;
  stack u256[7] st0, st1, st2, st3;
  reg u256 x0 x1 x2 x3 y0 y1 y2 y3;
  reg u256 t0 t1 t2 t3;
  inline int t;
  reg ptr u64[24] perm;

  //perm = KECCAK_ST_PINV;

  // state4x[0] are expanded into the first registers
  t0 = #VPBROADCAST_4u64(state4x[u64 0]);
  st0[0] = t0;
  t1 = #VPBROADCAST_4u64(state4x[u64 1]);
  st1[0] = t1;
  t2 = #VPBROADCAST_4u64(state4x[u64 2]);
  st2[0] = t2;
  t3 = #VPBROADCAST_4u64(state4x[u64 3]);
  st3[0] = t3;

  for i = 0 to 6 {
    t = __INV_P(4*i+0);
    t *=32;
    x0 = state4x.[t];
    t = __INV_P(4*i+1); //perm[4*i+1];
    t *=32;
    x1 = state4x.[t];
    t = __INV_P(4*i+2); //perm[4*i+2];
    t *=32;
    x2 = state4x.[t];
    t = __INV_P(4*i+3); //perm[4*i+3];
    t *=32;
    x3 = state4x.[t];

    x0, x1, x2, x3 = __4u64x4_u256x4(x0, x1, x2, x3);

    st0[i+1] = x0;
    st1[i+1] = x1;
    st2[i+1] = x2;
    st3[i+1] = x3;
  }

  return st0, st1, st2, st3;
}  

