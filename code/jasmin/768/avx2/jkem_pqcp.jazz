/* -------------------------------------------------------------------- */
/* After typing */

param int MLKEM_K = 3;

param int MLKEM_POLYCOMPRESSEDBYTES = 128;

param int MLKEM_POLYVECCOMPRESSEDBYTES = (MLKEM_K * 320);

param int MLKEM_Q = 3329;

param int MLKEM_N = 256;

param int MLKEM_VECN = (MLKEM_K * MLKEM_N);

param int MLKEM_SYMBYTES = 32;

param int MLKEM_SSBYTES = 32;

param int MLKEM_ETA1 = 2;

param int MLKEM_ETA2 = 2;

param int MLKEM_POLYBYTES = 384;

param int MLKEM_POLYVECBYTES = (MLKEM_K * MLKEM_POLYBYTES);

param int MLKEM_INDCPA_MSGBYTES = MLKEM_SYMBYTES;

param int MLKEM_INDCPA_PUBLICKEYBYTES = (MLKEM_POLYVECBYTES + MLKEM_SYMBYTES);

param int MLKEM_INDCPA_SECRETKEYBYTES = MLKEM_POLYVECBYTES;

param int MLKEM_INDCPA_CIPHERTEXTBYTES = (MLKEM_POLYVECCOMPRESSEDBYTES +
                                         MLKEM_POLYCOMPRESSEDBYTES);

param int MLKEM_PUBLICKEYBYTES = MLKEM_INDCPA_PUBLICKEYBYTES;

param int MLKEM_SECRETKEYBYTES = ((MLKEM_INDCPA_SECRETKEYBYTES +
                                  MLKEM_INDCPA_PUBLICKEYBYTES) +
                                 (2 * MLKEM_SYMBYTES));

param int MLKEM_CIPHERTEXTBYTES = MLKEM_INDCPA_CIPHERTEXTBYTES;

inline
fn __shuffle8 (reg u256 a, reg u256 b) -> (reg u256, reg u256)

{
  reg u256 r0;
  r0 = #VPERM2I128(a, b, ((8u) 32)); /*  */ 
  reg u256 r1;
  r1 = #VPERM2I128(a, b, ((8u) 49)); /*  */ 
  return (r0, r1);
}

inline
fn __shuffle4 (reg u256 a, reg u256 b) -> (reg u256, reg u256)

{
  reg u256 r0;
  r0 = #VPUNPCKL_4u64(a, b); /*  */ 
  reg u256 r1;
  r1 = #VPUNPCKH_4u64(a, b); /*  */ 
  return (r0, r1);
}

inline
fn __shuffle2 (reg u256 a, reg u256 b) -> (reg u256, reg u256)

{
  reg u256 t0;
  t0 = #VMOVSLDUP_256(b); /*  */ 
  t0 = #VPBLEND_8u32(a, t0, ((8u) 170)); /*  */ 
  a = #VPSRL_4u64(a, ((128u) 32)); /*  */ 
  reg u256 t1;
  t1 = #VPBLEND_8u32(a, b, ((8u) 170)); /*  */ 
  return (t0, t1);
}

inline
fn __shuffle1 (reg u256 a, reg u256 b) -> (reg u256, reg u256)

{
  reg u256 t0;
  t0 = #VPSLL_8u32(b, ((128u) 16)); /*  */ 
  reg u256 r0;
  r0 = #VPBLEND_16u16(a, t0, ((8u) 170)); /*  */ 
  reg u256 t1;
  t1 = #VPSRL_8u32(a, ((128u) 16)); /*  */ 
  reg u256 r1;
  r1 = #VPBLEND_16u16(t1, b, ((8u) 170)); /*  */ 
  return (r0, r1);
}

inline
fn __nttunpack128 (reg u256 r0, reg u256 r1, reg u256 r2, reg u256 r3,
                  reg u256 r4, reg u256 r5, reg u256 r6, reg u256 r7) -> 
(reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256,
reg u256)

{
  #[inline]
  (r0, r4) = __shuffle8(r0, r4);
  #[inline]
  (r1, r5) = __shuffle8(r1, r5);
  #[inline]
  (r2, r6) = __shuffle8(r2, r6);
  #[inline]
  (r3, r7) = __shuffle8(r3, r7);
  #[inline]
  (r0, r2) = __shuffle4(r0, r2);
  #[inline]
  (r4, r6) = __shuffle4(r4, r6);
  #[inline]
  (r1, r3) = __shuffle4(r1, r3);
  #[inline]
  (r5, r7) = __shuffle4(r5, r7);
  #[inline]
  (r0, r1) = __shuffle2(r0, r1);
  #[inline]
  (r2, r3) = __shuffle2(r2, r3);
  #[inline]
  (r4, r5) = __shuffle2(r4, r5);
  #[inline]
  (r6, r7) = __shuffle2(r6, r7);
  #[inline]
  (r0, r4) = __shuffle1(r0, r4);
  #[inline]
  (r1, r5) = __shuffle1(r1, r5);
  #[inline]
  (r2, r6) = __shuffle1(r2, r6);
  #[inline]
  (r3, r7) = __shuffle1(r3, r7);
  return (r0, r4, r1, r5, r2, r6, r3, r7);
}

fn _nttunpack (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N])

{
  reg u256 r0;
  r0 = rp.[#unaligned :u256 (32 * 0)]; /* u256 */ 
  reg u256 r1;
  r1 = rp.[#unaligned :u256 (32 * 1)]; /* u256 */ 
  reg u256 r2;
  r2 = rp.[#unaligned :u256 (32 * 2)]; /* u256 */ 
  reg u256 r3;
  r3 = rp.[#unaligned :u256 (32 * 3)]; /* u256 */ 
  reg u256 r4;
  r4 = rp.[#unaligned :u256 (32 * 4)]; /* u256 */ 
  reg u256 r5;
  r5 = rp.[#unaligned :u256 (32 * 5)]; /* u256 */ 
  reg u256 r6;
  r6 = rp.[#unaligned :u256 (32 * 6)]; /* u256 */ 
  reg u256 r7;
  r7 = rp.[#unaligned :u256 (32 * 7)]; /* u256 */ 
  #[inline]
  (r0, r1, r2, r3, r4, r5, r6, r7) =
    __nttunpack128(r0, r1, r2, r3, r4, r5, r6, r7);
  rp.[#unaligned :u256 (32 * 0)] = r0; /* u256 */ 
  rp.[#unaligned :u256 (32 * 1)] = r1; /* u256 */ 
  rp.[#unaligned :u256 (32 * 2)] = r2; /* u256 */ 
  rp.[#unaligned :u256 (32 * 3)] = r3; /* u256 */ 
  rp.[#unaligned :u256 (32 * 4)] = r4; /* u256 */ 
  rp.[#unaligned :u256 (32 * 5)] = r5; /* u256 */ 
  rp.[#unaligned :u256 (32 * 6)] = r6; /* u256 */ 
  rp.[#unaligned :u256 (32 * 7)] = r7; /* u256 */ 
  r0 = rp.[#unaligned :u256 (32 * 8)]; /* u256 */ 
  r1 = rp.[#unaligned :u256 (32 * 9)]; /* u256 */ 
  r2 = rp.[#unaligned :u256 (32 * 10)]; /* u256 */ 
  r3 = rp.[#unaligned :u256 (32 * 11)]; /* u256 */ 
  r4 = rp.[#unaligned :u256 (32 * 12)]; /* u256 */ 
  r5 = rp.[#unaligned :u256 (32 * 13)]; /* u256 */ 
  r6 = rp.[#unaligned :u256 (32 * 14)]; /* u256 */ 
  r7 = rp.[#unaligned :u256 (32 * 15)]; /* u256 */ 
  #[inline]
  (r0, r1, r2, r3, r4, r5, r6, r7) =
    __nttunpack128(r0, r1, r2, r3, r4, r5, r6, r7);
  rp.[#unaligned :u256 (32 * 8)] = r0; /* u256 */ 
  rp.[#unaligned :u256 (32 * 9)] = r1; /* u256 */ 
  rp.[#unaligned :u256 (32 * 10)] = r2; /* u256 */ 
  rp.[#unaligned :u256 (32 * 11)] = r3; /* u256 */ 
  rp.[#unaligned :u256 (32 * 12)] = r4; /* u256 */ 
  rp.[#unaligned :u256 (32 * 13)] = r5; /* u256 */ 
  rp.[#unaligned :u256 (32 * 14)] = r6; /* u256 */ 
  rp.[#unaligned :u256 (32 * 15)] = r7; /* u256 */ 
  return (rp);
}

u16[128] jzetas = {((16u) 2285), ((16u) 2571), ((16u) 2970), ((16u) 1812),
                   ((16u) 1493), ((16u) 1422), ((16u) 287), ((16u) 202),
                   ((16u) 3158), ((16u) 622), ((16u) 1577), ((16u) 182),
                   ((16u) 962), ((16u) 2127), ((16u) 1855), ((16u) 1468),
                   ((16u) 573), ((16u) 2004), ((16u) 264), ((16u) 383),
                   ((16u) 2500), ((16u) 1458), ((16u) 1727), ((16u) 3199),
                   ((16u) 2648), ((16u) 1017), ((16u) 732), ((16u) 608),
                   ((16u) 1787), ((16u) 411), ((16u) 3124), ((16u) 1758),
                   ((16u) 1223), ((16u) 652), ((16u) 2777), ((16u) 1015),
                   ((16u) 2036), ((16u) 1491), ((16u) 3047), ((16u) 1785),
                   ((16u) 516), ((16u) 3321), ((16u) 3009), ((16u) 2663),
                   ((16u) 1711), ((16u) 2167), ((16u) 126), ((16u) 1469),
                   ((16u) 2476), ((16u) 3239), ((16u) 3058), ((16u) 830),
                   ((16u) 107), ((16u) 1908), ((16u) 3082), ((16u) 2378),
                   ((16u) 2931), ((16u) 961), ((16u) 1821), ((16u) 2604),
                   ((16u) 448), ((16u) 2264), ((16u) 677), ((16u) 2054),
                   ((16u) 2226), ((16u) 430), ((16u) 555), ((16u) 843),
                   ((16u) 2078), ((16u) 871), ((16u) 1550), ((16u) 105),
                   ((16u) 422), ((16u) 587), ((16u) 177), ((16u) 3094),
                   ((16u) 3038), ((16u) 2869), ((16u) 1574), ((16u) 1653),
                   ((16u) 3083), ((16u) 778), ((16u) 1159), ((16u) 3182),
                   ((16u) 2552), ((16u) 1483), ((16u) 2727), ((16u) 1119),
                   ((16u) 1739), ((16u) 644), ((16u) 2457), ((16u) 349),
                   ((16u) 418), ((16u) 329), ((16u) 3173), ((16u) 3254),
                   ((16u) 817), ((16u) 1097), ((16u) 603), ((16u) 610),
                   ((16u) 1322), ((16u) 2044), ((16u) 1864), ((16u) 384),
                   ((16u) 2114), ((16u) 3193), ((16u) 1218), ((16u) 1994),
                   ((16u) 2455), ((16u) 220), ((16u) 2142), ((16u) 1670),
                   ((16u) 2144), ((16u) 1799), ((16u) 2051), ((16u) 794),
                   ((16u) 1819), ((16u) 2475), ((16u) 2459), ((16u) 478),
                   ((16u) 3221), ((16u) 3021), ((16u) 996), ((16u) 991),
                   ((16u) 958), ((16u) 1869), ((16u) 1522), ((16u) 1628)};

u16[128] jzetas_inv = {((16u) 1701), ((16u) 1807), ((16u) 1460),
                       ((16u) 2371), ((16u) 2338), ((16u) 2333), ((16u) 308),
                       ((16u) 108), ((16u) 2851), ((16u) 870), ((16u) 854),
                       ((16u) 1510), ((16u) 2535), ((16u) 1278),
                       ((16u) 1530), ((16u) 1185), ((16u) 1659),
                       ((16u) 1187), ((16u) 3109), ((16u) 874), ((16u) 1335),
                       ((16u) 2111), ((16u) 136), ((16u) 1215), ((16u) 2945),
                       ((16u) 1465), ((16u) 1285), ((16u) 2007),
                       ((16u) 2719), ((16u) 2726), ((16u) 2232),
                       ((16u) 2512), ((16u) 75), ((16u) 156), ((16u) 3000),
                       ((16u) 2911), ((16u) 2980), ((16u) 872), ((16u) 2685),
                       ((16u) 1590), ((16u) 2210), ((16u) 602), ((16u) 1846),
                       ((16u) 777), ((16u) 147), ((16u) 2170), ((16u) 2551),
                       ((16u) 246), ((16u) 1676), ((16u) 1755), ((16u) 460),
                       ((16u) 291), ((16u) 235), ((16u) 3152), ((16u) 2742),
                       ((16u) 2907), ((16u) 3224), ((16u) 1779),
                       ((16u) 2458), ((16u) 1251), ((16u) 2486),
                       ((16u) 2774), ((16u) 2899), ((16u) 1103),
                       ((16u) 1275), ((16u) 2652), ((16u) 1065),
                       ((16u) 2881), ((16u) 725), ((16u) 1508), ((16u) 2368),
                       ((16u) 398), ((16u) 951), ((16u) 247), ((16u) 1421),
                       ((16u) 3222), ((16u) 2499), ((16u) 271), ((16u) 90),
                       ((16u) 853), ((16u) 1860), ((16u) 3203), ((16u) 1162),
                       ((16u) 1618), ((16u) 666), ((16u) 320), ((16u) 8),
                       ((16u) 2813), ((16u) 1544), ((16u) 282), ((16u) 1838),
                       ((16u) 1293), ((16u) 2314), ((16u) 552), ((16u) 2677),
                       ((16u) 2106), ((16u) 1571), ((16u) 205), ((16u) 2918),
                       ((16u) 1542), ((16u) 2721), ((16u) 2597),
                       ((16u) 2312), ((16u) 681), ((16u) 130), ((16u) 1602),
                       ((16u) 1871), ((16u) 829), ((16u) 2946), ((16u) 3065),
                       ((16u) 1325), ((16u) 2756), ((16u) 1861),
                       ((16u) 1474), ((16u) 1202), ((16u) 2367),
                       ((16u) 3147), ((16u) 1752), ((16u) 2707), ((16u) 171),
                       ((16u) 3127), ((16u) 3042), ((16u) 1907),
                       ((16u) 1836), ((16u) 1517), ((16u) 359), ((16u) 758),
                       ((16u) 1441)};

u16[400] jzetas_exp = {((16u) 31499), ((16u) 31499), ((16u) 2571),
                       ((16u) 2571), ((16u) 14746), ((16u) 14746),
                       ((16u) 2970), ((16u) 2970), ((16u) 13525),
                       ((16u) 13525), ((16u) 13525), ((16u) 13525),
                       ((16u) 13525), ((16u) 13525), ((16u) 13525),
                       ((16u) 13525), ((16u) 53134), ((16u) 53134),
                       ((16u) 53134), ((16u) 53134), ((16u) 53134),
                       ((16u) 53134), ((16u) 53134), ((16u) 53134),
                       ((16u) 1493), ((16u) 1493), ((16u) 1493),
                       ((16u) 1493), ((16u) 1493), ((16u) 1493),
                       ((16u) 1493), ((16u) 1493), ((16u) 1422),
                       ((16u) 1422), ((16u) 1422), ((16u) 1422),
                       ((16u) 1422), ((16u) 1422), ((16u) 1422),
                       ((16u) 1422), ((16u) 44630), ((16u) 44630),
                       ((16u) 44630), ((16u) 44630), ((16u) 27758),
                       ((16u) 27758), ((16u) 27758), ((16u) 27758),
                       ((16u) 61737), ((16u) 61737), ((16u) 61737),
                       ((16u) 61737), ((16u) 49846), ((16u) 49846),
                       ((16u) 49846), ((16u) 49846), ((16u) 3158),
                       ((16u) 3158), ((16u) 3158), ((16u) 3158), ((16u) 622),
                       ((16u) 622), ((16u) 622), ((16u) 622), ((16u) 1577),
                       ((16u) 1577), ((16u) 1577), ((16u) 1577), ((16u) 182),
                       ((16u) 182), ((16u) 182), ((16u) 182), ((16u) 59709),
                       ((16u) 59709), ((16u) 17364), ((16u) 17364),
                       ((16u) 39176), ((16u) 39176), ((16u) 36479),
                       ((16u) 36479), ((16u) 5572), ((16u) 5572),
                       ((16u) 64434), ((16u) 64434), ((16u) 21439),
                       ((16u) 21439), ((16u) 39295), ((16u) 39295),
                       ((16u) 573), ((16u) 573), ((16u) 2004), ((16u) 2004),
                       ((16u) 264), ((16u) 264), ((16u) 383), ((16u) 383),
                       ((16u) 2500), ((16u) 2500), ((16u) 1458),
                       ((16u) 1458), ((16u) 1727), ((16u) 1727),
                       ((16u) 3199), ((16u) 3199), ((16u) 59847),
                       ((16u) 59020), ((16u) 1497), ((16u) 30967),
                       ((16u) 41972), ((16u) 20179), ((16u) 20711),
                       ((16u) 25081), ((16u) 52740), ((16u) 26617),
                       ((16u) 16065), ((16u) 53095), ((16u) 9135),
                       ((16u) 64887), ((16u) 39550), ((16u) 27837),
                       ((16u) 1223), ((16u) 652), ((16u) 2777), ((16u) 1015),
                       ((16u) 2036), ((16u) 1491), ((16u) 3047),
                       ((16u) 1785), ((16u) 516), ((16u) 3321), ((16u) 3009),
                       ((16u) 2663), ((16u) 1711), ((16u) 2167), ((16u) 126),
                       ((16u) 1469), ((16u) 65202), ((16u) 54059),
                       ((16u) 33310), ((16u) 20494), ((16u) 37798),
                       ((16u) 945), ((16u) 50654), ((16u) 6182),
                       ((16u) 32011), ((16u) 10631), ((16u) 29176),
                       ((16u) 36775), ((16u) 47051), ((16u) 17561),
                       ((16u) 51106), ((16u) 60261), ((16u) 2226),
                       ((16u) 555), ((16u) 2078), ((16u) 1550), ((16u) 422),
                       ((16u) 177), ((16u) 3038), ((16u) 1574), ((16u) 3083),
                       ((16u) 1159), ((16u) 2552), ((16u) 2727),
                       ((16u) 1739), ((16u) 2457), ((16u) 418), ((16u) 3173),
                       ((16u) 11182), ((16u) 13387), ((16u) 51303),
                       ((16u) 43881), ((16u) 13131), ((16u) 60950),
                       ((16u) 23093), ((16u) 5493), ((16u) 33034),
                       ((16u) 30318), ((16u) 46795), ((16u) 12639),
                       ((16u) 20100), ((16u) 18525), ((16u) 19529),
                       ((16u) 52918), ((16u) 430), ((16u) 843), ((16u) 871),
                       ((16u) 105), ((16u) 587), ((16u) 3094), ((16u) 2869),
                       ((16u) 1653), ((16u) 778), ((16u) 3182), ((16u) 1483),
                       ((16u) 1119), ((16u) 644), ((16u) 349), ((16u) 329),
                       ((16u) 3254), ((16u) 788), ((16u) 788), ((16u) 1812),
                       ((16u) 1812), ((16u) 28191), ((16u) 28191),
                       ((16u) 28191), ((16u) 28191), ((16u) 28191),
                       ((16u) 28191), ((16u) 28191), ((16u) 28191),
                       ((16u) 48842), ((16u) 48842), ((16u) 48842),
                       ((16u) 48842), ((16u) 48842), ((16u) 48842),
                       ((16u) 48842), ((16u) 48842), ((16u) 287),
                       ((16u) 287), ((16u) 287), ((16u) 287), ((16u) 287),
                       ((16u) 287), ((16u) 287), ((16u) 287), ((16u) 202),
                       ((16u) 202), ((16u) 202), ((16u) 202), ((16u) 202),
                       ((16u) 202), ((16u) 202), ((16u) 202), ((16u) 10690),
                       ((16u) 10690), ((16u) 10690), ((16u) 10690),
                       ((16u) 1359), ((16u) 1359), ((16u) 1359),
                       ((16u) 1359), ((16u) 54335), ((16u) 54335),
                       ((16u) 54335), ((16u) 54335), ((16u) 31164),
                       ((16u) 31164), ((16u) 31164), ((16u) 31164),
                       ((16u) 962), ((16u) 962), ((16u) 962), ((16u) 962),
                       ((16u) 2127), ((16u) 2127), ((16u) 2127),
                       ((16u) 2127), ((16u) 1855), ((16u) 1855),
                       ((16u) 1855), ((16u) 1855), ((16u) 1468),
                       ((16u) 1468), ((16u) 1468), ((16u) 1468),
                       ((16u) 37464), ((16u) 37464), ((16u) 24313),
                       ((16u) 24313), ((16u) 55004), ((16u) 55004),
                       ((16u) 8800), ((16u) 8800), ((16u) 18427),
                       ((16u) 18427), ((16u) 8859), ((16u) 8859),
                       ((16u) 26676), ((16u) 26676), ((16u) 49374),
                       ((16u) 49374), ((16u) 2648), ((16u) 2648),
                       ((16u) 1017), ((16u) 1017), ((16u) 732), ((16u) 732),
                       ((16u) 608), ((16u) 608), ((16u) 1787), ((16u) 1787),
                       ((16u) 411), ((16u) 411), ((16u) 3124), ((16u) 3124),
                       ((16u) 1758), ((16u) 1758), ((16u) 19884),
                       ((16u) 37287), ((16u) 49650), ((16u) 56638),
                       ((16u) 37227), ((16u) 9076), ((16u) 35338),
                       ((16u) 18250), ((16u) 13427), ((16u) 14017),
                       ((16u) 36381), ((16u) 52780), ((16u) 16832),
                       ((16u) 4312), ((16u) 41381), ((16u) 47622),
                       ((16u) 2476), ((16u) 3239), ((16u) 3058), ((16u) 830),
                       ((16u) 107), ((16u) 1908), ((16u) 3082), ((16u) 2378),
                       ((16u) 2931), ((16u) 961), ((16u) 1821), ((16u) 2604),
                       ((16u) 448), ((16u) 2264), ((16u) 677), ((16u) 2054),
                       ((16u) 34353), ((16u) 25435), ((16u) 58154),
                       ((16u) 24392), ((16u) 44610), ((16u) 10946),
                       ((16u) 24215), ((16u) 16990), ((16u) 10336),
                       ((16u) 57603), ((16u) 43035), ((16u) 10907),
                       ((16u) 31637), ((16u) 28644), ((16u) 23998),
                       ((16u) 48114), ((16u) 817), ((16u) 603), ((16u) 1322),
                       ((16u) 1864), ((16u) 2114), ((16u) 1218),
                       ((16u) 2455), ((16u) 2142), ((16u) 2144),
                       ((16u) 2051), ((16u) 1819), ((16u) 2459),
                       ((16u) 3221), ((16u) 996), ((16u) 958), ((16u) 1522),
                       ((16u) 20297), ((16u) 2146), ((16u) 15356),
                       ((16u) 33152), ((16u) 59257), ((16u) 50634),
                       ((16u) 54492), ((16u) 14470), ((16u) 44039),
                       ((16u) 45338), ((16u) 23211), ((16u) 48094),
                       ((16u) 41677), ((16u) 45279), ((16u) 7757),
                       ((16u) 23132), ((16u) 1097), ((16u) 610),
                       ((16u) 2044), ((16u) 384), ((16u) 3193), ((16u) 1994),
                       ((16u) 220), ((16u) 1670), ((16u) 1799), ((16u) 794),
                       ((16u) 2475), ((16u) 478), ((16u) 3021), ((16u) 991),
                       ((16u) 1869), ((16u) 1628), ((16u) 0), ((16u) 0),
                       ((16u) 0), ((16u) 0)};

u16[400] jzetas_inv_exp = {((16u) 42405), ((16u) 57780), ((16u) 20258),
                           ((16u) 23860), ((16u) 17443), ((16u) 42326),
                           ((16u) 20199), ((16u) 21498), ((16u) 51067),
                           ((16u) 11045), ((16u) 14903), ((16u) 6280),
                           ((16u) 32385), ((16u) 50181), ((16u) 63391),
                           ((16u) 45240), ((16u) 1701), ((16u) 1460),
                           ((16u) 2338), ((16u) 308), ((16u) 2851),
                           ((16u) 854), ((16u) 2535), ((16u) 1530),
                           ((16u) 1659), ((16u) 3109), ((16u) 1335),
                           ((16u) 136), ((16u) 2945), ((16u) 1285),
                           ((16u) 2719), ((16u) 2232), ((16u) 17423),
                           ((16u) 41539), ((16u) 36893), ((16u) 33900),
                           ((16u) 54630), ((16u) 22502), ((16u) 7934),
                           ((16u) 55201), ((16u) 48547), ((16u) 41322),
                           ((16u) 54591), ((16u) 20927), ((16u) 41145),
                           ((16u) 7383), ((16u) 40102), ((16u) 31184),
                           ((16u) 1807), ((16u) 2371), ((16u) 2333),
                           ((16u) 108), ((16u) 870), ((16u) 1510),
                           ((16u) 1278), ((16u) 1185), ((16u) 1187),
                           ((16u) 874), ((16u) 2111), ((16u) 1215),
                           ((16u) 1465), ((16u) 2007), ((16u) 2726),
                           ((16u) 2512), ((16u) 17915), ((16u) 24156),
                           ((16u) 61225), ((16u) 48705), ((16u) 12757),
                           ((16u) 29156), ((16u) 51520), ((16u) 52110),
                           ((16u) 47287), ((16u) 30199), ((16u) 56461),
                           ((16u) 28310), ((16u) 8899), ((16u) 15887),
                           ((16u) 28250), ((16u) 45653), ((16u) 1275),
                           ((16u) 2652), ((16u) 1065), ((16u) 2881),
                           ((16u) 725), ((16u) 1508), ((16u) 2368),
                           ((16u) 398), ((16u) 951), ((16u) 247),
                           ((16u) 1421), ((16u) 3222), ((16u) 2499),
                           ((16u) 271), ((16u) 90), ((16u) 853),
                           ((16u) 16163), ((16u) 16163), ((16u) 38861),
                           ((16u) 38861), ((16u) 56678), ((16u) 56678),
                           ((16u) 47110), ((16u) 47110), ((16u) 56737),
                           ((16u) 56737), ((16u) 10533), ((16u) 10533),
                           ((16u) 41224), ((16u) 41224), ((16u) 28073),
                           ((16u) 28073), ((16u) 1571), ((16u) 1571),
                           ((16u) 205), ((16u) 205), ((16u) 2918),
                           ((16u) 2918), ((16u) 1542), ((16u) 1542),
                           ((16u) 2721), ((16u) 2721), ((16u) 2597),
                           ((16u) 2597), ((16u) 2312), ((16u) 2312),
                           ((16u) 681), ((16u) 681), ((16u) 34373),
                           ((16u) 34373), ((16u) 34373), ((16u) 34373),
                           ((16u) 11202), ((16u) 11202), ((16u) 11202),
                           ((16u) 11202), ((16u) 64178), ((16u) 64178),
                           ((16u) 64178), ((16u) 64178), ((16u) 54847),
                           ((16u) 54847), ((16u) 54847), ((16u) 54847),
                           ((16u) 1861), ((16u) 1861), ((16u) 1861),
                           ((16u) 1861), ((16u) 1474), ((16u) 1474),
                           ((16u) 1474), ((16u) 1474), ((16u) 1202),
                           ((16u) 1202), ((16u) 1202), ((16u) 1202),
                           ((16u) 2367), ((16u) 2367), ((16u) 2367),
                           ((16u) 2367), ((16u) 16695), ((16u) 16695),
                           ((16u) 16695), ((16u) 16695), ((16u) 16695),
                           ((16u) 16695), ((16u) 16695), ((16u) 16695),
                           ((16u) 37346), ((16u) 37346), ((16u) 37346),
                           ((16u) 37346), ((16u) 37346), ((16u) 37346),
                           ((16u) 37346), ((16u) 37346), ((16u) 3127),
                           ((16u) 3127), ((16u) 3127), ((16u) 3127),
                           ((16u) 3127), ((16u) 3127), ((16u) 3127),
                           ((16u) 3127), ((16u) 3042), ((16u) 3042),
                           ((16u) 3042), ((16u) 3042), ((16u) 3042),
                           ((16u) 3042), ((16u) 3042), ((16u) 3042),
                           ((16u) 64749), ((16u) 64749), ((16u) 1517),
                           ((16u) 1517), ((16u) 12619), ((16u) 46008),
                           ((16u) 47012), ((16u) 45437), ((16u) 52898),
                           ((16u) 18742), ((16u) 35219), ((16u) 32503),
                           ((16u) 60044), ((16u) 42444), ((16u) 4587),
                           ((16u) 52406), ((16u) 21656), ((16u) 14234),
                           ((16u) 52150), ((16u) 54355), ((16u) 75),
                           ((16u) 3000), ((16u) 2980), ((16u) 2685),
                           ((16u) 2210), ((16u) 1846), ((16u) 147),
                           ((16u) 2551), ((16u) 1676), ((16u) 460),
                           ((16u) 235), ((16u) 2742), ((16u) 3224),
                           ((16u) 2458), ((16u) 2486), ((16u) 2899),
                           ((16u) 5276), ((16u) 14431), ((16u) 47976),
                           ((16u) 18486), ((16u) 28762), ((16u) 36361),
                           ((16u) 54906), ((16u) 33526), ((16u) 59355),
                           ((16u) 14883), ((16u) 64592), ((16u) 27739),
                           ((16u) 45043), ((16u) 32227), ((16u) 11478),
                           ((16u) 335), ((16u) 156), ((16u) 2911),
                           ((16u) 872), ((16u) 1590), ((16u) 602),
                           ((16u) 777), ((16u) 2170), ((16u) 246),
                           ((16u) 1755), ((16u) 291), ((16u) 3152),
                           ((16u) 2907), ((16u) 1779), ((16u) 1251),
                           ((16u) 2774), ((16u) 1103), ((16u) 37700),
                           ((16u) 25987), ((16u) 650), ((16u) 56402),
                           ((16u) 12442), ((16u) 49472), ((16u) 38920),
                           ((16u) 12797), ((16u) 40456), ((16u) 44826),
                           ((16u) 45358), ((16u) 23565), ((16u) 34570),
                           ((16u) 64040), ((16u) 6517), ((16u) 5690),
                           ((16u) 1860), ((16u) 3203), ((16u) 1162),
                           ((16u) 1618), ((16u) 666), ((16u) 320), ((16u) 8),
                           ((16u) 2813), ((16u) 1544), ((16u) 282),
                           ((16u) 1838), ((16u) 1293), ((16u) 2314),
                           ((16u) 552), ((16u) 2677), ((16u) 2106),
                           ((16u) 26242), ((16u) 26242), ((16u) 44098),
                           ((16u) 44098), ((16u) 1103), ((16u) 1103),
                           ((16u) 59965), ((16u) 59965), ((16u) 29058),
                           ((16u) 29058), ((16u) 26361), ((16u) 26361),
                           ((16u) 48173), ((16u) 48173), ((16u) 5828),
                           ((16u) 5828), ((16u) 130), ((16u) 130),
                           ((16u) 1602), ((16u) 1602), ((16u) 1871),
                           ((16u) 1871), ((16u) 829), ((16u) 829),
                           ((16u) 2946), ((16u) 2946), ((16u) 3065),
                           ((16u) 3065), ((16u) 1325), ((16u) 1325),
                           ((16u) 2756), ((16u) 2756), ((16u) 15691),
                           ((16u) 15691), ((16u) 15691), ((16u) 15691),
                           ((16u) 3800), ((16u) 3800), ((16u) 3800),
                           ((16u) 3800), ((16u) 37779), ((16u) 37779),
                           ((16u) 37779), ((16u) 37779), ((16u) 20907),
                           ((16u) 20907), ((16u) 20907), ((16u) 20907),
                           ((16u) 3147), ((16u) 3147), ((16u) 3147),
                           ((16u) 3147), ((16u) 1752), ((16u) 1752),
                           ((16u) 1752), ((16u) 1752), ((16u) 2707),
                           ((16u) 2707), ((16u) 2707), ((16u) 2707),
                           ((16u) 171), ((16u) 171), ((16u) 171),
                           ((16u) 171), ((16u) 12403), ((16u) 12403),
                           ((16u) 12403), ((16u) 12403), ((16u) 12403),
                           ((16u) 12403), ((16u) 12403), ((16u) 12403),
                           ((16u) 52012), ((16u) 52012), ((16u) 52012),
                           ((16u) 52012), ((16u) 52012), ((16u) 52012),
                           ((16u) 52012), ((16u) 52012), ((16u) 1907),
                           ((16u) 1907), ((16u) 1907), ((16u) 1907),
                           ((16u) 1907), ((16u) 1907), ((16u) 1907),
                           ((16u) 1907), ((16u) 1836), ((16u) 1836),
                           ((16u) 1836), ((16u) 1836), ((16u) 1836),
                           ((16u) 1836), ((16u) 1836), ((16u) 1836),
                           ((16u) 50791), ((16u) 50791), ((16u) 359),
                           ((16u) 359), ((16u) 60300), ((16u) 60300),
                           ((16u) 1932), ((16u) 1932), ((16u) 0), ((16u) 0),
                           ((16u) 0), ((16u) 0)};

u16[16] jqx16 = {((16u) MLKEM_Q), ((16u) MLKEM_Q), ((16u) MLKEM_Q),
                 ((16u) MLKEM_Q), ((16u) MLKEM_Q), ((16u) MLKEM_Q),
                 ((16u) MLKEM_Q), ((16u) MLKEM_Q), ((16u) MLKEM_Q),
                 ((16u) MLKEM_Q), ((16u) MLKEM_Q), ((16u) MLKEM_Q),
                 ((16u) MLKEM_Q), ((16u) MLKEM_Q), ((16u) MLKEM_Q),
                 ((16u) MLKEM_Q)};

u16[16] jqinvx16 = {((16u) 62209), ((16u) 62209), ((16u) 62209),
                    ((16u) 62209), ((16u) 62209), ((16u) 62209),
                    ((16u) 62209), ((16u) 62209), ((16u) 62209),
                    ((16u) 62209), ((16u) 62209), ((16u) 62209),
                    ((16u) 62209), ((16u) 62209), ((16u) 62209),
                    ((16u) 62209)};

u16[16] jvx16 = {((16u) 20159), ((16u) 20159), ((16u) 20159), ((16u) 20159),
                 ((16u) 20159), ((16u) 20159), ((16u) 20159), ((16u) 20159),
                 ((16u) 20159), ((16u) 20159), ((16u) 20159), ((16u) 20159),
                 ((16u) 20159), ((16u) 20159), ((16u) 20159), ((16u) 20159)};

u16[16] jfhix16 = {((16u) 1441), ((16u) 1441), ((16u) 1441), ((16u) 1441),
                   ((16u) 1441), ((16u) 1441), ((16u) 1441), ((16u) 1441),
                   ((16u) 1441), ((16u) 1441), ((16u) 1441), ((16u) 1441),
                   ((16u) 1441), ((16u) 1441), ((16u) 1441), ((16u) 1441)};

u16[16] jflox16 = {((16u) 55457), ((16u) 55457), ((16u) 55457),
                   ((16u) 55457), ((16u) 55457), ((16u) 55457),
                   ((16u) 55457), ((16u) 55457), ((16u) 55457),
                   ((16u) 55457), ((16u) 55457), ((16u) 55457),
                   ((16u) 55457), ((16u) 55457), ((16u) 55457), ((16u) 55457)};

u16[16] maskx16 = {((16u) 4095), ((16u) 4095), ((16u) 4095), ((16u) 4095),
                   ((16u) 4095), ((16u) 4095), ((16u) 4095), ((16u) 4095),
                   ((16u) 4095), ((16u) 4095), ((16u) 4095), ((16u) 4095),
                   ((16u) 4095), ((16u) 4095), ((16u) 4095), ((16u) 4095)};

u16[16] hqx16_p1 = {((16u) 1665), ((16u) 1665), ((16u) 1665), ((16u) 1665),
                    ((16u) 1665), ((16u) 1665), ((16u) 1665), ((16u) 1665),
                    ((16u) 1665), ((16u) 1665), ((16u) 1665), ((16u) 1665),
                    ((16u) 1665), ((16u) 1665), ((16u) 1665), ((16u) 1665)};

u16[16] hqx16_m1 = {((16u) 1664), ((16u) 1664), ((16u) 1664), ((16u) 1664),
                    ((16u) 1664), ((16u) 1664), ((16u) 1664), ((16u) 1664),
                    ((16u) 1664), ((16u) 1664), ((16u) 1664), ((16u) 1664),
                    ((16u) 1664), ((16u) 1664), ((16u) 1664), ((16u) 1664)};

u16[16] hhqx16 = {((16u) 832), ((16u) 832), ((16u) 832), ((16u) 832),
                  ((16u) 832), ((16u) 832), ((16u) 832), ((16u) 832),
                  ((16u) 832), ((16u) 832), ((16u) 832), ((16u) 832),
                  ((16u) 832), ((16u) 832), ((16u) 832), ((16u) 832)};

u16[16] mqinvx16 = {((16u) 80635), ((16u) 80635), ((16u) 80635),
                    ((16u) 80635), ((16u) 80635), ((16u) 80635),
                    ((16u) 80635), ((16u) 80635), ((16u) 80635),
                    ((16u) 80635), ((16u) 80635), ((16u) 80635),
                    ((16u) 80635), ((16u) 80635), ((16u) 80635),
                    ((16u) 80635)};

u16[16] jdmontx16 = {((16u) 1353), ((16u) 1353), ((16u) 1353), ((16u) 1353),
                     ((16u) 1353), ((16u) 1353), ((16u) 1353), ((16u) 1353),
                     ((16u) 1353), ((16u) 1353), ((16u) 1353), ((16u) 1353),
                     ((16u) 1353), ((16u) 1353), ((16u) 1353), ((16u) 1353)};

param int QINV = 62209;

param int MONT = 2285;

param int BARR = 20159;

inline
fn __csubq (reg u256 r, reg u256 qx16) -> (reg u256)

{
  r = #VPSUB_16u16(r, qx16); /*  */ 
  reg u256 t;
  t = #VPSRA_16u16(r, ((128u) 15)); /*  */ 
  t = #VPAND_256(t, qx16); /*  */ 
  r = #VPADD_16u16(t, r); /*  */ 
  return (r);
}

inline
fn __red16x (reg u256 r, reg u256 qx16, reg u256 vx16) -> (reg u256)

{
  reg u256 x;
  x = #VPMULH_16u16(r, vx16); /*  */ 
  x = #VPSRA_16u16(x, ((128u) 10)); /*  */ 
  x = #VPMULL_16u16(x, qx16); /*  */ 
  r = #VPSUB_16u16(r, x); /*  */ 
  return (r);
}

inline
fn __fqmulprecomp16x (reg u256 b, reg u256 al, reg u256 ah, reg u256 qx16) -> 
(reg u256)

{
  reg u256 x;
  x = #VPMULL_16u16(al, b); /*  */ 
  b = #VPMULH_16u16(ah, b); /*  */ 
  x = #VPMULH_16u16(x, qx16); /*  */ 
  b = #VPSUB_16u16(b, x); /*  */ 
  return (b);
}

inline
fn __fqmulx16 (reg u256 a, reg u256 b, reg u256 qx16, reg u256 qinvx16) -> 
(reg u256)

{
  reg u256 rhi;
  rhi = #VPMULH_16u16(a, b); /*  */ 
  reg u256 rlo;
  rlo = #VPMULL_16u16(a, b); /*  */ 
  rlo = #VPMULL_16u16(rlo, qinvx16); /*  */ 
  rlo = #VPMULH_16u16(rlo, qx16); /*  */ 
  reg u256 rd;
  rd = #VPSUB_16u16(rhi, rlo); /*  */ 
  return (rd);
}

param int KECCAK_ROUNDS = 24;

u64[24] KECCAK1600_RC = {((64u) 1), ((64u) 32898),
                         ((64u) 9223372036854808714),
                         ((64u) 9223372039002292224), ((64u) 32907),
                         ((64u) 2147483649), ((64u) 9223372039002292353),
                         ((64u) 9223372036854808585), ((64u) 138),
                         ((64u) 136), ((64u) 2147516425), ((64u) 2147483658),
                         ((64u) 2147516555), ((64u) 9223372036854775947),
                         ((64u) 9223372036854808713),
                         ((64u) 9223372036854808579),
                         ((64u) 9223372036854808578),
                         ((64u) 9223372036854775936), ((64u) 32778),
                         ((64u) 9223372039002259466),
                         ((64u) 9223372039002292353),
                         ((64u) 9223372036854808704), ((64u) 2147483649),
                         ((64u) 9223372039002292232)};

inline
fn keccakf1600_index (inline int x, inline int y) -> (inline int)

{
  inline int r;
  r = ((x % 5) + (5 * (y % 5))); /* int:i */ 
  return (r);
}

inline
fn keccakf1600_rho_offsets (inline int i) -> (inline int)

{
  inline int r;
  r = 0; /* int:i */ 
  inline int x;
  x = 1; /* int:i */ 
  inline int y;
  y = 0; /* int:i */ 
  inline int t;
  for t = 0 to 24 {
    if (i == (x + (5 * y))) {
      r = ((((t + 1) * (t + 2)) / 2) % 64); /* int:i */ 
    }
    inline int z;
    z = (((2 * x) + (3 * y)) % 5); /* int:i */ 
    x = y; /* int:i */ 
    y = z; /* int:i */ 
  }
  return (r);
}

inline
fn keccakf1600_rhotates (inline int x, inline int y) -> (inline int)

{
  inline int i;
  #[inline]
  i = keccakf1600_index(x, y);
  inline int r;
  #[inline]
  r = keccakf1600_rho_offsets(i);
  return (r);
}

u256[6] KECCAK_RHOTATES_LEFT = {(4u64)[41, 36, 18, 3], (4u64)[27, 28, 62, 1],
                                (4u64)[39, 56, 6, 45], (4u64)[8, 55, 61, 10],
                                (4u64)[20, 25, 15, 2], (4u64)[14, 21, 43, 44]};

u256[6] KECCAK_RHOTATES_RIGHT = {(4u64)[(64 - 41), (64 - 36), (64 - 18),
                                 (64 - 3)],
                                 (4u64)[(64 - 27), (64 - 28), (64 - 62),
                                 (64 - 1)],
                                 (4u64)[(64 - 39), (64 - 56), (64 - 6),
                                 (64 - 45)],
                                 (4u64)[(64 - 8), (64 - 55), (64 - 61),
                                 (64 - 10)],
                                 (4u64)[(64 - 20), (64 - 25), (64 - 15),
                                 (64 - 2)],
                                 (4u64)[(64 - 14), (64 - 21), (64 - 43),
                                 (64 - 44)]};

inline
fn __keccakf1600_pround_avx2 (reg u256[7] state) -> (reg u256[7])

{
  reg u256 c00;
  c00 = #VPSHUFD_256(state[2], (4u2)[1, 0, 3, 2]); /*  */ 
  reg u256 c14;
  c14 = (state[5] ^256u state[3]); /* u256 */ 
  reg u256 t2;
  t2 = (state[4] ^256u state[6]); /* u256 */ 
  c14 = (c14 ^256u state[1]); /* u256 */ 
  c14 = (c14 ^256u t2); /* u256 */ 
  reg u256 t4;
  t4 = #VPERMQ(c14, (4u2)[2, 1, 0, 3]); /*  */ 
  c00 = (c00 ^256u state[2]); /* u256 */ 
  reg u256 t0;
  t0 = #VPERMQ(c00, (4u2)[1, 0, 3, 2]); /*  */ 
  reg u256 t1;
  t1 = (c14 >>4u64 ((128u) 63)); /* u256 */ 
  t2 = (c14 +4u64 c14); /* u256 */ 
  t1 = (t1 |256u t2); /* u256 */ 
  reg u256 d14;
  d14 = #VPERMQ(t1, (4u2)[0, 3, 2, 1]); /*  */ 
  reg u256 d00;
  d00 = (t1 ^256u t4); /* u256 */ 
  d00 = #VPERMQ(d00, (4u2)[0, 0, 0, 0]); /*  */ 
  c00 = (c00 ^256u state[0]); /* u256 */ 
  c00 = (c00 ^256u t0); /* u256 */ 
  t0 = (c00 >>4u64 ((128u) 63)); /* u256 */ 
  t1 = (c00 +4u64 c00); /* u256 */ 
  t1 = (t1 |256u t0); /* u256 */ 
  state[2] = (state[2] ^256u d00); /* u256 */ 
  state[0] = (state[0] ^256u d00); /* u256 */ 
  d14 = #VPBLEND_8u32(d14, t1, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  t4 = #VPBLEND_8u32(t4, c00, (8u1)[0, 0, 0, 0, 0, 0, 1, 1]); /*  */ 
  d14 = (d14 ^256u t4); /* u256 */ 
  reg u256 t3;
  t3 = #VPSLLV_4u64(state[2], /* global: */ KECCAK_RHOTATES_LEFT[0]); /*  */ 
  state[2] =
    #VPSRLV_4u64(state[2], /* global: */ KECCAK_RHOTATES_RIGHT[0]); /*  */ 
  state[2] = (state[2] |256u t3); /* u256 */ 
  state[3] = (state[3] ^256u d14); /* u256 */ 
  t4 = #VPSLLV_4u64(state[3], /* global: */ KECCAK_RHOTATES_LEFT[2]); /*  */ 
  state[3] =
    #VPSRLV_4u64(state[3], /* global: */ KECCAK_RHOTATES_RIGHT[2]); /*  */ 
  state[3] = (state[3] |256u t4); /* u256 */ 
  state[4] = (state[4] ^256u d14); /* u256 */ 
  reg u256 t5;
  t5 = #VPSLLV_4u64(state[4], /* global: */ KECCAK_RHOTATES_LEFT[3]); /*  */ 
  state[4] =
    #VPSRLV_4u64(state[4], /* global: */ KECCAK_RHOTATES_RIGHT[3]); /*  */ 
  state[4] = (state[4] |256u t5); /* u256 */ 
  state[5] = (state[5] ^256u d14); /* u256 */ 
  reg u256 t6;
  t6 = #VPSLLV_4u64(state[5], /* global: */ KECCAK_RHOTATES_LEFT[4]); /*  */ 
  state[5] =
    #VPSRLV_4u64(state[5], /* global: */ KECCAK_RHOTATES_RIGHT[4]); /*  */ 
  state[5] = (state[5] |256u t6); /* u256 */ 
  state[6] = (state[6] ^256u d14); /* u256 */ 
  t3 = #VPERMQ(state[2], (4u2)[2, 0, 3, 1]); /*  */ 
  t4 = #VPERMQ(state[3], (4u2)[2, 0, 3, 1]); /*  */ 
  reg u256 t7;
  t7 = #VPSLLV_4u64(state[6], /* global: */ KECCAK_RHOTATES_LEFT[5]); /*  */ 
  t1 =
    #VPSRLV_4u64(state[6], /* global: */ KECCAK_RHOTATES_RIGHT[5]); /*  */ 
  t1 = (t1 |256u t7); /* u256 */ 
  state[1] = (state[1] ^256u d14); /* u256 */ 
  t5 = #VPERMQ(state[4], (4u2)[0, 1, 2, 3]); /*  */ 
  t6 = #VPERMQ(state[5], (4u2)[1, 3, 0, 2]); /*  */ 
  reg u256 t8;
  t8 = #VPSLLV_4u64(state[1], /* global: */ KECCAK_RHOTATES_LEFT[1]); /*  */ 
  t2 =
    #VPSRLV_4u64(state[1], /* global: */ KECCAK_RHOTATES_RIGHT[1]); /*  */ 
  t2 = (t2 |256u t8); /* u256 */ 
  t7 = #VPSRLDQ_256(t1, ((8u) 8)); /*  */ 
  t0 = ((!256u t1) &256u t7); /* u256 */ 
  state[3] = #VPBLEND_8u32(t2, t6, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */ 
  t8 = #VPBLEND_8u32(t4, t2, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */ 
  state[5] = #VPBLEND_8u32(t3, t4, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */ 
  t7 = #VPBLEND_8u32(t2, t3, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */ 
  state[3] =
    #VPBLEND_8u32(state[3], t4, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */ 
  t8 = #VPBLEND_8u32(t8, t5, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */ 
  state[5] =
    #VPBLEND_8u32(state[5], t2, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */ 
  t7 = #VPBLEND_8u32(t7, t6, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */ 
  state[3] =
    #VPBLEND_8u32(state[3], t5, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  t8 = #VPBLEND_8u32(t8, t6, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  state[5] =
    #VPBLEND_8u32(state[5], t6, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  t7 = #VPBLEND_8u32(t7, t4, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  state[3] = ((!256u state[3]) &256u t8); /* u256 */ 
  state[5] = ((!256u state[5]) &256u t7); /* u256 */ 
  state[6] = #VPBLEND_8u32(t5, t2, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */ 
  t8 = #VPBLEND_8u32(t3, t5, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */ 
  state[3] = (state[3] ^256u t3); /* u256 */ 
  state[6] =
    #VPBLEND_8u32(state[6], t3, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */ 
  t8 = #VPBLEND_8u32(t8, t4, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */ 
  state[5] = (state[5] ^256u t5); /* u256 */ 
  state[6] =
    #VPBLEND_8u32(state[6], t4, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  t8 = #VPBLEND_8u32(t8, t2, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  state[6] = ((!256u state[6]) &256u t8); /* u256 */ 
  state[6] = (state[6] ^256u t6); /* u256 */ 
  state[4] = #VPERMQ(t1, (4u2)[0, 1, 3, 2]); /*  */ 
  t8 =
    #VPBLEND_8u32(state[4], state[0], (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */ 
  state[1] = #VPERMQ(t1, (4u2)[0, 3, 2, 1]); /*  */ 
  state[1] =
    #VPBLEND_8u32(state[1], state[0], (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  state[1] = ((!256u state[1]) &256u t8); /* u256 */ 
  state[2] = #VPBLEND_8u32(t4, t5, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */ 
  t7 = #VPBLEND_8u32(t6, t4, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */ 
  state[2] =
    #VPBLEND_8u32(state[2], t6, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */ 
  t7 = #VPBLEND_8u32(t7, t3, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */ 
  state[2] =
    #VPBLEND_8u32(state[2], t3, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  t7 = #VPBLEND_8u32(t7, t5, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  state[2] = ((!256u state[2]) &256u t7); /* u256 */ 
  state[2] = (state[2] ^256u t2); /* u256 */ 
  t0 = #VPERMQ(t0, (4u2)[0, 0, 0, 0]); /*  */ 
  state[3] = #VPERMQ(state[3], (4u2)[0, 1, 2, 3]); /*  */ 
  state[5] = #VPERMQ(state[5], (4u2)[2, 0, 3, 1]); /*  */ 
  state[6] = #VPERMQ(state[6], (4u2)[1, 3, 0, 2]); /*  */ 
  state[4] = #VPBLEND_8u32(t6, t3, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */ 
  t7 = #VPBLEND_8u32(t5, t6, (8u1)[0, 0, 0, 0, 1, 1, 0, 0]); /*  */ 
  state[4] =
    #VPBLEND_8u32(state[4], t5, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */ 
  t7 = #VPBLEND_8u32(t7, t2, (8u1)[0, 0, 1, 1, 0, 0, 0, 0]); /*  */ 
  state[4] =
    #VPBLEND_8u32(state[4], t2, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  t7 = #VPBLEND_8u32(t7, t3, (8u1)[1, 1, 0, 0, 0, 0, 0, 0]); /*  */ 
  state[4] = ((!256u state[4]) &256u t7); /* u256 */ 
  state[0] = (state[0] ^256u t0); /* u256 */ 
  state[1] = (state[1] ^256u t1); /* u256 */ 
  state[4] = (state[4] ^256u t4); /* u256 */ 
  return (state);
}

fn __keccakf1600_avx2 (reg u256[7] state) -> (reg u256[7])

{
  reg mut ptr u64[24] round_constants;
  round_constants = /* global: */ KECCAK1600_RC; /* u64[24] */ 
  #[Internal::wint::unsigned]
  reg ui64 r;
  r = ((64ui /* of int */) 0); /* u64 */ 
  while {
    #[inline]
    state = __keccakf1600_pround_avx2(state);
    reg u256 rc;
    rc =
      #VPBROADCAST_4u64(round_constants[((uint /* of ui64 */) r)]); /*  */ 
    state[0] = (state[0] ^256u rc); /* u256 */ 
    r = (r +64ui ((64ui /* of int */) 1)); /* u64 */ 
  } ((r <64ui ((64ui /* of int */) KECCAK_ROUNDS)))
  return (state);
}

fn _keccakf1600_avx2 (reg u256[7] state) -> (reg u256[7])

{
  state = __keccakf1600_avx2(state);
  return (state);
}

inline
fn __stavx2_pack (reg const ptr u64[25] st) -> (reg u256[7])

{
  reg u256[7] state;
  state[0] = #VPBROADCAST_4u64(st.[#unaligned (8 * 0)]); /*  */ 
  state[1] = st.[#unaligned :u256 (1 * 8)]; /* u256 */ 
  reg u128 t128_0;
  t128_0 = #VMOV_64(st[5]); /*  */ 
  state[3] = st.[#unaligned :u256 (6 * 8)]; /* u256 */ 
  reg u128 t128_1;
  t128_1 = #VMOV_64(st[10]); /*  */ 
  state[4] = st.[#unaligned :u256 (11 * 8)]; /* u256 */ 
  reg u64 r;
  r = st[15]; /* u64 */ 
  t128_0 = #VPINSR_2u64(t128_0, r, ((8u) 1)); /*  */ 
  state[5] = st.[#unaligned :u256 (16 * 8)]; /* u256 */ 
  r = st[20]; /* u64 */ 
  t128_1 = #VPINSR_2u64(t128_1, r, ((8u) 1)); /*  */ 
  state[2] =
    (2u128)[((uint /* of u128 */) t128_0), ((uint /* of u128 */) t128_1)]; /* u256 */ 
  state[6] = st.[#unaligned :u256 (21 * 8)]; /* u256 */ 
  reg u256 t256_0;
  t256_0 =
    #VPBLEND_8u32(state[3], state[5], (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  reg u256 t256_1;
  t256_1 =
    #VPBLEND_8u32(state[6], state[4], (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  reg u256 t256_2;
  t256_2 =
    #VPBLEND_8u32(state[4], state[3], (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  state[3] =
    #VPBLEND_8u32(t256_0, t256_1, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  state[4] =
    #VPBLEND_8u32(t256_1, t256_0, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  t256_0 =
    #VPBLEND_8u32(state[5], state[6], (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  state[5] =
    #VPBLEND_8u32(t256_0, t256_2, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  state[6] =
    #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  return (state);
}

inline
fn __stavx2_unpack (reg mut ptr u64[25] st, reg u256[7] state) -> (reg mut ptr u64[25])

{
  reg u128 t128_0;
  t128_0 = state[0]; /* u128 */ 
  st[0] = #VMOVLPD(t128_0); /*  */ 
  st.[#unaligned :u256 (1 * 8)] = state[1]; /* u256 */ 
  reg u256 t256_0;
  t256_0 =
    #VPBLEND_8u32(state[3], state[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  reg u256 t256_1;
  t256_1 =
    #VPBLEND_8u32(state[4], state[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  reg u256 t256_2;
  t256_2 =
    #VPBLEND_8u32(state[5], state[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  reg u256 t256_3;
  t256_3 =
    #VPBLEND_8u32(state[6], state[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  reg u128 t128_1;
  t128_1 = #VEXTRACTI128(state[2], ((8u) 1)); /*  */ 
  st[5] = #VMOVLPD(t128_1); /*  */ 
  reg u256 t256_4;
  t256_4 =
    #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  st.[#unaligned :u256 (6 * 8)] = t256_4; /* u256 */ 
  t128_0 = state[2]; /* u128 */ 
  st[10] = #VMOVLPD(t128_0); /*  */ 
  t256_4 =
    #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  st.[#unaligned :u256 (11 * 8)] = t256_4; /* u256 */ 
  st[15] = #VMOVHPD(t128_1); /*  */ 
  t256_4 =
    #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  st.[#unaligned :u256 (16 * 8)] = t256_4; /* u256 */ 
  st[20] = #VMOVHPD(t128_0); /*  */ 
  t256_4 =
    #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  st.[#unaligned :u256 (21 * 8)] = t256_4; /* u256 */ 
  return (st);
}

fn _keccakf1600_st25_avx2 (reg mut ptr u64[25] st25) -> (reg mut ptr u64[25])

{
  reg u256[7] state;
  #[inline]
  state = __stavx2_pack(st25);
  state = __keccakf1600_avx2(state);
  #[inline]
  st25 = __stavx2_unpack(st25, state);
  return (st25);
}

param int R72 = 72;

param int R104 = 104;

param int R136 = 136;

param int R144 = 144;

param int R168 = 168;

param int UNFINISHED = 0;

param int SHA3 = 6;

param int RAWSHAKE = 7;

param int SHAKE = 31;

inline
fn __SHLQ (reg u64 x, inline int shbytes) -> (reg u64)

{
  if (shbytes != 0) {
    x = (x <<64u ((8u) (8 * shbytes))); /* u64 */ 
  }
  return (x);
}

inline
fn __SHLDQ (reg u128 x, inline int shbytes) -> (reg u128)

{
  if (shbytes != 0) {
    x = #VPSLLDQ_128(x, ((8u) shbytes)); /*  */ 
  }
  return (x);
}

inline
fn __SHLQ_256 (reg u256 x, inline int shbytes) -> (reg u256)

{
  if (shbytes != 0) {
    x = #VPSLL_4u64(x, ((128u) (8 * shbytes))); /*  */ 
  }
  return (x);
}

inline
fn __m_ilen_read_upto8_at (#[Internal::wint::unsigned] reg ui64 buf,
                          inline int LEN, inline int TRAIL, inline int CUR,
                          inline int AT) -> (reg ui64, inline int,
                                            inline int, inline int, reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w = [:u64 ((64u /* of ui64 */) buf)]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      buf = (buf +64ui ((64ui /* of int */) (8 - AT8))); /* u64 */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w = ((64u) [:u32 ((64u /* of ui64 */) buf)]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        buf =
          (buf +64ui
          ((64ui /* of int */) (((4 + AT8) >= 8) ? (8 - AT8) : 4))); /* u64 */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 = ((64u) [:u16 ((64u /* of ui64 */) buf)]); /* u64 */ 
        buf =
          (buf +64ui
          ((64ui /* of int */) (((2 + AT8) >= 8) ? (8 - AT8) : 2))); /* u64 */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 = ((64u) [:u8 ((64u /* of ui64 */) buf)]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          buf = (buf +64ui ((64ui /* of int */) 1)); /* u64 */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (buf, LEN, TRAIL, AT, w);
}

inline
fn __m_ilen_read_upto16_at (#[Internal::wint::unsigned] reg ui64 buf,
                           inline int LEN, inline int TRAIL, inline int CUR,
                           inline int AT) -> (reg ui64, inline int,
                                             inline int, inline int,
                                             reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w = [:u128 ((64u /* of ui64 */) buf)]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      buf = (buf +64ui ((64ui /* of int */) (16 - AT16))); /* u64 */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (buf, LEN, TRAIL, AT16, t64_1) =
          __m_ilen_read_upto8_at(buf, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (buf, LEN, TRAIL, AT16, t64_0) =
          __m_ilen_read_upto8_at(buf, LEN, TRAIL, 0, AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (buf, LEN, TRAIL, AT16, t64_1) =
          __m_ilen_read_upto8_at(buf, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (buf, LEN, TRAIL, AT, w);
}

inline
fn __m_ilen_read_upto32_at (#[Internal::wint::unsigned] reg ui64 buf,
                           inline int LEN, inline int TRAIL, inline int CUR,
                           inline int AT) -> (reg ui64, inline int,
                                             inline int, inline int,
                                             reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w = [:u256 ((64u /* of ui64 */) buf)]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      buf = (buf +64ui ((64ui /* of int */) 32)); /* u64 */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (buf, LEN, TRAIL, AT32, t128_1) =
          __m_ilen_read_upto16_at(buf, LEN, TRAIL, 16, AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (buf, LEN, TRAIL, AT32, t128_0) =
          __m_ilen_read_upto16_at(buf, LEN, TRAIL, 0, AT32);
        #[inline]
        (buf, LEN, TRAIL, AT32, t128_1) =
          __m_ilen_read_upto16_at(buf, LEN, TRAIL, 16, AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (buf, LEN, TRAIL, AT, w);
}

inline
fn __m_ilen_read_bcast_upto8_at (#[Internal::wint::unsigned] reg ui64 buf,
                                inline int LEN, inline int TRAIL,
                                inline int CUR, inline int AT) -> (reg ui64,
                                                                  inline int,
                                                                  inline int,
                                                                  inline int,
                                                                  reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 = #VPBROADCAST_4u64([:u64 ((64u /* of ui64 */) buf)]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      buf = (buf +64ui ((64ui /* of int */) (8 - AT8))); /* u64 */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (buf, LEN, TRAIL, AT, w) =
        __m_ilen_read_upto8_at(buf, LEN, TRAIL, CUR, AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      buf = (buf +64ui ((64ui /* of int */) (8 - AT8))); /* u64 */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (buf, LEN, TRAIL, AT, w256);
}

inline
fn __m_ilen_write_upto8 (#[Internal::wint::unsigned] reg ui64 buf,
                        inline int LEN, reg u64 w) -> (reg ui64, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      [:u64 ((64u /* of ui64 */) buf)] = w; /* u64 */ 
      buf = (buf +64ui ((64ui /* of int */) 8)); /* u64 */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        [:u32 ((64u /* of ui64 */) buf)] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        buf = (buf +64ui ((64ui /* of int */) 4)); /* u64 */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        [:u16 ((64u /* of ui64 */) buf)] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        buf = (buf +64ui ((64ui /* of int */) 2)); /* u64 */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        [:u8 ((64u /* of ui64 */) buf)] = w; /* u8 */ 
        buf = (buf +64ui ((64ui /* of int */) 1)); /* u64 */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, LEN);
}

inline
fn __m_ilen_write_upto16 (#[Internal::wint::unsigned] reg ui64 buf,
                         inline int LEN, reg u128 w) -> (reg ui64,
                                                        inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      [:u128 ((64u /* of ui64 */) buf)] = w; /* u128 */ 
      buf = (buf +64ui ((64ui /* of int */) 16)); /* u64 */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        [:u64 ((64u /* of ui64 */) buf)] = #MOVV_64(w); /*  */ 
        buf = (buf +64ui ((64ui /* of int */) 8)); /* u64 */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, LEN) = __m_ilen_write_upto8(buf, LEN, t64);
    }
  }
  return (buf, LEN);
}

inline
fn __m_ilen_write_upto32 (#[Internal::wint::unsigned] reg ui64 buf,
                         inline int LEN, reg u256 w) -> (reg ui64,
                                                        inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      [:u256 ((64u /* of ui64 */) buf)] = w; /* u256 */ 
      buf = (buf +64ui ((64ui /* of int */) 32)); /* u64 */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        [:u128 ((64u /* of ui64 */) buf)] = t128; /* u128 */ 
        buf = (buf +64ui ((64ui /* of int */) 16)); /* u64 */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, LEN) = __m_ilen_write_upto16(buf, LEN, t128);
    }
  }
  return (buf, LEN);
}

inline
fn __m_rlen_read_upto8 (#[Internal::wint::unsigned] reg ui64 buf,
                       #[Internal::wint::unsigned] reg ui64 len) -> (reg ui64,
                                                                    reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = [:u64 ((64u /* of ui64 */) buf)]; /* u64 */ 
    buf = (buf +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) [:u32 ((64u /* of ui64 */) buf)]); /* u64 */ 
      buf = (buf +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) [:u16 ((64u /* of ui64 */) buf)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      buf = (buf +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) [:u8 ((64u /* of ui64 */) buf)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      buf = (buf +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, w);
}

inline
fn __m_rlen_write_upto8 (#[Internal::wint::unsigned] reg ui64 buf,
                        reg u64 data, #[Internal::wint::unsigned]
                        reg ui64 len) -> (reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    [:u64 ((64u /* of ui64 */) buf)] = data; /* u64 */ 
    buf = (buf +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      [:u32 ((64u /* of ui64 */) buf)] = data; /* u32 */ 
      buf = (buf +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      [:u16 ((64u /* of ui64 */) buf)] = data; /* u16 */ 
      buf = (buf +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      [:u8 ((64u /* of ui64 */) buf)] = data; /* u8 */ 
      buf = (buf +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf);
}

inline
fn __u64_to_u256 (reg u64 x, inline int L) -> (reg u256)

{
  reg u128 t128;
  if ((L % 2) == 0) {
    t128 = ((128u) x); /* u128 */ 
  } else {
    t128 = #set0_128(); /*  */ 
    t128 = #VPINSR_2u64(t128, x, ((8u) 1)); /*  */ 
  }
  reg u256 t256;
  t256 = #set0_256(); /*  */ 
  if ((L / 2) == 0) {
    t256 = #VINSERTI128(t256, t128, ((8u) 0)); /*  */ 
  } else {
    t256 = #VINSERTI128(t256, t128, ((8u) 1)); /*  */ 
  }
  return (t256);
}

inline
fn __state_init_avx2 () -> (reg u256[7])

{
  inline int i;
  reg u256[7] st;
  for i = 0 to 7 {
    st[i] = #set0_256(); /*  */ 
  }
  return (st);
}

inline
fn __perm_reg3456_avx2 (reg u256 r3, reg u256 r4, reg u256 r5, reg u256 r6) -> 
(reg u256, reg u256, reg u256, reg u256)

{
  reg u256 t256_0;
  t256_0 = #VPBLEND_8u32(r3, r5, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  reg u256 t256_1;
  t256_1 = #VPBLEND_8u32(r6, r4, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  reg u256 t256_2;
  t256_2 = #VPBLEND_8u32(r4, r3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  reg u256 st3;
  st3 = #VPBLEND_8u32(t256_0, t256_1, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  reg u256 st4;
  st4 = #VPBLEND_8u32(t256_1, t256_0, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  t256_0 = #VPBLEND_8u32(r5, r6, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  reg u256 st5;
  st5 = #VPBLEND_8u32(t256_0, t256_2, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  reg u256 st6;
  st6 = #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  return (st3, st4, st5, st6);
}

inline
fn __unperm_reg3456_avx2 (reg u256 st3, reg u256 st4, reg u256 st5,
                         reg u256 st6) -> (reg u256, reg u256, reg u256,
                                          reg u256)

{
  reg u256 t256_0;
  t256_0 = #VPBLEND_8u32(st3, st4, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  reg u256 t256_1;
  t256_1 = #VPBLEND_8u32(st4, st3, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  reg u256 t256_2;
  t256_2 = #VPBLEND_8u32(st5, st6, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  reg u256 t256_3;
  t256_3 = #VPBLEND_8u32(st6, st5, (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
  reg u256 r3;
  r3 = #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  reg u256 r4;
  r4 = #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  reg u256 r5;
  r5 = #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  reg u256 r6;
  r6 = #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
  return (r3, r4, r5, r6);
}

inline
fn __addstate_r3456_avx2 (reg u256[7] st, reg u256 r3, reg u256 r4,
                         reg u256 r5, reg u256 r6) -> (reg u256[7])

{
  #[inline]
  (r3, r4, r5, r6) = __perm_reg3456_avx2(r3, r4, r5, r6);
  st[3] = (st[3] ^256u r3); /* u256 */ 
  st[4] = (st[4] ^256u r4); /* u256 */ 
  st[5] = (st[5] ^256u r5); /* u256 */ 
  st[6] = (st[6] ^256u r6); /* u256 */ 
  return (st);
}

inline
fn __stavx2_pos_avx2 (inline int POS) -> (inline int, inline int)

{
  inline int R;
  R = 0; /* int:i */ 
  inline int L;
  L = 0; /* int:i */ 
  if (0 < POS) {
    if (POS <= 4) {
      R = 1; /* int:i */ 
      L = (POS - 1); /* int:i */ 
    } else {
      if (POS == 10) {
        R = 2; /* int:i */ 
        L = 0; /* int:i */ 
      } else {
        if (POS == 20) {
          R = 2; /* int:i */ 
          L = 1; /* int:i */ 
        } else {
          if (POS == 5) {
            R = 2; /* int:i */ 
            L = 2; /* int:i */ 
          } else {
            if (POS == 15) {
              R = 2; /* int:i */ 
              L = 3; /* int:i */ 
            } else {
              if (POS == 16) {
                R = 3; /* int:i */ 
                L = 0; /* int:i */ 
              } else {
                if (POS == 7) {
                  R = 3; /* int:i */ 
                  L = 1; /* int:i */ 
                } else {
                  if (POS == 23) {
                    R = 3; /* int:i */ 
                    L = 2; /* int:i */ 
                  } else {
                    if (POS == 14) {
                      R = 3; /* int:i */ 
                      L = 3; /* int:i */ 
                    } else {
                      if (POS == 11) {
                        R = 4; /* int:i */ 
                        L = 0; /* int:i */ 
                      } else {
                        if (POS == 22) {
                          R = 4; /* int:i */ 
                          L = 1; /* int:i */ 
                        } else {
                          if (POS == 8) {
                            R = 4; /* int:i */ 
                            L = 2; /* int:i */ 
                          } else {
                            if (POS == 19) {
                              R = 4; /* int:i */ 
                              L = 3; /* int:i */ 
                            } else {
                              if (POS == 21) {
                                R = 5; /* int:i */ 
                                L = 0; /* int:i */ 
                              } else {
                                if (POS == 17) {
                                  R = 5; /* int:i */ 
                                  L = 1; /* int:i */ 
                                } else {
                                  if (POS == 13) {
                                    R = 5; /* int:i */ 
                                    L = 2; /* int:i */ 
                                  } else {
                                    if (POS == 9) {
                                      R = 5; /* int:i */ 
                                      L = 3; /* int:i */ 
                                    } else {
                                      if (POS == 6) {
                                        R = 6; /* int:i */ 
                                        L = 0; /* int:i */ 
                                      } else {
                                        if (POS == 12) {
                                          R = 6; /* int:i */ 
                                          L = 1; /* int:i */ 
                                        } else {
                                          if (POS == 18) {
                                            R = 6; /* int:i */ 
                                            L = 2; /* int:i */ 
                                          } else {
                                            if (POS == 24) {
                                              R = 6; /* int:i */ 
                                              L = 3; /* int:i */ 
                                            }
                                          }
                                        }
                                      }
                                    }
                                  }
                                }
                              }
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
  return (R, L);
}

inline
fn __addratebit_avx2 (reg u256[7] st, inline int RATE_8) -> (reg u256[7])

{
  reg u64 t64;
  t64 = ((64u) 1); /* u64 */ 
  t64 = (t64 <<64u ((8u) (((8 * RATE_8) - 1) % 64))); /* u64 */ 
  inline int R;
  inline int L;
  #[inline]
  (R, L) = __stavx2_pos_avx2(((RATE_8 - 1) / 8));
  reg u256 t256;
  if (R == 0) {
    t256 = #VPBROADCAST_4u64(t64); /*  */ 
  } else {
    #[inline]
    t256 = __u64_to_u256(t64, L);
  }
  st[R] = (st[R] ^256u t256); /* u256 */ 
  return (st);
}

inline
fn __pstate_init_avx2 (reg mut ptr u64[25] pst) -> (reg mut ptr u64[25],
                                                   reg u256[7])

{
  reg u256 z256;
  z256 = #set0_256(); /*  */ 
  inline int i;
  for i = 0 to (25 / 4) {
    pst[:u256 i] = z256; /* u256 */ 
  }
  reg u64 z64;
  z64 = ((64u) 0); /* u64 */ 
  pst[24] = z64; /* u64 */ 
  reg u256[7] st;
  #[inline]
  st = __state_init_avx2();
  return (pst, st);
}

inline
fn __addpst01_avx2 (reg u256[7] st, reg const ptr u64[25] pst) -> (reg u256[7])

{
  reg u256 t256;
  t256 = #VPBROADCAST_4u64(pst.[#unaligned 0]); /*  */ 
  st[0] = (st[0] ^256u t256); /* u256 */ 
  t256 = pst.[#unaligned :u256 (8 * 1)]; /* u256 */ 
  st[1] = (st[1] ^256u t256); /* u256 */ 
  return (st);
}

inline
fn __addpst23456_avx2 (reg u256[7] st, reg const ptr u64[25] pst) -> 
(reg u256[7])

{
  reg u128 t128_0;
  t128_0 = #VMOV_64(pst.[#unaligned (5 * 8)]); /*  */ 
  reg u256 r3;
  r3 = pst.[#unaligned :u256 (6 * 8)]; /* u256 */ 
  reg u128 t128_1;
  t128_1 = #VMOV_64(pst.[#unaligned (10 * 8)]); /*  */ 
  reg u256 r4;
  r4 = pst.[#unaligned :u256 (11 * 8)]; /* u256 */ 
  reg u64 t;
  t = pst.[#unaligned (15 * 8)]; /* u64 */ 
  t128_0 = #VPINSR_2u64(t128_0, t, ((8u) 1)); /*  */ 
  reg u256 r5;
  r5 = pst.[#unaligned :u256 (16 * 8)]; /* u256 */ 
  t = pst.[#unaligned (20 * 8)]; /* u64 */ 
  t128_1 = #VPINSR_2u64(t128_1, t, ((8u) 1)); /*  */ 
  reg u256 r2;
  r2 =
    (2u128)[((uint /* of u128 */) t128_0), ((uint /* of u128 */) t128_1)]; /* u256 */ 
  st[2] = (st[2] ^256u r2); /* u256 */ 
  reg u256 r6;
  r6 = pst.[#unaligned :u256 (21 * 8)]; /* u256 */ 
  #[inline]
  st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  return (st);
}

fn _addpstate_avx2 (reg u256[7] st, reg const ptr u64[25] pst) -> (reg u256[7])

{
  #[inline]
  st = __addpst01_avx2(st, pst);
  #[inline]
  st = __addpst23456_avx2(st, pst);
  return (st);
}

inline
fn __state_from_pstate_avx2 (reg const ptr u64[25] pst) -> (reg u256[7])

{
  reg u256[7] st;
  st[0] = #VPBROADCAST_4u64(pst.[#unaligned 0]); /*  */ 
  st[1] = pst.[#unaligned :u256 8]; /* u256 */ 
  reg u128 t128_0;
  t128_0 = #VMOV_64(pst.[#unaligned (5 * 8)]); /*  */ 
  st[3] = pst.[#unaligned :u256 (6 * 8)]; /* u256 */ 
  reg u128 t128_1;
  t128_1 = #VMOV_64(pst.[#unaligned (10 * 8)]); /*  */ 
  st[4] = pst.[#unaligned :u256 (11 * 8)]; /* u256 */ 
  reg u64 t;
  t = pst.[#unaligned (15 * 8)]; /* u64 */ 
  t128_0 = #VPINSR_2u64(t128_0, t, ((8u) 1)); /*  */ 
  st[5] = pst.[#unaligned :u256 (16 * 8)]; /* u256 */ 
  t = pst.[#unaligned (20 * 8)]; /* u64 */ 
  t128_1 = #VPINSR_2u64(t128_1, t, ((8u) 1)); /*  */ 
  st[2] =
    (2u128)[((uint /* of u128 */) t128_0), ((uint /* of u128 */) t128_1)]; /* u256 */ 
  st[6] = pst.[#unaligned :u256 (21 * 8)]; /* u256 */ 
  #[inline]
  (st[3], st[4], st[5], st[6]) =
    __perm_reg3456_avx2(st[3], st[4], st[5], st[6]);
  return (st);
}

inline
fn __addstate_m_avx2 (reg u256[7] st, inline int AT,
                     #[Internal::wint::unsigned] reg ui64 buf,
                     inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                             inline int,
                                                             reg ui64)

{
  reg u64 t64_1;
  #[inline]
  (buf, _LEN, _TRAILB, AT, t64_1) =
    __m_ilen_read_upto8_at(buf, _LEN, _TRAILB, 0, AT);
  reg u128 t128_0;
  t128_0 = ((128u) t64_1); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (buf, _LEN, _TRAILB, AT, r1) =
    __m_ilen_read_upto32_at(buf, _LEN, _TRAILB, 8, AT);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  reg u64 t64_2;
  #[inline]
  (buf, _LEN, _TRAILB, AT, t64_2) =
    __m_ilen_read_upto8_at(buf, _LEN, _TRAILB, 40, AT);
  reg u128 t128_1;
  t128_1 = ((128u) t64_2); /* u128 */ 
  reg u128 t128_2;
  t128_2 = #set0_128(); /*  */ 
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u256 r3;
    #[inline]
    (buf, _LEN, _TRAILB, AT, r3) =
      __m_ilen_read_upto32_at(buf, _LEN, _TRAILB, 48, AT);
    reg u64 t64_3;
    #[inline]
    (buf, _LEN, _TRAILB, AT, t64_3) =
      __m_ilen_read_upto8_at(buf, _LEN, _TRAILB, 80, AT);
    t128_2 = ((128u) t64_3); /* u128 */ 
    reg u256 r4;
    #[inline]
    (buf, _LEN, _TRAILB, AT, r4) =
      __m_ilen_read_upto32_at(buf, _LEN, _TRAILB, 88, AT);
    reg u64 t64_4;
    #[inline]
    (buf, _LEN, _TRAILB, AT, t64_4) =
      __m_ilen_read_upto8_at(buf, _LEN, _TRAILB, 120, AT);
    t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (buf, _LEN, _TRAILB, AT, r5) =
      __m_ilen_read_upto32_at(buf, _LEN, _TRAILB, 128, AT);
    reg u64 t64_5;
    #[inline]
    (buf, _LEN, _TRAILB, AT, t64_5) =
      __m_ilen_read_upto8_at(buf, _LEN, _TRAILB, 160, AT);
    t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
    reg u256 r6;
    #[inline]
    (buf, _LEN, _TRAILB, AT, r6) =
      __m_ilen_read_upto32_at(buf, _LEN, _TRAILB, 168, AT);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  reg u256 r2;
  r2 =
    (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
  st[2] = (st[2] ^256u r2); /* u256 */ 
  return (st, AT, buf);
}

inline
fn __absorb_m_avx2 (reg u256[7] st, inline int AT,
                   #[Internal::wint::unsigned] reg ui64 buf, inline int _LEN,
                   inline int _TRAILB, inline int _RATE8) -> (reg u256[7],
                                                             inline int)

{
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, buf) =
      __addstate_m_avx2(st, AT, buf, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, buf) = __addstate_m_avx2(st, 0, buf, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) = __addstate_m_avx2(st, AT, buf, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn __dumpstate_m_avx2 (#[Internal::wint::unsigned] reg ui64 buf,
                      inline int _LEN, reg u256[7] st) -> (reg ui64)

{
  if (8 <= _LEN) {
    #[inline]
    (buf, _ /* int */) = __m_ilen_write_upto32(buf, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, _LEN) = __m_ilen_write_upto32(buf, _LEN, st[0]);
  }
  #[inline]
  (buf, _LEN) = __m_ilen_write_upto32(buf, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, _LEN) = __m_ilen_write_upto8(buf, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, _LEN) = __m_ilen_write_upto32(buf, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, _LEN) = __m_ilen_write_upto8(buf, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, _LEN) = __m_ilen_write_upto32(buf, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, _LEN) = __m_ilen_write_upto8(buf, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, _LEN) = __m_ilen_write_upto32(buf, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, _LEN) = __m_ilen_write_upto8(buf, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, _LEN) = __m_ilen_write_upto32(buf, _LEN, t256_4);
      }
    }
  }
  return (buf);
}

inline
fn __squeeze_m_avx2 (reg u256[7] st, #[Internal::wint::unsigned]
                    reg ui64 buf, inline int _LEN, inline int _RATE8) -> 
(reg u256[7])

{
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    buf = __dumpstate_m_avx2(buf, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    buf = __dumpstate_m_avx2(buf, LO, st);
  }
  return (st);
}

fn _keccakf1600_4x_pround (reg mut ptr u256[25] e, reg const ptr u256[25] a,
                          reg u256 r8, reg u256 r56) -> (reg mut ptr u256[25])

{
  reg u256[5] c_571;
  c_571[0] = a[0]; /* u256 */ 
  c_571[1] = a[1]; /* u256 */ 
  c_571[2] = a[2]; /* u256 */ 
  c_571[3] = a[3]; /* u256 */ 
  c_571[4] = a[4]; /* u256 */ 
  c_571[0] = (c_571[0] ^256u a[5]); /* u256 */ 
  c_571[1] = (c_571[1] ^256u a[6]); /* u256 */ 
  c_571[2] = (c_571[2] ^256u a[7]); /* u256 */ 
  c_571[3] = (c_571[3] ^256u a[8]); /* u256 */ 
  c_571[4] = (c_571[4] ^256u a[9]); /* u256 */ 
  c_571[0] = (c_571[0] ^256u a[10]); /* u256 */ 
  c_571[1] = (c_571[1] ^256u a[11]); /* u256 */ 
  c_571[2] = (c_571[2] ^256u a[12]); /* u256 */ 
  c_571[3] = (c_571[3] ^256u a[13]); /* u256 */ 
  c_571[4] = (c_571[4] ^256u a[14]); /* u256 */ 
  c_571[0] = (c_571[0] ^256u a[15]); /* u256 */ 
  c_571[1] = (c_571[1] ^256u a[16]); /* u256 */ 
  c_571[2] = (c_571[2] ^256u a[17]); /* u256 */ 
  c_571[3] = (c_571[3] ^256u a[18]); /* u256 */ 
  c_571[4] = (c_571[4] ^256u a[19]); /* u256 */ 
  c_571[0] = (c_571[0] ^256u a[20]); /* u256 */ 
  c_571[1] = (c_571[1] ^256u a[21]); /* u256 */ 
  c_571[2] = (c_571[2] ^256u a[22]); /* u256 */ 
  c_571[3] = (c_571[3] ^256u a[23]); /* u256 */ 
  c_571[4] = (c_571[4] ^256u a[24]); /* u256 */ 
  reg u256[5] d_619;
  d_619[0] = c_571[1]; /* u256 */ 
  reg u256 t_574;
  t_574 = #VPSLL_4u64(d_619[0], ((128u) 1)); /*  */ 
  d_619[0] = #VPSRL_4u64(d_619[0], ((128u) 63)); /*  */ 
  d_619[0] = (d_619[0] |256u t_574); /* u256 */ 
  d_619[0] = (d_619[0] ^256u c_571[4]); /* u256 */ 
  d_619[1] = c_571[2]; /* u256 */ 
  reg u256 t_577;
  t_577 = #VPSLL_4u64(d_619[1], ((128u) 1)); /*  */ 
  d_619[1] = #VPSRL_4u64(d_619[1], ((128u) 63)); /*  */ 
  d_619[1] = (d_619[1] |256u t_577); /* u256 */ 
  d_619[1] = (d_619[1] ^256u c_571[0]); /* u256 */ 
  d_619[2] = c_571[3]; /* u256 */ 
  reg u256 t_580;
  t_580 = #VPSLL_4u64(d_619[2], ((128u) 1)); /*  */ 
  d_619[2] = #VPSRL_4u64(d_619[2], ((128u) 63)); /*  */ 
  d_619[2] = (d_619[2] |256u t_580); /* u256 */ 
  d_619[2] = (d_619[2] ^256u c_571[1]); /* u256 */ 
  d_619[3] = c_571[4]; /* u256 */ 
  reg u256 t_583;
  t_583 = #VPSLL_4u64(d_619[3], ((128u) 1)); /*  */ 
  d_619[3] = #VPSRL_4u64(d_619[3], ((128u) 63)); /*  */ 
  d_619[3] = (d_619[3] |256u t_583); /* u256 */ 
  d_619[3] = (d_619[3] ^256u c_571[2]); /* u256 */ 
  d_619[4] = c_571[0]; /* u256 */ 
  reg u256 t_586;
  t_586 = #VPSLL_4u64(d_619[4], ((128u) 1)); /*  */ 
  d_619[4] = #VPSRL_4u64(d_619[4], ((128u) 63)); /*  */ 
  d_619[4] = (d_619[4] |256u t_586); /* u256 */ 
  d_619[4] = (d_619[4] ^256u c_571[3]); /* u256 */ 
  reg u256[5] b_606;
  b_606[0] = a[0]; /* u256 */ 
  b_606[0] = (b_606[0] ^256u d_619[0]); /* u256 */ 
  b_606[1] = a[6]; /* u256 */ 
  b_606[1] = (b_606[1] ^256u d_619[1]); /* u256 */ 
  reg u256 t_593;
  t_593 = #VPSLL_4u64(b_606[1], ((128u) 44)); /*  */ 
  b_606[1] = #VPSRL_4u64(b_606[1], ((128u) 20)); /*  */ 
  b_606[1] = (b_606[1] |256u t_593); /* u256 */ 
  b_606[2] = a[12]; /* u256 */ 
  b_606[2] = (b_606[2] ^256u d_619[2]); /* u256 */ 
  reg u256 t_596;
  t_596 = #VPSLL_4u64(b_606[2], ((128u) 43)); /*  */ 
  b_606[2] = #VPSRL_4u64(b_606[2], ((128u) 21)); /*  */ 
  b_606[2] = (b_606[2] |256u t_596); /* u256 */ 
  b_606[3] = a[18]; /* u256 */ 
  b_606[3] = (b_606[3] ^256u d_619[3]); /* u256 */ 
  reg u256 t_599;
  t_599 = #VPSLL_4u64(b_606[3], ((128u) 21)); /*  */ 
  b_606[3] = #VPSRL_4u64(b_606[3], ((128u) 43)); /*  */ 
  b_606[3] = (b_606[3] |256u t_599); /* u256 */ 
  b_606[4] = a[24]; /* u256 */ 
  b_606[4] = (b_606[4] ^256u d_619[4]); /* u256 */ 
  reg u256 t_602;
  t_602 = #VPSLL_4u64(b_606[4], ((128u) 14)); /*  */ 
  b_606[4] = #VPSRL_4u64(b_606[4], ((128u) 50)); /*  */ 
  b_606[4] = (b_606[4] |256u t_602); /* u256 */ 
  reg u256 t_607;
  t_607 = #VPANDN_256(b_606[1], b_606[2]); /*  */ 
  reg u256 t_608;
  t_608 = (t_607 ^256u b_606[0]); /* u256 */ 
  e[0] = t_608; /* u256 */ 
  reg u256 t_609;
  t_609 = #VPANDN_256(b_606[2], b_606[3]); /*  */ 
  reg u256 t_610;
  t_610 = (t_609 ^256u b_606[1]); /* u256 */ 
  e[1] = t_610; /* u256 */ 
  reg u256 t_611;
  t_611 = #VPANDN_256(b_606[3], b_606[4]); /*  */ 
  reg u256 t_612;
  t_612 = (t_611 ^256u b_606[2]); /* u256 */ 
  e[2] = t_612; /* u256 */ 
  reg u256 t_613;
  t_613 = #VPANDN_256(b_606[4], b_606[0]); /*  */ 
  reg u256 t_614;
  t_614 = (t_613 ^256u b_606[3]); /* u256 */ 
  e[3] = t_614; /* u256 */ 
  reg u256 t_615;
  t_615 = #VPANDN_256(b_606[0], b_606[1]); /*  */ 
  reg u256 t_616;
  t_616 = (t_615 ^256u b_606[4]); /* u256 */ 
  e[4] = t_616; /* u256 */ 
  reg u256[5] b_638;
  b_638[0] = a[3]; /* u256 */ 
  b_638[0] = (b_638[0] ^256u d_619[3]); /* u256 */ 
  reg u256 t_622;
  t_622 = #VPSLL_4u64(b_638[0], ((128u) 28)); /*  */ 
  b_638[0] = #VPSRL_4u64(b_638[0], ((128u) 36)); /*  */ 
  b_638[0] = (b_638[0] |256u t_622); /* u256 */ 
  b_638[1] = a[9]; /* u256 */ 
  b_638[1] = (b_638[1] ^256u d_619[4]); /* u256 */ 
  reg u256 t_625;
  t_625 = #VPSLL_4u64(b_638[1], ((128u) 20)); /*  */ 
  b_638[1] = #VPSRL_4u64(b_638[1], ((128u) 44)); /*  */ 
  b_638[1] = (b_638[1] |256u t_625); /* u256 */ 
  b_638[2] = a[10]; /* u256 */ 
  b_638[2] = (b_638[2] ^256u d_619[0]); /* u256 */ 
  reg u256 t_628;
  t_628 = #VPSLL_4u64(b_638[2], ((128u) 3)); /*  */ 
  b_638[2] = #VPSRL_4u64(b_638[2], ((128u) 61)); /*  */ 
  b_638[2] = (b_638[2] |256u t_628); /* u256 */ 
  b_638[3] = a[16]; /* u256 */ 
  b_638[3] = (b_638[3] ^256u d_619[1]); /* u256 */ 
  reg u256 t_631;
  t_631 = #VPSLL_4u64(b_638[3], ((128u) 45)); /*  */ 
  b_638[3] = #VPSRL_4u64(b_638[3], ((128u) 19)); /*  */ 
  b_638[3] = (b_638[3] |256u t_631); /* u256 */ 
  b_638[4] = a[22]; /* u256 */ 
  b_638[4] = (b_638[4] ^256u d_619[2]); /* u256 */ 
  reg u256 t_634;
  t_634 = #VPSLL_4u64(b_638[4], ((128u) 61)); /*  */ 
  b_638[4] = #VPSRL_4u64(b_638[4], ((128u) 3)); /*  */ 
  b_638[4] = (b_638[4] |256u t_634); /* u256 */ 
  reg u256 t_639;
  t_639 = #VPANDN_256(b_638[1], b_638[2]); /*  */ 
  reg u256 t_640;
  t_640 = (t_639 ^256u b_638[0]); /* u256 */ 
  e[5] = t_640; /* u256 */ 
  reg u256 t_641;
  t_641 = #VPANDN_256(b_638[2], b_638[3]); /*  */ 
  reg u256 t_642;
  t_642 = (t_641 ^256u b_638[1]); /* u256 */ 
  e[6] = t_642; /* u256 */ 
  reg u256 t_643;
  t_643 = #VPANDN_256(b_638[3], b_638[4]); /*  */ 
  reg u256 t_644;
  t_644 = (t_643 ^256u b_638[2]); /* u256 */ 
  e[7] = t_644; /* u256 */ 
  reg u256 t_645;
  t_645 = #VPANDN_256(b_638[4], b_638[0]); /*  */ 
  reg u256 t_646;
  t_646 = (t_645 ^256u b_638[3]); /* u256 */ 
  e[8] = t_646; /* u256 */ 
  reg u256 t_647;
  t_647 = #VPANDN_256(b_638[0], b_638[1]); /*  */ 
  reg u256 t_648;
  t_648 = (t_647 ^256u b_638[4]); /* u256 */ 
  e[9] = t_648; /* u256 */ 
  reg u256[5] b_671;
  b_671[0] = a[1]; /* u256 */ 
  b_671[0] = (b_671[0] ^256u d_619[1]); /* u256 */ 
  reg u256 t_655;
  t_655 = #VPSLL_4u64(b_671[0], ((128u) 1)); /*  */ 
  b_671[0] = #VPSRL_4u64(b_671[0], ((128u) 63)); /*  */ 
  b_671[0] = (b_671[0] |256u t_655); /* u256 */ 
  b_671[1] = a[7]; /* u256 */ 
  b_671[1] = (b_671[1] ^256u d_619[2]); /* u256 */ 
  reg u256 t_658;
  t_658 = #VPSLL_4u64(b_671[1], ((128u) 6)); /*  */ 
  b_671[1] = #VPSRL_4u64(b_671[1], ((128u) 58)); /*  */ 
  b_671[1] = (b_671[1] |256u t_658); /* u256 */ 
  b_671[2] = a[13]; /* u256 */ 
  b_671[2] = (b_671[2] ^256u d_619[3]); /* u256 */ 
  reg u256 t_661;
  t_661 = #VPSLL_4u64(b_671[2], ((128u) 25)); /*  */ 
  b_671[2] = #VPSRL_4u64(b_671[2], ((128u) 39)); /*  */ 
  b_671[2] = (b_671[2] |256u t_661); /* u256 */ 
  b_671[3] = a[19]; /* u256 */ 
  b_671[3] = (b_671[3] ^256u d_619[4]); /* u256 */ 
  b_671[3] = #VPSHUFB_256(b_671[3], r8); /*  */ 
  b_671[4] = a[20]; /* u256 */ 
  b_671[4] = (b_671[4] ^256u d_619[0]); /* u256 */ 
  reg u256 t_667;
  t_667 = #VPSLL_4u64(b_671[4], ((128u) 18)); /*  */ 
  b_671[4] = #VPSRL_4u64(b_671[4], ((128u) 46)); /*  */ 
  b_671[4] = (b_671[4] |256u t_667); /* u256 */ 
  reg u256 t_672;
  t_672 = #VPANDN_256(b_671[1], b_671[2]); /*  */ 
  reg u256 t_673;
  t_673 = (t_672 ^256u b_671[0]); /* u256 */ 
  e[10] = t_673; /* u256 */ 
  reg u256 t_674;
  t_674 = #VPANDN_256(b_671[2], b_671[3]); /*  */ 
  reg u256 t_675;
  t_675 = (t_674 ^256u b_671[1]); /* u256 */ 
  e[11] = t_675; /* u256 */ 
  reg u256 t_676;
  t_676 = #VPANDN_256(b_671[3], b_671[4]); /*  */ 
  reg u256 t_677;
  t_677 = (t_676 ^256u b_671[2]); /* u256 */ 
  e[12] = t_677; /* u256 */ 
  reg u256 t_678;
  t_678 = #VPANDN_256(b_671[4], b_671[0]); /*  */ 
  reg u256 t_679;
  t_679 = (t_678 ^256u b_671[3]); /* u256 */ 
  e[13] = t_679; /* u256 */ 
  reg u256 t_680;
  t_680 = #VPANDN_256(b_671[0], b_671[1]); /*  */ 
  reg u256 t_681;
  t_681 = (t_680 ^256u b_671[4]); /* u256 */ 
  e[14] = t_681; /* u256 */ 
  reg u256[5] b_704;
  b_704[0] = a[4]; /* u256 */ 
  b_704[0] = (b_704[0] ^256u d_619[4]); /* u256 */ 
  reg u256 t_688;
  t_688 = #VPSLL_4u64(b_704[0], ((128u) 27)); /*  */ 
  b_704[0] = #VPSRL_4u64(b_704[0], ((128u) 37)); /*  */ 
  b_704[0] = (b_704[0] |256u t_688); /* u256 */ 
  b_704[1] = a[5]; /* u256 */ 
  b_704[1] = (b_704[1] ^256u d_619[0]); /* u256 */ 
  reg u256 t_691;
  t_691 = #VPSLL_4u64(b_704[1], ((128u) 36)); /*  */ 
  b_704[1] = #VPSRL_4u64(b_704[1], ((128u) 28)); /*  */ 
  b_704[1] = (b_704[1] |256u t_691); /* u256 */ 
  b_704[2] = a[11]; /* u256 */ 
  b_704[2] = (b_704[2] ^256u d_619[1]); /* u256 */ 
  reg u256 t_694;
  t_694 = #VPSLL_4u64(b_704[2], ((128u) 10)); /*  */ 
  b_704[2] = #VPSRL_4u64(b_704[2], ((128u) 54)); /*  */ 
  b_704[2] = (b_704[2] |256u t_694); /* u256 */ 
  b_704[3] = a[17]; /* u256 */ 
  b_704[3] = (b_704[3] ^256u d_619[2]); /* u256 */ 
  reg u256 t_697;
  t_697 = #VPSLL_4u64(b_704[3], ((128u) 15)); /*  */ 
  b_704[3] = #VPSRL_4u64(b_704[3], ((128u) 49)); /*  */ 
  b_704[3] = (b_704[3] |256u t_697); /* u256 */ 
  b_704[4] = a[23]; /* u256 */ 
  b_704[4] = (b_704[4] ^256u d_619[3]); /* u256 */ 
  b_704[4] = #VPSHUFB_256(b_704[4], r56); /*  */ 
  reg u256 t_705;
  t_705 = #VPANDN_256(b_704[1], b_704[2]); /*  */ 
  reg u256 t_706;
  t_706 = (t_705 ^256u b_704[0]); /* u256 */ 
  e[15] = t_706; /* u256 */ 
  reg u256 t_707;
  t_707 = #VPANDN_256(b_704[2], b_704[3]); /*  */ 
  reg u256 t_708;
  t_708 = (t_707 ^256u b_704[1]); /* u256 */ 
  e[16] = t_708; /* u256 */ 
  reg u256 t_709;
  t_709 = #VPANDN_256(b_704[3], b_704[4]); /*  */ 
  reg u256 t_710;
  t_710 = (t_709 ^256u b_704[2]); /* u256 */ 
  e[17] = t_710; /* u256 */ 
  reg u256 t_711;
  t_711 = #VPANDN_256(b_704[4], b_704[0]); /*  */ 
  reg u256 t_712;
  t_712 = (t_711 ^256u b_704[3]); /* u256 */ 
  e[18] = t_712; /* u256 */ 
  reg u256 t_713;
  t_713 = #VPANDN_256(b_704[0], b_704[1]); /*  */ 
  reg u256 t_714;
  t_714 = (t_713 ^256u b_704[4]); /* u256 */ 
  e[19] = t_714; /* u256 */ 
  reg u256[5] b_736;
  b_736[0] = a[2]; /* u256 */ 
  b_736[0] = (b_736[0] ^256u d_619[2]); /* u256 */ 
  reg u256 t_720;
  t_720 = #VPSLL_4u64(b_736[0], ((128u) 62)); /*  */ 
  b_736[0] = #VPSRL_4u64(b_736[0], ((128u) 2)); /*  */ 
  b_736[0] = (b_736[0] |256u t_720); /* u256 */ 
  b_736[1] = a[8]; /* u256 */ 
  b_736[1] = (b_736[1] ^256u d_619[3]); /* u256 */ 
  reg u256 t_723;
  t_723 = #VPSLL_4u64(b_736[1], ((128u) 55)); /*  */ 
  b_736[1] = #VPSRL_4u64(b_736[1], ((128u) 9)); /*  */ 
  b_736[1] = (b_736[1] |256u t_723); /* u256 */ 
  b_736[2] = a[14]; /* u256 */ 
  b_736[2] = (b_736[2] ^256u d_619[4]); /* u256 */ 
  reg u256 t_726;
  t_726 = #VPSLL_4u64(b_736[2], ((128u) 39)); /*  */ 
  b_736[2] = #VPSRL_4u64(b_736[2], ((128u) 25)); /*  */ 
  b_736[2] = (b_736[2] |256u t_726); /* u256 */ 
  b_736[3] = a[15]; /* u256 */ 
  b_736[3] = (b_736[3] ^256u d_619[0]); /* u256 */ 
  reg u256 t_729;
  t_729 = #VPSLL_4u64(b_736[3], ((128u) 41)); /*  */ 
  b_736[3] = #VPSRL_4u64(b_736[3], ((128u) 23)); /*  */ 
  b_736[3] = (b_736[3] |256u t_729); /* u256 */ 
  b_736[4] = a[21]; /* u256 */ 
  b_736[4] = (b_736[4] ^256u d_619[1]); /* u256 */ 
  reg u256 t_732;
  t_732 = #VPSLL_4u64(b_736[4], ((128u) 2)); /*  */ 
  b_736[4] = #VPSRL_4u64(b_736[4], ((128u) 62)); /*  */ 
  b_736[4] = (b_736[4] |256u t_732); /* u256 */ 
  reg u256 t_737;
  t_737 = #VPANDN_256(b_736[1], b_736[2]); /*  */ 
  reg u256 t_738;
  t_738 = (t_737 ^256u b_736[0]); /* u256 */ 
  e[20] = t_738; /* u256 */ 
  reg u256 t_739;
  t_739 = #VPANDN_256(b_736[2], b_736[3]); /*  */ 
  reg u256 t_740;
  t_740 = (t_739 ^256u b_736[1]); /* u256 */ 
  e[21] = t_740; /* u256 */ 
  reg u256 t_741;
  t_741 = #VPANDN_256(b_736[3], b_736[4]); /*  */ 
  reg u256 t_742;
  t_742 = (t_741 ^256u b_736[2]); /* u256 */ 
  e[22] = t_742; /* u256 */ 
  reg u256 t_743;
  t_743 = #VPANDN_256(b_736[4], b_736[0]); /*  */ 
  reg u256 t_744;
  t_744 = (t_743 ^256u b_736[3]); /* u256 */ 
  e[23] = t_744; /* u256 */ 
  reg u256 t_745;
  t_745 = #VPANDN_256(b_736[0], b_736[1]); /*  */ 
  reg u256 t_746;
  t_746 = (t_745 ^256u b_736[4]); /* u256 */ 
  e[24] = t_746; /* u256 */ 
  return (e);
}

u256 ROL56 = ((256u)
             10910488462195273559651782724632284871561478246514020268633800075540923875841);

u256 ROL8 = ((256u)
            13620818001941277694121380808605999856886653716761013959207994299728839901191);

inline
fn __keccakf1600_avx2x4 (reg mut ptr u256[25] a) -> (reg mut ptr u256[25])

{
  reg mut ptr u64[24] RC;
  RC = /* global: */ KECCAK1600_RC; /* u64[24] */ 
  stack u256[25] s_e;
  reg mut ptr u256[25] e;
  e = s_e; /* u256[25] */ 
  reg u256 r8;
  r8 = /* global: */ ROL8; /* u256 */ 
  reg u256 r56;
  r56 = /* global: */ ROL56; /* u256 */ 
  #[Internal::wint::unsigned]
  reg ui64 c;
  c = ((64ui /* of int */) 0); /* u64 */ 
  while ((c <64ui ((64ui /* of int */) KECCAK_ROUNDS))) {
    reg u256 rc;
    rc = #VPBROADCAST_4u64(RC[((uint /* of ui64 */) c)]); /*  */ 
    e = _keccakf1600_4x_pround(e, a, r8, r56);
    reg u256 t;
    t = (rc ^256u e[0]); /* u256 */ 
    e[0] = t; /* u256 */ 
    (a, e) = #swap(e, a); /*  */ 
    rc =
      #VPBROADCAST_4u64(RC[((uint /* of ui64 */)
                           (c +64ui ((64ui /* of int */) 1)))]); /*  */ 
    a = _keccakf1600_4x_pround(a, e, r8, r56);
    t = (rc ^256u a[0]); /* u256 */ 
    a[0] = t; /* u256 */ 
    (a, e) = #swap(e, a); /*  */ 
    c = (c +64ui ((64ui /* of int */) 2)); /* u64 */ 
  }
  return (a);
}

fn _keccakf1600_avx2x4 (reg mut ptr u256[25] a) -> (reg mut ptr u256[25])

{
  #[inline]
  a = __keccakf1600_avx2x4(a);
  return (a);
}

inline
fn _keccakf1600_avx2x4_ (reg mut ptr u256[25] a) -> (reg mut ptr u256[25])

{
  a = a; /* u256[25] */ 
  a = _keccakf1600_avx2x4(a);
  a = a; /* u256[25] */ 
  return (a);
}

inline
fn __u256x4_4u64x4 (reg u256 x0, reg u256 x1, reg u256 x2, reg u256 x3) -> 
(reg u256, reg u256, reg u256, reg u256)

{
  reg u256 y0;
  y0 = #VPUNPCKL_4u64(x0, x1); /*  */ 
  reg u256 y1;
  y1 = #VPUNPCKH_4u64(x0, x1); /*  */ 
  reg u256 y2;
  y2 = #VPUNPCKL_4u64(x2, x3); /*  */ 
  reg u256 y3;
  y3 = #VPUNPCKH_4u64(x2, x3); /*  */ 
  x0 = #VPERM2I128(y0, y2, ((8u) 32)); /*  */ 
  x1 = #VPERM2I128(y1, y3, ((8u) 32)); /*  */ 
  x2 = #VPERM2I128(y0, y2, ((8u) 49)); /*  */ 
  x3 = #VPERM2I128(y1, y3, ((8u) 49)); /*  */ 
  return (x0, x1, x2, x3);
}

inline
fn __st4x_pack (reg mut ptr u256[25] st4x, reg const ptr u64[25] st0,
               reg const ptr u64[25] st1, reg const ptr u64[25] st2,
               reg const ptr u64[25] st3) -> (reg mut ptr u256[25])

{
  inline int i;
  for i = 0 to 6 {
    reg u256 x0;
    x0 = st0[:u256 i]; /* u256 */ 
    reg u256 x1;
    x1 = st1[:u256 i]; /* u256 */ 
    reg u256 x2;
    x2 = st2[:u256 i]; /* u256 */ 
    reg u256 x3;
    x3 = st3[:u256 i]; /* u256 */ 
    #[inline]
    (x0, x1, x2, x3) = __u256x4_4u64x4(x0, x1, x2, x3);
    st4x[((4 * i) + 0)] = x0; /* u256 */ 
    st4x[((4 * i) + 1)] = x1; /* u256 */ 
    st4x[((4 * i) + 2)] = x2; /* u256 */ 
    st4x[((4 * i) + 3)] = x3; /* u256 */ 
  }
  reg u64 t0;
  t0 = st0[24]; /* u64 */ 
  reg u64 t1;
  t1 = st1[24]; /* u64 */ 
  reg u64 t2;
  t2 = st2[24]; /* u64 */ 
  reg u64 t3;
  t3 = st3[24]; /* u64 */ 
  st4x[:u64 ((4 * 24) + 0)] = t0; /* u64 */ 
  st4x[:u64 ((4 * 24) + 1)] = t1; /* u64 */ 
  st4x[:u64 ((4 * 24) + 2)] = t2; /* u64 */ 
  st4x[:u64 ((4 * 24) + 3)] = t3; /* u64 */ 
  return (st4x);
}

inline
fn __4u64x4_u256x4 (reg u256 y0, reg u256 y1, reg u256 y2, reg u256 y3) -> 
(reg u256, reg u256, reg u256, reg u256)

{
  reg u256 x0;
  x0 = #VPERM2I128(y0, y2, ((8u) 32)); /*  */ 
  reg u256 x1;
  x1 = #VPERM2I128(y1, y3, ((8u) 32)); /*  */ 
  reg u256 x2;
  x2 = #VPERM2I128(y0, y2, ((8u) 49)); /*  */ 
  reg u256 x3;
  x3 = #VPERM2I128(y1, y3, ((8u) 49)); /*  */ 
  y0 = #VPUNPCKL_4u64(x0, x1); /*  */ 
  y1 = #VPUNPCKH_4u64(x0, x1); /*  */ 
  y2 = #VPUNPCKL_4u64(x2, x3); /*  */ 
  y3 = #VPUNPCKH_4u64(x2, x3); /*  */ 
  return (y0, y1, y2, y3);
}

inline
fn __st4x_unpack (reg mut ptr u64[25] st0, reg mut ptr u64[25] st1,
                 reg mut ptr u64[25] st2, reg mut ptr u64[25] st3,
                 reg const ptr u256[25] st4x) -> (reg mut ptr u64[25],
                                                 reg mut ptr u64[25],
                                                 reg mut ptr u64[25],
                                                 reg mut ptr u64[25])

{
  inline int i;
  for i = 0 to 6 {
    reg u256 x0;
    x0 = st4x[((4 * i) + 0)]; /* u256 */ 
    reg u256 x1;
    x1 = st4x[((4 * i) + 1)]; /* u256 */ 
    reg u256 x2;
    x2 = st4x[((4 * i) + 2)]; /* u256 */ 
    reg u256 x3;
    x3 = st4x[((4 * i) + 3)]; /* u256 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    st0.[#unaligned :u256 ((4 * 8) * i)] = x0; /* u256 */ 
    st1.[#unaligned :u256 ((4 * 8) * i)] = x1; /* u256 */ 
    st2.[#unaligned :u256 ((4 * 8) * i)] = x2; /* u256 */ 
    st3.[#unaligned :u256 ((4 * 8) * i)] = x3; /* u256 */ 
  }
  reg u64 t0;
  t0 = st4x[:u64 ((4 * 24) + 0)]; /* u64 */ 
  reg u64 t1;
  t1 = st4x[:u64 ((4 * 24) + 1)]; /* u64 */ 
  reg u64 t2;
  t2 = st4x[:u64 ((4 * 24) + 2)]; /* u64 */ 
  reg u64 t3;
  t3 = st4x[:u64 ((4 * 24) + 3)]; /* u64 */ 
  st0[24] = t0; /* u64 */ 
  st1[24] = t1; /* u64 */ 
  st2[24] = t2; /* u64 */ 
  st3[24] = t3; /* u64 */ 
  return (st0, st1, st2, st3);
}

inline
fn __keccakf1600_pround_unpacked (reg mut ptr u64[25] st0,
                                 reg mut ptr u64[25] st1,
                                 reg mut ptr u64[25] st2,
                                 reg mut ptr u64[25] st3) -> (reg mut ptr u64[25],
                                                             reg mut ptr u64[25],
                                                             reg mut ptr u64[25],
                                                             reg mut ptr u64[25])

{
  reg u256 r8;
  r8 = /* global: */ ROL8; /* u256 */ 
  reg u256 r56;
  r56 = /* global: */ ROL56; /* u256 */ 
  stack u256[25] st4x1;
  #[inline]
  st4x1 = __st4x_pack(st4x1, st0, st1, st2, st3);
  stack u256[25] st4x2;
  st4x2 = _keccakf1600_4x_pround(st4x2, st4x1, r8, r56);
  #[inline]
  (st0, st1, st2, st3) = __st4x_unpack(st0, st1, st2, st3, st4x2);
  return (st0, st1, st2, st3);
}

inline
fn __keccakf1600_pround_equiv (reg mut ptr u256[25] e,
                              reg const ptr u256[25] a) -> (reg mut ptr u256[25])

{
  stack u64[25] st0;
  stack u64[25] st1;
  stack u64[25] st2;
  stack u64[25] st3;
  #[inline]
  (st0, st1, st2, st3) = __st4x_unpack(st0, st1, st2, st3, a);
  #[inline]
  (st0, st1, st2, st3) = __keccakf1600_pround_unpacked(st0, st1, st2, st3);
  #[inline]
  e = __st4x_pack(e, st0, st1, st2, st3);
  return (e);
}

inline
fn __state_init_avx2x4 (reg mut ptr u256[25] st) -> (reg mut ptr u256[25])

{
  reg u256 z256;
  z256 = #set0_256(); /*  */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * 25)))) {
    st.[#unaligned ((uint /* of ui64 */) i)] = z256; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  return (st);
}

inline
fn __addratebit_avx2x4 (reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u256[25])

{
  reg u64 t64;
  t64 = ((64u) 1); /* u64 */ 
  t64 = (t64 <<64u ((8u) (((8 * RATE8) - 1) % 64))); /* u64 */ 
  reg u128 t128;
  t128 = ((128u) t64); /* u128 */ 
  reg u256 t256;
  t256 = #VPBROADCAST_4u64(t128); /*  */ 
  t256 = (t256 ^256u st[((RATE8 - 1) / 8)]); /* u256 */ 
  st[((RATE8 - 1) / 8)] = t256; /* u256 */ 
  return (st);
}

inline
fn __addstate_m_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                             #[Internal::wint::unsigned] reg ui64 buf,
                             inline int _LEN, inline int _TRAILB) -> 
(reg mut ptr u256[25], inline int)

{
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u256 w;
  if ((AT8 % 8) != 0) {
    #[inline]
    (buf, _LEN, _TRAILB, AT8, w) =
      __m_ilen_read_bcast_upto8_at(buf, _LEN, _TRAILB, AT, AT8);
    w = (w ^256u st[(AT / 8)]); /* u256 */ 
    st[(AT / 8)] = w; /* u256 */ 
    AT = AT8; /* int:i */ 
  }
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (32 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) (32 * ((AT / 8) + (_LEN / 8)))))) {
    w = #VPBROADCAST_4u64([:u64 ((64u /* of ui64 */) buf)]); /*  */ 
    buf = (buf +64ui ((64ui /* of int */) 8)); /* u64 */ 
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    at = (at +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (buf, _LEN, _TRAILB, AT, w) =
      __m_ilen_read_bcast_upto8_at(buf, _LEN, _TRAILB, AT, AT);
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
  }
  return (st, AT);
}

inline
fn __absorb_m_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                           #[Internal::wint::unsigned] reg ui64 buf,
                           inline int _LEN, inline int _TRAILB,
                           inline int _RATE8) -> (reg mut ptr u256[25],
                                                 inline int)

{
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */) =
      __addstate_m_bcast_avx2x4(st, AT, buf, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */) = __addstate_m_bcast_avx2x4(st, 0, buf, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT) = __addstate_m_bcast_avx2x4(st, AT, buf, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn __addstate_m_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                       #[Internal::wint::unsigned] reg ui64 buf0,
                       #[Internal::wint::unsigned] reg ui64 buf1,
                       #[Internal::wint::unsigned] reg ui64 buf2,
                       #[Internal::wint::unsigned] reg ui64 buf3,
                       inline int _LEN, inline int _TRAILB) -> (reg mut ptr u256[25],
                                                               inline int,
                                                               reg ui64,
                                                               reg ui64,
                                                               reg ui64,
                                                               reg ui64)

{
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if ((AT8 % 8) != 0) {
    #[inline]
    (buf0, _ /* int */, _ /* int */, _ /* int */, t0) =
      __m_ilen_read_upto8_at(buf0, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 0)] =
      (st[:u64 ((4 * (AT / 8)) + 0)] ^64u t0); /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */, _ /* int */, t1) =
      __m_ilen_read_upto8_at(buf1, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 1)] =
      (st[:u64 ((4 * (AT / 8)) + 1)] ^64u t1); /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */, _ /* int */, t2) =
      __m_ilen_read_upto8_at(buf2, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 2)] =
      (st[:u64 ((4 * (AT / 8)) + 2)] ^64u t2); /* u64 */ 
    #[inline]
    (buf3, _LEN, _TRAILB, AT8, t3) =
      __m_ilen_read_upto8_at(buf3, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 3)] =
      (st[:u64 ((4 * (AT / 8)) + 3)] ^64u t3); /* u64 */ 
    AT = AT8; /* int:i */ 
  }
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (4 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) ((4 * (AT / 8)) + (4 * (_LEN / 8)))))) {
    t0 = [:u64 ((64u /* of ui64 */) buf0)]; /* u64 */ 
    buf0 = (buf0 +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    t1 = [:u64 ((64u /* of ui64 */) buf1)]; /* u64 */ 
    buf1 = (buf1 +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    t2 = [:u64 ((64u /* of ui64 */) buf2)]; /* u64 */ 
    buf2 = (buf2 +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    t3 = [:u64 ((64u /* of ui64 */) buf3)]; /* u64 */ 
    buf3 = (buf3 +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    at = (at +64ui ((64ui /* of int */) 4)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (buf0, _ /* int */, _ /* int */, _ /* int */, t0) =
      __m_ilen_read_upto8_at(buf0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */, _ /* int */, t1) =
      __m_ilen_read_upto8_at(buf1, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */, _ /* int */, t2) =
      __m_ilen_read_upto8_at(buf2, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */, AT, t3) =
      __m_ilen_read_upto8_at(buf3, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
  }
  return (st, AT, buf0, buf1, buf2, buf3);
}

inline
fn __absorb_m_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                     #[Internal::wint::unsigned] reg ui64 buf0,
                     #[Internal::wint::unsigned] reg ui64 buf1,
                     #[Internal::wint::unsigned] reg ui64 buf2,
                     #[Internal::wint::unsigned] reg ui64 buf3,
                     inline int _LEN, inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, buf0, buf1, buf2, buf3) =
      __addstate_m_avx2x4(st, AT, buf0, buf1, buf2, buf3, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, buf0, buf1, buf2, buf3) =
        __addstate_m_avx2x4(st, 0, buf0, buf1, buf2, buf3, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */, _ /* u64 */, _ /* u64 */, _ /* u64 */) =
    __addstate_m_avx2x4(st, AT, buf0, buf1, buf2, buf3, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn __dumpstate_m_avx2x4 (#[Internal::wint::unsigned] reg ui64 buf0,
                        #[Internal::wint::unsigned] reg ui64 buf1,
                        #[Internal::wint::unsigned] reg ui64 buf2,
                        #[Internal::wint::unsigned] reg ui64 buf3,
                        inline int _LEN, reg const ptr u256[25] st) -> 
(reg ui64, reg ui64, reg ui64, reg ui64)

{
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * (_LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (0 * 32))))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (1 * 32))))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (2 * 32))))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (3 * 32))))]; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    [:u256 ((64u /* of ui64 */) buf0)] = x0; /* u256 */ 
    buf0 = (buf0 +64ui ((64ui /* of int */) 32)); /* u64 */ 
    [:u256 ((64u /* of ui64 */) buf1)] = x1; /* u256 */ 
    buf1 = (buf1 +64ui ((64ui /* of int */) 32)); /* u64 */ 
    [:u256 ((64u /* of ui64 */) buf2)] = x2; /* u256 */ 
    buf2 = (buf2 +64ui ((64ui /* of int */) 32)); /* u64 */ 
    [:u256 ((64u /* of ui64 */) buf3)] = x3; /* u256 */ 
    buf3 = (buf3 +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <64ui ((64ui /* of int */) (8 * (_LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    [:u64 ((64u /* of ui64 */) (buf0 +64ui i))] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    [:u64 ((64u /* of ui64 */) (buf1 +64ui i))] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    [:u64 ((64u /* of ui64 */) (buf2 +64ui i))] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    [:u64 ((64u /* of ui64 */) (buf3 +64ui i))] = t3; /* u64 */ 
    i = (i +64ui ((64ui /* of int */) 8)); /* u64 */ 
  }
  buf0 = (buf0 +64ui i); /* u64 */ 
  buf1 = (buf1 +64ui i); /* u64 */ 
  buf2 = (buf2 +64ui i); /* u64 */ 
  buf3 = (buf3 +64ui i); /* u64 */ 
  if (0 < (_LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */) = __m_ilen_write_upto8(buf0, (_LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */) = __m_ilen_write_upto8(buf1, (_LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */) = __m_ilen_write_upto8(buf2, (_LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */) = __m_ilen_write_upto8(buf3, (_LEN % 8), t3);
  }
  return (buf0, buf1, buf2, buf3);
}

inline
fn __squeeze_m_avx2x4 (reg mut ptr u256[25] st, #[Internal::wint::unsigned]
                      reg ui64 buf0, #[Internal::wint::unsigned]
                      reg ui64 buf1, #[Internal::wint::unsigned]
                      reg ui64 buf2, #[Internal::wint::unsigned]
                      reg ui64 buf3, inline int _LEN, inline int _RATE8) -> 
(reg mut ptr u256[25])

{
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  if (0 < ITERS) {
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3) =
        __dumpstate_m_avx2x4(buf0, buf1, buf2, buf3, _RATE8, st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  if (0 < LO) {
    st = _keccakf1600_avx2x4(st);
    #[inline]
    (buf0, buf1, buf2, buf3) =
      __dumpstate_m_avx2x4(buf0, buf1, buf2, buf3, LO, st);
  }
  return (st);
}

param int A1::_ASIZE = 1;

inline
fn A1::__a_ilen_read_upto8_at (reg const ptr u8[A1::_ASIZE] buf,
                              #[Internal::wint::unsigned] reg ui64 offset,
                              inline int DELTA, inline int LEN,
                              inline int TRAIL, inline int CUR,
                              inline int AT) -> (inline int, inline int,
                                                inline int, inline int,
                                                reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w =
        buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                             ((64u) DELTA))]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u)
          buf.[#unaligned :u32 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        DELTA = (DELTA + (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 =
          ((64u)
          buf.[#unaligned :u16 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u)
            buf.[#unaligned (((64u /* of ui64 */) offset) +64u ((64u) DELTA))]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1::__a_ilen_read_upto16_at (reg const ptr u8[A1::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN,
                               inline int TRAIL, inline int CUR,
                               inline int AT) -> (inline int, inline int,
                                                 inline int, inline int,
                                                 reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w =
        buf.[#unaligned :u128 (((64u /* of ui64 */) offset) +64u
                              ((64u) DELTA))]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      DELTA = (DELTA + (16 - AT16)); /* int:i */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A1::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_0) =
          A1::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 0, AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A1::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1::__a_ilen_read_upto32_at (reg const ptr u8[A1::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN,
                               inline int TRAIL, inline int CUR,
                               inline int AT) -> (inline int, inline int,
                                                 inline int, inline int,
                                                 reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w =
        buf.[#unaligned :u256 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A1::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                      AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_0) =
          A1::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 0, AT32);
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A1::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                      AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1::__a_ilen_read_bcast_upto8_at (reg const ptr u8[A1::_ASIZE] buf,
                                    #[Internal::wint::unsigned]
                                    reg ui64 offset, inline int DELTA,
                                    inline int LEN, inline int TRAIL,
                                    inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */)
                                               (offset +64ui
                                               ((64ui /* of int */) DELTA)))]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (DELTA, LEN, TRAIL, AT, w) =
        A1::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, CUR, AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w256);
}

inline
fn A1::__a_ilen_write_upto8 (reg mut ptr u8[A1::_ASIZE] buf,
                            #[Internal::wint::unsigned] reg ui64 offset,
                            inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 ((uint /* of ui64 */)
                           (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned ((uint /* of ui64 */)
                        (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1::__a_ilen_write_upto16 (reg mut ptr u8[A1::_ASIZE] buf,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A1::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1::__a_ilen_write_upto8(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1::__a_ilen_write_upto32 (reg mut ptr u8[A1::_ASIZE] buf,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A1::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1::__a_ilen_write_upto16(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1::__a_rlen_read_upto8 (reg const ptr u8[A1::_ASIZE] a,
                           #[Internal::wint::unsigned] reg ui64 off,
                           #[Internal::wint::unsigned] reg ui64 len) -> 
(reg ui64, reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = a.[#unaligned :u64 ((uint /* of ui64 */) off)]; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) a.[#unaligned :u32 ((uint /* of ui64 */) off)]); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) a.[#unaligned :u16 ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) a.[#unaligned ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (off, w);
}

inline
fn A1::__a_rlen_write_upto8 (reg mut ptr u8[A1::_ASIZE] buf,
                            #[Internal::wint::unsigned] reg ui64 off,
                            reg u64 data, #[Internal::wint::unsigned]
                            reg ui64 len) -> (reg mut ptr u8[A1::_ASIZE],
                                             reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    buf.[#unaligned :u64 ((uint /* of ui64 */) off)] = data; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u32 ((uint /* of ui64 */) off)] = data; /* u32 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u16 ((uint /* of ui64 */) off)] = data; /* u16 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      buf.[#unaligned ((uint /* of ui64 */) off)] = data; /* u8 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, off);
}

inline
fn A1::__aread_subu64 (reg const ptr u8[A1::_ASIZE] buf, reg u64 offset,
                      inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64)

{
  inline int ILEN;
  ILEN = LEN; /* int:i */ 
  reg u64 w;
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */ 
    TRAIL = 0; /* int:i */ 
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      reg u64 t16;
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      } else {
        t16 = ((64u) 0); /* u64 */ 
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */ 
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          }
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */ 
        }
        TRAIL = 0; /* int:i */ 
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */ 
        t16 = (t16 |64u t8); /* u64 */ 
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */ 
      w = (w |64u t16); /* u64 */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1::__aread_bcast_4subu64 (reg const ptr u8[A1::_ASIZE] buf,
                             reg u64 offset, inline int DELTA,
                             inline int LEN, inline int TRAIL) -> (inline int,
                                                                  inline int,
                                                                  inline int,
                                                                  reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      reg u64 t64;
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A1::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      reg u128 t128;
      t128 = ((128u) t64); /* u128 */ 
      w = #VPBROADCAST_4u64(t128); /*  */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1::__aread_subu128 (reg const ptr u8[A1::_ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128)

{
  reg u128 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */ 
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      reg u64 t64;
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */ 
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1::__aread_subu256 (reg const ptr u8[A1::_ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_0;
      reg u128 t128_1;
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A1::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      } else {
        t128_1 = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A1::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1::__awrite_subu64 (reg mut ptr u8[A1::_ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1::__awrite_subu128 (reg mut ptr u8[A1::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A1::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) = A1::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1::__awrite_subu256 (reg mut ptr u8[A1::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A1::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1::__addstate_avx2 (reg u256[7] st, inline int AT,
                       reg const ptr u8[A1::_ASIZE] buf,
                       #[Internal::wint::unsigned] reg ui64 offset,
                       inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                               inline int,
                                                               reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (AT < 8) {
    reg u256 r0;
    if ((AT == 0) && (8 <= _LEN)) {
      r0 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                                               ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      _LEN = (_LEN - 8); /* int:i */ 
      AT = 8; /* int:i */ 
    } else {
      reg u64 t64_1;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_1) =
        A1::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 0, AT);
      reg u128 t128_0;
      t128_0 = ((128u) t64_1); /* u128 */ 
      r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
    }
    st[0] = (st[0] ^256u r0); /* u256 */ 
  }
  if ((AT < 40) && ((0 < _LEN) || (_TRAILB != 0))) {
    reg u256 r1;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, r1) =
      A1::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 8, AT);
    st[1] = (st[1] ^256u r1); /* u256 */ 
  }
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u64 t64_2;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t64_2) =
      A1::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 40, AT);
    reg u128 t128_1;
    t128_1 = ((128u) t64_2); /* u128 */ 
    reg u128 t128_2;
    t128_2 = #set0_128(); /*  */ 
    if ((0 < _LEN) || (_TRAILB != 0)) {
      reg u256 r3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r3) =
        A1::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 48, AT);
      reg u64 t64_3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_3) =
        A1::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 80, AT);
      t128_2 = ((128u) t64_3); /* u128 */ 
      reg u256 r4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r4) =
        A1::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 88, AT);
      reg u64 t64_4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_4) =
        A1::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 120, AT);
      t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
      reg u256 r5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r5) =
        A1::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 128,
                                    AT);
      reg u64 t64_5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_5) =
        A1::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 160, AT);
      t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
      reg u256 r6;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r6) =
        A1::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 168,
                                    AT);
      #[inline]
      st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
    }
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (st, AT, offset);
}

inline
fn A1::__absorb_avx2 (reg u256[7] st, inline int AT,
                     reg const ptr u8[A1::_ASIZE] buf, inline int _TRAILB,
                     inline int _RATE8) -> (reg u256[7], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1::__addstate_avx2(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1::__addstate_avx2(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1::__addstate_avx2(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1::__dumpstate_avx2 (reg mut ptr u8[A1::_ASIZE] buf,
                        #[Internal::wint::unsigned] reg ui64 offset,
                        inline int _LEN, reg u256[7] st) -> (reg mut ptr u8[A1::_ASIZE],
                                                            reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= _LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1::__a_ilen_write_upto32(buf, offset, DELTA, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, _LEN) =
      A1::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, _LEN) =
    A1::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, _LEN) =
      A1::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, _LEN) =
        A1::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
    }
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A1::__squeeze_avx2 (reg u256[7] st, reg mut ptr u8[A1::_ASIZE] buf,
                      inline int _RATE8) -> (reg u256[7],
                                            reg mut ptr u8[A1::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A1::__dumpstate_avx2(buf, offset, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A1::__dumpstate_avx2(buf, offset, LO, st);
  }
  return (st, buf);
}

inline
fn A1::__addstate_array_avx2 (reg u256[7] st,
                             reg const ptr u8[A1::_ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t64;
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  reg u128 t128_0;
  t128_0 = ((128u) t64); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A1::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    reg u128 t128_1;
    t128_1 = ((128u) t64); /* u128 */ 
    reg u256 r3;
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A1::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */ 
    reg u256 r4;
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A1::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A1::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */ 
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
    reg u256 r6;
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A1::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (st, offset);
}

inline
fn A1::__absorb_array_avx2 (reg u256[7] st, reg const ptr u8[A1::_ASIZE] buf,
                           reg u64 offset, inline int LEN, inline int RATE8,
                           inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */ 
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  if (0 < ITERS) {
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
  }
  LEN = (LEN % RATE8); /* int:i */ 
  #[inline]
  (st, offset) = A1::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A1::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                           reg const ptr u8[A1::_ASIZE] buf, reg u64 offset,
                           inline int LEN, inline int TRAILB) -> (reg mut ptr u64[25],
                                                                 inline int,
                                                                 reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (AT / 8)); /* u64 */ 
  reg u64 t64;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A1::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A1::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      at = (at +64u ((64u) 1)); /* u64 */ 
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      reg u256 t256;
      t256 = buf.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = (LEN % 32); /* int:i */ 
  }
  if (16 <= LEN) {
    reg u128 t128;
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
    DELTA = (DELTA + 16); /* int:i */ 
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */ 
    at = (at +64u ((64u) 2)); /* u64 */ 
    LEN = (LEN - 16); /* int:i */ 
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
    DELTA = (DELTA + 8); /* int:i */ 
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */ 
    at = (at +64u ((64u) 1)); /* u64 */ 
    LEN = (LEN - 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A1::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */ 
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (pst, ALL, offset);
}

inline
fn A1::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                            reg u256[7] st, reg const ptr u8[A1::_ASIZE] buf,
                            reg u64 offset, inline int LEN, inline int RATE8,
                            inline int TRAILB) -> (reg mut ptr u64[25],
                                                  inline int, reg u256[7],
                                                  reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  reg u64 i;
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A1::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */ 
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */ 
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A1::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    if (TRAILB != 0) {
      #[inline]
      (st, offset) = A1::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A1::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A1::__dumpstate_array_avx2 (reg mut ptr u8[A1::_ASIZE] buf,
                              reg u64 offset, inline int LEN, reg u256[7] st) -> 
(reg mut ptr u8[A1::_ASIZE], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, LEN) = A1::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A1::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, LEN) = A1::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A1::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A1::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A1::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A1::__squeeze_array_avx2 (reg mut ptr u8[A1::_ASIZE] buf, reg u64 offset,
                            inline int LEN, reg u256[7] st, inline int RATE8) -> 
(reg mut ptr u8[A1::_ASIZE], reg u256[7])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A1::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A1::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A1::__addstate_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                               reg const ptr u8[A1::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int _LEN, inline int _TRAILB) -> 
(reg mut ptr u256[25], inline int, reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u256 w;
  if ((AT8 % 8) != 0) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, w) =
      A1::__a_ilen_read_bcast_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, AT,
                                       AT8);
    w = (w ^256u st[(AT / 8)]); /* u256 */ 
    st[(AT / 8)] = w; /* u256 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (32 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) (32 * ((AT / 8) + (_LEN / 8)))))) {
    w =
      #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */) offset)]); /*  */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    at = (at +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, w) =
      A1::__a_ilen_read_bcast_upto8_at(buf, offset, 0, _LEN, _TRAILB, AT, AT);
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A1::__absorb_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                             reg const ptr u8[A1::_ASIZE] buf,
                             inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1::__addstate_bcast_avx2x4(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1::__addstate_bcast_avx2x4(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1::__addstate_bcast_avx2x4(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1::__addstate_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                         reg const ptr u8[A1::_ASIZE] buf0,
                         reg const ptr u8[A1::_ASIZE] buf1,
                         reg const ptr u8[A1::_ASIZE] buf2,
                         reg const ptr u8[A1::_ASIZE] buf3,
                         #[Internal::wint::unsigned] reg ui64 offset,
                         inline int _LEN, inline int _TRAILB) -> (reg mut ptr u256[25],
                                                                 inline int,
                                                                 reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if ((AT8 % 8) != 0) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A1::__a_ilen_read_upto8_at(buf0, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 0)] =
      (st[:u64 ((4 * (AT / 8)) + 0)] ^64u t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A1::__a_ilen_read_upto8_at(buf1, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 1)] =
      (st[:u64 ((4 * (AT / 8)) + 1)] ^64u t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A1::__a_ilen_read_upto8_at(buf2, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 2)] =
      (st[:u64 ((4 * (AT / 8)) + 2)] ^64u t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, t3) =
      A1::__a_ilen_read_upto8_at(buf3, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 3)] =
      (st[:u64 ((4 * (AT / 8)) + 3)] ^64u t3); /* u64 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (4 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) ((4 * (AT / 8)) + (4 * (_LEN / 8)))))) {
    t0 = buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    t1 = buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    t2 = buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    t3 = buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    at = (at +64ui ((64ui /* of int */) 4)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A1::__a_ilen_read_upto8_at(buf0, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A1::__a_ilen_read_upto8_at(buf1, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A1::__a_ilen_read_upto8_at(buf2, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t3) =
      A1::__a_ilen_read_upto8_at(buf3, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A1::__absorb_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                       reg const ptr u8[A1::_ASIZE] buf0,
                       reg const ptr u8[A1::_ASIZE] buf1,
                       reg const ptr u8[A1::_ASIZE] buf2,
                       reg const ptr u8[A1::_ASIZE] buf3, inline int _TRAILB,
                       inline int _RATE8) -> (reg mut ptr u256[25],
                                             inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                            (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1::__addstate_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset, _RATE8,
                              0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, _LEN,
                          _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1::__dumpstate_avx2x4 (reg mut ptr u8[A1::_ASIZE] buf0,
                          reg mut ptr u8[A1::_ASIZE] buf1,
                          reg mut ptr u8[A1::_ASIZE] buf2,
                          reg mut ptr u8[A1::_ASIZE] buf3,
                          #[Internal::wint::unsigned] reg ui64 offset,
                          inline int _LEN, reg const ptr u256[25] st) -> 
(reg mut ptr u8[A1::_ASIZE], reg mut ptr u8[A1::_ASIZE],
reg mut ptr u8[A1::_ASIZE], reg mut ptr u8[A1::_ASIZE], reg ui64)

{
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * (_LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (0 * 32))))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (1 * 32))))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (2 * 32))))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (3 * 32))))]; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x0; /* u256 */ 
    buf1.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x1; /* u256 */ 
    buf2.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x2; /* u256 */ 
    buf3.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x3; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <64ui ((64ui /* of int */) (8 * (_LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t3; /* u64 */ 
    i = (i +64ui ((64ui /* of int */) 8)); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
  }
  if (0 < (_LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1::__a_ilen_write_upto8(buf0, offset, 0, (_LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1::__a_ilen_write_upto8(buf1, offset, 0, (_LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1::__a_ilen_write_upto8(buf2, offset, 0, (_LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1::__a_ilen_write_upto8(buf3, offset, 0, (_LEN % 8), t3);
    offset = (offset +64ui ((64ui /* of int */) (_LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1::__squeeze_avx2x4 (reg mut ptr u256[25] st,
                        reg mut ptr u8[A1::_ASIZE] buf0,
                        reg mut ptr u8[A1::_ASIZE] buf1,
                        reg mut ptr u8[A1::_ASIZE] buf2,
                        reg mut ptr u8[A1::_ASIZE] buf3, inline int _RATE8) -> 
(reg mut ptr u256[25], reg mut ptr u8[A1::_ASIZE],
reg mut ptr u8[A1::_ASIZE], reg mut ptr u8[A1::_ASIZE],
reg mut ptr u8[A1::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  if (0 < ITERS) {
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, _RATE8, st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  if (0 < LO) {
    st = _keccakf1600_avx2x4(st);
    #[inline]
    (buf0, buf1, buf2, buf3, offset) =
      A1::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
  }
  return (st, buf0, buf1, buf2, buf3);
}

inline
fn A1::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                     reg const ptr u8[A1::_ASIZE] buf,
                                     reg u64 offset, inline int LEN,
                                     inline int TRAILB) -> (reg mut ptr u256[25],
                                                           inline int,
                                                           reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (32 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u256 t256;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A1::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A1::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A1::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
    st.[#unaligned at] = t256; /* u256 */ 
  }
  return (st, ALL, offset);
}

inline
fn A1::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                   reg const ptr u8[A1::_ASIZE] buf,
                                   reg u64 offset, inline int LEN,
                                   inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1::__addstate_bcast_array_avx2x4(st, AT, buf, offset, (RATE8 - AT),
                                          0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A1::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                               reg const ptr u8[A1::_ASIZE] buf0,
                               reg const ptr u8[A1::_ASIZE] buf1,
                               reg const ptr u8[A1::_ASIZE] buf2,
                               reg const ptr u8[A1::_ASIZE] buf3,
                               reg u64 offset, inline int LEN,
                               inline int TRAILB) -> (reg mut ptr u256[25],
                                                     inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (4 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A1::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A1::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A1::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A1::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
      TRAILB = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */ 
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A1::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A1::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A1::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A1::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      reg u256 t256_0;
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_1;
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_2;
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_3;
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 =
        (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */ 
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */ 
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */ 
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */ 
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */ 
      at = (at +64u ((64u) 16)); /* u64 */ 
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A1::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A1::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A1::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A1::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
      TRAILB = 0; /* int:i */ 
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
  }
  return (st, ALL, offset);
}

inline
fn A1::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                             reg const ptr u8[A1::_ASIZE] buf0,
                             reg const ptr u8[A1::_ASIZE] buf1,
                             reg const ptr u8[A1::_ASIZE] buf2,
                             reg const ptr u8[A1::_ASIZE] buf3,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                  LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                    (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                    RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A1::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset, LEN,
                                  TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1::__dumpstate_array_avx2x4 (reg mut ptr u8[A1::_ASIZE] buf0,
                                reg mut ptr u8[A1::_ASIZE] buf1,
                                reg mut ptr u8[A1::_ASIZE] buf2,
                                reg mut ptr u8[A1::_ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                reg const ptr u256[25] st) -> (reg mut ptr u8[A1::_ASIZE],
                                                              reg mut ptr u8[A1::_ASIZE],
                                                              reg mut ptr u8[A1::_ASIZE],
                                                              reg mut ptr u8[A1::_ASIZE],
                                                              reg u64)

{
  reg u64 i;
  i = ((64u) 0); /* u64 */ 
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */ 
    i = (i +64u ((64u) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */ 
    buf1.[#unaligned :u256 offset] = x1; /* u256 */ 
    buf2.[#unaligned :u256 offset] = x2; /* u256 */ 
    buf3.[#unaligned :u256 offset] = x3; /* u256 */ 
    offset = (offset +64u ((64u) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    buf0.[#unaligned :u64 offset] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    buf1.[#unaligned :u64 offset] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    buf2.[#unaligned :u64 offset] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    buf3.[#unaligned :u64 offset] = t3; /* u64 */ 
    i = (i +64u ((64u) 8)); /* u64 */ 
    offset = (offset +64u ((64u) 8)); /* u64 */ 
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1::__squeeze_array_avx2x4 (reg mut ptr u8[A1::_ASIZE] buf0,
                              reg mut ptr u8[A1::_ASIZE] buf1,
                              reg mut ptr u8[A1::_ASIZE] buf2,
                              reg mut ptr u8[A1::_ASIZE] buf3,
                              reg u64 offset, inline int LEN,
                              reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A1::_ASIZE], reg mut ptr u8[A1::_ASIZE],
reg mut ptr u8[A1::_ASIZE], reg mut ptr u8[A1::_ASIZE], reg u64,
reg mut ptr u256[25])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A1::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, RATE8,
                                       st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A2::_ASIZE = 2;

inline
fn A2::__a_ilen_read_upto8_at (reg const ptr u8[A2::_ASIZE] buf,
                              #[Internal::wint::unsigned] reg ui64 offset,
                              inline int DELTA, inline int LEN,
                              inline int TRAIL, inline int CUR,
                              inline int AT) -> (inline int, inline int,
                                                inline int, inline int,
                                                reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w =
        buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                             ((64u) DELTA))]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u)
          buf.[#unaligned :u32 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        DELTA = (DELTA + (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 =
          ((64u)
          buf.[#unaligned :u16 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u)
            buf.[#unaligned (((64u /* of ui64 */) offset) +64u ((64u) DELTA))]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A2::__a_ilen_read_upto16_at (reg const ptr u8[A2::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN,
                               inline int TRAIL, inline int CUR,
                               inline int AT) -> (inline int, inline int,
                                                 inline int, inline int,
                                                 reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w =
        buf.[#unaligned :u128 (((64u /* of ui64 */) offset) +64u
                              ((64u) DELTA))]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      DELTA = (DELTA + (16 - AT16)); /* int:i */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A2::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_0) =
          A2::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 0, AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A2::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A2::__a_ilen_read_upto32_at (reg const ptr u8[A2::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN,
                               inline int TRAIL, inline int CUR,
                               inline int AT) -> (inline int, inline int,
                                                 inline int, inline int,
                                                 reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w =
        buf.[#unaligned :u256 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A2::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                      AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_0) =
          A2::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 0, AT32);
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A2::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                      AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A2::__a_ilen_read_bcast_upto8_at (reg const ptr u8[A2::_ASIZE] buf,
                                    #[Internal::wint::unsigned]
                                    reg ui64 offset, inline int DELTA,
                                    inline int LEN, inline int TRAIL,
                                    inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */)
                                               (offset +64ui
                                               ((64ui /* of int */) DELTA)))]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (DELTA, LEN, TRAIL, AT, w) =
        A2::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, CUR, AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w256);
}

inline
fn A2::__a_ilen_write_upto8 (reg mut ptr u8[A2::_ASIZE] buf,
                            #[Internal::wint::unsigned] reg ui64 offset,
                            inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A2::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 ((uint /* of ui64 */)
                           (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned ((uint /* of ui64 */)
                        (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A2::__a_ilen_write_upto16 (reg mut ptr u8[A2::_ASIZE] buf,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A2::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A2::__a_ilen_write_upto8(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A2::__a_ilen_write_upto32 (reg mut ptr u8[A2::_ASIZE] buf,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A2::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A2::__a_ilen_write_upto16(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A2::__a_rlen_read_upto8 (reg const ptr u8[A2::_ASIZE] a,
                           #[Internal::wint::unsigned] reg ui64 off,
                           #[Internal::wint::unsigned] reg ui64 len) -> 
(reg ui64, reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = a.[#unaligned :u64 ((uint /* of ui64 */) off)]; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) a.[#unaligned :u32 ((uint /* of ui64 */) off)]); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) a.[#unaligned :u16 ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) a.[#unaligned ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (off, w);
}

inline
fn A2::__a_rlen_write_upto8 (reg mut ptr u8[A2::_ASIZE] buf,
                            #[Internal::wint::unsigned] reg ui64 off,
                            reg u64 data, #[Internal::wint::unsigned]
                            reg ui64 len) -> (reg mut ptr u8[A2::_ASIZE],
                                             reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    buf.[#unaligned :u64 ((uint /* of ui64 */) off)] = data; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u32 ((uint /* of ui64 */) off)] = data; /* u32 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u16 ((uint /* of ui64 */) off)] = data; /* u16 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      buf.[#unaligned ((uint /* of ui64 */) off)] = data; /* u8 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, off);
}

inline
fn A2::__aread_subu64 (reg const ptr u8[A2::_ASIZE] buf, reg u64 offset,
                      inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64)

{
  inline int ILEN;
  ILEN = LEN; /* int:i */ 
  reg u64 w;
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */ 
    TRAIL = 0; /* int:i */ 
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      reg u64 t16;
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      } else {
        t16 = ((64u) 0); /* u64 */ 
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */ 
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          }
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */ 
        }
        TRAIL = 0; /* int:i */ 
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */ 
        t16 = (t16 |64u t8); /* u64 */ 
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */ 
      w = (w |64u t16); /* u64 */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A2::__aread_bcast_4subu64 (reg const ptr u8[A2::_ASIZE] buf,
                             reg u64 offset, inline int DELTA,
                             inline int LEN, inline int TRAIL) -> (inline int,
                                                                  inline int,
                                                                  inline int,
                                                                  reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      reg u64 t64;
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A2::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      reg u128 t128;
      t128 = ((128u) t64); /* u128 */ 
      w = #VPBROADCAST_4u64(t128); /*  */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A2::__aread_subu128 (reg const ptr u8[A2::_ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128)

{
  reg u128 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */ 
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      reg u64 t64;
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A2::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */ 
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A2::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A2::__aread_subu256 (reg const ptr u8[A2::_ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_0;
      reg u128 t128_1;
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A2::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      } else {
        t128_1 = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A2::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A2::__awrite_subu64 (reg mut ptr u8[A2::_ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A2::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A2::__awrite_subu128 (reg mut ptr u8[A2::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A2::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) = A2::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A2::__awrite_subu256 (reg mut ptr u8[A2::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A2::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A2::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A2::__addstate_avx2 (reg u256[7] st, inline int AT,
                       reg const ptr u8[A2::_ASIZE] buf,
                       #[Internal::wint::unsigned] reg ui64 offset,
                       inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                               inline int,
                                                               reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (AT < 8) {
    reg u256 r0;
    if ((AT == 0) && (8 <= _LEN)) {
      r0 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                                               ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      _LEN = (_LEN - 8); /* int:i */ 
      AT = 8; /* int:i */ 
    } else {
      reg u64 t64_1;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_1) =
        A2::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 0, AT);
      reg u128 t128_0;
      t128_0 = ((128u) t64_1); /* u128 */ 
      r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
    }
    st[0] = (st[0] ^256u r0); /* u256 */ 
  }
  if ((AT < 40) && ((0 < _LEN) || (_TRAILB != 0))) {
    reg u256 r1;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, r1) =
      A2::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 8, AT);
    st[1] = (st[1] ^256u r1); /* u256 */ 
  }
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u64 t64_2;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t64_2) =
      A2::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 40, AT);
    reg u128 t128_1;
    t128_1 = ((128u) t64_2); /* u128 */ 
    reg u128 t128_2;
    t128_2 = #set0_128(); /*  */ 
    if ((0 < _LEN) || (_TRAILB != 0)) {
      reg u256 r3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r3) =
        A2::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 48, AT);
      reg u64 t64_3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_3) =
        A2::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 80, AT);
      t128_2 = ((128u) t64_3); /* u128 */ 
      reg u256 r4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r4) =
        A2::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 88, AT);
      reg u64 t64_4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_4) =
        A2::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 120, AT);
      t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
      reg u256 r5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r5) =
        A2::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 128,
                                    AT);
      reg u64 t64_5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_5) =
        A2::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 160, AT);
      t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
      reg u256 r6;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r6) =
        A2::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 168,
                                    AT);
      #[inline]
      st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
    }
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (st, AT, offset);
}

inline
fn A2::__absorb_avx2 (reg u256[7] st, inline int AT,
                     reg const ptr u8[A2::_ASIZE] buf, inline int _TRAILB,
                     inline int _RATE8) -> (reg u256[7], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A2::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A2::__addstate_avx2(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A2::__addstate_avx2(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A2::__addstate_avx2(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A2::__dumpstate_avx2 (reg mut ptr u8[A2::_ASIZE] buf,
                        #[Internal::wint::unsigned] reg ui64 offset,
                        inline int _LEN, reg u256[7] st) -> (reg mut ptr u8[A2::_ASIZE],
                                                            reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= _LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A2::__a_ilen_write_upto32(buf, offset, DELTA, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, _LEN) =
      A2::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, _LEN) =
    A2::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, _LEN) =
      A2::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, _LEN) =
        A2::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A2::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A2::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A2::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A2::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A2::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A2::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
    }
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A2::__squeeze_avx2 (reg u256[7] st, reg mut ptr u8[A2::_ASIZE] buf,
                      inline int _RATE8) -> (reg u256[7],
                                            reg mut ptr u8[A2::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A2::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A2::__dumpstate_avx2(buf, offset, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A2::__dumpstate_avx2(buf, offset, LO, st);
  }
  return (st, buf);
}

inline
fn A2::__addstate_array_avx2 (reg u256[7] st,
                             reg const ptr u8[A2::_ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t64;
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  reg u128 t128_0;
  t128_0 = ((128u) t64); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A2::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    reg u128 t128_1;
    t128_1 = ((128u) t64); /* u128 */ 
    reg u256 r3;
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A2::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */ 
    reg u256 r4;
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A2::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A2::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */ 
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
    reg u256 r6;
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A2::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (st, offset);
}

inline
fn A2::__absorb_array_avx2 (reg u256[7] st, reg const ptr u8[A2::_ASIZE] buf,
                           reg u64 offset, inline int LEN, inline int RATE8,
                           inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */ 
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  if (0 < ITERS) {
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A2::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
  }
  LEN = (LEN % RATE8); /* int:i */ 
  #[inline]
  (st, offset) = A2::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A2::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                           reg const ptr u8[A2::_ASIZE] buf, reg u64 offset,
                           inline int LEN, inline int TRAILB) -> (reg mut ptr u64[25],
                                                                 inline int,
                                                                 reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (AT / 8)); /* u64 */ 
  reg u64 t64;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A2::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A2::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      at = (at +64u ((64u) 1)); /* u64 */ 
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      reg u256 t256;
      t256 = buf.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = (LEN % 32); /* int:i */ 
  }
  if (16 <= LEN) {
    reg u128 t128;
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
    DELTA = (DELTA + 16); /* int:i */ 
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */ 
    at = (at +64u ((64u) 2)); /* u64 */ 
    LEN = (LEN - 16); /* int:i */ 
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
    DELTA = (DELTA + 8); /* int:i */ 
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */ 
    at = (at +64u ((64u) 1)); /* u64 */ 
    LEN = (LEN - 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A2::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */ 
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (pst, ALL, offset);
}

inline
fn A2::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                            reg u256[7] st, reg const ptr u8[A2::_ASIZE] buf,
                            reg u64 offset, inline int LEN, inline int RATE8,
                            inline int TRAILB) -> (reg mut ptr u64[25],
                                                  inline int, reg u256[7],
                                                  reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  reg u64 i;
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A2::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */ 
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */ 
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A2::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A2::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    if (TRAILB != 0) {
      #[inline]
      (st, offset) = A2::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A2::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A2::__dumpstate_array_avx2 (reg mut ptr u8[A2::_ASIZE] buf,
                              reg u64 offset, inline int LEN, reg u256[7] st) -> 
(reg mut ptr u8[A2::_ASIZE], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A2::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, LEN) = A2::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A2::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, LEN) = A2::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, LEN) =
        A2::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A2::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A2::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A2::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A2::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A2::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A2::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A2::__squeeze_array_avx2 (reg mut ptr u8[A2::_ASIZE] buf, reg u64 offset,
                            inline int LEN, reg u256[7] st, inline int RATE8) -> 
(reg mut ptr u8[A2::_ASIZE], reg u256[7])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A2::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A2::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A2::__addstate_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                               reg const ptr u8[A2::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int _LEN, inline int _TRAILB) -> 
(reg mut ptr u256[25], inline int, reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u256 w;
  if ((AT8 % 8) != 0) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, w) =
      A2::__a_ilen_read_bcast_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, AT,
                                       AT8);
    w = (w ^256u st[(AT / 8)]); /* u256 */ 
    st[(AT / 8)] = w; /* u256 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (32 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) (32 * ((AT / 8) + (_LEN / 8)))))) {
    w =
      #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */) offset)]); /*  */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    at = (at +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, w) =
      A2::__a_ilen_read_bcast_upto8_at(buf, offset, 0, _LEN, _TRAILB, AT, AT);
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A2::__absorb_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                             reg const ptr u8[A2::_ASIZE] buf,
                             inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A2::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A2::__addstate_bcast_avx2x4(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A2::__addstate_bcast_avx2x4(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A2::__addstate_bcast_avx2x4(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A2::__addstate_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                         reg const ptr u8[A2::_ASIZE] buf0,
                         reg const ptr u8[A2::_ASIZE] buf1,
                         reg const ptr u8[A2::_ASIZE] buf2,
                         reg const ptr u8[A2::_ASIZE] buf3,
                         #[Internal::wint::unsigned] reg ui64 offset,
                         inline int _LEN, inline int _TRAILB) -> (reg mut ptr u256[25],
                                                                 inline int,
                                                                 reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if ((AT8 % 8) != 0) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A2::__a_ilen_read_upto8_at(buf0, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 0)] =
      (st[:u64 ((4 * (AT / 8)) + 0)] ^64u t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A2::__a_ilen_read_upto8_at(buf1, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 1)] =
      (st[:u64 ((4 * (AT / 8)) + 1)] ^64u t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A2::__a_ilen_read_upto8_at(buf2, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 2)] =
      (st[:u64 ((4 * (AT / 8)) + 2)] ^64u t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, t3) =
      A2::__a_ilen_read_upto8_at(buf3, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 3)] =
      (st[:u64 ((4 * (AT / 8)) + 3)] ^64u t3); /* u64 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (4 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) ((4 * (AT / 8)) + (4 * (_LEN / 8)))))) {
    t0 = buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    t1 = buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    t2 = buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    t3 = buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    at = (at +64ui ((64ui /* of int */) 4)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A2::__a_ilen_read_upto8_at(buf0, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A2::__a_ilen_read_upto8_at(buf1, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A2::__a_ilen_read_upto8_at(buf2, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t3) =
      A2::__a_ilen_read_upto8_at(buf3, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A2::__absorb_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                       reg const ptr u8[A2::_ASIZE] buf0,
                       reg const ptr u8[A2::_ASIZE] buf1,
                       reg const ptr u8[A2::_ASIZE] buf2,
                       reg const ptr u8[A2::_ASIZE] buf3, inline int _TRAILB,
                       inline int _RATE8) -> (reg mut ptr u256[25],
                                             inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A2::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A2::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                            (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A2::__addstate_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset, _RATE8,
                              0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A2::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, _LEN,
                          _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A2::__dumpstate_avx2x4 (reg mut ptr u8[A2::_ASIZE] buf0,
                          reg mut ptr u8[A2::_ASIZE] buf1,
                          reg mut ptr u8[A2::_ASIZE] buf2,
                          reg mut ptr u8[A2::_ASIZE] buf3,
                          #[Internal::wint::unsigned] reg ui64 offset,
                          inline int _LEN, reg const ptr u256[25] st) -> 
(reg mut ptr u8[A2::_ASIZE], reg mut ptr u8[A2::_ASIZE],
reg mut ptr u8[A2::_ASIZE], reg mut ptr u8[A2::_ASIZE], reg ui64)

{
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * (_LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (0 * 32))))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (1 * 32))))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (2 * 32))))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (3 * 32))))]; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x0; /* u256 */ 
    buf1.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x1; /* u256 */ 
    buf2.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x2; /* u256 */ 
    buf3.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x3; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <64ui ((64ui /* of int */) (8 * (_LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t3; /* u64 */ 
    i = (i +64ui ((64ui /* of int */) 8)); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
  }
  if (0 < (_LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A2::__a_ilen_write_upto8(buf0, offset, 0, (_LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A2::__a_ilen_write_upto8(buf1, offset, 0, (_LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A2::__a_ilen_write_upto8(buf2, offset, 0, (_LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A2::__a_ilen_write_upto8(buf3, offset, 0, (_LEN % 8), t3);
    offset = (offset +64ui ((64ui /* of int */) (_LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A2::__squeeze_avx2x4 (reg mut ptr u256[25] st,
                        reg mut ptr u8[A2::_ASIZE] buf0,
                        reg mut ptr u8[A2::_ASIZE] buf1,
                        reg mut ptr u8[A2::_ASIZE] buf2,
                        reg mut ptr u8[A2::_ASIZE] buf3, inline int _RATE8) -> 
(reg mut ptr u256[25], reg mut ptr u8[A2::_ASIZE],
reg mut ptr u8[A2::_ASIZE], reg mut ptr u8[A2::_ASIZE],
reg mut ptr u8[A2::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A2::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  if (0 < ITERS) {
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A2::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, _RATE8, st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  if (0 < LO) {
    st = _keccakf1600_avx2x4(st);
    #[inline]
    (buf0, buf1, buf2, buf3, offset) =
      A2::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
  }
  return (st, buf0, buf1, buf2, buf3);
}

inline
fn A2::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                     reg const ptr u8[A2::_ASIZE] buf,
                                     reg u64 offset, inline int LEN,
                                     inline int TRAILB) -> (reg mut ptr u256[25],
                                                           inline int,
                                                           reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (32 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u256 t256;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A2::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A2::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A2::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
    st.[#unaligned at] = t256; /* u256 */ 
  }
  return (st, ALL, offset);
}

inline
fn A2::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                   reg const ptr u8[A2::_ASIZE] buf,
                                   reg u64 offset, inline int LEN,
                                   inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A2::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A2::__addstate_bcast_array_avx2x4(st, AT, buf, offset, (RATE8 - AT),
                                          0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A2::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A2::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A2::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                               reg const ptr u8[A2::_ASIZE] buf0,
                               reg const ptr u8[A2::_ASIZE] buf1,
                               reg const ptr u8[A2::_ASIZE] buf2,
                               reg const ptr u8[A2::_ASIZE] buf3,
                               reg u64 offset, inline int LEN,
                               inline int TRAILB) -> (reg mut ptr u256[25],
                                                     inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (4 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A2::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A2::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A2::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A2::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
      TRAILB = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */ 
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A2::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A2::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A2::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A2::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      reg u256 t256_0;
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_1;
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_2;
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_3;
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 =
        (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */ 
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */ 
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */ 
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */ 
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */ 
      at = (at +64u ((64u) 16)); /* u64 */ 
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A2::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A2::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A2::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A2::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
      TRAILB = 0; /* int:i */ 
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
  }
  return (st, ALL, offset);
}

inline
fn A2::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                             reg const ptr u8[A2::_ASIZE] buf0,
                             reg const ptr u8[A2::_ASIZE] buf1,
                             reg const ptr u8[A2::_ASIZE] buf2,
                             reg const ptr u8[A2::_ASIZE] buf3,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A2::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                  LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A2::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                    (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A2::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                    RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A2::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset, LEN,
                                  TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A2::__dumpstate_array_avx2x4 (reg mut ptr u8[A2::_ASIZE] buf0,
                                reg mut ptr u8[A2::_ASIZE] buf1,
                                reg mut ptr u8[A2::_ASIZE] buf2,
                                reg mut ptr u8[A2::_ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                reg const ptr u256[25] st) -> (reg mut ptr u8[A2::_ASIZE],
                                                              reg mut ptr u8[A2::_ASIZE],
                                                              reg mut ptr u8[A2::_ASIZE],
                                                              reg mut ptr u8[A2::_ASIZE],
                                                              reg u64)

{
  reg u64 i;
  i = ((64u) 0); /* u64 */ 
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */ 
    i = (i +64u ((64u) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */ 
    buf1.[#unaligned :u256 offset] = x1; /* u256 */ 
    buf2.[#unaligned :u256 offset] = x2; /* u256 */ 
    buf3.[#unaligned :u256 offset] = x3; /* u256 */ 
    offset = (offset +64u ((64u) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    buf0.[#unaligned :u64 offset] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    buf1.[#unaligned :u64 offset] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    buf2.[#unaligned :u64 offset] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    buf3.[#unaligned :u64 offset] = t3; /* u64 */ 
    i = (i +64u ((64u) 8)); /* u64 */ 
    offset = (offset +64u ((64u) 8)); /* u64 */ 
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A2::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A2::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A2::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A2::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A2::__squeeze_array_avx2x4 (reg mut ptr u8[A2::_ASIZE] buf0,
                              reg mut ptr u8[A2::_ASIZE] buf1,
                              reg mut ptr u8[A2::_ASIZE] buf2,
                              reg mut ptr u8[A2::_ASIZE] buf3,
                              reg u64 offset, inline int LEN,
                              reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A2::_ASIZE], reg mut ptr u8[A2::_ASIZE],
reg mut ptr u8[A2::_ASIZE], reg mut ptr u8[A2::_ASIZE], reg u64,
reg mut ptr u256[25])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A2::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, RATE8,
                                       st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A2::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A32::_ASIZE = 32;

inline
fn A32::__a_ilen_read_upto8_at (reg const ptr u8[A32::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN,
                               inline int TRAIL, inline int CUR,
                               inline int AT) -> (inline int, inline int,
                                                 inline int, inline int,
                                                 reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w =
        buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                             ((64u) DELTA))]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u)
          buf.[#unaligned :u32 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        DELTA = (DELTA + (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 =
          ((64u)
          buf.[#unaligned :u16 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u)
            buf.[#unaligned (((64u /* of ui64 */) offset) +64u ((64u) DELTA))]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A32::__a_ilen_read_upto16_at (reg const ptr u8[A32::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN,
                                inline int TRAIL, inline int CUR,
                                inline int AT) -> (inline int, inline int,
                                                  inline int, inline int,
                                                  reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w =
        buf.[#unaligned :u128 (((64u /* of ui64 */) offset) +64u
                              ((64u) DELTA))]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      DELTA = (DELTA + (16 - AT16)); /* int:i */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A32::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_0) =
          A32::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 0, AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A32::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A32::__a_ilen_read_upto32_at (reg const ptr u8[A32::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN,
                                inline int TRAIL, inline int CUR,
                                inline int AT) -> (inline int, inline int,
                                                  inline int, inline int,
                                                  reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w =
        buf.[#unaligned :u256 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A32::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                       AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_0) =
          A32::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                       AT32);
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A32::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                       AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A32::__a_ilen_read_bcast_upto8_at (reg const ptr u8[A32::_ASIZE] buf,
                                     #[Internal::wint::unsigned]
                                     reg ui64 offset, inline int DELTA,
                                     inline int LEN, inline int TRAIL,
                                     inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */)
                                               (offset +64ui
                                               ((64ui /* of int */) DELTA)))]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (DELTA, LEN, TRAIL, AT, w) =
        A32::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, CUR, AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w256);
}

inline
fn A32::__a_ilen_write_upto8 (reg mut ptr u8[A32::_ASIZE] buf,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A32::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 ((uint /* of ui64 */)
                           (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned ((uint /* of ui64 */)
                        (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A32::__a_ilen_write_upto16 (reg mut ptr u8[A32::_ASIZE] buf,
                              #[Internal::wint::unsigned] reg ui64 offset,
                              inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A32::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A32::__a_ilen_write_upto8(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A32::__a_ilen_write_upto32 (reg mut ptr u8[A32::_ASIZE] buf,
                              #[Internal::wint::unsigned] reg ui64 offset,
                              inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A32::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A32::__a_ilen_write_upto16(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A32::__a_rlen_read_upto8 (reg const ptr u8[A32::_ASIZE] a,
                            #[Internal::wint::unsigned] reg ui64 off,
                            #[Internal::wint::unsigned] reg ui64 len) -> 
(reg ui64, reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = a.[#unaligned :u64 ((uint /* of ui64 */) off)]; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) a.[#unaligned :u32 ((uint /* of ui64 */) off)]); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) a.[#unaligned :u16 ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) a.[#unaligned ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (off, w);
}

inline
fn A32::__a_rlen_write_upto8 (reg mut ptr u8[A32::_ASIZE] buf,
                             #[Internal::wint::unsigned] reg ui64 off,
                             reg u64 data, #[Internal::wint::unsigned]
                             reg ui64 len) -> (reg mut ptr u8[A32::_ASIZE],
                                              reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    buf.[#unaligned :u64 ((uint /* of ui64 */) off)] = data; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u32 ((uint /* of ui64 */) off)] = data; /* u32 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u16 ((uint /* of ui64 */) off)] = data; /* u16 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      buf.[#unaligned ((uint /* of ui64 */) off)] = data; /* u8 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, off);
}

inline
fn A32::__aread_subu64 (reg const ptr u8[A32::_ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64)

{
  inline int ILEN;
  ILEN = LEN; /* int:i */ 
  reg u64 w;
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */ 
    TRAIL = 0; /* int:i */ 
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      reg u64 t16;
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      } else {
        t16 = ((64u) 0); /* u64 */ 
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */ 
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          }
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */ 
        }
        TRAIL = 0; /* int:i */ 
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */ 
        t16 = (t16 |64u t8); /* u64 */ 
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */ 
      w = (w |64u t16); /* u64 */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A32::__aread_bcast_4subu64 (reg const ptr u8[A32::_ASIZE] buf,
                              reg u64 offset, inline int DELTA,
                              inline int LEN, inline int TRAIL) -> (inline int,
                                                                   inline int,
                                                                   inline int,
                                                                   reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      reg u64 t64;
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A32::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      reg u128 t128;
      t128 = ((128u) t64); /* u128 */ 
      w = #VPBROADCAST_4u64(t128); /*  */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A32::__aread_subu128 (reg const ptr u8[A32::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128)

{
  reg u128 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */ 
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      reg u64 t64;
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A32::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */ 
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A32::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A32::__aread_subu256 (reg const ptr u8[A32::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_0;
      reg u128 t128_1;
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A32::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      } else {
        t128_1 = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A32::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A32::__awrite_subu64 (reg mut ptr u8[A32::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A32::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A32::__awrite_subu128 (reg mut ptr u8[A32::_ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A32::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) = A32::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A32::__awrite_subu256 (reg mut ptr u8[A32::_ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A32::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A32::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A32::__addstate_avx2 (reg u256[7] st, inline int AT,
                        reg const ptr u8[A32::_ASIZE] buf,
                        #[Internal::wint::unsigned] reg ui64 offset,
                        inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                                inline int,
                                                                reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (AT < 8) {
    reg u256 r0;
    if ((AT == 0) && (8 <= _LEN)) {
      r0 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                                               ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      _LEN = (_LEN - 8); /* int:i */ 
      AT = 8; /* int:i */ 
    } else {
      reg u64 t64_1;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_1) =
        A32::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 0, AT);
      reg u128 t128_0;
      t128_0 = ((128u) t64_1); /* u128 */ 
      r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
    }
    st[0] = (st[0] ^256u r0); /* u256 */ 
  }
  if ((AT < 40) && ((0 < _LEN) || (_TRAILB != 0))) {
    reg u256 r1;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, r1) =
      A32::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 8, AT);
    st[1] = (st[1] ^256u r1); /* u256 */ 
  }
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u64 t64_2;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t64_2) =
      A32::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 40, AT);
    reg u128 t128_1;
    t128_1 = ((128u) t64_2); /* u128 */ 
    reg u128 t128_2;
    t128_2 = #set0_128(); /*  */ 
    if ((0 < _LEN) || (_TRAILB != 0)) {
      reg u256 r3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r3) =
        A32::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 48,
                                     AT);
      reg u64 t64_3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_3) =
        A32::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 80, AT);
      t128_2 = ((128u) t64_3); /* u128 */ 
      reg u256 r4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r4) =
        A32::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 88,
                                     AT);
      reg u64 t64_4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_4) =
        A32::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 120,
                                    AT);
      t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
      reg u256 r5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r5) =
        A32::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 128,
                                     AT);
      reg u64 t64_5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_5) =
        A32::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 160,
                                    AT);
      t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
      reg u256 r6;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r6) =
        A32::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 168,
                                     AT);
      #[inline]
      st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
    }
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (st, AT, offset);
}

inline
fn A32::__absorb_avx2 (reg u256[7] st, inline int AT,
                      reg const ptr u8[A32::_ASIZE] buf, inline int _TRAILB,
                      inline int _RATE8) -> (reg u256[7], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A32::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A32::__addstate_avx2(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A32::__addstate_avx2(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A32::__addstate_avx2(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A32::__dumpstate_avx2 (reg mut ptr u8[A32::_ASIZE] buf,
                         #[Internal::wint::unsigned] reg ui64 offset,
                         inline int _LEN, reg u256[7] st) -> (reg mut ptr u8[A32::_ASIZE],
                                                             reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= _LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A32::__a_ilen_write_upto32(buf, offset, DELTA, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, _LEN) =
      A32::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, _LEN) =
    A32::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, _LEN) =
      A32::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, _LEN) =
        A32::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A32::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A32::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A32::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A32::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A32::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A32::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
    }
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A32::__squeeze_avx2 (reg u256[7] st, reg mut ptr u8[A32::_ASIZE] buf,
                       inline int _RATE8) -> (reg u256[7],
                                             reg mut ptr u8[A32::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A32::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A32::__dumpstate_avx2(buf, offset, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A32::__dumpstate_avx2(buf, offset, LO, st);
  }
  return (st, buf);
}

inline
fn A32::__addstate_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A32::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t64;
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  reg u128 t128_0;
  t128_0 = ((128u) t64); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A32::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    reg u128 t128_1;
    t128_1 = ((128u) t64); /* u128 */ 
    reg u256 r3;
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A32::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */ 
    reg u256 r4;
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A32::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A32::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */ 
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
    reg u256 r6;
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A32::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (st, offset);
}

inline
fn A32::__absorb_array_avx2 (reg u256[7] st,
                            reg const ptr u8[A32::_ASIZE] buf,
                            reg u64 offset, inline int LEN, inline int RATE8,
                            inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */ 
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  if (0 < ITERS) {
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A32::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
  }
  LEN = (LEN % RATE8); /* int:i */ 
  #[inline]
  (st, offset) = A32::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A32::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                            reg const ptr u8[A32::_ASIZE] buf,
                            reg u64 offset, inline int LEN,
                            inline int TRAILB) -> (reg mut ptr u64[25],
                                                  inline int, reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (AT / 8)); /* u64 */ 
  reg u64 t64;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A32::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A32::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      at = (at +64u ((64u) 1)); /* u64 */ 
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      reg u256 t256;
      t256 = buf.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = (LEN % 32); /* int:i */ 
  }
  if (16 <= LEN) {
    reg u128 t128;
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
    DELTA = (DELTA + 16); /* int:i */ 
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */ 
    at = (at +64u ((64u) 2)); /* u64 */ 
    LEN = (LEN - 16); /* int:i */ 
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
    DELTA = (DELTA + 8); /* int:i */ 
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */ 
    at = (at +64u ((64u) 1)); /* u64 */ 
    LEN = (LEN - 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A32::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */ 
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (pst, ALL, offset);
}

inline
fn A32::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                             reg u256[7] st,
                             reg const ptr u8[A32::_ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  reg u64 i;
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A32::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */ 
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */ 
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A32::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A32::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A32::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A32::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A32::__dumpstate_array_avx2 (reg mut ptr u8[A32::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st) -> (reg mut ptr u8[A32::_ASIZE],
                                                  reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A32::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A32::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A32::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, LEN) = A32::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, LEN) =
        A32::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A32::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A32::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A32::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A32::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A32::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A32::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A32::__squeeze_array_avx2 (reg mut ptr u8[A32::_ASIZE] buf,
                             reg u64 offset, inline int LEN, reg u256[7] st,
                             inline int RATE8) -> (reg mut ptr u8[A32::_ASIZE],
                                                  reg u256[7])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A32::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A32::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A32::__addstate_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A32::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int _LEN, inline int _TRAILB) -> 
(reg mut ptr u256[25], inline int, reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u256 w;
  if ((AT8 % 8) != 0) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, w) =
      A32::__a_ilen_read_bcast_upto8_at(buf, offset, DELTA, _LEN, _TRAILB,
                                        AT, AT8);
    w = (w ^256u st[(AT / 8)]); /* u256 */ 
    st[(AT / 8)] = w; /* u256 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (32 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) (32 * ((AT / 8) + (_LEN / 8)))))) {
    w =
      #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */) offset)]); /*  */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    at = (at +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, w) =
      A32::__a_ilen_read_bcast_upto8_at(buf, offset, 0, _LEN, _TRAILB, AT, AT);
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A32::__absorb_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                              reg const ptr u8[A32::_ASIZE] buf,
                              inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A32::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A32::__addstate_bcast_avx2x4(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A32::__addstate_bcast_avx2x4(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A32::__addstate_bcast_avx2x4(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A32::__addstate_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                          reg const ptr u8[A32::_ASIZE] buf0,
                          reg const ptr u8[A32::_ASIZE] buf1,
                          reg const ptr u8[A32::_ASIZE] buf2,
                          reg const ptr u8[A32::_ASIZE] buf3,
                          #[Internal::wint::unsigned] reg ui64 offset,
                          inline int _LEN, inline int _TRAILB) -> (reg mut ptr u256[25],
                                                                  inline int,
                                                                  reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if ((AT8 % 8) != 0) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A32::__a_ilen_read_upto8_at(buf0, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 0)] =
      (st[:u64 ((4 * (AT / 8)) + 0)] ^64u t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A32::__a_ilen_read_upto8_at(buf1, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 1)] =
      (st[:u64 ((4 * (AT / 8)) + 1)] ^64u t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A32::__a_ilen_read_upto8_at(buf2, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 2)] =
      (st[:u64 ((4 * (AT / 8)) + 2)] ^64u t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, t3) =
      A32::__a_ilen_read_upto8_at(buf3, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 3)] =
      (st[:u64 ((4 * (AT / 8)) + 3)] ^64u t3); /* u64 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (4 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) ((4 * (AT / 8)) + (4 * (_LEN / 8)))))) {
    t0 = buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    t1 = buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    t2 = buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    t3 = buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    at = (at +64ui ((64ui /* of int */) 4)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A32::__a_ilen_read_upto8_at(buf0, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A32::__a_ilen_read_upto8_at(buf1, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A32::__a_ilen_read_upto8_at(buf2, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t3) =
      A32::__a_ilen_read_upto8_at(buf3, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A32::__absorb_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                        reg const ptr u8[A32::_ASIZE] buf0,
                        reg const ptr u8[A32::_ASIZE] buf1,
                        reg const ptr u8[A32::_ASIZE] buf2,
                        reg const ptr u8[A32::_ASIZE] buf3,
                        inline int _TRAILB, inline int _RATE8) -> (reg mut ptr u256[25],
                                                                  inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A32::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A32::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                             (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A32::__addstate_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset, _RATE8,
                               0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A32::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, _LEN,
                           _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A32::__dumpstate_avx2x4 (reg mut ptr u8[A32::_ASIZE] buf0,
                           reg mut ptr u8[A32::_ASIZE] buf1,
                           reg mut ptr u8[A32::_ASIZE] buf2,
                           reg mut ptr u8[A32::_ASIZE] buf3,
                           #[Internal::wint::unsigned] reg ui64 offset,
                           inline int _LEN, reg const ptr u256[25] st) -> 
(reg mut ptr u8[A32::_ASIZE], reg mut ptr u8[A32::_ASIZE],
reg mut ptr u8[A32::_ASIZE], reg mut ptr u8[A32::_ASIZE], reg ui64)

{
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * (_LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (0 * 32))))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (1 * 32))))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (2 * 32))))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (3 * 32))))]; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x0; /* u256 */ 
    buf1.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x1; /* u256 */ 
    buf2.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x2; /* u256 */ 
    buf3.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x3; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <64ui ((64ui /* of int */) (8 * (_LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t3; /* u64 */ 
    i = (i +64ui ((64ui /* of int */) 8)); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
  }
  if (0 < (_LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A32::__a_ilen_write_upto8(buf0, offset, 0, (_LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A32::__a_ilen_write_upto8(buf1, offset, 0, (_LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A32::__a_ilen_write_upto8(buf2, offset, 0, (_LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A32::__a_ilen_write_upto8(buf3, offset, 0, (_LEN % 8), t3);
    offset = (offset +64ui ((64ui /* of int */) (_LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A32::__squeeze_avx2x4 (reg mut ptr u256[25] st,
                         reg mut ptr u8[A32::_ASIZE] buf0,
                         reg mut ptr u8[A32::_ASIZE] buf1,
                         reg mut ptr u8[A32::_ASIZE] buf2,
                         reg mut ptr u8[A32::_ASIZE] buf3, inline int _RATE8) -> 
(reg mut ptr u256[25], reg mut ptr u8[A32::_ASIZE],
reg mut ptr u8[A32::_ASIZE], reg mut ptr u8[A32::_ASIZE],
reg mut ptr u8[A32::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A32::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  if (0 < ITERS) {
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A32::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, _RATE8, st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  if (0 < LO) {
    st = _keccakf1600_avx2x4(st);
    #[inline]
    (buf0, buf1, buf2, buf3, offset) =
      A32::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
  }
  return (st, buf0, buf1, buf2, buf3);
}

inline
fn A32::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A32::_ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int TRAILB) -> (reg mut ptr u256[25],
                                                            inline int,
                                                            reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (32 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u256 t256;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A32::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A32::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A32::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
    st.[#unaligned at] = t256; /* u256 */ 
  }
  return (st, ALL, offset);
}

inline
fn A32::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                    reg const ptr u8[A32::_ASIZE] buf,
                                    reg u64 offset, inline int LEN,
                                    inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A32::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A32::__addstate_bcast_array_avx2x4(st, AT, buf, offset, (RATE8 - AT),
                                           0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A32::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A32::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A32::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A32::_ASIZE] buf0,
                                reg const ptr u8[A32::_ASIZE] buf1,
                                reg const ptr u8[A32::_ASIZE] buf2,
                                reg const ptr u8[A32::_ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg mut ptr u256[25],
                                                      inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (4 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A32::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A32::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A32::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A32::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
      TRAILB = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */ 
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A32::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A32::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A32::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A32::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      reg u256 t256_0;
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_1;
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_2;
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_3;
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 =
        (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */ 
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */ 
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */ 
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */ 
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */ 
      at = (at +64u ((64u) 16)); /* u64 */ 
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A32::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A32::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A32::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A32::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
      TRAILB = 0; /* int:i */ 
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
  }
  return (st, ALL, offset);
}

inline
fn A32::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                              reg const ptr u8[A32::_ASIZE] buf0,
                              reg const ptr u8[A32::_ASIZE] buf1,
                              reg const ptr u8[A32::_ASIZE] buf2,
                              reg const ptr u8[A32::_ASIZE] buf3,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A32::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                   LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A32::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A32::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A32::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                   LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A32::__dumpstate_array_avx2x4 (reg mut ptr u8[A32::_ASIZE] buf0,
                                 reg mut ptr u8[A32::_ASIZE] buf1,
                                 reg mut ptr u8[A32::_ASIZE] buf2,
                                 reg mut ptr u8[A32::_ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg const ptr u256[25] st) -> (reg mut ptr u8[A32::_ASIZE],
                                                               reg mut ptr u8[A32::_ASIZE],
                                                               reg mut ptr u8[A32::_ASIZE],
                                                               reg mut ptr u8[A32::_ASIZE],
                                                               reg u64)

{
  reg u64 i;
  i = ((64u) 0); /* u64 */ 
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */ 
    i = (i +64u ((64u) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */ 
    buf1.[#unaligned :u256 offset] = x1; /* u256 */ 
    buf2.[#unaligned :u256 offset] = x2; /* u256 */ 
    buf3.[#unaligned :u256 offset] = x3; /* u256 */ 
    offset = (offset +64u ((64u) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    buf0.[#unaligned :u64 offset] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    buf1.[#unaligned :u64 offset] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    buf2.[#unaligned :u64 offset] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    buf3.[#unaligned :u64 offset] = t3; /* u64 */ 
    i = (i +64u ((64u) 8)); /* u64 */ 
    offset = (offset +64u ((64u) 8)); /* u64 */ 
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A32::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A32::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A32::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A32::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A32::__squeeze_array_avx2x4 (reg mut ptr u8[A32::_ASIZE] buf0,
                               reg mut ptr u8[A32::_ASIZE] buf1,
                               reg mut ptr u8[A32::_ASIZE] buf2,
                               reg mut ptr u8[A32::_ASIZE] buf3,
                               reg u64 offset, inline int LEN,
                               reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A32::_ASIZE], reg mut ptr u8[A32::_ASIZE],
reg mut ptr u8[A32::_ASIZE], reg mut ptr u8[A32::_ASIZE], reg u64,
reg mut ptr u256[25])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A32::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                        RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A32::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A33::_ASIZE = 33;

inline
fn A33::__a_ilen_read_upto8_at (reg const ptr u8[A33::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN,
                               inline int TRAIL, inline int CUR,
                               inline int AT) -> (inline int, inline int,
                                                 inline int, inline int,
                                                 reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w =
        buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                             ((64u) DELTA))]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u)
          buf.[#unaligned :u32 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        DELTA = (DELTA + (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 =
          ((64u)
          buf.[#unaligned :u16 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u)
            buf.[#unaligned (((64u /* of ui64 */) offset) +64u ((64u) DELTA))]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A33::__a_ilen_read_upto16_at (reg const ptr u8[A33::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN,
                                inline int TRAIL, inline int CUR,
                                inline int AT) -> (inline int, inline int,
                                                  inline int, inline int,
                                                  reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w =
        buf.[#unaligned :u128 (((64u /* of ui64 */) offset) +64u
                              ((64u) DELTA))]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      DELTA = (DELTA + (16 - AT16)); /* int:i */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A33::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_0) =
          A33::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 0, AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A33::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A33::__a_ilen_read_upto32_at (reg const ptr u8[A33::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN,
                                inline int TRAIL, inline int CUR,
                                inline int AT) -> (inline int, inline int,
                                                  inline int, inline int,
                                                  reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w =
        buf.[#unaligned :u256 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A33::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                       AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_0) =
          A33::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                       AT32);
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A33::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                       AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A33::__a_ilen_read_bcast_upto8_at (reg const ptr u8[A33::_ASIZE] buf,
                                     #[Internal::wint::unsigned]
                                     reg ui64 offset, inline int DELTA,
                                     inline int LEN, inline int TRAIL,
                                     inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */)
                                               (offset +64ui
                                               ((64ui /* of int */) DELTA)))]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (DELTA, LEN, TRAIL, AT, w) =
        A33::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, CUR, AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w256);
}

inline
fn A33::__a_ilen_write_upto8 (reg mut ptr u8[A33::_ASIZE] buf,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A33::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 ((uint /* of ui64 */)
                           (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned ((uint /* of ui64 */)
                        (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A33::__a_ilen_write_upto16 (reg mut ptr u8[A33::_ASIZE] buf,
                              #[Internal::wint::unsigned] reg ui64 offset,
                              inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A33::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A33::__a_ilen_write_upto8(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A33::__a_ilen_write_upto32 (reg mut ptr u8[A33::_ASIZE] buf,
                              #[Internal::wint::unsigned] reg ui64 offset,
                              inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A33::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A33::__a_ilen_write_upto16(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A33::__a_rlen_read_upto8 (reg const ptr u8[A33::_ASIZE] a,
                            #[Internal::wint::unsigned] reg ui64 off,
                            #[Internal::wint::unsigned] reg ui64 len) -> 
(reg ui64, reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = a.[#unaligned :u64 ((uint /* of ui64 */) off)]; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) a.[#unaligned :u32 ((uint /* of ui64 */) off)]); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) a.[#unaligned :u16 ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) a.[#unaligned ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (off, w);
}

inline
fn A33::__a_rlen_write_upto8 (reg mut ptr u8[A33::_ASIZE] buf,
                             #[Internal::wint::unsigned] reg ui64 off,
                             reg u64 data, #[Internal::wint::unsigned]
                             reg ui64 len) -> (reg mut ptr u8[A33::_ASIZE],
                                              reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    buf.[#unaligned :u64 ((uint /* of ui64 */) off)] = data; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u32 ((uint /* of ui64 */) off)] = data; /* u32 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u16 ((uint /* of ui64 */) off)] = data; /* u16 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      buf.[#unaligned ((uint /* of ui64 */) off)] = data; /* u8 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, off);
}

inline
fn A33::__aread_subu64 (reg const ptr u8[A33::_ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64)

{
  inline int ILEN;
  ILEN = LEN; /* int:i */ 
  reg u64 w;
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */ 
    TRAIL = 0; /* int:i */ 
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      reg u64 t16;
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      } else {
        t16 = ((64u) 0); /* u64 */ 
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */ 
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          }
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */ 
        }
        TRAIL = 0; /* int:i */ 
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */ 
        t16 = (t16 |64u t8); /* u64 */ 
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */ 
      w = (w |64u t16); /* u64 */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A33::__aread_bcast_4subu64 (reg const ptr u8[A33::_ASIZE] buf,
                              reg u64 offset, inline int DELTA,
                              inline int LEN, inline int TRAIL) -> (inline int,
                                                                   inline int,
                                                                   inline int,
                                                                   reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      reg u64 t64;
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A33::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      reg u128 t128;
      t128 = ((128u) t64); /* u128 */ 
      w = #VPBROADCAST_4u64(t128); /*  */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A33::__aread_subu128 (reg const ptr u8[A33::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128)

{
  reg u128 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */ 
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      reg u64 t64;
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A33::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */ 
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A33::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A33::__aread_subu256 (reg const ptr u8[A33::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_0;
      reg u128 t128_1;
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A33::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      } else {
        t128_1 = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A33::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A33::__awrite_subu64 (reg mut ptr u8[A33::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A33::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A33::__awrite_subu128 (reg mut ptr u8[A33::_ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A33::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) = A33::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A33::__awrite_subu256 (reg mut ptr u8[A33::_ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A33::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A33::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A33::__addstate_avx2 (reg u256[7] st, inline int AT,
                        reg const ptr u8[A33::_ASIZE] buf,
                        #[Internal::wint::unsigned] reg ui64 offset,
                        inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                                inline int,
                                                                reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (AT < 8) {
    reg u256 r0;
    if ((AT == 0) && (8 <= _LEN)) {
      r0 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                                               ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      _LEN = (_LEN - 8); /* int:i */ 
      AT = 8; /* int:i */ 
    } else {
      reg u64 t64_1;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_1) =
        A33::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 0, AT);
      reg u128 t128_0;
      t128_0 = ((128u) t64_1); /* u128 */ 
      r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
    }
    st[0] = (st[0] ^256u r0); /* u256 */ 
  }
  if ((AT < 40) && ((0 < _LEN) || (_TRAILB != 0))) {
    reg u256 r1;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, r1) =
      A33::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 8, AT);
    st[1] = (st[1] ^256u r1); /* u256 */ 
  }
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u64 t64_2;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t64_2) =
      A33::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 40, AT);
    reg u128 t128_1;
    t128_1 = ((128u) t64_2); /* u128 */ 
    reg u128 t128_2;
    t128_2 = #set0_128(); /*  */ 
    if ((0 < _LEN) || (_TRAILB != 0)) {
      reg u256 r3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r3) =
        A33::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 48,
                                     AT);
      reg u64 t64_3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_3) =
        A33::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 80, AT);
      t128_2 = ((128u) t64_3); /* u128 */ 
      reg u256 r4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r4) =
        A33::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 88,
                                     AT);
      reg u64 t64_4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_4) =
        A33::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 120,
                                    AT);
      t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
      reg u256 r5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r5) =
        A33::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 128,
                                     AT);
      reg u64 t64_5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_5) =
        A33::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 160,
                                    AT);
      t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
      reg u256 r6;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r6) =
        A33::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 168,
                                     AT);
      #[inline]
      st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
    }
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (st, AT, offset);
}

inline
fn A33::__absorb_avx2 (reg u256[7] st, inline int AT,
                      reg const ptr u8[A33::_ASIZE] buf, inline int _TRAILB,
                      inline int _RATE8) -> (reg u256[7], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A33::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A33::__addstate_avx2(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A33::__addstate_avx2(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A33::__addstate_avx2(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A33::__dumpstate_avx2 (reg mut ptr u8[A33::_ASIZE] buf,
                         #[Internal::wint::unsigned] reg ui64 offset,
                         inline int _LEN, reg u256[7] st) -> (reg mut ptr u8[A33::_ASIZE],
                                                             reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= _LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A33::__a_ilen_write_upto32(buf, offset, DELTA, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, _LEN) =
      A33::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, _LEN) =
    A33::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, _LEN) =
      A33::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, _LEN) =
        A33::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A33::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A33::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A33::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A33::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A33::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A33::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
    }
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A33::__squeeze_avx2 (reg u256[7] st, reg mut ptr u8[A33::_ASIZE] buf,
                       inline int _RATE8) -> (reg u256[7],
                                             reg mut ptr u8[A33::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A33::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A33::__dumpstate_avx2(buf, offset, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A33::__dumpstate_avx2(buf, offset, LO, st);
  }
  return (st, buf);
}

inline
fn A33::__addstate_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A33::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t64;
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  reg u128 t128_0;
  t128_0 = ((128u) t64); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A33::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    reg u128 t128_1;
    t128_1 = ((128u) t64); /* u128 */ 
    reg u256 r3;
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A33::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */ 
    reg u256 r4;
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A33::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A33::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */ 
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
    reg u256 r6;
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A33::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (st, offset);
}

inline
fn A33::__absorb_array_avx2 (reg u256[7] st,
                            reg const ptr u8[A33::_ASIZE] buf,
                            reg u64 offset, inline int LEN, inline int RATE8,
                            inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */ 
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  if (0 < ITERS) {
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A33::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
  }
  LEN = (LEN % RATE8); /* int:i */ 
  #[inline]
  (st, offset) = A33::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A33::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                            reg const ptr u8[A33::_ASIZE] buf,
                            reg u64 offset, inline int LEN,
                            inline int TRAILB) -> (reg mut ptr u64[25],
                                                  inline int, reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (AT / 8)); /* u64 */ 
  reg u64 t64;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A33::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A33::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      at = (at +64u ((64u) 1)); /* u64 */ 
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      reg u256 t256;
      t256 = buf.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = (LEN % 32); /* int:i */ 
  }
  if (16 <= LEN) {
    reg u128 t128;
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
    DELTA = (DELTA + 16); /* int:i */ 
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */ 
    at = (at +64u ((64u) 2)); /* u64 */ 
    LEN = (LEN - 16); /* int:i */ 
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
    DELTA = (DELTA + 8); /* int:i */ 
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */ 
    at = (at +64u ((64u) 1)); /* u64 */ 
    LEN = (LEN - 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A33::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */ 
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (pst, ALL, offset);
}

inline
fn A33::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                             reg u256[7] st,
                             reg const ptr u8[A33::_ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  reg u64 i;
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A33::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */ 
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */ 
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A33::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A33::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A33::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A33::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A33::__dumpstate_array_avx2 (reg mut ptr u8[A33::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st) -> (reg mut ptr u8[A33::_ASIZE],
                                                  reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A33::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A33::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A33::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, LEN) = A33::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, LEN) =
        A33::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A33::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A33::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A33::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A33::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A33::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A33::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A33::__squeeze_array_avx2 (reg mut ptr u8[A33::_ASIZE] buf,
                             reg u64 offset, inline int LEN, reg u256[7] st,
                             inline int RATE8) -> (reg mut ptr u8[A33::_ASIZE],
                                                  reg u256[7])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A33::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A33::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A33::__addstate_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A33::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int _LEN, inline int _TRAILB) -> 
(reg mut ptr u256[25], inline int, reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u256 w;
  if ((AT8 % 8) != 0) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, w) =
      A33::__a_ilen_read_bcast_upto8_at(buf, offset, DELTA, _LEN, _TRAILB,
                                        AT, AT8);
    w = (w ^256u st[(AT / 8)]); /* u256 */ 
    st[(AT / 8)] = w; /* u256 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (32 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) (32 * ((AT / 8) + (_LEN / 8)))))) {
    w =
      #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */) offset)]); /*  */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    at = (at +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, w) =
      A33::__a_ilen_read_bcast_upto8_at(buf, offset, 0, _LEN, _TRAILB, AT, AT);
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A33::__absorb_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                              reg const ptr u8[A33::_ASIZE] buf,
                              inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A33::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A33::__addstate_bcast_avx2x4(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A33::__addstate_bcast_avx2x4(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A33::__addstate_bcast_avx2x4(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A33::__addstate_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                          reg const ptr u8[A33::_ASIZE] buf0,
                          reg const ptr u8[A33::_ASIZE] buf1,
                          reg const ptr u8[A33::_ASIZE] buf2,
                          reg const ptr u8[A33::_ASIZE] buf3,
                          #[Internal::wint::unsigned] reg ui64 offset,
                          inline int _LEN, inline int _TRAILB) -> (reg mut ptr u256[25],
                                                                  inline int,
                                                                  reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if ((AT8 % 8) != 0) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A33::__a_ilen_read_upto8_at(buf0, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 0)] =
      (st[:u64 ((4 * (AT / 8)) + 0)] ^64u t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A33::__a_ilen_read_upto8_at(buf1, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 1)] =
      (st[:u64 ((4 * (AT / 8)) + 1)] ^64u t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A33::__a_ilen_read_upto8_at(buf2, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 2)] =
      (st[:u64 ((4 * (AT / 8)) + 2)] ^64u t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, t3) =
      A33::__a_ilen_read_upto8_at(buf3, offset, DELTA, _LEN, _TRAILB, AT, AT8);
    st[:u64 ((4 * (AT / 8)) + 3)] =
      (st[:u64 ((4 * (AT / 8)) + 3)] ^64u t3); /* u64 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (4 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) ((4 * (AT / 8)) + (4 * (_LEN / 8)))))) {
    t0 = buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    t1 = buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    t2 = buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    t3 = buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    at = (at +64ui ((64ui /* of int */) 4)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A33::__a_ilen_read_upto8_at(buf0, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A33::__a_ilen_read_upto8_at(buf1, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A33::__a_ilen_read_upto8_at(buf2, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t3) =
      A33::__a_ilen_read_upto8_at(buf3, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A33::__absorb_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                        reg const ptr u8[A33::_ASIZE] buf0,
                        reg const ptr u8[A33::_ASIZE] buf1,
                        reg const ptr u8[A33::_ASIZE] buf2,
                        reg const ptr u8[A33::_ASIZE] buf3,
                        inline int _TRAILB, inline int _RATE8) -> (reg mut ptr u256[25],
                                                                  inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A33::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A33::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                             (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A33::__addstate_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset, _RATE8,
                               0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A33::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, _LEN,
                           _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A33::__dumpstate_avx2x4 (reg mut ptr u8[A33::_ASIZE] buf0,
                           reg mut ptr u8[A33::_ASIZE] buf1,
                           reg mut ptr u8[A33::_ASIZE] buf2,
                           reg mut ptr u8[A33::_ASIZE] buf3,
                           #[Internal::wint::unsigned] reg ui64 offset,
                           inline int _LEN, reg const ptr u256[25] st) -> 
(reg mut ptr u8[A33::_ASIZE], reg mut ptr u8[A33::_ASIZE],
reg mut ptr u8[A33::_ASIZE], reg mut ptr u8[A33::_ASIZE], reg ui64)

{
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * (_LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (0 * 32))))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (1 * 32))))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (2 * 32))))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (3 * 32))))]; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x0; /* u256 */ 
    buf1.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x1; /* u256 */ 
    buf2.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x2; /* u256 */ 
    buf3.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x3; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <64ui ((64ui /* of int */) (8 * (_LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t3; /* u64 */ 
    i = (i +64ui ((64ui /* of int */) 8)); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
  }
  if (0 < (_LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A33::__a_ilen_write_upto8(buf0, offset, 0, (_LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A33::__a_ilen_write_upto8(buf1, offset, 0, (_LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A33::__a_ilen_write_upto8(buf2, offset, 0, (_LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A33::__a_ilen_write_upto8(buf3, offset, 0, (_LEN % 8), t3);
    offset = (offset +64ui ((64ui /* of int */) (_LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A33::__squeeze_avx2x4 (reg mut ptr u256[25] st,
                         reg mut ptr u8[A33::_ASIZE] buf0,
                         reg mut ptr u8[A33::_ASIZE] buf1,
                         reg mut ptr u8[A33::_ASIZE] buf2,
                         reg mut ptr u8[A33::_ASIZE] buf3, inline int _RATE8) -> 
(reg mut ptr u256[25], reg mut ptr u8[A33::_ASIZE],
reg mut ptr u8[A33::_ASIZE], reg mut ptr u8[A33::_ASIZE],
reg mut ptr u8[A33::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A33::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  if (0 < ITERS) {
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A33::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, _RATE8, st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  if (0 < LO) {
    st = _keccakf1600_avx2x4(st);
    #[inline]
    (buf0, buf1, buf2, buf3, offset) =
      A33::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
  }
  return (st, buf0, buf1, buf2, buf3);
}

inline
fn A33::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A33::_ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int TRAILB) -> (reg mut ptr u256[25],
                                                            inline int,
                                                            reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (32 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u256 t256;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A33::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A33::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A33::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
    st.[#unaligned at] = t256; /* u256 */ 
  }
  return (st, ALL, offset);
}

inline
fn A33::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                    reg const ptr u8[A33::_ASIZE] buf,
                                    reg u64 offset, inline int LEN,
                                    inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A33::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A33::__addstate_bcast_array_avx2x4(st, AT, buf, offset, (RATE8 - AT),
                                           0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A33::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A33::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A33::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A33::_ASIZE] buf0,
                                reg const ptr u8[A33::_ASIZE] buf1,
                                reg const ptr u8[A33::_ASIZE] buf2,
                                reg const ptr u8[A33::_ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg mut ptr u256[25],
                                                      inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (4 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A33::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A33::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A33::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A33::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
      TRAILB = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */ 
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A33::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A33::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A33::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A33::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      reg u256 t256_0;
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_1;
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_2;
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_3;
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 =
        (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */ 
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */ 
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */ 
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */ 
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */ 
      at = (at +64u ((64u) 16)); /* u64 */ 
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A33::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A33::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A33::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A33::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
      TRAILB = 0; /* int:i */ 
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
  }
  return (st, ALL, offset);
}

inline
fn A33::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                              reg const ptr u8[A33::_ASIZE] buf0,
                              reg const ptr u8[A33::_ASIZE] buf1,
                              reg const ptr u8[A33::_ASIZE] buf2,
                              reg const ptr u8[A33::_ASIZE] buf3,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A33::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                   LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A33::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A33::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A33::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                   LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A33::__dumpstate_array_avx2x4 (reg mut ptr u8[A33::_ASIZE] buf0,
                                 reg mut ptr u8[A33::_ASIZE] buf1,
                                 reg mut ptr u8[A33::_ASIZE] buf2,
                                 reg mut ptr u8[A33::_ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg const ptr u256[25] st) -> (reg mut ptr u8[A33::_ASIZE],
                                                               reg mut ptr u8[A33::_ASIZE],
                                                               reg mut ptr u8[A33::_ASIZE],
                                                               reg mut ptr u8[A33::_ASIZE],
                                                               reg u64)

{
  reg u64 i;
  i = ((64u) 0); /* u64 */ 
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */ 
    i = (i +64u ((64u) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */ 
    buf1.[#unaligned :u256 offset] = x1; /* u256 */ 
    buf2.[#unaligned :u256 offset] = x2; /* u256 */ 
    buf3.[#unaligned :u256 offset] = x3; /* u256 */ 
    offset = (offset +64u ((64u) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    buf0.[#unaligned :u64 offset] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    buf1.[#unaligned :u64 offset] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    buf2.[#unaligned :u64 offset] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    buf3.[#unaligned :u64 offset] = t3; /* u64 */ 
    i = (i +64u ((64u) 8)); /* u64 */ 
    offset = (offset +64u ((64u) 8)); /* u64 */ 
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A33::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A33::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A33::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A33::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A33::__squeeze_array_avx2x4 (reg mut ptr u8[A33::_ASIZE] buf0,
                               reg mut ptr u8[A33::_ASIZE] buf1,
                               reg mut ptr u8[A33::_ASIZE] buf2,
                               reg mut ptr u8[A33::_ASIZE] buf3,
                               reg u64 offset, inline int LEN,
                               reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A33::_ASIZE], reg mut ptr u8[A33::_ASIZE],
reg mut ptr u8[A33::_ASIZE], reg mut ptr u8[A33::_ASIZE], reg u64,
reg mut ptr u256[25])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A33::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                        RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A33::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A64::_ASIZE = 64;

inline
fn A64::__a_ilen_read_upto8_at (reg const ptr u8[A64::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN,
                               inline int TRAIL, inline int CUR,
                               inline int AT) -> (inline int, inline int,
                                                 inline int, inline int,
                                                 reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w =
        buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                             ((64u) DELTA))]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u)
          buf.[#unaligned :u32 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        DELTA = (DELTA + (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 =
          ((64u)
          buf.[#unaligned :u16 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u)
            buf.[#unaligned (((64u /* of ui64 */) offset) +64u ((64u) DELTA))]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A64::__a_ilen_read_upto16_at (reg const ptr u8[A64::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN,
                                inline int TRAIL, inline int CUR,
                                inline int AT) -> (inline int, inline int,
                                                  inline int, inline int,
                                                  reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w =
        buf.[#unaligned :u128 (((64u /* of ui64 */) offset) +64u
                              ((64u) DELTA))]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      DELTA = (DELTA + (16 - AT16)); /* int:i */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A64::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_0) =
          A64::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 0, AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A64::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8, AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A64::__a_ilen_read_upto32_at (reg const ptr u8[A64::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN,
                                inline int TRAIL, inline int CUR,
                                inline int AT) -> (inline int, inline int,
                                                  inline int, inline int,
                                                  reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w =
        buf.[#unaligned :u256 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A64::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                       AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_0) =
          A64::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                       AT32);
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A64::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                       AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A64::__a_ilen_read_bcast_upto8_at (reg const ptr u8[A64::_ASIZE] buf,
                                     #[Internal::wint::unsigned]
                                     reg ui64 offset, inline int DELTA,
                                     inline int LEN, inline int TRAIL,
                                     inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */)
                                               (offset +64ui
                                               ((64ui /* of int */) DELTA)))]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (DELTA, LEN, TRAIL, AT, w) =
        A64::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, CUR, AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w256);
}

inline
fn A64::__a_ilen_write_upto8 (reg mut ptr u8[A64::_ASIZE] buf,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A64::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 ((uint /* of ui64 */)
                           (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned ((uint /* of ui64 */)
                        (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A64::__a_ilen_write_upto16 (reg mut ptr u8[A64::_ASIZE] buf,
                              #[Internal::wint::unsigned] reg ui64 offset,
                              inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A64::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A64::__a_ilen_write_upto8(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A64::__a_ilen_write_upto32 (reg mut ptr u8[A64::_ASIZE] buf,
                              #[Internal::wint::unsigned] reg ui64 offset,
                              inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A64::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A64::__a_ilen_write_upto16(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A64::__a_rlen_read_upto8 (reg const ptr u8[A64::_ASIZE] a,
                            #[Internal::wint::unsigned] reg ui64 off,
                            #[Internal::wint::unsigned] reg ui64 len) -> 
(reg ui64, reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = a.[#unaligned :u64 ((uint /* of ui64 */) off)]; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) a.[#unaligned :u32 ((uint /* of ui64 */) off)]); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) a.[#unaligned :u16 ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) a.[#unaligned ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (off, w);
}

inline
fn A64::__a_rlen_write_upto8 (reg mut ptr u8[A64::_ASIZE] buf,
                             #[Internal::wint::unsigned] reg ui64 off,
                             reg u64 data, #[Internal::wint::unsigned]
                             reg ui64 len) -> (reg mut ptr u8[A64::_ASIZE],
                                              reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    buf.[#unaligned :u64 ((uint /* of ui64 */) off)] = data; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u32 ((uint /* of ui64 */) off)] = data; /* u32 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u16 ((uint /* of ui64 */) off)] = data; /* u16 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      buf.[#unaligned ((uint /* of ui64 */) off)] = data; /* u8 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, off);
}

inline
fn A64::__aread_subu64 (reg const ptr u8[A64::_ASIZE] buf, reg u64 offset,
                       inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64)

{
  inline int ILEN;
  ILEN = LEN; /* int:i */ 
  reg u64 w;
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */ 
    TRAIL = 0; /* int:i */ 
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      reg u64 t16;
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      } else {
        t16 = ((64u) 0); /* u64 */ 
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */ 
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          }
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */ 
        }
        TRAIL = 0; /* int:i */ 
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */ 
        t16 = (t16 |64u t8); /* u64 */ 
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */ 
      w = (w |64u t16); /* u64 */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A64::__aread_bcast_4subu64 (reg const ptr u8[A64::_ASIZE] buf,
                              reg u64 offset, inline int DELTA,
                              inline int LEN, inline int TRAIL) -> (inline int,
                                                                   inline int,
                                                                   inline int,
                                                                   reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      reg u64 t64;
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A64::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      reg u128 t128;
      t128 = ((128u) t64); /* u128 */ 
      w = #VPBROADCAST_4u64(t128); /*  */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A64::__aread_subu128 (reg const ptr u8[A64::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128)

{
  reg u128 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */ 
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      reg u64 t64;
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A64::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */ 
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A64::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A64::__aread_subu256 (reg const ptr u8[A64::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_0;
      reg u128 t128_1;
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A64::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      } else {
        t128_1 = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A64::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A64::__awrite_subu64 (reg mut ptr u8[A64::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A64::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A64::__awrite_subu128 (reg mut ptr u8[A64::_ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A64::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) = A64::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A64::__awrite_subu256 (reg mut ptr u8[A64::_ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A64::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A64::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A64::__addstate_avx2 (reg u256[7] st, inline int AT,
                        reg const ptr u8[A64::_ASIZE] buf,
                        #[Internal::wint::unsigned] reg ui64 offset,
                        inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                                inline int,
                                                                reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (AT < 8) {
    reg u256 r0;
    if ((AT == 0) && (8 <= _LEN)) {
      r0 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                                               ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      _LEN = (_LEN - 8); /* int:i */ 
      AT = 8; /* int:i */ 
    } else {
      reg u64 t64_1;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_1) =
        A64::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 0, AT);
      reg u128 t128_0;
      t128_0 = ((128u) t64_1); /* u128 */ 
      r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
    }
    st[0] = (st[0] ^256u r0); /* u256 */ 
  }
  if ((AT < 40) && ((0 < _LEN) || (_TRAILB != 0))) {
    reg u256 r1;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, r1) =
      A64::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 8, AT);
    st[1] = (st[1] ^256u r1); /* u256 */ 
  }
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u64 t64_2;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t64_2) =
      A64::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 40, AT);
    reg u128 t128_1;
    t128_1 = ((128u) t64_2); /* u128 */ 
    reg u128 t128_2;
    t128_2 = #set0_128(); /*  */ 
    if ((0 < _LEN) || (_TRAILB != 0)) {
      reg u256 r3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r3) =
        A64::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 48,
                                     AT);
      reg u64 t64_3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_3) =
        A64::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 80, AT);
      t128_2 = ((128u) t64_3); /* u128 */ 
      reg u256 r4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r4) =
        A64::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 88,
                                     AT);
      reg u64 t64_4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_4) =
        A64::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 120,
                                    AT);
      t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
      reg u256 r5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r5) =
        A64::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 128,
                                     AT);
      reg u64 t64_5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_5) =
        A64::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 160,
                                    AT);
      t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
      reg u256 r6;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r6) =
        A64::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 168,
                                     AT);
      #[inline]
      st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
    }
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (st, AT, offset);
}

inline
fn A64::__absorb_avx2 (reg u256[7] st, inline int AT,
                      reg const ptr u8[A64::_ASIZE] buf, inline int _TRAILB,
                      inline int _RATE8) -> (reg u256[7], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A64::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A64::__addstate_avx2(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A64::__addstate_avx2(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A64::__addstate_avx2(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A64::__dumpstate_avx2 (reg mut ptr u8[A64::_ASIZE] buf,
                         #[Internal::wint::unsigned] reg ui64 offset,
                         inline int _LEN, reg u256[7] st) -> (reg mut ptr u8[A64::_ASIZE],
                                                             reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= _LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A64::__a_ilen_write_upto32(buf, offset, DELTA, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, _LEN) =
      A64::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, _LEN) =
    A64::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, _LEN) =
      A64::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, _LEN) =
        A64::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A64::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A64::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A64::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A64::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A64::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A64::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
    }
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A64::__squeeze_avx2 (reg u256[7] st, reg mut ptr u8[A64::_ASIZE] buf,
                       inline int _RATE8) -> (reg u256[7],
                                             reg mut ptr u8[A64::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A64::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A64::__dumpstate_avx2(buf, offset, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A64::__dumpstate_avx2(buf, offset, LO, st);
  }
  return (st, buf);
}

inline
fn A64::__addstate_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A64::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t64;
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  reg u128 t128_0;
  t128_0 = ((128u) t64); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A64::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    reg u128 t128_1;
    t128_1 = ((128u) t64); /* u128 */ 
    reg u256 r3;
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A64::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */ 
    reg u256 r4;
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A64::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A64::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */ 
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
    reg u256 r6;
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A64::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (st, offset);
}

inline
fn A64::__absorb_array_avx2 (reg u256[7] st,
                            reg const ptr u8[A64::_ASIZE] buf,
                            reg u64 offset, inline int LEN, inline int RATE8,
                            inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */ 
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  if (0 < ITERS) {
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A64::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
  }
  LEN = (LEN % RATE8); /* int:i */ 
  #[inline]
  (st, offset) = A64::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A64::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                            reg const ptr u8[A64::_ASIZE] buf,
                            reg u64 offset, inline int LEN,
                            inline int TRAILB) -> (reg mut ptr u64[25],
                                                  inline int, reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (AT / 8)); /* u64 */ 
  reg u64 t64;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A64::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A64::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      at = (at +64u ((64u) 1)); /* u64 */ 
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      reg u256 t256;
      t256 = buf.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = (LEN % 32); /* int:i */ 
  }
  if (16 <= LEN) {
    reg u128 t128;
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
    DELTA = (DELTA + 16); /* int:i */ 
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */ 
    at = (at +64u ((64u) 2)); /* u64 */ 
    LEN = (LEN - 16); /* int:i */ 
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
    DELTA = (DELTA + 8); /* int:i */ 
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */ 
    at = (at +64u ((64u) 1)); /* u64 */ 
    LEN = (LEN - 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A64::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */ 
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (pst, ALL, offset);
}

inline
fn A64::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                             reg u256[7] st,
                             reg const ptr u8[A64::_ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  reg u64 i;
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A64::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */ 
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */ 
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A64::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A64::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A64::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A64::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A64::__dumpstate_array_avx2 (reg mut ptr u8[A64::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st) -> (reg mut ptr u8[A64::_ASIZE],
                                                  reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A64::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A64::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A64::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, LEN) = A64::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, LEN) =
        A64::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A64::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A64::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A64::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A64::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) = A64::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A64::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A64::__squeeze_array_avx2 (reg mut ptr u8[A64::_ASIZE] buf,
                             reg u64 offset, inline int LEN, reg u256[7] st,
                             inline int RATE8) -> (reg mut ptr u8[A64::_ASIZE],
                                                  reg u256[7])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A64::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A64::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

param int A128::_ASIZE = 128;

inline
fn A128::__a_ilen_read_upto8_at (reg const ptr u8[A128::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN,
                                inline int TRAIL, inline int CUR,
                                inline int AT) -> (inline int, inline int,
                                                  inline int, inline int,
                                                  reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w =
        buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                             ((64u) DELTA))]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u)
          buf.[#unaligned :u32 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        DELTA = (DELTA + (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 =
          ((64u)
          buf.[#unaligned :u16 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u)
            buf.[#unaligned (((64u /* of ui64 */) offset) +64u ((64u) DELTA))]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A128::__a_ilen_read_upto16_at (reg const ptr u8[A128::_ASIZE] buf,
                                 #[Internal::wint::unsigned] reg ui64 offset,
                                 inline int DELTA, inline int LEN,
                                 inline int TRAIL, inline int CUR,
                                 inline int AT) -> (inline int, inline int,
                                                   inline int, inline int,
                                                   reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w =
        buf.[#unaligned :u128 (((64u /* of ui64 */) offset) +64u
                              ((64u) DELTA))]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      DELTA = (DELTA + (16 - AT16)); /* int:i */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A128::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                       AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_0) =
          A128::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                       AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A128::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                       AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A128::__a_ilen_read_upto32_at (reg const ptr u8[A128::_ASIZE] buf,
                                 #[Internal::wint::unsigned] reg ui64 offset,
                                 inline int DELTA, inline int LEN,
                                 inline int TRAIL, inline int CUR,
                                 inline int AT) -> (inline int, inline int,
                                                   inline int, inline int,
                                                   reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w =
        buf.[#unaligned :u256 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A128::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                        AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_0) =
          A128::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                        AT32);
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A128::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                        AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A128::__a_ilen_read_bcast_upto8_at (reg const ptr u8[A128::_ASIZE] buf,
                                      #[Internal::wint::unsigned]
                                      reg ui64 offset, inline int DELTA,
                                      inline int LEN, inline int TRAIL,
                                      inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */)
                                               (offset +64ui
                                               ((64ui /* of int */) DELTA)))]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (DELTA, LEN, TRAIL, AT, w) =
        A128::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, CUR, AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w256);
}

inline
fn A128::__a_ilen_write_upto8 (reg mut ptr u8[A128::_ASIZE] buf,
                              #[Internal::wint::unsigned] reg ui64 offset,
                              inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A128::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 ((uint /* of ui64 */)
                           (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned ((uint /* of ui64 */)
                        (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A128::__a_ilen_write_upto16 (reg mut ptr u8[A128::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A128::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A128::__a_ilen_write_upto8(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A128::__a_ilen_write_upto32 (reg mut ptr u8[A128::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A128::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A128::__a_ilen_write_upto16(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A128::__a_rlen_read_upto8 (reg const ptr u8[A128::_ASIZE] a,
                             #[Internal::wint::unsigned] reg ui64 off,
                             #[Internal::wint::unsigned] reg ui64 len) -> 
(reg ui64, reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = a.[#unaligned :u64 ((uint /* of ui64 */) off)]; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) a.[#unaligned :u32 ((uint /* of ui64 */) off)]); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) a.[#unaligned :u16 ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) a.[#unaligned ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (off, w);
}

inline
fn A128::__a_rlen_write_upto8 (reg mut ptr u8[A128::_ASIZE] buf,
                              #[Internal::wint::unsigned] reg ui64 off,
                              reg u64 data, #[Internal::wint::unsigned]
                              reg ui64 len) -> (reg mut ptr u8[A128::_ASIZE],
                                               reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    buf.[#unaligned :u64 ((uint /* of ui64 */) off)] = data; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u32 ((uint /* of ui64 */) off)] = data; /* u32 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u16 ((uint /* of ui64 */) off)] = data; /* u16 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      buf.[#unaligned ((uint /* of ui64 */) off)] = data; /* u8 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, off);
}

inline
fn A128::__aread_subu64 (reg const ptr u8[A128::_ASIZE] buf, reg u64 offset,
                        inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u64)

{
  inline int ILEN;
  ILEN = LEN; /* int:i */ 
  reg u64 w;
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */ 
    TRAIL = 0; /* int:i */ 
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      reg u64 t16;
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      } else {
        t16 = ((64u) 0); /* u64 */ 
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */ 
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          }
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */ 
        }
        TRAIL = 0; /* int:i */ 
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */ 
        t16 = (t16 |64u t8); /* u64 */ 
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */ 
      w = (w |64u t16); /* u64 */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A128::__aread_bcast_4subu64 (reg const ptr u8[A128::_ASIZE] buf,
                               reg u64 offset, inline int DELTA,
                               inline int LEN, inline int TRAIL) -> (inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      reg u64 t64;
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A128::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      reg u128 t128;
      t128 = ((128u) t64); /* u128 */ 
      w = #VPBROADCAST_4u64(t128); /*  */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A128::__aread_subu128 (reg const ptr u8[A128::_ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u128)

{
  reg u128 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */ 
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      reg u64 t64;
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A128::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */ 
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A128::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A128::__aread_subu256 (reg const ptr u8[A128::_ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_0;
      reg u128 t128_1;
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A128::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      } else {
        t128_1 = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A128::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A128::__awrite_subu64 (reg mut ptr u8[A128::_ASIZE] buf, reg u64 offset,
                         inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A128::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A128::__awrite_subu128 (reg mut ptr u8[A128::_ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A128::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A128::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A128::__awrite_subu256 (reg mut ptr u8[A128::_ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A128::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A128::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A128::__addstate_avx2 (reg u256[7] st, inline int AT,
                         reg const ptr u8[A128::_ASIZE] buf,
                         #[Internal::wint::unsigned] reg ui64 offset,
                         inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                                 inline int,
                                                                 reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (AT < 8) {
    reg u256 r0;
    if ((AT == 0) && (8 <= _LEN)) {
      r0 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                                               ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      _LEN = (_LEN - 8); /* int:i */ 
      AT = 8; /* int:i */ 
    } else {
      reg u64 t64_1;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_1) =
        A128::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 0, AT);
      reg u128 t128_0;
      t128_0 = ((128u) t64_1); /* u128 */ 
      r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
    }
    st[0] = (st[0] ^256u r0); /* u256 */ 
  }
  if ((AT < 40) && ((0 < _LEN) || (_TRAILB != 0))) {
    reg u256 r1;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, r1) =
      A128::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 8, AT);
    st[1] = (st[1] ^256u r1); /* u256 */ 
  }
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u64 t64_2;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t64_2) =
      A128::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 40, AT);
    reg u128 t128_1;
    t128_1 = ((128u) t64_2); /* u128 */ 
    reg u128 t128_2;
    t128_2 = #set0_128(); /*  */ 
    if ((0 < _LEN) || (_TRAILB != 0)) {
      reg u256 r3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r3) =
        A128::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 48,
                                      AT);
      reg u64 t64_3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_3) =
        A128::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 80,
                                     AT);
      t128_2 = ((128u) t64_3); /* u128 */ 
      reg u256 r4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r4) =
        A128::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 88,
                                      AT);
      reg u64 t64_4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_4) =
        A128::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 120,
                                     AT);
      t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
      reg u256 r5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r5) =
        A128::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 128,
                                      AT);
      reg u64 t64_5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_5) =
        A128::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 160,
                                     AT);
      t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
      reg u256 r6;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r6) =
        A128::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 168,
                                      AT);
      #[inline]
      st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
    }
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (st, AT, offset);
}

inline
fn A128::__absorb_avx2 (reg u256[7] st, inline int AT,
                       reg const ptr u8[A128::_ASIZE] buf,
                       inline int _TRAILB, inline int _RATE8) -> (reg u256[7],
                                                                 inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A128::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A128::__addstate_avx2(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A128::__addstate_avx2(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A128::__addstate_avx2(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A128::__dumpstate_avx2 (reg mut ptr u8[A128::_ASIZE] buf,
                          #[Internal::wint::unsigned] reg ui64 offset,
                          inline int _LEN, reg u256[7] st) -> (reg mut ptr u8[A128::_ASIZE],
                                                              reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= _LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A128::__a_ilen_write_upto32(buf, offset, DELTA, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, _LEN) =
      A128::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, _LEN) =
    A128::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, _LEN) =
      A128::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, _LEN) =
        A128::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A128::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A128::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A128::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A128::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A128::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A128::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
    }
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A128::__squeeze_avx2 (reg u256[7] st, reg mut ptr u8[A128::_ASIZE] buf,
                        inline int _RATE8) -> (reg u256[7],
                                              reg mut ptr u8[A128::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A128::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A128::__dumpstate_avx2(buf, offset, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A128::__dumpstate_avx2(buf, offset, LO, st);
  }
  return (st, buf);
}

inline
fn A128::__addstate_array_avx2 (reg u256[7] st,
                               reg const ptr u8[A128::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t64;
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  reg u128 t128_0;
  t128_0 = ((128u) t64); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A128::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    reg u128 t128_1;
    t128_1 = ((128u) t64); /* u128 */ 
    reg u256 r3;
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A128::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */ 
    reg u256 r4;
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A128::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A128::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */ 
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
    reg u256 r6;
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A128::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (st, offset);
}

inline
fn A128::__absorb_array_avx2 (reg u256[7] st,
                             reg const ptr u8[A128::_ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */ 
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  if (0 < ITERS) {
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A128::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
  }
  LEN = (LEN % RATE8); /* int:i */ 
  #[inline]
  (st, offset) = A128::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A128::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                             reg const ptr u8[A128::_ASIZE] buf,
                             reg u64 offset, inline int LEN,
                             inline int TRAILB) -> (reg mut ptr u64[25],
                                                   inline int, reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (AT / 8)); /* u64 */ 
  reg u64 t64;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A128::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A128::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      at = (at +64u ((64u) 1)); /* u64 */ 
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      reg u256 t256;
      t256 = buf.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = (LEN % 32); /* int:i */ 
  }
  if (16 <= LEN) {
    reg u128 t128;
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
    DELTA = (DELTA + 16); /* int:i */ 
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */ 
    at = (at +64u ((64u) 2)); /* u64 */ 
    LEN = (LEN - 16); /* int:i */ 
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
    DELTA = (DELTA + 8); /* int:i */ 
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */ 
    at = (at +64u ((64u) 1)); /* u64 */ 
    LEN = (LEN - 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A128::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */ 
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (pst, ALL, offset);
}

inline
fn A128::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                              reg u256[7] st,
                              reg const ptr u8[A128::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  reg u64 i;
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A128::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */ 
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */ 
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A128::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A128::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A128::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A128::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A128::__dumpstate_array_avx2 (reg mut ptr u8[A128::_ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                reg u256[7] st) -> (reg mut ptr u8[A128::_ASIZE],
                                                   reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A128::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A128::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) = A128::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, LEN) = A128::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, LEN) =
        A128::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A128::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A128::__squeeze_array_avx2 (reg mut ptr u8[A128::_ASIZE] buf,
                              reg u64 offset, inline int LEN, reg u256[7] st,
                              inline int RATE8) -> (reg mut ptr u8[A128::_ASIZE],
                                                   reg u256[7])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) = A128::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A128::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A128::__addstate_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                 reg const ptr u8[A128::_ASIZE] buf,
                                 #[Internal::wint::unsigned] reg ui64 offset,
                                 inline int _LEN, inline int _TRAILB) -> 
(reg mut ptr u256[25], inline int, reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u256 w;
  if ((AT8 % 8) != 0) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, w) =
      A128::__a_ilen_read_bcast_upto8_at(buf, offset, DELTA, _LEN, _TRAILB,
                                         AT, AT8);
    w = (w ^256u st[(AT / 8)]); /* u256 */ 
    st[(AT / 8)] = w; /* u256 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (32 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) (32 * ((AT / 8) + (_LEN / 8)))))) {
    w =
      #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */) offset)]); /*  */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    at = (at +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, w) =
      A128::__a_ilen_read_bcast_upto8_at(buf, offset, 0, _LEN, _TRAILB, AT,
                                         AT);
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A128::__absorb_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                               reg const ptr u8[A128::_ASIZE] buf,
                               inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A128::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A128::__addstate_bcast_avx2x4(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A128::__addstate_bcast_avx2x4(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A128::__addstate_bcast_avx2x4(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A128::__addstate_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                           reg const ptr u8[A128::_ASIZE] buf0,
                           reg const ptr u8[A128::_ASIZE] buf1,
                           reg const ptr u8[A128::_ASIZE] buf2,
                           reg const ptr u8[A128::_ASIZE] buf3,
                           #[Internal::wint::unsigned] reg ui64 offset,
                           inline int _LEN, inline int _TRAILB) -> (reg mut ptr u256[25],
                                                                   inline int,
                                                                   reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if ((AT8 % 8) != 0) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A128::__a_ilen_read_upto8_at(buf0, offset, DELTA, _LEN, _TRAILB, AT,
                                   AT8);
    st[:u64 ((4 * (AT / 8)) + 0)] =
      (st[:u64 ((4 * (AT / 8)) + 0)] ^64u t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A128::__a_ilen_read_upto8_at(buf1, offset, DELTA, _LEN, _TRAILB, AT,
                                   AT8);
    st[:u64 ((4 * (AT / 8)) + 1)] =
      (st[:u64 ((4 * (AT / 8)) + 1)] ^64u t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A128::__a_ilen_read_upto8_at(buf2, offset, DELTA, _LEN, _TRAILB, AT,
                                   AT8);
    st[:u64 ((4 * (AT / 8)) + 2)] =
      (st[:u64 ((4 * (AT / 8)) + 2)] ^64u t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, t3) =
      A128::__a_ilen_read_upto8_at(buf3, offset, DELTA, _LEN, _TRAILB, AT,
                                   AT8);
    st[:u64 ((4 * (AT / 8)) + 3)] =
      (st[:u64 ((4 * (AT / 8)) + 3)] ^64u t3); /* u64 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (4 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) ((4 * (AT / 8)) + (4 * (_LEN / 8)))))) {
    t0 = buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    t1 = buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    t2 = buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    t3 = buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    at = (at +64ui ((64ui /* of int */) 4)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A128::__a_ilen_read_upto8_at(buf0, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A128::__a_ilen_read_upto8_at(buf1, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A128::__a_ilen_read_upto8_at(buf2, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t3) =
      A128::__a_ilen_read_upto8_at(buf3, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A128::__absorb_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                         reg const ptr u8[A128::_ASIZE] buf0,
                         reg const ptr u8[A128::_ASIZE] buf1,
                         reg const ptr u8[A128::_ASIZE] buf2,
                         reg const ptr u8[A128::_ASIZE] buf3,
                         inline int _TRAILB, inline int _RATE8) -> (reg mut ptr u256[25],
                                                                   inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A128::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A128::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                              (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A128::__addstate_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A128::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, _LEN,
                            _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A128::__dumpstate_avx2x4 (reg mut ptr u8[A128::_ASIZE] buf0,
                            reg mut ptr u8[A128::_ASIZE] buf1,
                            reg mut ptr u8[A128::_ASIZE] buf2,
                            reg mut ptr u8[A128::_ASIZE] buf3,
                            #[Internal::wint::unsigned] reg ui64 offset,
                            inline int _LEN, reg const ptr u256[25] st) -> 
(reg mut ptr u8[A128::_ASIZE], reg mut ptr u8[A128::_ASIZE],
reg mut ptr u8[A128::_ASIZE], reg mut ptr u8[A128::_ASIZE], reg ui64)

{
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * (_LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (0 * 32))))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (1 * 32))))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (2 * 32))))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (3 * 32))))]; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x0; /* u256 */ 
    buf1.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x1; /* u256 */ 
    buf2.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x2; /* u256 */ 
    buf3.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x3; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <64ui ((64ui /* of int */) (8 * (_LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t3; /* u64 */ 
    i = (i +64ui ((64ui /* of int */) 8)); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
  }
  if (0 < (_LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A128::__a_ilen_write_upto8(buf0, offset, 0, (_LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A128::__a_ilen_write_upto8(buf1, offset, 0, (_LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A128::__a_ilen_write_upto8(buf2, offset, 0, (_LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A128::__a_ilen_write_upto8(buf3, offset, 0, (_LEN % 8), t3);
    offset = (offset +64ui ((64ui /* of int */) (_LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A128::__squeeze_avx2x4 (reg mut ptr u256[25] st,
                          reg mut ptr u8[A128::_ASIZE] buf0,
                          reg mut ptr u8[A128::_ASIZE] buf1,
                          reg mut ptr u8[A128::_ASIZE] buf2,
                          reg mut ptr u8[A128::_ASIZE] buf3,
                          inline int _RATE8) -> (reg mut ptr u256[25],
                                                reg mut ptr u8[A128::_ASIZE],
                                                reg mut ptr u8[A128::_ASIZE],
                                                reg mut ptr u8[A128::_ASIZE],
                                                reg mut ptr u8[A128::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A128::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  if (0 < ITERS) {
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A128::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, _RATE8, st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  if (0 < LO) {
    st = _keccakf1600_avx2x4(st);
    #[inline]
    (buf0, buf1, buf2, buf3, offset) =
      A128::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
  }
  return (st, buf0, buf1, buf2, buf3);
}

inline
fn A128::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                       inline int AT,
                                       reg const ptr u8[A128::_ASIZE] buf,
                                       reg u64 offset, inline int LEN,
                                       inline int TRAILB) -> (reg mut ptr u256[25],
                                                             inline int,
                                                             reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (32 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u256 t256;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A128::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A128::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A128::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
    st.[#unaligned at] = t256; /* u256 */ 
  }
  return (st, ALL, offset);
}

inline
fn A128::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                     reg const ptr u8[A128::_ASIZE] buf,
                                     reg u64 offset, inline int LEN,
                                     inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A128::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A128::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                            (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A128::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A128::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A128::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                 reg const ptr u8[A128::_ASIZE] buf0,
                                 reg const ptr u8[A128::_ASIZE] buf1,
                                 reg const ptr u8[A128::_ASIZE] buf2,
                                 reg const ptr u8[A128::_ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 inline int TRAILB) -> (reg mut ptr u256[25],
                                                       inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (4 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A128::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A128::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A128::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A128::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
      TRAILB = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */ 
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A128::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A128::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A128::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A128::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      reg u256 t256_0;
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_1;
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_2;
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_3;
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 =
        (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */ 
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */ 
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */ 
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */ 
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */ 
      at = (at +64u ((64u) 16)); /* u64 */ 
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A128::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A128::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A128::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A128::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
      TRAILB = 0; /* int:i */ 
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
  }
  return (st, ALL, offset);
}

inline
fn A128::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                               reg const ptr u8[A128::_ASIZE] buf0,
                               reg const ptr u8[A128::_ASIZE] buf1,
                               reg const ptr u8[A128::_ASIZE] buf2,
                               reg const ptr u8[A128::_ASIZE] buf3,
                               reg u64 offset, inline int LEN,
                               inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A128::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                    LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A128::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                      (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A128::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                      RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A128::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                    LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A128::__dumpstate_array_avx2x4 (reg mut ptr u8[A128::_ASIZE] buf0,
                                  reg mut ptr u8[A128::_ASIZE] buf1,
                                  reg mut ptr u8[A128::_ASIZE] buf2,
                                  reg mut ptr u8[A128::_ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  reg const ptr u256[25] st) -> (reg mut ptr u8[A128::_ASIZE],
                                                                reg mut ptr u8[A128::_ASIZE],
                                                                reg mut ptr u8[A128::_ASIZE],
                                                                reg mut ptr u8[A128::_ASIZE],
                                                                reg u64)

{
  reg u64 i;
  i = ((64u) 0); /* u64 */ 
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */ 
    i = (i +64u ((64u) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */ 
    buf1.[#unaligned :u256 offset] = x1; /* u256 */ 
    buf2.[#unaligned :u256 offset] = x2; /* u256 */ 
    buf3.[#unaligned :u256 offset] = x3; /* u256 */ 
    offset = (offset +64u ((64u) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    buf0.[#unaligned :u64 offset] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    buf1.[#unaligned :u64 offset] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    buf2.[#unaligned :u64 offset] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    buf3.[#unaligned :u64 offset] = t3; /* u64 */ 
    i = (i +64u ((64u) 8)); /* u64 */ 
    offset = (offset +64u ((64u) 8)); /* u64 */ 
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A128::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A128::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A128::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A128::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A128::__squeeze_array_avx2x4 (reg mut ptr u8[A128::_ASIZE] buf0,
                                reg mut ptr u8[A128::_ASIZE] buf1,
                                reg mut ptr u8[A128::_ASIZE] buf2,
                                reg mut ptr u8[A128::_ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A128::_ASIZE], reg mut ptr u8[A128::_ASIZE],
reg mut ptr u8[A128::_ASIZE], reg mut ptr u8[A128::_ASIZE], reg u64,
reg mut ptr u256[25])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A128::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                         RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A128::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A1184::_ASIZE = 1184;

inline
fn A1184::__a_ilen_read_upto8_at (reg const ptr u8[A1184::_ASIZE] buf,
                                 #[Internal::wint::unsigned] reg ui64 offset,
                                 inline int DELTA, inline int LEN,
                                 inline int TRAIL, inline int CUR,
                                 inline int AT) -> (inline int, inline int,
                                                   inline int, inline int,
                                                   reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w =
        buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                             ((64u) DELTA))]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u)
          buf.[#unaligned :u32 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        DELTA = (DELTA + (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 =
          ((64u)
          buf.[#unaligned :u16 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u)
            buf.[#unaligned (((64u /* of ui64 */) offset) +64u ((64u) DELTA))]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1184::__a_ilen_read_upto16_at (reg const ptr u8[A1184::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int DELTA,
                                  inline int LEN, inline int TRAIL,
                                  inline int CUR, inline int AT) -> (inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w =
        buf.[#unaligned :u128 (((64u /* of ui64 */) offset) +64u
                              ((64u) DELTA))]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      DELTA = (DELTA + (16 - AT16)); /* int:i */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A1184::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                        AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_0) =
          A1184::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                        AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A1184::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                        AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1184::__a_ilen_read_upto32_at (reg const ptr u8[A1184::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int DELTA,
                                  inline int LEN, inline int TRAIL,
                                  inline int CUR, inline int AT) -> (inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w =
        buf.[#unaligned :u256 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A1184::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                         AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_0) =
          A1184::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                         AT32);
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A1184::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                         AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1184::__a_ilen_read_bcast_upto8_at (reg const ptr u8[A1184::_ASIZE] buf,
                                       #[Internal::wint::unsigned]
                                       reg ui64 offset, inline int DELTA,
                                       inline int LEN, inline int TRAIL,
                                       inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */)
                                               (offset +64ui
                                               ((64ui /* of int */) DELTA)))]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (DELTA, LEN, TRAIL, AT, w) =
        A1184::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, CUR, AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w256);
}

inline
fn A1184::__a_ilen_write_upto8 (reg mut ptr u8[A1184::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1184::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 ((uint /* of ui64 */)
                           (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned ((uint /* of ui64 */)
                        (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1184::__a_ilen_write_upto16 (reg mut ptr u8[A1184::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A1184::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1184::__a_ilen_write_upto8(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1184::__a_ilen_write_upto32 (reg mut ptr u8[A1184::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A1184::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1184::__a_ilen_write_upto16(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1184::__a_rlen_read_upto8 (reg const ptr u8[A1184::_ASIZE] a,
                              #[Internal::wint::unsigned] reg ui64 off,
                              #[Internal::wint::unsigned] reg ui64 len) -> 
(reg ui64, reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = a.[#unaligned :u64 ((uint /* of ui64 */) off)]; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) a.[#unaligned :u32 ((uint /* of ui64 */) off)]); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) a.[#unaligned :u16 ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) a.[#unaligned ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (off, w);
}

inline
fn A1184::__a_rlen_write_upto8 (reg mut ptr u8[A1184::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 off,
                               reg u64 data, #[Internal::wint::unsigned]
                               reg ui64 len) -> (reg mut ptr u8[A1184::_ASIZE],
                                                reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    buf.[#unaligned :u64 ((uint /* of ui64 */) off)] = data; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u32 ((uint /* of ui64 */) off)] = data; /* u32 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u16 ((uint /* of ui64 */) off)] = data; /* u16 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      buf.[#unaligned ((uint /* of ui64 */) off)] = data; /* u8 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, off);
}

inline
fn A1184::__aread_subu64 (reg const ptr u8[A1184::_ASIZE] buf,
                         reg u64 offset, inline int DELTA, inline int LEN,
                         inline int TRAIL) -> (inline int, inline int,
                                              inline int, reg u64)

{
  inline int ILEN;
  ILEN = LEN; /* int:i */ 
  reg u64 w;
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */ 
    TRAIL = 0; /* int:i */ 
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      reg u64 t16;
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      } else {
        t16 = ((64u) 0); /* u64 */ 
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */ 
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          }
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */ 
        }
        TRAIL = 0; /* int:i */ 
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */ 
        t16 = (t16 |64u t8); /* u64 */ 
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */ 
      w = (w |64u t16); /* u64 */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1184::__aread_bcast_4subu64 (reg const ptr u8[A1184::_ASIZE] buf,
                                reg u64 offset, inline int DELTA,
                                inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      reg u64 t64;
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      reg u128 t128;
      t128 = ((128u) t64); /* u128 */ 
      w = #VPBROADCAST_4u64(t128); /*  */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1184::__aread_subu128 (reg const ptr u8[A1184::_ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u128)

{
  reg u128 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */ 
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      reg u64 t64;
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */ 
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1184::__aread_subu256 (reg const ptr u8[A1184::_ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_0;
      reg u128 t128_1;
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A1184::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      } else {
        t128_1 = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A1184::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1184::__awrite_subu64 (reg mut ptr u8[A1184::_ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1184::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1184::__awrite_subu128 (reg mut ptr u8[A1184::_ASIZE] buf,
                           reg u64 offset, inline int DELTA, inline int LEN,
                           reg u128 w) -> (reg mut ptr u8[A1184::_ASIZE],
                                          inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1184::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1184::__awrite_subu256 (reg mut ptr u8[A1184::_ASIZE] buf,
                           reg u64 offset, inline int DELTA, inline int LEN,
                           reg u256 w) -> (reg mut ptr u8[A1184::_ASIZE],
                                          inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1184::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1184::__addstate_avx2 (reg u256[7] st, inline int AT,
                          reg const ptr u8[A1184::_ASIZE] buf,
                          #[Internal::wint::unsigned] reg ui64 offset,
                          inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                                  inline int,
                                                                  reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (AT < 8) {
    reg u256 r0;
    if ((AT == 0) && (8 <= _LEN)) {
      r0 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                                               ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      _LEN = (_LEN - 8); /* int:i */ 
      AT = 8; /* int:i */ 
    } else {
      reg u64 t64_1;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_1) =
        A1184::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 0,
                                      AT);
      reg u128 t128_0;
      t128_0 = ((128u) t64_1); /* u128 */ 
      r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
    }
    st[0] = (st[0] ^256u r0); /* u256 */ 
  }
  if ((AT < 40) && ((0 < _LEN) || (_TRAILB != 0))) {
    reg u256 r1;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, r1) =
      A1184::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 8, AT);
    st[1] = (st[1] ^256u r1); /* u256 */ 
  }
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u64 t64_2;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t64_2) =
      A1184::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 40, AT);
    reg u128 t128_1;
    t128_1 = ((128u) t64_2); /* u128 */ 
    reg u128 t128_2;
    t128_2 = #set0_128(); /*  */ 
    if ((0 < _LEN) || (_TRAILB != 0)) {
      reg u256 r3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r3) =
        A1184::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 48,
                                       AT);
      reg u64 t64_3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_3) =
        A1184::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 80,
                                      AT);
      t128_2 = ((128u) t64_3); /* u128 */ 
      reg u256 r4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r4) =
        A1184::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 88,
                                       AT);
      reg u64 t64_4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_4) =
        A1184::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 120,
                                      AT);
      t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
      reg u256 r5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r5) =
        A1184::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                       128, AT);
      reg u64 t64_5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_5) =
        A1184::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 160,
                                      AT);
      t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
      reg u256 r6;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r6) =
        A1184::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                       168, AT);
      #[inline]
      st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
    }
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (st, AT, offset);
}

inline
fn A1184::__absorb_avx2 (reg u256[7] st, inline int AT,
                        reg const ptr u8[A1184::_ASIZE] buf,
                        inline int _TRAILB, inline int _RATE8) -> (reg u256[7],
                                                                  inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1184::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1184::__addstate_avx2(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1184::__addstate_avx2(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1184::__addstate_avx2(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1184::__dumpstate_avx2 (reg mut ptr u8[A1184::_ASIZE] buf,
                           #[Internal::wint::unsigned] reg ui64 offset,
                           inline int _LEN, reg u256[7] st) -> (reg mut ptr u8[A1184::_ASIZE],
                                                               reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= _LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1184::__a_ilen_write_upto32(buf, offset, DELTA, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, _LEN) =
      A1184::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, _LEN) =
    A1184::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, _LEN) =
      A1184::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, _LEN) =
        A1184::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1184::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1184::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1184::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1184::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1184::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1184::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
    }
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A1184::__squeeze_avx2 (reg u256[7] st, reg mut ptr u8[A1184::_ASIZE] buf,
                         inline int _RATE8) -> (reg u256[7],
                                               reg mut ptr u8[A1184::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1184::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A1184::__dumpstate_avx2(buf, offset, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A1184::__dumpstate_avx2(buf, offset, LO, st);
  }
  return (st, buf);
}

inline
fn A1184::__addstate_array_avx2 (reg u256[7] st,
                                reg const ptr u8[A1184::_ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t64;
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  reg u128 t128_0;
  t128_0 = ((128u) t64); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A1184::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    reg u128 t128_1;
    t128_1 = ((128u) t64); /* u128 */ 
    reg u256 r3;
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A1184::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */ 
    reg u256 r4;
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A1184::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A1184::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */ 
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
    reg u256 r6;
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A1184::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (st, offset);
}

inline
fn A1184::__absorb_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A1184::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */ 
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  if (0 < ITERS) {
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1184::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
  }
  LEN = (LEN % RATE8); /* int:i */ 
  #[inline]
  (st, offset) = A1184::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A1184::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                              reg const ptr u8[A1184::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg mut ptr u64[25],
                                                    inline int, reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (AT / 8)); /* u64 */ 
  reg u64 t64;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A1184::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A1184::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      at = (at +64u ((64u) 1)); /* u64 */ 
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      reg u256 t256;
      t256 = buf.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = (LEN % 32); /* int:i */ 
  }
  if (16 <= LEN) {
    reg u128 t128;
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
    DELTA = (DELTA + 16); /* int:i */ 
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */ 
    at = (at +64u ((64u) 2)); /* u64 */ 
    LEN = (LEN - 16); /* int:i */ 
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
    DELTA = (DELTA + 8); /* int:i */ 
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */ 
    at = (at +64u ((64u) 1)); /* u64 */ 
    LEN = (LEN - 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A1184::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */ 
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (pst, ALL, offset);
}

inline
fn A1184::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                               reg u256[7] st,
                               reg const ptr u8[A1184::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  reg u64 i;
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A1184::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */ 
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */ 
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A1184::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1184::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A1184::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A1184::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A1184::__dumpstate_array_avx2 (reg mut ptr u8[A1184::_ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 reg u256[7] st) -> (reg mut ptr u8[A1184::_ASIZE],
                                                    reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1184::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A1184::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) =
    A1184::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, LEN) = A1184::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1184::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1184::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A1184::__squeeze_array_avx2 (reg mut ptr u8[A1184::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st, inline int RATE8) -> (reg mut ptr u8[A1184::_ASIZE],
                                                                    reg u256[7])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) =
          A1184::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A1184::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A1184::__addstate_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1184::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int _LEN,
                                  inline int _TRAILB) -> (reg mut ptr u256[25],
                                                         inline int,
                                                         reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u256 w;
  if ((AT8 % 8) != 0) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, w) =
      A1184::__a_ilen_read_bcast_upto8_at(buf, offset, DELTA, _LEN, _TRAILB,
                                          AT, AT8);
    w = (w ^256u st[(AT / 8)]); /* u256 */ 
    st[(AT / 8)] = w; /* u256 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (32 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) (32 * ((AT / 8) + (_LEN / 8)))))) {
    w =
      #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */) offset)]); /*  */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    at = (at +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, w) =
      A1184::__a_ilen_read_bcast_upto8_at(buf, offset, 0, _LEN, _TRAILB, AT,
                                          AT);
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A1184::__absorb_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1184::_ASIZE] buf,
                                inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1184::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1184::__addstate_bcast_avx2x4(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1184::__addstate_bcast_avx2x4(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1184::__addstate_bcast_avx2x4(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1184::__addstate_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                            reg const ptr u8[A1184::_ASIZE] buf0,
                            reg const ptr u8[A1184::_ASIZE] buf1,
                            reg const ptr u8[A1184::_ASIZE] buf2,
                            reg const ptr u8[A1184::_ASIZE] buf3,
                            #[Internal::wint::unsigned] reg ui64 offset,
                            inline int _LEN, inline int _TRAILB) -> (reg mut ptr u256[25],
                                                                    inline int,
                                                                    reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if ((AT8 % 8) != 0) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A1184::__a_ilen_read_upto8_at(buf0, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 0)] =
      (st[:u64 ((4 * (AT / 8)) + 0)] ^64u t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A1184::__a_ilen_read_upto8_at(buf1, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 1)] =
      (st[:u64 ((4 * (AT / 8)) + 1)] ^64u t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A1184::__a_ilen_read_upto8_at(buf2, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 2)] =
      (st[:u64 ((4 * (AT / 8)) + 2)] ^64u t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, t3) =
      A1184::__a_ilen_read_upto8_at(buf3, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 3)] =
      (st[:u64 ((4 * (AT / 8)) + 3)] ^64u t3); /* u64 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (4 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) ((4 * (AT / 8)) + (4 * (_LEN / 8)))))) {
    t0 = buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    t1 = buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    t2 = buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    t3 = buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    at = (at +64ui ((64ui /* of int */) 4)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A1184::__a_ilen_read_upto8_at(buf0, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A1184::__a_ilen_read_upto8_at(buf1, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A1184::__a_ilen_read_upto8_at(buf2, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t3) =
      A1184::__a_ilen_read_upto8_at(buf3, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A1184::__absorb_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                          reg const ptr u8[A1184::_ASIZE] buf0,
                          reg const ptr u8[A1184::_ASIZE] buf1,
                          reg const ptr u8[A1184::_ASIZE] buf2,
                          reg const ptr u8[A1184::_ASIZE] buf3,
                          inline int _TRAILB, inline int _RATE8) -> (reg mut ptr u256[25],
                                                                    inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1184::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1184::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                               (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1184::__addstate_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                 _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1184::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, _LEN,
                             _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1184::__dumpstate_avx2x4 (reg mut ptr u8[A1184::_ASIZE] buf0,
                             reg mut ptr u8[A1184::_ASIZE] buf1,
                             reg mut ptr u8[A1184::_ASIZE] buf2,
                             reg mut ptr u8[A1184::_ASIZE] buf3,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int _LEN, reg const ptr u256[25] st) -> 
(reg mut ptr u8[A1184::_ASIZE], reg mut ptr u8[A1184::_ASIZE],
reg mut ptr u8[A1184::_ASIZE], reg mut ptr u8[A1184::_ASIZE], reg ui64)

{
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * (_LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (0 * 32))))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (1 * 32))))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (2 * 32))))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (3 * 32))))]; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x0; /* u256 */ 
    buf1.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x1; /* u256 */ 
    buf2.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x2; /* u256 */ 
    buf3.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x3; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <64ui ((64ui /* of int */) (8 * (_LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t3; /* u64 */ 
    i = (i +64ui ((64ui /* of int */) 8)); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
  }
  if (0 < (_LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1184::__a_ilen_write_upto8(buf0, offset, 0, (_LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1184::__a_ilen_write_upto8(buf1, offset, 0, (_LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1184::__a_ilen_write_upto8(buf2, offset, 0, (_LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1184::__a_ilen_write_upto8(buf3, offset, 0, (_LEN % 8), t3);
    offset = (offset +64ui ((64ui /* of int */) (_LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1184::__squeeze_avx2x4 (reg mut ptr u256[25] st,
                           reg mut ptr u8[A1184::_ASIZE] buf0,
                           reg mut ptr u8[A1184::_ASIZE] buf1,
                           reg mut ptr u8[A1184::_ASIZE] buf2,
                           reg mut ptr u8[A1184::_ASIZE] buf3,
                           inline int _RATE8) -> (reg mut ptr u256[25],
                                                 reg mut ptr u8[A1184::_ASIZE],
                                                 reg mut ptr u8[A1184::_ASIZE],
                                                 reg mut ptr u8[A1184::_ASIZE],
                                                 reg mut ptr u8[A1184::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1184::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  if (0 < ITERS) {
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1184::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, _RATE8, st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  if (0 < LO) {
    st = _keccakf1600_avx2x4(st);
    #[inline]
    (buf0, buf1, buf2, buf3, offset) =
      A1184::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
  }
  return (st, buf0, buf1, buf2, buf3);
}

inline
fn A1184::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                        inline int AT,
                                        reg const ptr u8[A1184::_ASIZE] buf,
                                        reg u64 offset, inline int LEN,
                                        inline int TRAILB) -> (reg mut ptr u256[25],
                                                              inline int,
                                                              reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (32 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u256 t256;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A1184::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A1184::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A1184::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
    st.[#unaligned at] = t256; /* u256 */ 
  }
  return (st, ALL, offset);
}

inline
fn A1184::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A1184::_ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1184::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1184::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                             (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1184::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A1184::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1184::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1184::_ASIZE] buf0,
                                  reg const ptr u8[A1184::_ASIZE] buf1,
                                  reg const ptr u8[A1184::_ASIZE] buf2,
                                  reg const ptr u8[A1184::_ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  inline int TRAILB) -> (reg mut ptr u256[25],
                                                        inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (4 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A1184::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A1184::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A1184::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A1184::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
      TRAILB = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */ 
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A1184::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A1184::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A1184::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A1184::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      reg u256 t256_0;
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_1;
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_2;
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_3;
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 =
        (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */ 
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */ 
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */ 
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */ 
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */ 
      at = (at +64u ((64u) 16)); /* u64 */ 
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A1184::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A1184::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A1184::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A1184::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
      TRAILB = 0; /* int:i */ 
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
  }
  return (st, ALL, offset);
}

inline
fn A1184::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1184::_ASIZE] buf0,
                                reg const ptr u8[A1184::_ASIZE] buf1,
                                reg const ptr u8[A1184::_ASIZE] buf2,
                                reg const ptr u8[A1184::_ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1184::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1184::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                       offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1184::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                       RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A1184::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1184::__dumpstate_array_avx2x4 (reg mut ptr u8[A1184::_ASIZE] buf0,
                                   reg mut ptr u8[A1184::_ASIZE] buf1,
                                   reg mut ptr u8[A1184::_ASIZE] buf2,
                                   reg mut ptr u8[A1184::_ASIZE] buf3,
                                   reg u64 offset, inline int LEN,
                                   reg const ptr u256[25] st) -> (reg mut ptr u8[A1184::_ASIZE],
                                                                 reg mut ptr u8[A1184::_ASIZE],
                                                                 reg mut ptr u8[A1184::_ASIZE],
                                                                 reg mut ptr u8[A1184::_ASIZE],
                                                                 reg u64)

{
  reg u64 i;
  i = ((64u) 0); /* u64 */ 
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */ 
    i = (i +64u ((64u) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */ 
    buf1.[#unaligned :u256 offset] = x1; /* u256 */ 
    buf2.[#unaligned :u256 offset] = x2; /* u256 */ 
    buf3.[#unaligned :u256 offset] = x3; /* u256 */ 
    offset = (offset +64u ((64u) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    buf0.[#unaligned :u64 offset] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    buf1.[#unaligned :u64 offset] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    buf2.[#unaligned :u64 offset] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    buf3.[#unaligned :u64 offset] = t3; /* u64 */ 
    i = (i +64u ((64u) 8)); /* u64 */ 
    offset = (offset +64u ((64u) 8)); /* u64 */ 
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1184::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1184::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1184::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1184::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1184::__squeeze_array_avx2x4 (reg mut ptr u8[A1184::_ASIZE] buf0,
                                 reg mut ptr u8[A1184::_ASIZE] buf1,
                                 reg mut ptr u8[A1184::_ASIZE] buf2,
                                 reg mut ptr u8[A1184::_ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A1184::_ASIZE], reg mut ptr u8[A1184::_ASIZE],
reg mut ptr u8[A1184::_ASIZE], reg mut ptr u8[A1184::_ASIZE], reg u64,
reg mut ptr u256[25])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A1184::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                          RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1184::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO,
                                        st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A1568::_ASIZE = 1568;

inline
fn A1568::__a_ilen_read_upto8_at (reg const ptr u8[A1568::_ASIZE] buf,
                                 #[Internal::wint::unsigned] reg ui64 offset,
                                 inline int DELTA, inline int LEN,
                                 inline int TRAIL, inline int CUR,
                                 inline int AT) -> (inline int, inline int,
                                                   inline int, inline int,
                                                   reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w =
        buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                             ((64u) DELTA))]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u)
          buf.[#unaligned :u32 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        DELTA = (DELTA + (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 =
          ((64u)
          buf.[#unaligned :u16 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u)
            buf.[#unaligned (((64u /* of ui64 */) offset) +64u ((64u) DELTA))]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1568::__a_ilen_read_upto16_at (reg const ptr u8[A1568::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int DELTA,
                                  inline int LEN, inline int TRAIL,
                                  inline int CUR, inline int AT) -> (inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w =
        buf.[#unaligned :u128 (((64u /* of ui64 */) offset) +64u
                              ((64u) DELTA))]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      DELTA = (DELTA + (16 - AT16)); /* int:i */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A1568::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                        AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_0) =
          A1568::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                        AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A1568::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                        AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1568::__a_ilen_read_upto32_at (reg const ptr u8[A1568::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int DELTA,
                                  inline int LEN, inline int TRAIL,
                                  inline int CUR, inline int AT) -> (inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w =
        buf.[#unaligned :u256 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A1568::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                         AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_0) =
          A1568::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                         AT32);
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A1568::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                         AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1568::__a_ilen_read_bcast_upto8_at (reg const ptr u8[A1568::_ASIZE] buf,
                                       #[Internal::wint::unsigned]
                                       reg ui64 offset, inline int DELTA,
                                       inline int LEN, inline int TRAIL,
                                       inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */)
                                               (offset +64ui
                                               ((64ui /* of int */) DELTA)))]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (DELTA, LEN, TRAIL, AT, w) =
        A1568::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, CUR, AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w256);
}

inline
fn A1568::__a_ilen_write_upto8 (reg mut ptr u8[A1568::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1568::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 ((uint /* of ui64 */)
                           (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned ((uint /* of ui64 */)
                        (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1568::__a_ilen_write_upto16 (reg mut ptr u8[A1568::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A1568::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1568::__a_ilen_write_upto8(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1568::__a_ilen_write_upto32 (reg mut ptr u8[A1568::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A1568::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1568::__a_ilen_write_upto16(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1568::__a_rlen_read_upto8 (reg const ptr u8[A1568::_ASIZE] a,
                              #[Internal::wint::unsigned] reg ui64 off,
                              #[Internal::wint::unsigned] reg ui64 len) -> 
(reg ui64, reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = a.[#unaligned :u64 ((uint /* of ui64 */) off)]; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) a.[#unaligned :u32 ((uint /* of ui64 */) off)]); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) a.[#unaligned :u16 ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) a.[#unaligned ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (off, w);
}

inline
fn A1568::__a_rlen_write_upto8 (reg mut ptr u8[A1568::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 off,
                               reg u64 data, #[Internal::wint::unsigned]
                               reg ui64 len) -> (reg mut ptr u8[A1568::_ASIZE],
                                                reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    buf.[#unaligned :u64 ((uint /* of ui64 */) off)] = data; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u32 ((uint /* of ui64 */) off)] = data; /* u32 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u16 ((uint /* of ui64 */) off)] = data; /* u16 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      buf.[#unaligned ((uint /* of ui64 */) off)] = data; /* u8 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, off);
}

inline
fn A1568::__aread_subu64 (reg const ptr u8[A1568::_ASIZE] buf,
                         reg u64 offset, inline int DELTA, inline int LEN,
                         inline int TRAIL) -> (inline int, inline int,
                                              inline int, reg u64)

{
  inline int ILEN;
  ILEN = LEN; /* int:i */ 
  reg u64 w;
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */ 
    TRAIL = 0; /* int:i */ 
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      reg u64 t16;
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      } else {
        t16 = ((64u) 0); /* u64 */ 
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */ 
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          }
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */ 
        }
        TRAIL = 0; /* int:i */ 
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */ 
        t16 = (t16 |64u t8); /* u64 */ 
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */ 
      w = (w |64u t16); /* u64 */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1568::__aread_bcast_4subu64 (reg const ptr u8[A1568::_ASIZE] buf,
                                reg u64 offset, inline int DELTA,
                                inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      reg u64 t64;
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      reg u128 t128;
      t128 = ((128u) t64); /* u128 */ 
      w = #VPBROADCAST_4u64(t128); /*  */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1568::__aread_subu128 (reg const ptr u8[A1568::_ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u128)

{
  reg u128 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */ 
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      reg u64 t64;
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */ 
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1568::__aread_subu256 (reg const ptr u8[A1568::_ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_0;
      reg u128 t128_1;
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A1568::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      } else {
        t128_1 = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A1568::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1568::__awrite_subu64 (reg mut ptr u8[A1568::_ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1568::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1568::__awrite_subu128 (reg mut ptr u8[A1568::_ASIZE] buf,
                           reg u64 offset, inline int DELTA, inline int LEN,
                           reg u128 w) -> (reg mut ptr u8[A1568::_ASIZE],
                                          inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1568::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1568::__awrite_subu256 (reg mut ptr u8[A1568::_ASIZE] buf,
                           reg u64 offset, inline int DELTA, inline int LEN,
                           reg u256 w) -> (reg mut ptr u8[A1568::_ASIZE],
                                          inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1568::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1568::__addstate_avx2 (reg u256[7] st, inline int AT,
                          reg const ptr u8[A1568::_ASIZE] buf,
                          #[Internal::wint::unsigned] reg ui64 offset,
                          inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                                  inline int,
                                                                  reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (AT < 8) {
    reg u256 r0;
    if ((AT == 0) && (8 <= _LEN)) {
      r0 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                                               ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      _LEN = (_LEN - 8); /* int:i */ 
      AT = 8; /* int:i */ 
    } else {
      reg u64 t64_1;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_1) =
        A1568::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 0,
                                      AT);
      reg u128 t128_0;
      t128_0 = ((128u) t64_1); /* u128 */ 
      r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
    }
    st[0] = (st[0] ^256u r0); /* u256 */ 
  }
  if ((AT < 40) && ((0 < _LEN) || (_TRAILB != 0))) {
    reg u256 r1;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, r1) =
      A1568::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 8, AT);
    st[1] = (st[1] ^256u r1); /* u256 */ 
  }
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u64 t64_2;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t64_2) =
      A1568::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 40, AT);
    reg u128 t128_1;
    t128_1 = ((128u) t64_2); /* u128 */ 
    reg u128 t128_2;
    t128_2 = #set0_128(); /*  */ 
    if ((0 < _LEN) || (_TRAILB != 0)) {
      reg u256 r3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r3) =
        A1568::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 48,
                                       AT);
      reg u64 t64_3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_3) =
        A1568::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 80,
                                      AT);
      t128_2 = ((128u) t64_3); /* u128 */ 
      reg u256 r4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r4) =
        A1568::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 88,
                                       AT);
      reg u64 t64_4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_4) =
        A1568::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 120,
                                      AT);
      t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
      reg u256 r5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r5) =
        A1568::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                       128, AT);
      reg u64 t64_5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_5) =
        A1568::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 160,
                                      AT);
      t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
      reg u256 r6;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r6) =
        A1568::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                       168, AT);
      #[inline]
      st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
    }
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (st, AT, offset);
}

inline
fn A1568::__absorb_avx2 (reg u256[7] st, inline int AT,
                        reg const ptr u8[A1568::_ASIZE] buf,
                        inline int _TRAILB, inline int _RATE8) -> (reg u256[7],
                                                                  inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1568::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1568::__addstate_avx2(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1568::__addstate_avx2(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1568::__addstate_avx2(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1568::__dumpstate_avx2 (reg mut ptr u8[A1568::_ASIZE] buf,
                           #[Internal::wint::unsigned] reg ui64 offset,
                           inline int _LEN, reg u256[7] st) -> (reg mut ptr u8[A1568::_ASIZE],
                                                               reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= _LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1568::__a_ilen_write_upto32(buf, offset, DELTA, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, _LEN) =
      A1568::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, _LEN) =
    A1568::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, _LEN) =
      A1568::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, _LEN) =
        A1568::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1568::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1568::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1568::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1568::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1568::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1568::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
    }
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A1568::__squeeze_avx2 (reg u256[7] st, reg mut ptr u8[A1568::_ASIZE] buf,
                         inline int _RATE8) -> (reg u256[7],
                                               reg mut ptr u8[A1568::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1568::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A1568::__dumpstate_avx2(buf, offset, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A1568::__dumpstate_avx2(buf, offset, LO, st);
  }
  return (st, buf);
}

inline
fn A1568::__addstate_array_avx2 (reg u256[7] st,
                                reg const ptr u8[A1568::_ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t64;
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  reg u128 t128_0;
  t128_0 = ((128u) t64); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A1568::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    reg u128 t128_1;
    t128_1 = ((128u) t64); /* u128 */ 
    reg u256 r3;
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A1568::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */ 
    reg u256 r4;
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A1568::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A1568::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */ 
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
    reg u256 r6;
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A1568::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (st, offset);
}

inline
fn A1568::__absorb_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A1568::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */ 
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  if (0 < ITERS) {
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1568::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
  }
  LEN = (LEN % RATE8); /* int:i */ 
  #[inline]
  (st, offset) = A1568::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A1568::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                              reg const ptr u8[A1568::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg mut ptr u64[25],
                                                    inline int, reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (AT / 8)); /* u64 */ 
  reg u64 t64;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A1568::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A1568::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      at = (at +64u ((64u) 1)); /* u64 */ 
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      reg u256 t256;
      t256 = buf.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = (LEN % 32); /* int:i */ 
  }
  if (16 <= LEN) {
    reg u128 t128;
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
    DELTA = (DELTA + 16); /* int:i */ 
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */ 
    at = (at +64u ((64u) 2)); /* u64 */ 
    LEN = (LEN - 16); /* int:i */ 
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
    DELTA = (DELTA + 8); /* int:i */ 
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */ 
    at = (at +64u ((64u) 1)); /* u64 */ 
    LEN = (LEN - 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A1568::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */ 
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (pst, ALL, offset);
}

inline
fn A1568::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                               reg u256[7] st,
                               reg const ptr u8[A1568::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  reg u64 i;
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A1568::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */ 
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */ 
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A1568::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1568::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A1568::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A1568::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A1568::__dumpstate_array_avx2 (reg mut ptr u8[A1568::_ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 reg u256[7] st) -> (reg mut ptr u8[A1568::_ASIZE],
                                                    reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1568::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A1568::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) =
    A1568::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, LEN) = A1568::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1568::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1568::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A1568::__squeeze_array_avx2 (reg mut ptr u8[A1568::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st, inline int RATE8) -> (reg mut ptr u8[A1568::_ASIZE],
                                                                    reg u256[7])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) =
          A1568::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A1568::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A1568::__addstate_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1568::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int _LEN,
                                  inline int _TRAILB) -> (reg mut ptr u256[25],
                                                         inline int,
                                                         reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u256 w;
  if ((AT8 % 8) != 0) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, w) =
      A1568::__a_ilen_read_bcast_upto8_at(buf, offset, DELTA, _LEN, _TRAILB,
                                          AT, AT8);
    w = (w ^256u st[(AT / 8)]); /* u256 */ 
    st[(AT / 8)] = w; /* u256 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (32 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) (32 * ((AT / 8) + (_LEN / 8)))))) {
    w =
      #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */) offset)]); /*  */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    at = (at +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, w) =
      A1568::__a_ilen_read_bcast_upto8_at(buf, offset, 0, _LEN, _TRAILB, AT,
                                          AT);
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A1568::__absorb_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1568::_ASIZE] buf,
                                inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1568::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1568::__addstate_bcast_avx2x4(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1568::__addstate_bcast_avx2x4(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1568::__addstate_bcast_avx2x4(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1568::__addstate_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                            reg const ptr u8[A1568::_ASIZE] buf0,
                            reg const ptr u8[A1568::_ASIZE] buf1,
                            reg const ptr u8[A1568::_ASIZE] buf2,
                            reg const ptr u8[A1568::_ASIZE] buf3,
                            #[Internal::wint::unsigned] reg ui64 offset,
                            inline int _LEN, inline int _TRAILB) -> (reg mut ptr u256[25],
                                                                    inline int,
                                                                    reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if ((AT8 % 8) != 0) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A1568::__a_ilen_read_upto8_at(buf0, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 0)] =
      (st[:u64 ((4 * (AT / 8)) + 0)] ^64u t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A1568::__a_ilen_read_upto8_at(buf1, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 1)] =
      (st[:u64 ((4 * (AT / 8)) + 1)] ^64u t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A1568::__a_ilen_read_upto8_at(buf2, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 2)] =
      (st[:u64 ((4 * (AT / 8)) + 2)] ^64u t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, t3) =
      A1568::__a_ilen_read_upto8_at(buf3, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 3)] =
      (st[:u64 ((4 * (AT / 8)) + 3)] ^64u t3); /* u64 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (4 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) ((4 * (AT / 8)) + (4 * (_LEN / 8)))))) {
    t0 = buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    t1 = buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    t2 = buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    t3 = buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    at = (at +64ui ((64ui /* of int */) 4)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A1568::__a_ilen_read_upto8_at(buf0, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A1568::__a_ilen_read_upto8_at(buf1, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A1568::__a_ilen_read_upto8_at(buf2, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t3) =
      A1568::__a_ilen_read_upto8_at(buf3, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A1568::__absorb_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                          reg const ptr u8[A1568::_ASIZE] buf0,
                          reg const ptr u8[A1568::_ASIZE] buf1,
                          reg const ptr u8[A1568::_ASIZE] buf2,
                          reg const ptr u8[A1568::_ASIZE] buf3,
                          inline int _TRAILB, inline int _RATE8) -> (reg mut ptr u256[25],
                                                                    inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1568::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1568::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                               (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1568::__addstate_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                 _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1568::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, _LEN,
                             _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1568::__dumpstate_avx2x4 (reg mut ptr u8[A1568::_ASIZE] buf0,
                             reg mut ptr u8[A1568::_ASIZE] buf1,
                             reg mut ptr u8[A1568::_ASIZE] buf2,
                             reg mut ptr u8[A1568::_ASIZE] buf3,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int _LEN, reg const ptr u256[25] st) -> 
(reg mut ptr u8[A1568::_ASIZE], reg mut ptr u8[A1568::_ASIZE],
reg mut ptr u8[A1568::_ASIZE], reg mut ptr u8[A1568::_ASIZE], reg ui64)

{
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * (_LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (0 * 32))))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (1 * 32))))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (2 * 32))))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (3 * 32))))]; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x0; /* u256 */ 
    buf1.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x1; /* u256 */ 
    buf2.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x2; /* u256 */ 
    buf3.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x3; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <64ui ((64ui /* of int */) (8 * (_LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t3; /* u64 */ 
    i = (i +64ui ((64ui /* of int */) 8)); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
  }
  if (0 < (_LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1568::__a_ilen_write_upto8(buf0, offset, 0, (_LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1568::__a_ilen_write_upto8(buf1, offset, 0, (_LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1568::__a_ilen_write_upto8(buf2, offset, 0, (_LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1568::__a_ilen_write_upto8(buf3, offset, 0, (_LEN % 8), t3);
    offset = (offset +64ui ((64ui /* of int */) (_LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1568::__squeeze_avx2x4 (reg mut ptr u256[25] st,
                           reg mut ptr u8[A1568::_ASIZE] buf0,
                           reg mut ptr u8[A1568::_ASIZE] buf1,
                           reg mut ptr u8[A1568::_ASIZE] buf2,
                           reg mut ptr u8[A1568::_ASIZE] buf3,
                           inline int _RATE8) -> (reg mut ptr u256[25],
                                                 reg mut ptr u8[A1568::_ASIZE],
                                                 reg mut ptr u8[A1568::_ASIZE],
                                                 reg mut ptr u8[A1568::_ASIZE],
                                                 reg mut ptr u8[A1568::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1568::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  if (0 < ITERS) {
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1568::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, _RATE8, st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  if (0 < LO) {
    st = _keccakf1600_avx2x4(st);
    #[inline]
    (buf0, buf1, buf2, buf3, offset) =
      A1568::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
  }
  return (st, buf0, buf1, buf2, buf3);
}

inline
fn A1568::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                        inline int AT,
                                        reg const ptr u8[A1568::_ASIZE] buf,
                                        reg u64 offset, inline int LEN,
                                        inline int TRAILB) -> (reg mut ptr u256[25],
                                                              inline int,
                                                              reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (32 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u256 t256;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A1568::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A1568::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A1568::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
    st.[#unaligned at] = t256; /* u256 */ 
  }
  return (st, ALL, offset);
}

inline
fn A1568::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A1568::_ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1568::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1568::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                             (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1568::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A1568::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1568::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1568::_ASIZE] buf0,
                                  reg const ptr u8[A1568::_ASIZE] buf1,
                                  reg const ptr u8[A1568::_ASIZE] buf2,
                                  reg const ptr u8[A1568::_ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  inline int TRAILB) -> (reg mut ptr u256[25],
                                                        inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (4 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A1568::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A1568::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A1568::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A1568::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
      TRAILB = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */ 
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A1568::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A1568::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A1568::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A1568::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      reg u256 t256_0;
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_1;
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_2;
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_3;
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 =
        (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */ 
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */ 
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */ 
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */ 
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */ 
      at = (at +64u ((64u) 16)); /* u64 */ 
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A1568::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A1568::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A1568::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A1568::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
      TRAILB = 0; /* int:i */ 
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
  }
  return (st, ALL, offset);
}

inline
fn A1568::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1568::_ASIZE] buf0,
                                reg const ptr u8[A1568::_ASIZE] buf1,
                                reg const ptr u8[A1568::_ASIZE] buf2,
                                reg const ptr u8[A1568::_ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1568::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1568::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                       offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1568::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                       RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A1568::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1568::__dumpstate_array_avx2x4 (reg mut ptr u8[A1568::_ASIZE] buf0,
                                   reg mut ptr u8[A1568::_ASIZE] buf1,
                                   reg mut ptr u8[A1568::_ASIZE] buf2,
                                   reg mut ptr u8[A1568::_ASIZE] buf3,
                                   reg u64 offset, inline int LEN,
                                   reg const ptr u256[25] st) -> (reg mut ptr u8[A1568::_ASIZE],
                                                                 reg mut ptr u8[A1568::_ASIZE],
                                                                 reg mut ptr u8[A1568::_ASIZE],
                                                                 reg mut ptr u8[A1568::_ASIZE],
                                                                 reg u64)

{
  reg u64 i;
  i = ((64u) 0); /* u64 */ 
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */ 
    i = (i +64u ((64u) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */ 
    buf1.[#unaligned :u256 offset] = x1; /* u256 */ 
    buf2.[#unaligned :u256 offset] = x2; /* u256 */ 
    buf3.[#unaligned :u256 offset] = x3; /* u256 */ 
    offset = (offset +64u ((64u) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    buf0.[#unaligned :u64 offset] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    buf1.[#unaligned :u64 offset] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    buf2.[#unaligned :u64 offset] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    buf3.[#unaligned :u64 offset] = t3; /* u64 */ 
    i = (i +64u ((64u) 8)); /* u64 */ 
    offset = (offset +64u ((64u) 8)); /* u64 */ 
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1568::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1568::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1568::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1568::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1568::__squeeze_array_avx2x4 (reg mut ptr u8[A1568::_ASIZE] buf0,
                                 reg mut ptr u8[A1568::_ASIZE] buf1,
                                 reg mut ptr u8[A1568::_ASIZE] buf2,
                                 reg mut ptr u8[A1568::_ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A1568::_ASIZE], reg mut ptr u8[A1568::_ASIZE],
reg mut ptr u8[A1568::_ASIZE], reg mut ptr u8[A1568::_ASIZE], reg u64,
reg mut ptr u256[25])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A1568::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                          RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1568::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO,
                                        st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A1120::_ASIZE = 1120;

inline
fn A1120::__a_ilen_read_upto8_at (reg const ptr u8[A1120::_ASIZE] buf,
                                 #[Internal::wint::unsigned] reg ui64 offset,
                                 inline int DELTA, inline int LEN,
                                 inline int TRAIL, inline int CUR,
                                 inline int AT) -> (inline int, inline int,
                                                   inline int, inline int,
                                                   reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w =
        buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                             ((64u) DELTA))]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u)
          buf.[#unaligned :u32 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        DELTA = (DELTA + (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 =
          ((64u)
          buf.[#unaligned :u16 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u)
            buf.[#unaligned (((64u /* of ui64 */) offset) +64u ((64u) DELTA))]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1120::__a_ilen_read_upto16_at (reg const ptr u8[A1120::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int DELTA,
                                  inline int LEN, inline int TRAIL,
                                  inline int CUR, inline int AT) -> (inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w =
        buf.[#unaligned :u128 (((64u /* of ui64 */) offset) +64u
                              ((64u) DELTA))]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      DELTA = (DELTA + (16 - AT16)); /* int:i */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A1120::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                        AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_0) =
          A1120::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                        AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A1120::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                        AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1120::__a_ilen_read_upto32_at (reg const ptr u8[A1120::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int DELTA,
                                  inline int LEN, inline int TRAIL,
                                  inline int CUR, inline int AT) -> (inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w =
        buf.[#unaligned :u256 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A1120::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                         AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_0) =
          A1120::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                         AT32);
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A1120::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                         AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1120::__a_ilen_read_bcast_upto8_at (reg const ptr u8[A1120::_ASIZE] buf,
                                       #[Internal::wint::unsigned]
                                       reg ui64 offset, inline int DELTA,
                                       inline int LEN, inline int TRAIL,
                                       inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */)
                                               (offset +64ui
                                               ((64ui /* of int */) DELTA)))]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (DELTA, LEN, TRAIL, AT, w) =
        A1120::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, CUR, AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w256);
}

inline
fn A1120::__a_ilen_write_upto8 (reg mut ptr u8[A1120::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1120::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 ((uint /* of ui64 */)
                           (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned ((uint /* of ui64 */)
                        (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1120::__a_ilen_write_upto16 (reg mut ptr u8[A1120::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A1120::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1120::__a_ilen_write_upto8(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1120::__a_ilen_write_upto32 (reg mut ptr u8[A1120::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A1120::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1120::__a_ilen_write_upto16(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1120::__a_rlen_read_upto8 (reg const ptr u8[A1120::_ASIZE] a,
                              #[Internal::wint::unsigned] reg ui64 off,
                              #[Internal::wint::unsigned] reg ui64 len) -> 
(reg ui64, reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = a.[#unaligned :u64 ((uint /* of ui64 */) off)]; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) a.[#unaligned :u32 ((uint /* of ui64 */) off)]); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) a.[#unaligned :u16 ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) a.[#unaligned ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (off, w);
}

inline
fn A1120::__a_rlen_write_upto8 (reg mut ptr u8[A1120::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 off,
                               reg u64 data, #[Internal::wint::unsigned]
                               reg ui64 len) -> (reg mut ptr u8[A1120::_ASIZE],
                                                reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    buf.[#unaligned :u64 ((uint /* of ui64 */) off)] = data; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u32 ((uint /* of ui64 */) off)] = data; /* u32 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u16 ((uint /* of ui64 */) off)] = data; /* u16 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      buf.[#unaligned ((uint /* of ui64 */) off)] = data; /* u8 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, off);
}

inline
fn A1120::__aread_subu64 (reg const ptr u8[A1120::_ASIZE] buf,
                         reg u64 offset, inline int DELTA, inline int LEN,
                         inline int TRAIL) -> (inline int, inline int,
                                              inline int, reg u64)

{
  inline int ILEN;
  ILEN = LEN; /* int:i */ 
  reg u64 w;
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */ 
    TRAIL = 0; /* int:i */ 
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      reg u64 t16;
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      } else {
        t16 = ((64u) 0); /* u64 */ 
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */ 
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          }
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */ 
        }
        TRAIL = 0; /* int:i */ 
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */ 
        t16 = (t16 |64u t8); /* u64 */ 
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */ 
      w = (w |64u t16); /* u64 */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1120::__aread_bcast_4subu64 (reg const ptr u8[A1120::_ASIZE] buf,
                                reg u64 offset, inline int DELTA,
                                inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      reg u64 t64;
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      reg u128 t128;
      t128 = ((128u) t64); /* u128 */ 
      w = #VPBROADCAST_4u64(t128); /*  */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1120::__aread_subu128 (reg const ptr u8[A1120::_ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u128)

{
  reg u128 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */ 
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      reg u64 t64;
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */ 
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1120::__aread_subu256 (reg const ptr u8[A1120::_ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_0;
      reg u128 t128_1;
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A1120::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      } else {
        t128_1 = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A1120::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1120::__awrite_subu64 (reg mut ptr u8[A1120::_ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1120::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1120::__awrite_subu128 (reg mut ptr u8[A1120::_ASIZE] buf,
                           reg u64 offset, inline int DELTA, inline int LEN,
                           reg u128 w) -> (reg mut ptr u8[A1120::_ASIZE],
                                          inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1120::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1120::__awrite_subu256 (reg mut ptr u8[A1120::_ASIZE] buf,
                           reg u64 offset, inline int DELTA, inline int LEN,
                           reg u256 w) -> (reg mut ptr u8[A1120::_ASIZE],
                                          inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1120::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1120::__addstate_avx2 (reg u256[7] st, inline int AT,
                          reg const ptr u8[A1120::_ASIZE] buf,
                          #[Internal::wint::unsigned] reg ui64 offset,
                          inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                                  inline int,
                                                                  reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (AT < 8) {
    reg u256 r0;
    if ((AT == 0) && (8 <= _LEN)) {
      r0 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                                               ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      _LEN = (_LEN - 8); /* int:i */ 
      AT = 8; /* int:i */ 
    } else {
      reg u64 t64_1;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_1) =
        A1120::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 0,
                                      AT);
      reg u128 t128_0;
      t128_0 = ((128u) t64_1); /* u128 */ 
      r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
    }
    st[0] = (st[0] ^256u r0); /* u256 */ 
  }
  if ((AT < 40) && ((0 < _LEN) || (_TRAILB != 0))) {
    reg u256 r1;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, r1) =
      A1120::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 8, AT);
    st[1] = (st[1] ^256u r1); /* u256 */ 
  }
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u64 t64_2;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t64_2) =
      A1120::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 40, AT);
    reg u128 t128_1;
    t128_1 = ((128u) t64_2); /* u128 */ 
    reg u128 t128_2;
    t128_2 = #set0_128(); /*  */ 
    if ((0 < _LEN) || (_TRAILB != 0)) {
      reg u256 r3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r3) =
        A1120::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 48,
                                       AT);
      reg u64 t64_3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_3) =
        A1120::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 80,
                                      AT);
      t128_2 = ((128u) t64_3); /* u128 */ 
      reg u256 r4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r4) =
        A1120::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 88,
                                       AT);
      reg u64 t64_4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_4) =
        A1120::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 120,
                                      AT);
      t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
      reg u256 r5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r5) =
        A1120::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                       128, AT);
      reg u64 t64_5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_5) =
        A1120::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 160,
                                      AT);
      t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
      reg u256 r6;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r6) =
        A1120::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                       168, AT);
      #[inline]
      st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
    }
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (st, AT, offset);
}

inline
fn A1120::__absorb_avx2 (reg u256[7] st, inline int AT,
                        reg const ptr u8[A1120::_ASIZE] buf,
                        inline int _TRAILB, inline int _RATE8) -> (reg u256[7],
                                                                  inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1120::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1120::__addstate_avx2(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1120::__addstate_avx2(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1120::__addstate_avx2(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1120::__dumpstate_avx2 (reg mut ptr u8[A1120::_ASIZE] buf,
                           #[Internal::wint::unsigned] reg ui64 offset,
                           inline int _LEN, reg u256[7] st) -> (reg mut ptr u8[A1120::_ASIZE],
                                                               reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= _LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1120::__a_ilen_write_upto32(buf, offset, DELTA, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, _LEN) =
      A1120::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, _LEN) =
    A1120::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, _LEN) =
      A1120::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, _LEN) =
        A1120::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1120::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1120::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1120::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1120::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1120::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1120::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
    }
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A1120::__squeeze_avx2 (reg u256[7] st, reg mut ptr u8[A1120::_ASIZE] buf,
                         inline int _RATE8) -> (reg u256[7],
                                               reg mut ptr u8[A1120::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1120::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A1120::__dumpstate_avx2(buf, offset, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A1120::__dumpstate_avx2(buf, offset, LO, st);
  }
  return (st, buf);
}

inline
fn A1120::__addstate_array_avx2 (reg u256[7] st,
                                reg const ptr u8[A1120::_ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t64;
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  reg u128 t128_0;
  t128_0 = ((128u) t64); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A1120::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    reg u128 t128_1;
    t128_1 = ((128u) t64); /* u128 */ 
    reg u256 r3;
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A1120::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */ 
    reg u256 r4;
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A1120::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A1120::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */ 
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
    reg u256 r6;
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A1120::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (st, offset);
}

inline
fn A1120::__absorb_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A1120::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */ 
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  if (0 < ITERS) {
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1120::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
  }
  LEN = (LEN % RATE8); /* int:i */ 
  #[inline]
  (st, offset) = A1120::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A1120::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                              reg const ptr u8[A1120::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg mut ptr u64[25],
                                                    inline int, reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (AT / 8)); /* u64 */ 
  reg u64 t64;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A1120::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A1120::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      at = (at +64u ((64u) 1)); /* u64 */ 
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      reg u256 t256;
      t256 = buf.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = (LEN % 32); /* int:i */ 
  }
  if (16 <= LEN) {
    reg u128 t128;
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
    DELTA = (DELTA + 16); /* int:i */ 
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */ 
    at = (at +64u ((64u) 2)); /* u64 */ 
    LEN = (LEN - 16); /* int:i */ 
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
    DELTA = (DELTA + 8); /* int:i */ 
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */ 
    at = (at +64u ((64u) 1)); /* u64 */ 
    LEN = (LEN - 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A1120::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */ 
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (pst, ALL, offset);
}

inline
fn A1120::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                               reg u256[7] st,
                               reg const ptr u8[A1120::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  reg u64 i;
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A1120::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */ 
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */ 
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A1120::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1120::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A1120::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A1120::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A1120::__dumpstate_array_avx2 (reg mut ptr u8[A1120::_ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 reg u256[7] st) -> (reg mut ptr u8[A1120::_ASIZE],
                                                    reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1120::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A1120::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) =
    A1120::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, LEN) = A1120::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1120::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1120::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A1120::__squeeze_array_avx2 (reg mut ptr u8[A1120::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st, inline int RATE8) -> (reg mut ptr u8[A1120::_ASIZE],
                                                                    reg u256[7])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) =
          A1120::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A1120::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A1120::__addstate_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1120::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int _LEN,
                                  inline int _TRAILB) -> (reg mut ptr u256[25],
                                                         inline int,
                                                         reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u256 w;
  if ((AT8 % 8) != 0) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, w) =
      A1120::__a_ilen_read_bcast_upto8_at(buf, offset, DELTA, _LEN, _TRAILB,
                                          AT, AT8);
    w = (w ^256u st[(AT / 8)]); /* u256 */ 
    st[(AT / 8)] = w; /* u256 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (32 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) (32 * ((AT / 8) + (_LEN / 8)))))) {
    w =
      #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */) offset)]); /*  */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    at = (at +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, w) =
      A1120::__a_ilen_read_bcast_upto8_at(buf, offset, 0, _LEN, _TRAILB, AT,
                                          AT);
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A1120::__absorb_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1120::_ASIZE] buf,
                                inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1120::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1120::__addstate_bcast_avx2x4(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1120::__addstate_bcast_avx2x4(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1120::__addstate_bcast_avx2x4(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1120::__addstate_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                            reg const ptr u8[A1120::_ASIZE] buf0,
                            reg const ptr u8[A1120::_ASIZE] buf1,
                            reg const ptr u8[A1120::_ASIZE] buf2,
                            reg const ptr u8[A1120::_ASIZE] buf3,
                            #[Internal::wint::unsigned] reg ui64 offset,
                            inline int _LEN, inline int _TRAILB) -> (reg mut ptr u256[25],
                                                                    inline int,
                                                                    reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if ((AT8 % 8) != 0) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A1120::__a_ilen_read_upto8_at(buf0, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 0)] =
      (st[:u64 ((4 * (AT / 8)) + 0)] ^64u t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A1120::__a_ilen_read_upto8_at(buf1, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 1)] =
      (st[:u64 ((4 * (AT / 8)) + 1)] ^64u t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A1120::__a_ilen_read_upto8_at(buf2, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 2)] =
      (st[:u64 ((4 * (AT / 8)) + 2)] ^64u t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, t3) =
      A1120::__a_ilen_read_upto8_at(buf3, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 3)] =
      (st[:u64 ((4 * (AT / 8)) + 3)] ^64u t3); /* u64 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (4 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) ((4 * (AT / 8)) + (4 * (_LEN / 8)))))) {
    t0 = buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    t1 = buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    t2 = buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    t3 = buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    at = (at +64ui ((64ui /* of int */) 4)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A1120::__a_ilen_read_upto8_at(buf0, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A1120::__a_ilen_read_upto8_at(buf1, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A1120::__a_ilen_read_upto8_at(buf2, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t3) =
      A1120::__a_ilen_read_upto8_at(buf3, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A1120::__absorb_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                          reg const ptr u8[A1120::_ASIZE] buf0,
                          reg const ptr u8[A1120::_ASIZE] buf1,
                          reg const ptr u8[A1120::_ASIZE] buf2,
                          reg const ptr u8[A1120::_ASIZE] buf3,
                          inline int _TRAILB, inline int _RATE8) -> (reg mut ptr u256[25],
                                                                    inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1120::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1120::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                               (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1120::__addstate_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                 _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1120::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, _LEN,
                             _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1120::__dumpstate_avx2x4 (reg mut ptr u8[A1120::_ASIZE] buf0,
                             reg mut ptr u8[A1120::_ASIZE] buf1,
                             reg mut ptr u8[A1120::_ASIZE] buf2,
                             reg mut ptr u8[A1120::_ASIZE] buf3,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int _LEN, reg const ptr u256[25] st) -> 
(reg mut ptr u8[A1120::_ASIZE], reg mut ptr u8[A1120::_ASIZE],
reg mut ptr u8[A1120::_ASIZE], reg mut ptr u8[A1120::_ASIZE], reg ui64)

{
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * (_LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (0 * 32))))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (1 * 32))))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (2 * 32))))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (3 * 32))))]; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x0; /* u256 */ 
    buf1.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x1; /* u256 */ 
    buf2.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x2; /* u256 */ 
    buf3.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x3; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <64ui ((64ui /* of int */) (8 * (_LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t3; /* u64 */ 
    i = (i +64ui ((64ui /* of int */) 8)); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
  }
  if (0 < (_LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1120::__a_ilen_write_upto8(buf0, offset, 0, (_LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1120::__a_ilen_write_upto8(buf1, offset, 0, (_LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1120::__a_ilen_write_upto8(buf2, offset, 0, (_LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1120::__a_ilen_write_upto8(buf3, offset, 0, (_LEN % 8), t3);
    offset = (offset +64ui ((64ui /* of int */) (_LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1120::__squeeze_avx2x4 (reg mut ptr u256[25] st,
                           reg mut ptr u8[A1120::_ASIZE] buf0,
                           reg mut ptr u8[A1120::_ASIZE] buf1,
                           reg mut ptr u8[A1120::_ASIZE] buf2,
                           reg mut ptr u8[A1120::_ASIZE] buf3,
                           inline int _RATE8) -> (reg mut ptr u256[25],
                                                 reg mut ptr u8[A1120::_ASIZE],
                                                 reg mut ptr u8[A1120::_ASIZE],
                                                 reg mut ptr u8[A1120::_ASIZE],
                                                 reg mut ptr u8[A1120::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1120::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  if (0 < ITERS) {
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1120::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, _RATE8, st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  if (0 < LO) {
    st = _keccakf1600_avx2x4(st);
    #[inline]
    (buf0, buf1, buf2, buf3, offset) =
      A1120::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
  }
  return (st, buf0, buf1, buf2, buf3);
}

inline
fn A1120::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                        inline int AT,
                                        reg const ptr u8[A1120::_ASIZE] buf,
                                        reg u64 offset, inline int LEN,
                                        inline int TRAILB) -> (reg mut ptr u256[25],
                                                              inline int,
                                                              reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (32 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u256 t256;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A1120::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A1120::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A1120::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
    st.[#unaligned at] = t256; /* u256 */ 
  }
  return (st, ALL, offset);
}

inline
fn A1120::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A1120::_ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1120::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1120::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                             (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1120::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A1120::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1120::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1120::_ASIZE] buf0,
                                  reg const ptr u8[A1120::_ASIZE] buf1,
                                  reg const ptr u8[A1120::_ASIZE] buf2,
                                  reg const ptr u8[A1120::_ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  inline int TRAILB) -> (reg mut ptr u256[25],
                                                        inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (4 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A1120::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A1120::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A1120::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A1120::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
      TRAILB = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */ 
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A1120::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A1120::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A1120::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A1120::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      reg u256 t256_0;
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_1;
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_2;
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_3;
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 =
        (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */ 
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */ 
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */ 
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */ 
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */ 
      at = (at +64u ((64u) 16)); /* u64 */ 
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A1120::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A1120::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A1120::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A1120::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
      TRAILB = 0; /* int:i */ 
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
  }
  return (st, ALL, offset);
}

inline
fn A1120::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1120::_ASIZE] buf0,
                                reg const ptr u8[A1120::_ASIZE] buf1,
                                reg const ptr u8[A1120::_ASIZE] buf2,
                                reg const ptr u8[A1120::_ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1120::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1120::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                       offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1120::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                       RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A1120::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1120::__dumpstate_array_avx2x4 (reg mut ptr u8[A1120::_ASIZE] buf0,
                                   reg mut ptr u8[A1120::_ASIZE] buf1,
                                   reg mut ptr u8[A1120::_ASIZE] buf2,
                                   reg mut ptr u8[A1120::_ASIZE] buf3,
                                   reg u64 offset, inline int LEN,
                                   reg const ptr u256[25] st) -> (reg mut ptr u8[A1120::_ASIZE],
                                                                 reg mut ptr u8[A1120::_ASIZE],
                                                                 reg mut ptr u8[A1120::_ASIZE],
                                                                 reg mut ptr u8[A1120::_ASIZE],
                                                                 reg u64)

{
  reg u64 i;
  i = ((64u) 0); /* u64 */ 
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */ 
    i = (i +64u ((64u) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */ 
    buf1.[#unaligned :u256 offset] = x1; /* u256 */ 
    buf2.[#unaligned :u256 offset] = x2; /* u256 */ 
    buf3.[#unaligned :u256 offset] = x3; /* u256 */ 
    offset = (offset +64u ((64u) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    buf0.[#unaligned :u64 offset] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    buf1.[#unaligned :u64 offset] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    buf2.[#unaligned :u64 offset] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    buf3.[#unaligned :u64 offset] = t3; /* u64 */ 
    i = (i +64u ((64u) 8)); /* u64 */ 
    offset = (offset +64u ((64u) 8)); /* u64 */ 
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1120::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1120::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1120::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1120::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1120::__squeeze_array_avx2x4 (reg mut ptr u8[A1120::_ASIZE] buf0,
                                 reg mut ptr u8[A1120::_ASIZE] buf1,
                                 reg mut ptr u8[A1120::_ASIZE] buf2,
                                 reg mut ptr u8[A1120::_ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A1120::_ASIZE], reg mut ptr u8[A1120::_ASIZE],
reg mut ptr u8[A1120::_ASIZE], reg mut ptr u8[A1120::_ASIZE], reg u64,
reg mut ptr u256[25])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A1120::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                          RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1120::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO,
                                        st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int A1600::_ASIZE = 1600;

inline
fn A1600::__a_ilen_read_upto8_at (reg const ptr u8[A1600::_ASIZE] buf,
                                 #[Internal::wint::unsigned] reg ui64 offset,
                                 inline int DELTA, inline int LEN,
                                 inline int TRAIL, inline int CUR,
                                 inline int AT) -> (inline int, inline int,
                                                   inline int, inline int,
                                                   reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w =
        buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                             ((64u) DELTA))]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u)
          buf.[#unaligned :u32 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        DELTA = (DELTA + (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 =
          ((64u)
          buf.[#unaligned :u16 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u)
            buf.[#unaligned (((64u /* of ui64 */) offset) +64u ((64u) DELTA))]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1600::__a_ilen_read_upto16_at (reg const ptr u8[A1600::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int DELTA,
                                  inline int LEN, inline int TRAIL,
                                  inline int CUR, inline int AT) -> (inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w =
        buf.[#unaligned :u128 (((64u /* of ui64 */) offset) +64u
                              ((64u) DELTA))]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      DELTA = (DELTA + (16 - AT16)); /* int:i */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A1600::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                        AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_0) =
          A1600::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                        AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          A1600::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                        AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1600::__a_ilen_read_upto32_at (reg const ptr u8[A1600::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int DELTA,
                                  inline int LEN, inline int TRAIL,
                                  inline int CUR, inline int AT) -> (inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    inline int,
                                                                    reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w =
        buf.[#unaligned :u256 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A1600::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                         AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_0) =
          A1600::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                         AT32);
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          A1600::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 16,
                                         AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn A1600::__a_ilen_read_bcast_upto8_at (reg const ptr u8[A1600::_ASIZE] buf,
                                       #[Internal::wint::unsigned]
                                       reg ui64 offset, inline int DELTA,
                                       inline int LEN, inline int TRAIL,
                                       inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */)
                                               (offset +64ui
                                               ((64ui /* of int */) DELTA)))]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (DELTA, LEN, TRAIL, AT, w) =
        A1600::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, CUR, AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w256);
}

inline
fn A1600::__a_ilen_write_upto8 (reg mut ptr u8[A1600::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1600::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 ((uint /* of ui64 */)
                           (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned ((uint /* of ui64 */)
                        (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1600::__a_ilen_write_upto16 (reg mut ptr u8[A1600::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN, reg u128 w) -> 
(reg mut ptr u8[A1600::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1600::__a_ilen_write_upto8(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1600::__a_ilen_write_upto32 (reg mut ptr u8[A1600::_ASIZE] buf,
                                #[Internal::wint::unsigned] reg ui64 offset,
                                inline int DELTA, inline int LEN, reg u256 w) -> 
(reg mut ptr u8[A1600::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1600::__a_ilen_write_upto16(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1600::__a_rlen_read_upto8 (reg const ptr u8[A1600::_ASIZE] a,
                              #[Internal::wint::unsigned] reg ui64 off,
                              #[Internal::wint::unsigned] reg ui64 len) -> 
(reg ui64, reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = a.[#unaligned :u64 ((uint /* of ui64 */) off)]; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) a.[#unaligned :u32 ((uint /* of ui64 */) off)]); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) a.[#unaligned :u16 ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) a.[#unaligned ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (off, w);
}

inline
fn A1600::__a_rlen_write_upto8 (reg mut ptr u8[A1600::_ASIZE] buf,
                               #[Internal::wint::unsigned] reg ui64 off,
                               reg u64 data, #[Internal::wint::unsigned]
                               reg ui64 len) -> (reg mut ptr u8[A1600::_ASIZE],
                                                reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    buf.[#unaligned :u64 ((uint /* of ui64 */) off)] = data; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u32 ((uint /* of ui64 */) off)] = data; /* u32 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u16 ((uint /* of ui64 */) off)] = data; /* u16 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      buf.[#unaligned ((uint /* of ui64 */) off)] = data; /* u8 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, off);
}

inline
fn A1600::__aread_subu64 (reg const ptr u8[A1600::_ASIZE] buf,
                         reg u64 offset, inline int DELTA, inline int LEN,
                         inline int TRAIL) -> (inline int, inline int,
                                              inline int, reg u64)

{
  inline int ILEN;
  ILEN = LEN; /* int:i */ 
  reg u64 w;
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */ 
    TRAIL = 0; /* int:i */ 
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      reg u64 t16;
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      } else {
        t16 = ((64u) 0); /* u64 */ 
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */ 
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          }
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */ 
        }
        TRAIL = 0; /* int:i */ 
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */ 
        t16 = (t16 |64u t8); /* u64 */ 
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */ 
      w = (w |64u t16); /* u64 */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1600::__aread_bcast_4subu64 (reg const ptr u8[A1600::_ASIZE] buf,
                                reg u64 offset, inline int DELTA,
                                inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      reg u64 t64;
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      reg u128 t128;
      t128 = ((128u) t64); /* u128 */ 
      w = #VPBROADCAST_4u64(t128); /*  */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1600::__aread_subu128 (reg const ptr u8[A1600::_ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u128)

{
  reg u128 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */ 
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      reg u64 t64;
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */ 
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1600::__aread_subu256 (reg const ptr u8[A1600::_ASIZE] buf,
                          reg u64 offset, inline int DELTA, inline int LEN,
                          inline int TRAIL) -> (inline int, inline int,
                                               inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_0;
      reg u128 t128_1;
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          A1600::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      } else {
        t128_1 = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          A1600::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn A1600::__awrite_subu64 (reg mut ptr u8[A1600::_ASIZE] buf, reg u64 offset,
                          inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[A1600::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1600::__awrite_subu128 (reg mut ptr u8[A1600::_ASIZE] buf,
                           reg u64 offset, inline int DELTA, inline int LEN,
                           reg u128 w) -> (reg mut ptr u8[A1600::_ASIZE],
                                          inline int, inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1600::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1600::__awrite_subu256 (reg mut ptr u8[A1600::_ASIZE] buf,
                           reg u64 offset, inline int DELTA, inline int LEN,
                           reg u256 w) -> (reg mut ptr u8[A1600::_ASIZE],
                                          inline int, inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        A1600::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn A1600::__addstate_avx2 (reg u256[7] st, inline int AT,
                          reg const ptr u8[A1600::_ASIZE] buf,
                          #[Internal::wint::unsigned] reg ui64 offset,
                          inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                                  inline int,
                                                                  reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (AT < 8) {
    reg u256 r0;
    if ((AT == 0) && (8 <= _LEN)) {
      r0 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                                               ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      _LEN = (_LEN - 8); /* int:i */ 
      AT = 8; /* int:i */ 
    } else {
      reg u64 t64_1;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_1) =
        A1600::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 0,
                                      AT);
      reg u128 t128_0;
      t128_0 = ((128u) t64_1); /* u128 */ 
      r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
    }
    st[0] = (st[0] ^256u r0); /* u256 */ 
  }
  if ((AT < 40) && ((0 < _LEN) || (_TRAILB != 0))) {
    reg u256 r1;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, r1) =
      A1600::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 8, AT);
    st[1] = (st[1] ^256u r1); /* u256 */ 
  }
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u64 t64_2;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t64_2) =
      A1600::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 40, AT);
    reg u128 t128_1;
    t128_1 = ((128u) t64_2); /* u128 */ 
    reg u128 t128_2;
    t128_2 = #set0_128(); /*  */ 
    if ((0 < _LEN) || (_TRAILB != 0)) {
      reg u256 r3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r3) =
        A1600::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 48,
                                       AT);
      reg u64 t64_3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_3) =
        A1600::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 80,
                                      AT);
      t128_2 = ((128u) t64_3); /* u128 */ 
      reg u256 r4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r4) =
        A1600::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 88,
                                       AT);
      reg u64 t64_4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_4) =
        A1600::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 120,
                                      AT);
      t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
      reg u256 r5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r5) =
        A1600::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                       128, AT);
      reg u64 t64_5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_5) =
        A1600::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 160,
                                      AT);
      t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
      reg u256 r6;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r6) =
        A1600::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                       168, AT);
      #[inline]
      st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
    }
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (st, AT, offset);
}

inline
fn A1600::__absorb_avx2 (reg u256[7] st, inline int AT,
                        reg const ptr u8[A1600::_ASIZE] buf,
                        inline int _TRAILB, inline int _RATE8) -> (reg u256[7],
                                                                  inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1600::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1600::__addstate_avx2(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1600::__addstate_avx2(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1600::__addstate_avx2(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1600::__dumpstate_avx2 (reg mut ptr u8[A1600::_ASIZE] buf,
                           #[Internal::wint::unsigned] reg ui64 offset,
                           inline int _LEN, reg u256[7] st) -> (reg mut ptr u8[A1600::_ASIZE],
                                                               reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= _LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1600::__a_ilen_write_upto32(buf, offset, DELTA, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, _LEN) =
      A1600::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, _LEN) =
    A1600::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, _LEN) =
      A1600::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, _LEN) =
        A1600::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1600::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1600::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1600::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1600::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1600::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          A1600::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
    }
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A1600::__squeeze_avx2 (reg u256[7] st, reg mut ptr u8[A1600::_ASIZE] buf,
                         inline int _RATE8) -> (reg u256[7],
                                               reg mut ptr u8[A1600::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1600::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A1600::__dumpstate_avx2(buf, offset, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = A1600::__dumpstate_avx2(buf, offset, LO, st);
  }
  return (st, buf);
}

inline
fn A1600::__addstate_array_avx2 (reg u256[7] st,
                                reg const ptr u8[A1600::_ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg u256[7], reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t64;
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  reg u128 t128_0;
  t128_0 = ((128u) t64); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    A1600::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    reg u128 t128_1;
    t128_1 = ((128u) t64); /* u128 */ 
    reg u256 r3;
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      A1600::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */ 
    reg u256 r4;
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      A1600::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      A1600::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */ 
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
    reg u256 r6;
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      A1600::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (st, offset);
}

inline
fn A1600::__absorb_array_avx2 (reg u256[7] st,
                              reg const ptr u8[A1600::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */ 
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  if (0 < ITERS) {
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1600::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
  }
  LEN = (LEN % RATE8); /* int:i */ 
  #[inline]
  (st, offset) = A1600::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn A1600::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                              reg const ptr u8[A1600::_ASIZE] buf,
                              reg u64 offset, inline int LEN,
                              inline int TRAILB) -> (reg mut ptr u64[25],
                                                    inline int, reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (AT / 8)); /* u64 */ 
  reg u64 t64;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        A1600::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          A1600::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      at = (at +64u ((64u) 1)); /* u64 */ 
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      reg u256 t256;
      t256 = buf.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = (LEN % 32); /* int:i */ 
  }
  if (16 <= LEN) {
    reg u128 t128;
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
    DELTA = (DELTA + 16); /* int:i */ 
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */ 
    at = (at +64u ((64u) 2)); /* u64 */ 
    LEN = (LEN - 16); /* int:i */ 
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
    DELTA = (DELTA + 8); /* int:i */ 
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */ 
    at = (at +64u ((64u) 1)); /* u64 */ 
    LEN = (LEN - 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      A1600::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */ 
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (pst, ALL, offset);
}

inline
fn A1600::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                               reg u256[7] st,
                               reg const ptr u8[A1600::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  reg u64 i;
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      A1600::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */ 
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */ 
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        A1600::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) = A1600::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        A1600::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          A1600::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn A1600::__dumpstate_array_avx2 (reg mut ptr u8[A1600::_ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 reg u256[7] st) -> (reg mut ptr u8[A1600::_ASIZE],
                                                    reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      A1600::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      A1600::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) =
    A1600::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, LEN) = A1600::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, LEN) =
        A1600::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          A1600::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn A1600::__squeeze_array_avx2 (reg mut ptr u8[A1600::_ASIZE] buf,
                               reg u64 offset, inline int LEN,
                               reg u256[7] st, inline int RATE8) -> (reg mut ptr u8[A1600::_ASIZE],
                                                                    reg u256[7])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) =
          A1600::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = A1600::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn A1600::__addstate_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1600::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int _LEN,
                                  inline int _TRAILB) -> (reg mut ptr u256[25],
                                                         inline int,
                                                         reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u256 w;
  if ((AT8 % 8) != 0) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, w) =
      A1600::__a_ilen_read_bcast_upto8_at(buf, offset, DELTA, _LEN, _TRAILB,
                                          AT, AT8);
    w = (w ^256u st[(AT / 8)]); /* u256 */ 
    st[(AT / 8)] = w; /* u256 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (32 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) (32 * ((AT / 8) + (_LEN / 8)))))) {
    w =
      #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */) offset)]); /*  */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    at = (at +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, w) =
      A1600::__a_ilen_read_bcast_upto8_at(buf, offset, 0, _LEN, _TRAILB, AT,
                                          AT);
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A1600::__absorb_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1600::_ASIZE] buf,
                                inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1600::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1600::__addstate_bcast_avx2x4(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1600::__addstate_bcast_avx2x4(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1600::__addstate_bcast_avx2x4(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1600::__addstate_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                            reg const ptr u8[A1600::_ASIZE] buf0,
                            reg const ptr u8[A1600::_ASIZE] buf1,
                            reg const ptr u8[A1600::_ASIZE] buf2,
                            reg const ptr u8[A1600::_ASIZE] buf3,
                            #[Internal::wint::unsigned] reg ui64 offset,
                            inline int _LEN, inline int _TRAILB) -> (reg mut ptr u256[25],
                                                                    inline int,
                                                                    reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if ((AT8 % 8) != 0) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A1600::__a_ilen_read_upto8_at(buf0, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 0)] =
      (st[:u64 ((4 * (AT / 8)) + 0)] ^64u t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A1600::__a_ilen_read_upto8_at(buf1, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 1)] =
      (st[:u64 ((4 * (AT / 8)) + 1)] ^64u t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A1600::__a_ilen_read_upto8_at(buf2, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 2)] =
      (st[:u64 ((4 * (AT / 8)) + 2)] ^64u t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, t3) =
      A1600::__a_ilen_read_upto8_at(buf3, offset, DELTA, _LEN, _TRAILB, AT,
                                    AT8);
    st[:u64 ((4 * (AT / 8)) + 3)] =
      (st[:u64 ((4 * (AT / 8)) + 3)] ^64u t3); /* u64 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (4 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) ((4 * (AT / 8)) + (4 * (_LEN / 8)))))) {
    t0 = buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    t1 = buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    t2 = buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    t3 = buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    at = (at +64ui ((64ui /* of int */) 4)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      A1600::__a_ilen_read_upto8_at(buf0, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      A1600::__a_ilen_read_upto8_at(buf1, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      A1600::__a_ilen_read_upto8_at(buf2, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t3) =
      A1600::__a_ilen_read_upto8_at(buf3, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn A1600::__absorb_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                          reg const ptr u8[A1600::_ASIZE] buf0,
                          reg const ptr u8[A1600::_ASIZE] buf1,
                          reg const ptr u8[A1600::_ASIZE] buf2,
                          reg const ptr u8[A1600::_ASIZE] buf3,
                          inline int _TRAILB, inline int _RATE8) -> (reg mut ptr u256[25],
                                                                    inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1600::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      A1600::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                               (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1600::__addstate_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                 _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    A1600::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, _LEN,
                             _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn A1600::__dumpstate_avx2x4 (reg mut ptr u8[A1600::_ASIZE] buf0,
                             reg mut ptr u8[A1600::_ASIZE] buf1,
                             reg mut ptr u8[A1600::_ASIZE] buf2,
                             reg mut ptr u8[A1600::_ASIZE] buf3,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int _LEN, reg const ptr u256[25] st) -> 
(reg mut ptr u8[A1600::_ASIZE], reg mut ptr u8[A1600::_ASIZE],
reg mut ptr u8[A1600::_ASIZE], reg mut ptr u8[A1600::_ASIZE], reg ui64)

{
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * (_LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (0 * 32))))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (1 * 32))))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (2 * 32))))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (3 * 32))))]; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x0; /* u256 */ 
    buf1.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x1; /* u256 */ 
    buf2.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x2; /* u256 */ 
    buf3.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x3; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <64ui ((64ui /* of int */) (8 * (_LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t3; /* u64 */ 
    i = (i +64ui ((64ui /* of int */) 8)); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
  }
  if (0 < (_LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1600::__a_ilen_write_upto8(buf0, offset, 0, (_LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1600::__a_ilen_write_upto8(buf1, offset, 0, (_LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1600::__a_ilen_write_upto8(buf2, offset, 0, (_LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1600::__a_ilen_write_upto8(buf3, offset, 0, (_LEN % 8), t3);
    offset = (offset +64ui ((64ui /* of int */) (_LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1600::__squeeze_avx2x4 (reg mut ptr u256[25] st,
                           reg mut ptr u8[A1600::_ASIZE] buf0,
                           reg mut ptr u8[A1600::_ASIZE] buf1,
                           reg mut ptr u8[A1600::_ASIZE] buf2,
                           reg mut ptr u8[A1600::_ASIZE] buf3,
                           inline int _RATE8) -> (reg mut ptr u256[25],
                                                 reg mut ptr u8[A1600::_ASIZE],
                                                 reg mut ptr u8[A1600::_ASIZE],
                                                 reg mut ptr u8[A1600::_ASIZE],
                                                 reg mut ptr u8[A1600::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = A1600::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  if (0 < ITERS) {
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1600::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, _RATE8, st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  if (0 < LO) {
    st = _keccakf1600_avx2x4(st);
    #[inline]
    (buf0, buf1, buf2, buf3, offset) =
      A1600::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
  }
  return (st, buf0, buf1, buf2, buf3);
}

inline
fn A1600::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                        inline int AT,
                                        reg const ptr u8[A1600::_ASIZE] buf,
                                        reg u64 offset, inline int LEN,
                                        inline int TRAILB) -> (reg mut ptr u256[25],
                                                              inline int,
                                                              reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (32 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u256 t256;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        A1600::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          A1600::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      A1600::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
    st.[#unaligned at] = t256; /* u256 */ 
  }
  return (st, ALL, offset);
}

inline
fn A1600::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                      inline int AT,
                                      reg const ptr u8[A1600::_ASIZE] buf,
                                      reg u64 offset, inline int LEN,
                                      inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1600::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1600::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                             (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1600::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A1600::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1600::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[A1600::_ASIZE] buf0,
                                  reg const ptr u8[A1600::_ASIZE] buf1,
                                  reg const ptr u8[A1600::_ASIZE] buf2,
                                  reg const ptr u8[A1600::_ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  inline int TRAILB) -> (reg mut ptr u256[25],
                                                        inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (4 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        A1600::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        A1600::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        A1600::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        A1600::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
      TRAILB = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */ 
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          A1600::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          A1600::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          A1600::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          A1600::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      reg u256 t256_0;
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_1;
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_2;
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_3;
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 =
        (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */ 
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */ 
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */ 
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */ 
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */ 
      at = (at +64u ((64u) 16)); /* u64 */ 
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      A1600::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      A1600::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      A1600::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      A1600::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
      TRAILB = 0; /* int:i */ 
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
  }
  return (st, ALL, offset);
}

inline
fn A1600::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                reg const ptr u8[A1600::_ASIZE] buf0,
                                reg const ptr u8[A1600::_ASIZE] buf1,
                                reg const ptr u8[A1600::_ASIZE] buf2,
                                reg const ptr u8[A1600::_ASIZE] buf3,
                                reg u64 offset, inline int LEN,
                                inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      A1600::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        A1600::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                       offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        A1600::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                       RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      A1600::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                     LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn A1600::__dumpstate_array_avx2x4 (reg mut ptr u8[A1600::_ASIZE] buf0,
                                   reg mut ptr u8[A1600::_ASIZE] buf1,
                                   reg mut ptr u8[A1600::_ASIZE] buf2,
                                   reg mut ptr u8[A1600::_ASIZE] buf3,
                                   reg u64 offset, inline int LEN,
                                   reg const ptr u256[25] st) -> (reg mut ptr u8[A1600::_ASIZE],
                                                                 reg mut ptr u8[A1600::_ASIZE],
                                                                 reg mut ptr u8[A1600::_ASIZE],
                                                                 reg mut ptr u8[A1600::_ASIZE],
                                                                 reg u64)

{
  reg u64 i;
  i = ((64u) 0); /* u64 */ 
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */ 
    i = (i +64u ((64u) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */ 
    buf1.[#unaligned :u256 offset] = x1; /* u256 */ 
    buf2.[#unaligned :u256 offset] = x2; /* u256 */ 
    buf3.[#unaligned :u256 offset] = x3; /* u256 */ 
    offset = (offset +64u ((64u) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    buf0.[#unaligned :u64 offset] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    buf1.[#unaligned :u64 offset] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    buf2.[#unaligned :u64 offset] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    buf3.[#unaligned :u64 offset] = t3; /* u64 */ 
    i = (i +64u ((64u) 8)); /* u64 */ 
    offset = (offset +64u ((64u) 8)); /* u64 */ 
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      A1600::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      A1600::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      A1600::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      A1600::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn A1600::__squeeze_array_avx2x4 (reg mut ptr u8[A1600::_ASIZE] buf0,
                                 reg mut ptr u8[A1600::_ASIZE] buf1,
                                 reg mut ptr u8[A1600::_ASIZE] buf2,
                                 reg mut ptr u8[A1600::_ASIZE] buf3,
                                 reg u64 offset, inline int LEN,
                                 reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[A1600::_ASIZE], reg mut ptr u8[A1600::_ASIZE],
reg mut ptr u8[A1600::_ASIZE], reg mut ptr u8[A1600::_ASIZE], reg u64,
reg mut ptr u256[25])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          A1600::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                          RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        A1600::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO,
                                        st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

param int ABUFLEN::_ASIZE = 536;

inline
fn ABUFLEN::__a_ilen_read_upto8_at (reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                   #[Internal::wint::unsigned]
                                   reg ui64 offset, inline int DELTA,
                                   inline int LEN, inline int TRAIL,
                                   inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u64)

{
  reg u64 w;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = ((64u) 0); /* u64 */ 
  } else {
    inline int AT8;
    AT8 = (AT - CUR); /* int:i */ 
    if (8 <= LEN) {
      w =
        buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                             ((64u) DELTA))]; /* u64 */ 
      #[inline]
      w = __SHLQ(w, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u)
          buf.[#unaligned :u32 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        #[inline]
        w = __SHLQ(w, AT8);
        DELTA = (DELTA + (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        LEN = (LEN - (((4 + AT8) >= 8) ? (8 - AT8) : 4)); /* int:i */ 
        AT8 = (((4 + AT8) >= 8) ? 8 : (4 + AT8)); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      if ((AT8 < 8) && (2 <= LEN)) {
        reg u64 t16;
        t16 =
          ((64u)
          buf.[#unaligned :u16 (((64u /* of ui64 */) offset) +64u
                               ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        LEN = (LEN - (((2 + AT8) >= 8) ? (8 - AT8) : 2)); /* int:i */ 
        #[inline]
        t16 = __SHLQ(t16, AT8);
        w = (w |64u t16); /* u64 */ 
        AT8 = (((2 + AT8) >= 8) ? 8 : (2 + AT8)); /* int:i */ 
      }
      if (AT8 < 8) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u)
            buf.[#unaligned (((64u /* of ui64 */) offset) +64u ((64u) DELTA))]); /* u64 */ 
          t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
          #[inline]
          t8 = __SHLQ(t8, AT8);
          w = (w |64u t8); /* u64 */ 
          AT8 = (AT8 + 1); /* int:i */ 
          if ((AT8 < 8) && ((TRAIL % 256) != 0)) {
            AT8 = (AT8 + 1); /* int:i */ 
            TRAIL = 0; /* int:i */ 
          }
        } else {
          if ((TRAIL % 256) != 0) {
            t8 = ((64u) (TRAIL % 256)); /* u64 */ 
            #[inline]
            t8 = __SHLQ(t8, AT8);
            w = (w |64u t8); /* u64 */ 
            TRAIL = 0; /* int:i */ 
            AT8 = (AT8 + 1); /* int:i */ 
          }
        }
      }
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn ABUFLEN::__a_ilen_read_upto16_at (reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                    #[Internal::wint::unsigned]
                                    reg ui64 offset, inline int DELTA,
                                    inline int LEN, inline int TRAIL,
                                    inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u128)

{
  reg u128 w;
  if (((AT < CUR) || ((CUR + 16) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_128(); /*  */ 
  } else {
    inline int AT16;
    AT16 = (AT - CUR); /* int:i */ 
    if (16 <= LEN) {
      w =
        buf.[#unaligned :u128 (((64u /* of ui64 */) offset) +64u
                              ((64u) DELTA))]; /* u128 */ 
      #[inline]
      w = __SHLDQ(w, AT16);
      DELTA = (DELTA + (16 - AT16)); /* int:i */ 
      LEN = (LEN - (16 - AT16)); /* int:i */ 
      AT16 = 16; /* int:i */ 
    } else {
      reg u64 t64_1;
      if (8 <= AT16) {
        w = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          ABUFLEN::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                          AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      } else {
        reg u64 t64_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_0) =
          ABUFLEN::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                          AT16);
        w = ((128u) t64_0); /* u128 */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT16, t64_1) =
          ABUFLEN::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, 8,
                                          AT16);
        w = #VPINSR_2u64(w, t64_1, ((8u) 1)); /*  */ 
      }
    }
    AT = (CUR + AT16); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn ABUFLEN::__a_ilen_read_upto32_at (reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                    #[Internal::wint::unsigned]
                                    reg ui64 offset, inline int DELTA,
                                    inline int LEN, inline int TRAIL,
                                    inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if (((AT < CUR) || ((CUR + 32) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w = #set0_256(); /*  */ 
  } else {
    inline int AT32;
    AT32 = (AT - CUR); /* int:i */ 
    if ((AT32 == 0) && (32 <= LEN)) {
      w =
        buf.[#unaligned :u256 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))]; /* u256 */ 
      AT32 = (AT32 + 32); /* int:i */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_1;
      if (16 <= AT32) {
        w = #set0_256(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          ABUFLEN::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL,
                                           16, AT32);
        w = #VINSERTI128(w, t128_1, ((8u) 1)); /*  */ 
      } else {
        reg u128 t128_0;
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_0) =
          ABUFLEN::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL, 0,
                                           AT32);
        #[inline]
        (DELTA, LEN, TRAIL, AT32, t128_1) =
          ABUFLEN::__a_ilen_read_upto16_at(buf, offset, DELTA, LEN, TRAIL,
                                           16, AT32);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
    AT = (CUR + AT32); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w);
}

inline
fn ABUFLEN::__a_ilen_read_bcast_upto8_at (reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                         #[Internal::wint::unsigned]
                                         reg ui64 offset, inline int DELTA,
                                         inline int LEN, inline int TRAIL,
                                         inline int CUR, inline int AT) -> 
(inline int, inline int, inline int, inline int, reg u256)

{
  reg u256 w256;
  if (((AT < CUR) || ((CUR + 8) <= AT)) || ((LEN == 0) && (TRAIL == 0))) {
    w256 = #set0_256(); /*  */ 
  } else {
    inline int AT8;
    if (8 <= LEN) {
      AT8 = (AT - CUR); /* int:i */ 
      w256 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */)
                                               (offset +64ui
                                               ((64ui /* of int */) DELTA)))]); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    } else {
      AT8 = (AT - CUR); /* int:i */ 
      reg u64 w;
      #[inline]
      (DELTA, LEN, TRAIL, AT, w) =
        ABUFLEN::__a_ilen_read_upto8_at(buf, offset, DELTA, LEN, TRAIL, CUR,
                                        AT);
      reg u128 t128;
      t128 = ((128u) w); /* u128 */ 
      w256 = #VPBROADCAST_4u64(t128); /*  */ 
      #[inline]
      w256 = __SHLQ_256(w256, AT8);
      DELTA = (DELTA + (8 - AT8)); /* int:i */ 
      LEN = (LEN - (8 - AT8)); /* int:i */ 
      AT8 = 8; /* int:i */ 
    }
    AT = (CUR + AT8); /* int:i */ 
  }
  return (DELTA, LEN, TRAIL, AT, w256);
}

inline
fn ABUFLEN::__a_ilen_write_upto8 (reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                                 #[Internal::wint::unsigned] reg ui64 offset,
                                 inline int DELTA, inline int LEN, reg u64 w) -> 
(reg mut ptr u8[ABUFLEN::_ASIZE], inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 ((uint /* of ui64 */)
                           (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned ((uint /* of ui64 */)
                        (offset +64ui ((64ui /* of int */) DELTA)))] =
          w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn ABUFLEN::__a_ilen_write_upto16 (reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int DELTA,
                                  inline int LEN, reg u128 w) -> (reg mut ptr u8[ABUFLEN::_ASIZE],
                                                                 inline int,
                                                                 inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 ((uint /* of ui64 */)
                             (offset +64ui ((64ui /* of int */) DELTA)))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        ABUFLEN::__a_ilen_write_upto8(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn ABUFLEN::__a_ilen_write_upto32 (reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                                  #[Internal::wint::unsigned]
                                  reg ui64 offset, inline int DELTA,
                                  inline int LEN, reg u256 w) -> (reg mut ptr u8[ABUFLEN::_ASIZE],
                                                                 inline int,
                                                                 inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 ((uint /* of ui64 */)
                            (offset +64ui ((64ui /* of int */) DELTA)))] =
        w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 ((uint /* of ui64 */)
                              (offset +64ui ((64ui /* of int */) DELTA)))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        ABUFLEN::__a_ilen_write_upto16(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn ABUFLEN::__a_rlen_read_upto8 (reg const ptr u8[ABUFLEN::_ASIZE] a,
                                #[Internal::wint::unsigned] reg ui64 off,
                                #[Internal::wint::unsigned] reg ui64 len) -> 
(reg ui64, reg u64)

{
  reg u64 w;
  if (len >=64ui ((64ui /* of int */) 8)) {
    w = a.[#unaligned :u64 ((uint /* of ui64 */) off)]; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    reg u8 sh;
    if (! zf) {
      w = ((64u) a.[#unaligned :u32 ((uint /* of ui64 */) off)]); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      sh = ((8u) 32); /* u8 */ 
    } else {
      w = ((64u) 0); /* u64 */ 
      sh = ((8u) 0); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    reg u64 x;
    if (! zf) {
      x = ((64u) a.[#unaligned :u16 ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      sh = (sh +8u ((8u) 16)); /* u8 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      x = ((64u) a.[#unaligned ((uint /* of ui64 */) off)]); /* u64 */ 
      x = (x <<64u (sh &8u ((8u) 63))); /* u64 */ 
      w = (w +64u x); /* u64 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (off, w);
}

inline
fn ABUFLEN::__a_rlen_write_upto8 (reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                                 #[Internal::wint::unsigned] reg ui64 off,
                                 reg u64 data, #[Internal::wint::unsigned]
                                 reg ui64 len) -> (reg mut ptr u8[ABUFLEN::_ASIZE],
                                                  reg ui64)

{
  if (len >=64ui ((64ui /* of int */) 8)) {
    buf.[#unaligned :u64 ((uint /* of ui64 */) off)] = data; /* u64 */ 
    off = (off +64ui ((64ui /* of int */) 8)); /* u64 */ 
  } else {
    reg bool zf;
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 4)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u32 ((uint /* of ui64 */) off)] = data; /* u32 */ 
      off = (off +64ui ((64ui /* of int */) 4)); /* u64 */ 
      data = (data >>64u ((8u) 32)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 2)); /*  */ 
    if (! zf) {
      buf.[#unaligned :u16 ((uint /* of ui64 */) off)] = data; /* u16 */ 
      off = (off +64ui ((64ui /* of int */) 2)); /* u64 */ 
      data = (data >>64u ((8u) 16)); /* u64 */ 
    }
    (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
      #TEST_64(((64u /* of ui64 */) len), ((64u) 1)); /*  */ 
    if (! zf) {
      buf.[#unaligned ((uint /* of ui64 */) off)] = data; /* u8 */ 
      off = (off +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  return (buf, off);
}

inline
fn ABUFLEN::__aread_subu64 (reg const ptr u8[ABUFLEN::_ASIZE] buf,
                           reg u64 offset, inline int DELTA, inline int LEN,
                           inline int TRAIL) -> (inline int, inline int,
                                                inline int, reg u64)

{
  inline int ILEN;
  ILEN = LEN; /* int:i */ 
  reg u64 w;
  if (LEN <= 0) {
    w = ((64u) (TRAIL % 256)); /* u64 */ 
    TRAIL = 0; /* int:i */ 
  } else {
    if (8 <= LEN) {
      w = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        w =
          ((64u) buf.[#unaligned :u32 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      } else {
        w = ((64u) 0); /* u64 */ 
      }
      reg u64 t16;
      if (2 <= LEN) {
        t16 =
          ((64u) buf.[#unaligned :u16 (offset +64u ((64u) DELTA))]); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      } else {
        t16 = ((64u) 0); /* u64 */ 
      }
      if ((1 <= LEN) || ((TRAIL % 256) != 0)) {
        reg u64 t8;
        if (1 <= LEN) {
          t8 =
            ((64u) buf.[#unaligned (offset +64u ((64u) DELTA))]); /* u64 */ 
          if ((TRAIL % 256) != 0) {
            t8 = (t8 |64u ((64u) (256 * (TRAIL % 256)))); /* u64 */ 
          }
          DELTA = (DELTA + 1); /* int:i */ 
          LEN = (LEN - 1); /* int:i */ 
        } else {
          t8 = ((64u) (TRAIL % 256)); /* u64 */ 
        }
        TRAIL = 0; /* int:i */ 
        t8 = (t8 <<64u ((8u) (8 * (2 * ((ILEN / 2) % 2))))); /* u64 */ 
        t16 = (t16 |64u t8); /* u64 */ 
      }
      t16 = (t16 <<64u ((8u) (8 * (4 * ((ILEN / 4) % 2))))); /* u64 */ 
      w = (w |64u t16); /* u64 */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn ABUFLEN::__aread_bcast_4subu64 (reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                  reg u64 offset, inline int DELTA,
                                  inline int LEN, inline int TRAIL) -> 
(inline int, inline int, inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (8 <= LEN) {
      w =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      reg u64 t64;
      #[inline]
      (DELTA, LEN, TRAIL, t64) =
        ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
      reg u128 t128;
      t128 = ((128u) t64); /* u128 */ 
      w = #VPBROADCAST_4u64(t128); /*  */ 
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn ABUFLEN::__aread_subu128 (reg const ptr u8[ABUFLEN::_ASIZE] buf,
                            reg u64 offset, inline int DELTA, inline int LEN,
                            inline int TRAIL) -> (inline int, inline int,
                                                 inline int, reg u128)

{
  reg u128 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_128(); /*  */ 
  } else {
    if (16 <= LEN) {
      w = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      reg u64 t64;
      if (8 <= LEN) {
        w =
          #VMOV_64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = #VPINSR_2u64(w, t64, ((8u) 1)); /*  */ 
      } else {
        #[inline]
        (DELTA, LEN, TRAIL, t64) =
          ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAIL);
        w = ((128u) t64); /* u128 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn ABUFLEN::__aread_subu256 (reg const ptr u8[ABUFLEN::_ASIZE] buf,
                            reg u64 offset, inline int DELTA, inline int LEN,
                            inline int TRAIL) -> (inline int, inline int,
                                                 inline int, reg u256)

{
  reg u256 w;
  if ((LEN <= 0) && ((TRAIL % 256) == 0)) {
    w = #set0_256(); /*  */ 
  } else {
    if (32 <= LEN) {
      w = buf.[#unaligned :u256 (offset +64u ((64u) DELTA))]; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128_0;
      reg u128 t128_1;
      if (16 <= LEN) {
        t128_0 =
          buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_1) =
          ABUFLEN::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      } else {
        t128_1 = #set0_128(); /*  */ 
        #[inline]
        (DELTA, LEN, TRAIL, t128_0) =
          ABUFLEN::__aread_subu128(buf, offset, DELTA, LEN, TRAIL);
        w =
          (2u128)[((uint /* of u128 */) t128_1),
          ((uint /* of u128 */) t128_0)]; /* u256 */ 
      }
    }
  }
  return (DELTA, LEN, TRAIL, w);
}

inline
fn ABUFLEN::__awrite_subu64 (reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                            reg u64 offset, inline int DELTA, inline int LEN,
                            reg u64 w) -> (reg mut ptr u8[ABUFLEN::_ASIZE],
                                          inline int, inline int)

{
  if (0 < LEN) {
    if (8 <= LEN) {
      buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] = w; /* u64 */ 
      DELTA = (DELTA + 8); /* int:i */ 
      LEN = (LEN - 8); /* int:i */ 
    } else {
      if (4 <= LEN) {
        buf.[#unaligned :u32 (offset +64u ((64u) DELTA))] = w; /* u32 */ 
        w = (w >>64u ((8u) 32)); /* u64 */ 
        DELTA = (DELTA + 4); /* int:i */ 
        LEN = (LEN - 4); /* int:i */ 
      }
      if (2 <= LEN) {
        buf.[#unaligned :u16 (offset +64u ((64u) DELTA))] = w; /* u16 */ 
        w = (w >>64u ((8u) 16)); /* u64 */ 
        DELTA = (DELTA + 2); /* int:i */ 
        LEN = (LEN - 2); /* int:i */ 
      }
      if (1 <= LEN) {
        buf.[#unaligned (offset +64u ((64u) DELTA))] = w; /* u8 */ 
        DELTA = (DELTA + 1); /* int:i */ 
        LEN = (LEN - 1); /* int:i */ 
      }
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn ABUFLEN::__awrite_subu128 (reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                             reg u64 offset, inline int DELTA,
                             inline int LEN, reg u128 w) -> (reg mut ptr u8[ABUFLEN::_ASIZE],
                                                            inline int,
                                                            inline int)

{
  if (0 < LEN) {
    if (16 <= LEN) {
      buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] = w; /* u128 */ 
      DELTA = (DELTA + 16); /* int:i */ 
      LEN = (LEN - 16); /* int:i */ 
    } else {
      if (8 <= LEN) {
        buf.[#unaligned :u64 (offset +64u ((64u) DELTA))] =
          #MOVV_64(w); /*  */ 
        DELTA = (DELTA + 8); /* int:i */ 
        LEN = (LEN - 8); /* int:i */ 
        w = #VPUNPCKH_2u64(w, w); /*  */ 
      }
      reg u64 t64;
      t64 = w; /* u64 */ 
      #[inline]
      (buf, DELTA, LEN) =
        ABUFLEN::__awrite_subu64(buf, offset, DELTA, LEN, t64);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn ABUFLEN::__awrite_subu256 (reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                             reg u64 offset, inline int DELTA,
                             inline int LEN, reg u256 w) -> (reg mut ptr u8[ABUFLEN::_ASIZE],
                                                            inline int,
                                                            inline int)

{
  if (0 < LEN) {
    if (32 <= LEN) {
      buf.[#unaligned :u256 (offset +64u ((64u) DELTA))] = w; /* u256 */ 
      DELTA = (DELTA + 32); /* int:i */ 
      LEN = (LEN - 32); /* int:i */ 
    } else {
      reg u128 t128;
      t128 = w; /* u128 */ 
      if (16 <= LEN) {
        buf.[#unaligned :u128 (offset +64u ((64u) DELTA))] =
          t128; /* u128 */ 
        DELTA = (DELTA + 16); /* int:i */ 
        LEN = (LEN - 16); /* int:i */ 
        t128 = #VEXTRACTI128(w, ((8u) 1)); /*  */ 
      }
      #[inline]
      (buf, DELTA, LEN) =
        ABUFLEN::__awrite_subu128(buf, offset, DELTA, LEN, t128);
    }
  }
  return (buf, DELTA, LEN);
}

inline
fn ABUFLEN::__addstate_avx2 (reg u256[7] st, inline int AT,
                            reg const ptr u8[ABUFLEN::_ASIZE] buf,
                            #[Internal::wint::unsigned] reg ui64 offset,
                            inline int _LEN, inline int _TRAILB) -> (reg u256[7],
                                                                    inline int,
                                                                    reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (AT < 8) {
    reg u256 r0;
    if ((AT == 0) && (8 <= _LEN)) {
      r0 =
        #VPBROADCAST_4u64(buf.[#unaligned :u64 (((64u /* of ui64 */) offset) +64u
                                               ((64u) DELTA))]); /*  */ 
      DELTA = (DELTA + 8); /* int:i */ 
      _LEN = (_LEN - 8); /* int:i */ 
      AT = 8; /* int:i */ 
    } else {
      reg u64 t64_1;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_1) =
        ABUFLEN::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 0,
                                        AT);
      reg u128 t128_0;
      t128_0 = ((128u) t64_1); /* u128 */ 
      r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
    }
    st[0] = (st[0] ^256u r0); /* u256 */ 
  }
  if ((AT < 40) && ((0 < _LEN) || (_TRAILB != 0))) {
    reg u256 r1;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, r1) =
      ABUFLEN::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB, 8,
                                       AT);
    st[1] = (st[1] ^256u r1); /* u256 */ 
  }
  if ((0 < _LEN) || (_TRAILB != 0)) {
    reg u64 t64_2;
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t64_2) =
      ABUFLEN::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB, 40,
                                      AT);
    reg u128 t128_1;
    t128_1 = ((128u) t64_2); /* u128 */ 
    reg u128 t128_2;
    t128_2 = #set0_128(); /*  */ 
    if ((0 < _LEN) || (_TRAILB != 0)) {
      reg u256 r3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r3) =
        ABUFLEN::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                         48, AT);
      reg u64 t64_3;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_3) =
        ABUFLEN::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB,
                                        80, AT);
      t128_2 = ((128u) t64_3); /* u128 */ 
      reg u256 r4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r4) =
        ABUFLEN::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                         88, AT);
      reg u64 t64_4;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_4) =
        ABUFLEN::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB,
                                        120, AT);
      t128_1 = #VPINSR_2u64(t128_1, t64_4, ((8u) 1)); /*  */ 
      reg u256 r5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r5) =
        ABUFLEN::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                         128, AT);
      reg u64 t64_5;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, t64_5) =
        ABUFLEN::__a_ilen_read_upto8_at(buf, offset, DELTA, _LEN, _TRAILB,
                                        160, AT);
      t128_2 = #VPINSR_2u64(t128_2, t64_5, ((8u) 1)); /*  */ 
      reg u256 r6;
      #[inline]
      (DELTA, _LEN, _TRAILB, AT, r6) =
        ABUFLEN::__a_ilen_read_upto32_at(buf, offset, DELTA, _LEN, _TRAILB,
                                         168, AT);
      #[inline]
      st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
    }
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_2)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (st, AT, offset);
}

inline
fn ABUFLEN::__absorb_avx2 (reg u256[7] st, inline int AT,
                          reg const ptr u8[ABUFLEN::_ASIZE] buf,
                          inline int _TRAILB, inline int _RATE8) -> (reg u256[7],
                                                                    inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = ABUFLEN::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      ABUFLEN::__addstate_avx2(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        ABUFLEN::__addstate_avx2(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    ABUFLEN::__addstate_avx2(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, _RATE8);
  }
  return (st, AT);
}

inline
fn ABUFLEN::__dumpstate_avx2 (reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                             #[Internal::wint::unsigned] reg ui64 offset,
                             inline int _LEN, reg u256[7] st) -> (reg mut ptr u8[ABUFLEN::_ASIZE],
                                                                 reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= _LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      ABUFLEN::__a_ilen_write_upto32(buf, offset, DELTA, 8, st[0]);
    _LEN = (_LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, _LEN) =
      ABUFLEN::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, _LEN) =
    ABUFLEN::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, st[1]);
  if (0 < _LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, _LEN) =
      ABUFLEN::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < _LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, _LEN) =
        ABUFLEN::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          ABUFLEN::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          ABUFLEN::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          ABUFLEN::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          ABUFLEN::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
      if (0 < _LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, _LEN) =
          ABUFLEN::__a_ilen_write_upto8(buf, offset, DELTA, _LEN, t);
      }
      if (0 < _LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, _LEN) =
          ABUFLEN::__a_ilen_write_upto32(buf, offset, DELTA, _LEN, t256_4);
      }
    }
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn ABUFLEN::__squeeze_avx2 (reg u256[7] st,
                           reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                           inline int _RATE8) -> (reg u256[7],
                                                 reg mut ptr u8[ABUFLEN::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = ABUFLEN::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) ITERS))) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = ABUFLEN::__dumpstate_avx2(buf, offset, _RATE8, st);
    i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
  }
  if (0 < LO) {
    st = _keccakf1600_avx2(st);
    #[inline]
    (buf, offset) = ABUFLEN::__dumpstate_avx2(buf, offset, LO, st);
  }
  return (st, buf);
}

inline
fn ABUFLEN::__addstate_array_avx2 (reg u256[7] st,
                                  reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                  reg u64 offset, inline int LEN,
                                  inline int TRAILB) -> (reg u256[7],
                                                        reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t64;
  #[inline]
  (DELTA, LEN, TRAILB, t64) =
    ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
  reg u128 t128_0;
  t128_0 = ((128u) t64); /* u128 */ 
  reg u256 r0;
  r0 = #VPBROADCAST_4u64(t128_0); /*  */ 
  st[0] = (st[0] ^256u r0); /* u256 */ 
  reg u256 r1;
  #[inline]
  (DELTA, LEN, TRAILB, r1) =
    ABUFLEN::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
  st[1] = (st[1] ^256u r1); /* u256 */ 
  if (0 < LEN) {
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    reg u128 t128_1;
    t128_1 = ((128u) t64); /* u128 */ 
    reg u256 r3;
    #[inline]
    (DELTA, LEN, TRAILB, r3) =
      ABUFLEN::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = ((128u) t64); /* u128 */ 
    reg u256 r4;
    #[inline]
    (DELTA, LEN, TRAILB, r4) =
      ABUFLEN::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_1 = #VPINSR_2u64(t128_1, t64, ((8u) 1)); /*  */ 
    reg u256 r5;
    #[inline]
    (DELTA, LEN, TRAILB, r5) =
      ABUFLEN::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    (DELTA, LEN, TRAILB, t64) =
      ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
    t128_0 = #VPINSR_2u64(t128_0, t64, ((8u) 1)); /*  */ 
    reg u256 r2;
    r2 =
      (2u128)[((uint /* of u128 */) t128_1), ((uint /* of u128 */) t128_0)]; /* u256 */ 
    st[2] = (st[2] ^256u r2); /* u256 */ 
    reg u256 r6;
    #[inline]
    (DELTA, LEN, TRAILB, r6) =
      ABUFLEN::__aread_subu256(buf, offset, DELTA, LEN, TRAILB);
    #[inline]
    st = __addstate_r3456_avx2(st, r3, r4, r5, r6);
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (st, offset);
}

inline
fn ABUFLEN::__absorb_array_avx2 (reg u256[7] st,
                                reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int RATE8, inline int TRAILB) -> 
(reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (LEN + ((TRAILB != 0) ? 1 : 0)); /* int:i */ 
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  if (0 < ITERS) {
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) =
        ABUFLEN::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
  }
  LEN = (LEN % RATE8); /* int:i */ 
  #[inline]
  (st, offset) =
    ABUFLEN::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
  if (TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2(st, RATE8);
  }
  return (st, offset);
}

inline
fn ABUFLEN::__pstate_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                                reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                reg u64 offset, inline int LEN,
                                inline int TRAILB) -> (reg mut ptr u64[25],
                                                      inline int, reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (AT / 8)); /* u64 */ 
  reg u64 t64;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t64) =
        ABUFLEN::__aread_subu64(buf, offset, DELTA, LEN, TRAILB);
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t64) =
          ABUFLEN::__aread_subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t64 = (t64 <<64u ((8u) (8 * LO))); /* u64 */ 
      pst[at] = (pst[at] ^64u t64); /* u64 */ 
      at = (at +64u ((64u) 1)); /* u64 */ 
    }
  }
  if (32 <= LEN) {
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
    while ((at <u ((64u) ((AT / 8) + (4 * (LEN / 32)))))) {
      reg u256 t256;
      t256 = buf.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      pst.[#unaligned :u256 (((64u) 8) *64u at)] = t256; /* u256 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = (LEN % 32); /* int:i */ 
  }
  if (16 <= LEN) {
    reg u128 t128;
    t128 = buf.[#unaligned :u128 (offset +64u ((64u) DELTA))]; /* u128 */ 
    DELTA = (DELTA + 16); /* int:i */ 
    pst.[#unaligned :u128 (((64u) 8) *64u at)] = t128; /* u128 */ 
    at = (at +64u ((64u) 2)); /* u64 */ 
    LEN = (LEN - 16); /* int:i */ 
  }
  if (8 <= LEN) {
    t64 = buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
    DELTA = (DELTA + 8); /* int:i */ 
    pst.[#unaligned (((64u) 8) *64u at)] = t64; /* u64 */ 
    at = (at +64u ((64u) 1)); /* u64 */ 
    LEN = (LEN - 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t64) =
      ABUFLEN::__aread_subu64(buf, offset, DELTA, LO, TRAILB);
    pst[(ALL / 8)] = t64; /* u64 */ 
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (pst, ALL, offset);
}

inline
fn ABUFLEN::__pabsorb_array_avx2 (reg mut ptr u64[25] pst, inline int AT,
                                 reg u256[7] st,
                                 reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u64[25], inline int, reg u256[7], reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  reg u64 i;
  if ((AT + LEN) < RATE8) {
    #[inline]
    (pst, AT, offset) =
      ABUFLEN::__pstate_array_avx2(pst, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      i = ((64u) ((AT / 8) + 1)); /* u64 */ 
      if (AT <= (5 * 8)) {
        while ((i <u ((64u) 5))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        #[inline]
        st = __addpst01_avx2(st, pst);
        #[inline]
        st = __addratebit_avx2(st, RATE8);
      } else {
        while ((i <u ((64u) (RATE8 / 8)))) {
          pst[i] = ((64u) 0); /* u64 */ 
          i = (i +64u ((64u) 1)); /* u64 */ 
        }
        pst[:u8 (RATE8 - 1)] =
          (pst[:u8 (RATE8 - 1)] ^8u ((8u) 128)); /* u8 */ 
        st = _addpstate_avx2(st, pst);
      }
    }
  } else {
    if (AT != 0) {
      #[inline]
      (pst, _ /* int */, offset) =
        ABUFLEN::__pstate_array_avx2(pst, AT, buf, offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _addpstate_avx2(st, pst);
      st = _keccakf1600_avx2(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, offset) =
        ABUFLEN::__addstate_array_avx2(st, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    if (TRAILB != 0) {
      #[inline]
      (st, offset) =
        ABUFLEN::__addstate_array_avx2(st, buf, offset, LEN, TRAILB);
      #[inline]
      st = __addratebit_avx2(st, RATE8);
    } else {
      if (LEN != 0) {
        #[inline]
        (pst, AT, offset) =
          ABUFLEN::__pstate_array_avx2(pst, 0, buf, offset, LEN, TRAILB);
      }
    }
  }
  return (pst, AT, st, offset);
}

inline
fn ABUFLEN::__dumpstate_array_avx2 (reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                                   reg u64 offset, inline int LEN,
                                   reg u256[7] st) -> (reg mut ptr u8[ABUFLEN::_ASIZE],
                                                      reg u64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    #[inline]
    (buf, DELTA, _ /* int */) =
      ABUFLEN::__awrite_subu256(buf, offset, DELTA, 8, st[0]);
    LEN = (LEN - 8); /* int:i */ 
  } else {
    #[inline]
    (buf, DELTA, LEN) =
      ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, st[0]);
  }
  #[inline]
  (buf, DELTA, LEN) =
    ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, st[1]);
  if (0 < LEN) {
    reg u128 t128_0;
    t128_0 = st[2]; /* u128 */ 
    reg u128 t128_1;
    t128_1 = #VEXTRACTI128(st[2], ((8u) 1)); /*  */ 
    reg u64 t;
    t = t128_1; /* u64 */ 
    #[inline]
    (buf, DELTA, LEN) = ABUFLEN::__awrite_subu64(buf, offset, DELTA, LEN, t);
    t128_1 = #VPUNPCKH_2u64(t128_1, t128_1); /*  */ 
    if (0 < LEN) {
      reg u256 t256_0;
      t256_0 =
        #VPBLEND_8u32(st[3], st[4], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_1;
      t256_1 =
        #VPBLEND_8u32(st[4], st[3], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_2;
      t256_2 =
        #VPBLEND_8u32(st[5], st[6], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_3;
      t256_3 =
        #VPBLEND_8u32(st[6], st[5], (8u1)[1, 1, 1, 1, 0, 0, 0, 0]); /*  */ 
      reg u256 t256_4;
      t256_4 =
        #VPBLEND_8u32(t256_0, t256_3, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
      #[inline]
      (buf, DELTA, LEN) =
        ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu64(buf, offset, DELTA, LEN, t);
        t128_0 = #VPUNPCKH_2u64(t128_0, t128_0); /*  */ 
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_3, t256_1, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_1; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_2, t256_0, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
      if (0 < LEN) {
        t = t128_0; /* u64 */ 
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu64(buf, offset, DELTA, LEN, t);
      }
      if (0 < LEN) {
        t256_4 =
          #VPBLEND_8u32(t256_1, t256_2, (8u1)[1, 1, 0, 0, 0, 0, 1, 1]); /*  */ 
        #[inline]
        (buf, DELTA, LEN) =
          ABUFLEN::__awrite_subu256(buf, offset, DELTA, LEN, t256_4);
      }
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  return (buf, offset);
}

inline
fn ABUFLEN::__squeeze_array_avx2 (reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                                 reg u64 offset, inline int LEN,
                                 reg u256[7] st, inline int RATE8) -> 
(reg mut ptr u8[ABUFLEN::_ASIZE], reg u256[7])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2(st);
        #[inline]
        (buf, offset) =
          ABUFLEN::__dumpstate_array_avx2(buf, offset, RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2(st);
      #[inline]
      (buf, offset) = ABUFLEN::__dumpstate_array_avx2(buf, offset, LO, st);
    }
  }
  return (buf, st);
}

inline
fn ABUFLEN::__addstate_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                    reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                    #[Internal::wint::unsigned]
                                    reg ui64 offset, inline int _LEN,
                                    inline int _TRAILB) -> (reg mut ptr u256[25],
                                                           inline int,
                                                           reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u256 w;
  if ((AT8 % 8) != 0) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, w) =
      ABUFLEN::__a_ilen_read_bcast_upto8_at(buf, offset, DELTA, _LEN,
                                            _TRAILB, AT, AT8);
    w = (w ^256u st[(AT / 8)]); /* u256 */ 
    st[(AT / 8)] = w; /* u256 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (32 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) (32 * ((AT / 8) + (_LEN / 8)))))) {
    w =
      #VPBROADCAST_4u64(buf.[#unaligned :u64 ((uint /* of ui64 */) offset)]); /*  */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    at = (at +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, w) =
      ABUFLEN::__a_ilen_read_bcast_upto8_at(buf, offset, 0, _LEN, _TRAILB,
                                            AT, AT);
    w = (w ^256u st.[#unaligned ((uint /* of ui64 */) at)]); /* u256 */ 
    st.[#unaligned ((uint /* of ui64 */) at)] = w; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn ABUFLEN::__absorb_bcast_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                  inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = ABUFLEN::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      ABUFLEN::__addstate_bcast_avx2x4(st, AT, buf, offset, (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        ABUFLEN::__addstate_bcast_avx2x4(st, 0, buf, offset, _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    ABUFLEN::__addstate_bcast_avx2x4(st, AT, buf, offset, _LEN, _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn ABUFLEN::__addstate_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                              reg const ptr u8[ABUFLEN::_ASIZE] buf0,
                              reg const ptr u8[ABUFLEN::_ASIZE] buf1,
                              reg const ptr u8[ABUFLEN::_ASIZE] buf2,
                              reg const ptr u8[ABUFLEN::_ASIZE] buf3,
                              #[Internal::wint::unsigned] reg ui64 offset,
                              inline int _LEN, inline int _TRAILB) -> 
(reg mut ptr u256[25], inline int, reg ui64)

{
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  inline int AT8;
  AT8 = AT; /* int:i */ 
  AT = (8 * (AT / 8)); /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if ((AT8 % 8) != 0) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      ABUFLEN::__a_ilen_read_upto8_at(buf0, offset, DELTA, _LEN, _TRAILB, AT,
                                      AT8);
    st[:u64 ((4 * (AT / 8)) + 0)] =
      (st[:u64 ((4 * (AT / 8)) + 0)] ^64u t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      ABUFLEN::__a_ilen_read_upto8_at(buf1, offset, DELTA, _LEN, _TRAILB, AT,
                                      AT8);
    st[:u64 ((4 * (AT / 8)) + 1)] =
      (st[:u64 ((4 * (AT / 8)) + 1)] ^64u t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      ABUFLEN::__a_ilen_read_upto8_at(buf2, offset, DELTA, _LEN, _TRAILB, AT,
                                      AT8);
    st[:u64 ((4 * (AT / 8)) + 2)] =
      (st[:u64 ((4 * (AT / 8)) + 2)] ^64u t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT8, t3) =
      ABUFLEN::__a_ilen_read_upto8_at(buf3, offset, DELTA, _LEN, _TRAILB, AT,
                                      AT8);
    st[:u64 ((4 * (AT / 8)) + 3)] =
      (st[:u64 ((4 * (AT / 8)) + 3)] ^64u t3); /* u64 */ 
    AT = AT8; /* int:i */ 
  }
  offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  #[Internal::wint::unsigned]
  reg ui64 at;
  at = ((64ui /* of int */) (4 * (AT / 8))); /* u64 */ 
  while ((at <64ui ((64ui /* of int */) ((4 * (AT / 8)) + (4 * (_LEN / 8)))))) {
    t0 = buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    t1 = buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    t2 = buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    t3 = buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)]; /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    at = (at +64ui ((64ui /* of int */) 4)); /* u64 */ 
  }
  AT = (AT + (8 * (_LEN / 8))); /* int:i */ 
  _LEN = (_LEN % 8); /* int:i */ 
  if ((0 < _LEN) || ((_TRAILB % 256) != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t0) =
      ABUFLEN::__a_ilen_read_upto8_at(buf0, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 0)))] ^64u
      t0); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t1) =
      ABUFLEN::__a_ilen_read_upto8_at(buf1, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 1)))] ^64u
      t1); /* u64 */ 
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, _ /* int */, t2) =
      ABUFLEN::__a_ilen_read_upto8_at(buf2, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 2)))] ^64u
      t2); /* u64 */ 
    #[inline]
    (DELTA, _LEN, _TRAILB, AT, t3) =
      ABUFLEN::__a_ilen_read_upto8_at(buf3, offset, 0, _LEN, _TRAILB, AT, AT);
    st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] =
      (st[:u64 ((uint /* of ui64 */) (at +64ui ((64ui /* of int */) 3)))] ^64u
      t3); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) DELTA)); /* u64 */ 
  }
  return (st, AT, offset);
}

inline
fn ABUFLEN::__absorb_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                            reg const ptr u8[ABUFLEN::_ASIZE] buf0,
                            reg const ptr u8[ABUFLEN::_ASIZE] buf1,
                            reg const ptr u8[ABUFLEN::_ASIZE] buf2,
                            reg const ptr u8[ABUFLEN::_ASIZE] buf3,
                            inline int _TRAILB, inline int _RATE8) -> 
(reg mut ptr u256[25], inline int)

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = ABUFLEN::_ASIZE; /* int:i */ 
  if ((AT + _LEN) >= _RATE8) {
    #[inline]
    (st, _ /* int */, offset) =
      ABUFLEN::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset,
                                 (_RATE8 - AT), 0);
    _LEN = (_LEN - (_RATE8 - AT)); /* int:i */ 
    AT = 0; /* int:i */ 
    st = _keccakf1600_avx2x4(st);
    inline int ITERS;
    ITERS = (_LEN / _RATE8); /* int:i */ 
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        ABUFLEN::__addstate_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                   _RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
    _LEN = (_LEN % _RATE8); /* int:i */ 
  }
  #[inline]
  (st, AT, _ /* u64 */) =
    ABUFLEN::__addstate_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, _LEN,
                               _TRAILB);
  if (_TRAILB != 0) {
    #[inline]
    st = __addratebit_avx2x4(st, _RATE8);
  }
  return (st, AT);
}

inline
fn ABUFLEN::__dumpstate_avx2x4 (reg mut ptr u8[ABUFLEN::_ASIZE] buf0,
                               reg mut ptr u8[ABUFLEN::_ASIZE] buf1,
                               reg mut ptr u8[ABUFLEN::_ASIZE] buf2,
                               reg mut ptr u8[ABUFLEN::_ASIZE] buf3,
                               #[Internal::wint::unsigned] reg ui64 offset,
                               inline int _LEN, reg const ptr u256[25] st) -> 
(reg mut ptr u8[ABUFLEN::_ASIZE], reg mut ptr u8[ABUFLEN::_ASIZE],
reg mut ptr u8[ABUFLEN::_ASIZE], reg mut ptr u8[ABUFLEN::_ASIZE], reg ui64)

{
  #[Internal::wint::unsigned]
  reg ui64 i;
  i = ((64ui /* of int */) 0); /* u64 */ 
  while ((i <64ui ((64ui /* of int */) (32 * (_LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (0 * 32))))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (1 * 32))))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (2 * 32))))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((uint /* of ui64 */)
                     ((((64ui /* of int */) 4) *64ui i) +64ui
                     ((64ui /* of int */) (3 * 32))))]; /* u256 */ 
    i = (i +64ui ((64ui /* of int */) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x0; /* u256 */ 
    buf1.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x1; /* u256 */ 
    buf2.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x2; /* u256 */ 
    buf3.[#unaligned :u256 ((uint /* of ui64 */) offset)] = x3; /* u256 */ 
    offset = (offset +64ui ((64ui /* of int */) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <64ui ((64ui /* of int */) (8 * (_LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    buf0.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    buf1.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    buf2.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    buf3.[#unaligned :u64 ((uint /* of ui64 */) offset)] = t3; /* u64 */ 
    i = (i +64ui ((64ui /* of int */) 8)); /* u64 */ 
    offset = (offset +64ui ((64ui /* of int */) 8)); /* u64 */ 
  }
  if (0 < (_LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (0 * 8))))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      ABUFLEN::__a_ilen_write_upto8(buf0, offset, 0, (_LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (1 * 8))))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      ABUFLEN::__a_ilen_write_upto8(buf1, offset, 0, (_LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (2 * 8))))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      ABUFLEN::__a_ilen_write_upto8(buf2, offset, 0, (_LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((uint /* of ui64 */)
                          ((((64ui /* of int */) 4) *64ui i) +64ui
                          ((64ui /* of int */) (3 * 8))))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      ABUFLEN::__a_ilen_write_upto8(buf3, offset, 0, (_LEN % 8), t3);
    offset = (offset +64ui ((64ui /* of int */) (_LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn ABUFLEN::__squeeze_avx2x4 (reg mut ptr u256[25] st,
                             reg mut ptr u8[ABUFLEN::_ASIZE] buf0,
                             reg mut ptr u8[ABUFLEN::_ASIZE] buf1,
                             reg mut ptr u8[ABUFLEN::_ASIZE] buf2,
                             reg mut ptr u8[ABUFLEN::_ASIZE] buf3,
                             inline int _RATE8) -> (reg mut ptr u256[25],
                                                   reg mut ptr u8[ABUFLEN::_ASIZE],
                                                   reg mut ptr u8[ABUFLEN::_ASIZE],
                                                   reg mut ptr u8[ABUFLEN::_ASIZE],
                                                   reg mut ptr u8[ABUFLEN::_ASIZE])

{
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  inline int _LEN;
  _LEN = ABUFLEN::_ASIZE; /* int:i */ 
  inline int ITERS;
  ITERS = (_LEN / _RATE8); /* int:i */ 
  inline int LO;
  LO = (_LEN % _RATE8); /* int:i */ 
  if (0 < ITERS) {
    #[Internal::wint::unsigned]
    reg ui64 i;
    i = ((64ui /* of int */) 0); /* u64 */ 
    while ((i <64ui ((64ui /* of int */) ITERS))) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        ABUFLEN::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, _RATE8,
                                    st);
      i = (i +64ui ((64ui /* of int */) 1)); /* u64 */ 
    }
  }
  if (0 < LO) {
    st = _keccakf1600_avx2x4(st);
    #[inline]
    (buf0, buf1, buf2, buf3, offset) =
      ABUFLEN::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
  }
  return (st, buf0, buf1, buf2, buf3);
}

inline
fn ABUFLEN::__addstate_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                          inline int AT,
                                          reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                          reg u64 offset, inline int LEN,
                                          inline int TRAILB) -> (reg mut ptr u256[25],
                                                                inline int,
                                                                reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (32 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u256 t256;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (DELTA, _ /* int */, TRAILB, t256) =
        ABUFLEN::__aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t256 =
          #VPBROADCAST_4u64(buf.[#unaligned :u64 (offset +64u ((64u) DELTA))]); /*  */ 
        DELTA = (DELTA + (8 - LO)); /* int:i */ 
      } else {
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t256) =
          ABUFLEN::__aread_bcast_4subu64(buf, offset, DELTA, (8 - LO), 0);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t256 = #VPSLL_4u64(t256, ((128u) (8 * LO))); /*  */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    DELTA = 0; /* int:i */ 
  }
  if (8 <= LEN) {
    while ((at <u ((64u) ((32 * (AT / 8)) + (32 * (LEN / 8)))))) {
      t256 = #VPBROADCAST_4u64(buf.[#unaligned :u64 offset]); /*  */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
      st.[#unaligned at] = t256; /* u256 */ 
      at = (at +64u ((64u) 32)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
    }
    #[inline]
    (DELTA, _ /* int */, TRAILB, t256) =
      ABUFLEN::__aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    t256 = (t256 ^256u st.[#unaligned at]); /* u256 */ 
    st.[#unaligned at] = t256; /* u256 */ 
  }
  return (st, ALL, offset);
}

inline
fn ABUFLEN::__absorb_bcast_array_avx2x4 (reg mut ptr u256[25] st,
                                        inline int AT,
                                        reg const ptr u8[ABUFLEN::_ASIZE] buf,
                                        reg u64 offset, inline int LEN,
                                        inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      ABUFLEN::__addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        ABUFLEN::__addstate_bcast_array_avx2x4(st, AT, buf, offset,
                                               (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        ABUFLEN::__addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      ABUFLEN::__addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn ABUFLEN::__addstate_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                    reg const ptr u8[ABUFLEN::_ASIZE] buf0,
                                    reg const ptr u8[ABUFLEN::_ASIZE] buf1,
                                    reg const ptr u8[ABUFLEN::_ASIZE] buf2,
                                    reg const ptr u8[ABUFLEN::_ASIZE] buf3,
                                    reg u64 offset, inline int LEN,
                                    inline int TRAILB) -> (reg mut ptr u256[25],
                                                          inline int,
                                                          reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  inline int LO;
  LO = (AT % 8); /* int:i */ 
  reg u64 at;
  at = ((64u) (4 * (AT / 8))); /* u64 */ 
  inline int DELTA;
  DELTA = 0; /* int:i */ 
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  if (0 < LO) {
    if ((LO + LEN) < 8) {
      if (TRAILB != 0) {
        ALL = (ALL + 1); /* int:i */ 
      }
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t0) =
        ABUFLEN::__aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t1) =
        ABUFLEN::__aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      #[inline]
      (_ /* int */, _ /* int */, _ /* int */, t2) =
        ABUFLEN::__aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      #[inline]
      (DELTA, _ /* int */, _ /* int */, t3) =
        ABUFLEN::__aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      LO = 0; /* int:i */ 
      AT = 0; /* int:i */ 
      LEN = 0; /* int:i */ 
      TRAILB = 0; /* int:i */ 
    } else {
      if (8 <= LEN) {
        t0 = buf0.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t1 = buf1.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t2 = buf2.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        t3 = buf3.[#unaligned :u64 (offset +64u ((64u) DELTA))]; /* u64 */ 
        offset = (offset +64u ((64u) (8 - LO))); /* u64 */ 
      } else {
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t0) =
          ABUFLEN::__aread_subu64(buf0, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t1) =
          ABUFLEN::__aread_subu64(buf1, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (_ /* int */, _ /* int */, _ /* int */, t2) =
          ABUFLEN::__aread_subu64(buf2, offset, DELTA, (8 - LO), TRAILB);
        #[inline]
        (DELTA, _ /* int */, _ /* int */, t3) =
          ABUFLEN::__aread_subu64(buf3, offset, DELTA, (8 - LO), TRAILB);
      }
      LEN = (LEN - (8 - LO)); /* int:i */ 
      AT = (AT + (8 - LO)); /* int:i */ 
      t0 = (t0 <<64u ((8u) (8 * LO))); /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = (t1 <<64u ((8u) (8 * LO))); /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = (t2 <<64u ((8u) (8 * LO))); /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = (t3 <<64u ((8u) (8 * LO))); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
  }
  offset = (offset +64u ((64u) DELTA)); /* u64 */ 
  DELTA = 0; /* int:i */ 
  if (8 <= LEN) {
    while ((at <u ((64u) ((4 * (AT / 8)) + (16 * (LEN / 32)))))) {
      reg u256 t256_0;
      t256_0 = buf0.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_1;
      t256_1 = buf1.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_2;
      t256_2 = buf2.[#unaligned :u256 offset]; /* u256 */ 
      reg u256 t256_3;
      t256_3 = buf3.[#unaligned :u256 offset]; /* u256 */ 
      offset = (offset +64u ((64u) 32)); /* u64 */ 
      #[inline]
      (t256_0, t256_1, t256_2, t256_3) =
        __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      t256_0 =
        (t256_0 ^256u st.[#unaligned (((64u) 8) *64u at)]); /* u256 */ 
      st.[#unaligned (((64u) 8) *64u at)] = t256_0; /* u256 */ 
      t256_1 =
        (t256_1 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 32))] =
        t256_1; /* u256 */ 
      t256_2 =
        (t256_2 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 64))] =
        t256_2; /* u256 */ 
      t256_3 =
        (t256_3 ^256u st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))]); /* u256 */ 
      st.[#unaligned ((((64u) 8) *64u at) +64u ((64u) 96))] =
        t256_3; /* u256 */ 
      at = (at +64u ((64u) 16)); /* u64 */ 
    }
    while ((at <u ((64u) ((4 * (AT / 8)) + (4 * (LEN / 8)))))) {
      t0 = buf0.[#unaligned :u64 offset]; /* u64 */ 
      t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
      t1 = buf1.[#unaligned :u64 offset]; /* u64 */ 
      t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
      t2 = buf2.[#unaligned :u64 offset]; /* u64 */ 
      t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
      t3 = buf3.[#unaligned :u64 offset]; /* u64 */ 
      offset = (offset +64u ((64u) 8)); /* u64 */ 
      t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
      st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
      at = (at +64u ((64u) 4)); /* u64 */ 
    }
    LEN = ((AT + LEN) % 8); /* int:i */ 
  }
  LO = ((AT + LEN) % 8); /* int:i */ 
  if ((0 < LO) || (TRAILB != 0)) {
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t0) =
      ABUFLEN::__aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t1) =
      ABUFLEN::__aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    #[inline]
    (_ /* int */, _ /* int */, _ /* int */, t2) =
      ABUFLEN::__aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    #[inline]
    (DELTA, _ /* int */, _ /* int */, t3) =
      ABUFLEN::__aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset = (offset +64u ((64u) DELTA)); /* u64 */ 
    if (TRAILB != 0) {
      ALL = (ALL + 1); /* int:i */ 
      TRAILB = 0; /* int:i */ 
    }
    t0 = (t0 ^64u st[:u64 (at +64u ((64u) 0))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 0))] = t0; /* u64 */ 
    t1 = (t1 ^64u st[:u64 (at +64u ((64u) 1))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 1))] = t1; /* u64 */ 
    t2 = (t2 ^64u st[:u64 (at +64u ((64u) 2))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 2))] = t2; /* u64 */ 
    t3 = (t3 ^64u st[:u64 (at +64u ((64u) 3))]); /* u64 */ 
    st[:u64 (at +64u ((64u) 3))] = t3; /* u64 */ 
  }
  return (st, ALL, offset);
}

inline
fn ABUFLEN::__absorb_array_avx2x4 (reg mut ptr u256[25] st, inline int AT,
                                  reg const ptr u8[ABUFLEN::_ASIZE] buf0,
                                  reg const ptr u8[ABUFLEN::_ASIZE] buf1,
                                  reg const ptr u8[ABUFLEN::_ASIZE] buf2,
                                  reg const ptr u8[ABUFLEN::_ASIZE] buf3,
                                  reg u64 offset, inline int LEN,
                                  inline int RATE8, inline int TRAILB) -> 
(reg mut ptr u256[25], inline int, reg u64)

{
  inline int ALL;
  ALL = (AT + LEN); /* int:i */ 
  if ((AT + LEN) < RATE8) {
    #[inline]
    (st, AT, offset) =
      ABUFLEN::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                       offset, LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else {
    if (AT != 0) {
      #[inline]
      (st, _ /* int */, offset) =
        ABUFLEN::__addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3,
                                         offset, (RATE8 - AT), 0);
      LEN = (LEN - (RATE8 - AT)); /* int:i */ 
      st = _keccakf1600_avx2x4(st);
      AT = 0; /* int:i */ 
    }
    inline int ITERS;
    ITERS = (LEN / RATE8); /* int:i */ 
    reg u64 i;
    i = ((64u) 0); /* u64 */ 
    while ((i <u ((64u) ITERS))) {
      #[inline]
      (st, _ /* int */, offset) =
        ABUFLEN::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3,
                                         offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i = (i +64u ((64u) 1)); /* u64 */ 
    }
    LEN = (ALL % RATE8); /* int:i */ 
    #[inline]
    (st, AT, offset) =
      ABUFLEN::__addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset,
                                       LEN, TRAILB);
    if (TRAILB != 0) {
      #[inline]
      st = __addratebit_avx2x4(st, RATE8);
    }
  }
  return (st, AT, offset);
}

inline
fn ABUFLEN::__dumpstate_array_avx2x4 (reg mut ptr u8[ABUFLEN::_ASIZE] buf0,
                                     reg mut ptr u8[ABUFLEN::_ASIZE] buf1,
                                     reg mut ptr u8[ABUFLEN::_ASIZE] buf2,
                                     reg mut ptr u8[ABUFLEN::_ASIZE] buf3,
                                     reg u64 offset, inline int LEN,
                                     reg const ptr u256[25] st) -> (reg mut ptr u8[ABUFLEN::_ASIZE],
                                                                   reg mut ptr u8[ABUFLEN::_ASIZE],
                                                                   reg mut ptr u8[ABUFLEN::_ASIZE],
                                                                   reg mut ptr u8[ABUFLEN::_ASIZE],
                                                                   reg u64)

{
  reg u64 i;
  i = ((64u) 0); /* u64 */ 
  while ((i <s ((64u) (32 * (LEN / 32))))) {
    reg u256 x0;
    x0 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (0 * 32)))]; /* u256 */ 
    reg u256 x1;
    x1 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (1 * 32)))]; /* u256 */ 
    reg u256 x2;
    x2 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (2 * 32)))]; /* u256 */ 
    reg u256 x3;
    x3 =
      st.[#unaligned ((((64u) 4) *64u i) +64u ((64u) (3 * 32)))]; /* u256 */ 
    i = (i +64u ((64u) 32)); /* u64 */ 
    #[inline]
    (x0, x1, x2, x3) = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[#unaligned :u256 offset] = x0; /* u256 */ 
    buf1.[#unaligned :u256 offset] = x1; /* u256 */ 
    buf2.[#unaligned :u256 offset] = x2; /* u256 */ 
    buf3.[#unaligned :u256 offset] = x3; /* u256 */ 
    offset = (offset +64u ((64u) 32)); /* u64 */ 
  }
  reg u64 t0;
  reg u64 t1;
  reg u64 t2;
  reg u64 t3;
  while ((i <s ((64u) (8 * (LEN / 8))))) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    buf0.[#unaligned :u64 offset] = t0; /* u64 */ 
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    buf1.[#unaligned :u64 offset] = t1; /* u64 */ 
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    buf2.[#unaligned :u64 offset] = t2; /* u64 */ 
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    buf3.[#unaligned :u64 offset] = t3; /* u64 */ 
    i = (i +64u ((64u) 8)); /* u64 */ 
    offset = (offset +64u ((64u) 8)); /* u64 */ 
  }
  if (0 < (LEN % 8)) {
    t0 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (0 * 8)))]; /* u64 */ 
    #[inline]
    (buf0, _ /* int */, _ /* int */) =
      ABUFLEN::__awrite_subu64(buf0, offset, 0, (LEN % 8), t0);
    t1 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (1 * 8)))]; /* u64 */ 
    #[inline]
    (buf1, _ /* int */, _ /* int */) =
      ABUFLEN::__awrite_subu64(buf1, offset, 0, (LEN % 8), t1);
    t2 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (2 * 8)))]; /* u64 */ 
    #[inline]
    (buf2, _ /* int */, _ /* int */) =
      ABUFLEN::__awrite_subu64(buf2, offset, 0, (LEN % 8), t2);
    t3 =
      st.[#unaligned :u64 ((((64u) 4) *64u i) +64u ((64u) (3 * 8)))]; /* u64 */ 
    #[inline]
    (buf3, _ /* int */, _ /* int */) =
      ABUFLEN::__awrite_subu64(buf3, offset, 0, (LEN % 8), t3);
    offset = (offset +64u ((64u) (LEN % 8))); /* u64 */ 
  }
  return (buf0, buf1, buf2, buf3, offset);
}

inline
fn ABUFLEN::__squeeze_array_avx2x4 (reg mut ptr u8[ABUFLEN::_ASIZE] buf0,
                                   reg mut ptr u8[ABUFLEN::_ASIZE] buf1,
                                   reg mut ptr u8[ABUFLEN::_ASIZE] buf2,
                                   reg mut ptr u8[ABUFLEN::_ASIZE] buf3,
                                   reg u64 offset, inline int LEN,
                                   reg mut ptr u256[25] st, inline int RATE8) -> 
(reg mut ptr u8[ABUFLEN::_ASIZE], reg mut ptr u8[ABUFLEN::_ASIZE],
reg mut ptr u8[ABUFLEN::_ASIZE], reg mut ptr u8[ABUFLEN::_ASIZE], reg u64,
reg mut ptr u256[25])

{
  inline int ITERS;
  ITERS = (LEN / RATE8); /* int:i */ 
  inline int LO;
  LO = (LEN % RATE8); /* int:i */ 
  if (0 < LEN) {
    if (0 < ITERS) {
      reg u64 i;
      i = ((64u) 0); /* u64 */ 
      while ((i <u ((64u) ITERS))) {
        st = _keccakf1600_avx2x4(st);
        #[inline]
        (buf0, buf1, buf2, buf3, offset) =
          ABUFLEN::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset,
                                            RATE8, st);
        i = (i +64u ((64u) 1)); /* u64 */ 
      }
    }
    if (0 < LO) {
      st = _keccakf1600_avx2x4(st);
      #[inline]
      (buf0, buf1, buf2, buf3, offset) =
        ABUFLEN::__dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO,
                                          st);
    }
  }
  return (buf0, buf1, buf2, buf3, offset, st);
}

fn _sha3_512A_A33 (#[spill_to_mmx] reg mut ptr u8[64] out,
                  reg const ptr u8[33] in) -> (reg mut ptr u8[64])
requires #[prover=Cas] {is_arr_init(in, 0, 33)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, 64)} 
{
  reg u256[7] st;
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* int */) = A33::__absorb_avx2(st, 0, in, SHA3, R72);
  #[inline]
  (_ /* u256[?] */, out) = A64::__squeeze_avx2(st, out, R72);
  return (out);
}

fn _sha3_512A_A64 (reg mut ptr u8[64] out, reg const ptr u8[64] in) -> 
(reg mut ptr u8[64])
requires #[prover=Cas] {is_arr_init(in, 0, 64)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, 64)} 
{
  reg u256[7] st;
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* int */) = A64::__absorb_avx2(st, 0, in, SHA3, R72);
  #[inline]
  (_ /* u256[?] */, out) = A64::__squeeze_avx2(st, out, R72);
  return (out);
}

fn _shake256_A128__A32_A1 (reg mut ptr u8[128] out,
                          reg const ptr u8[32] seed,
                          reg const ptr u8[1] nonce) -> (reg mut ptr u8[128])
requires #[prover=Cas] {(is_arr_init(seed, 0, 32) &&
                        is_arr_init(nonce, 0, 1))}

ensures #[prover=Cas] {is_arr_init(result.0, 0, 128)} 
{
  reg u256[7] st;
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* int */) = A32::__absorb_avx2(st, 0, seed, UNFINISHED, R136);
  #[inline]
  (st, _ /* int */) = A1::__absorb_avx2(st, 32, nonce, SHAKE, R136);
  #[inline]
  (_ /* u256[?] */, out) = A128::__squeeze_avx2(st, out, R136);
  return (out);
}

fn _shake256x4_A128__A32_A1 (reg mut ptr u8[128] out0,
                            reg mut ptr u8[128] out1,
                            reg mut ptr u8[128] out2,
                            reg mut ptr u8[128] out3,
                            reg const ptr u8[32] seed,
                            reg const ptr u8[4] nonces) -> (reg mut ptr u8[128],
                                                           reg mut ptr u8[128],
                                                           reg mut ptr u8[128],
                                                           reg mut ptr u8[128])
requires #[prover=Cas] {(is_arr_init(seed, 0, 32) &&
                        is_arr_init(nonces, 0, 4))}

ensures #[prover=Cas] {(((is_arr_init(result.0, 0, 128) &&
                         is_arr_init(result.1, 0, 128)) &&
                        is_arr_init(result.2, 0, 128)) &&
                       is_arr_init(result.3, 0, 128))}

{
  stack u256[25] st_s;
  reg mut ptr u256[25] st;
  st = st_s; /* u256[25] */ 
  #[inline]
  st = __state_init_avx2x4(st);
  #[inline]
  (st, _ /* int */) =
    A32::__absorb_bcast_avx2x4(st, 0, seed, UNFINISHED, R136);
  #[inline]
  (st, _ /* int */) =
    A1::__absorb_avx2x4(st, 32, nonces[0 : 1], nonces[1 : 1], nonces[2 : 1],
                        nonces[3 : 1], SHAKE, R136);
  #[inline]
  (st, out0, out1, out2, out3) =
    A128::__squeeze_avx2x4(st, out0, out1, out2, out3, R136);
  return (out0, out1, out2, out3);
}

fn _shake128_absorb_A32_A2 (reg const ptr u8[32] seed,
                           reg const ptr u8[2] pos) -> (reg u256[7])
requires #[prover=Cas] {(is_arr_init(seed, 0, 32) && is_arr_init(pos, 0, 2))} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, (32 * 7))} 
{
  reg u256[7] st;
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* int */) = A32::__absorb_avx2(st, 0, seed, UNFINISHED, R168);
  #[inline]
  (st, _ /* int */) = A2::__absorb_avx2(st, 32, pos, SHAKE, R168);
  return (st);
}

fn _shake128x4_absorb_A32_A2 (reg mut ptr u256[25] st,
                             reg const ptr u8[32] seed,
                             reg const ptr u8[(4 * 2)] pos) -> (reg mut ptr u256[25])
requires #[prover=Cas] {(is_arr_init(seed, 0, 32) &&
                        is_arr_init(pos, 0, (4 * 2)))}

ensures #[prover=Cas] {is_arr_init(result.0, 0, (32 * 25))} 
{
  #[inline]
  st = __state_init_avx2x4(st);
  inline int AT;
  #[inline]
  (st, AT) = A32::__absorb_bcast_avx2x4(st, 0, seed, UNFINISHED, R168);
  #[inline]
  (st, _ /* int */) =
    A2::__absorb_avx2x4(st, 32, pos[0 : 2], pos[2 : 2], pos[4 : 2],
                        pos[6 : 2], SHAKE, R168);
  return (st);
}

fn _shake128_squeeze3blocks (reg mut ptr u8[ABUFLEN::_ASIZE] buf,
                            reg u256[7] st) -> (reg mut ptr u8[ABUFLEN::_ASIZE])
requires #[prover=Cas] {is_arr_init(st, 0, (32 * 7))} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, ABUFLEN::_ASIZE)} 
{
  st = _keccakf1600_avx2(st);
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  #[inline]
  (buf, offset) = ABUFLEN::__dumpstate_avx2(buf, offset, R168, st);
  st = _keccakf1600_avx2(st);
  #[inline]
  (buf, offset) = ABUFLEN::__dumpstate_avx2(buf, offset, R168, st);
  st = _keccakf1600_avx2(st);
  #[inline]
  (buf, offset) = ABUFLEN::__dumpstate_avx2(buf, offset, 200, st);
  return (buf);
}

fn _shake128_next_state (reg mut ptr u8[ABUFLEN::_ASIZE] buf) -> (reg mut ptr u8[ABUFLEN::_ASIZE])
requires #[prover=Cas] {is_arr_init(buf, 0, ABUFLEN::_ASIZE)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, ABUFLEN::_ASIZE)} 
{
  reg mut ptr u64[25] pst;
  pst = buf[:u64 (2 * (168 / 8)) : 25]; /* u64[25] */ 
  pst = _keccakf1600_st25_avx2(pst);
  buf[:u64 (2 * (168 / 8)) : 25] = pst; /* u64[25] */ 
  return (buf);
}

fn _shake128x4_squeeze3blocks (reg mut ptr u256[25] st,
                              reg mut ptr u8[(4 * ABUFLEN::_ASIZE)] buf) -> 
(reg mut ptr u256[25], reg mut ptr u8[(4 * ABUFLEN::_ASIZE)])
requires #[prover=Cas] {is_arr_init(st, 0, (32 * 25))} 
ensures #[prover=Cas] {is_arr_init(result.1, 0, (4 * ABUFLEN::_ASIZE))} 
{
  reg mut ptr u8[ABUFLEN::_ASIZE] buf0;
  buf0 =
    buf[(0 * ABUFLEN::_ASIZE) : ABUFLEN::_ASIZE]; /* u8[ABUFLEN::_ASIZE] */ 
  reg mut ptr u8[ABUFLEN::_ASIZE] buf1;
  buf1 =
    buf[(1 * ABUFLEN::_ASIZE) : ABUFLEN::_ASIZE]; /* u8[ABUFLEN::_ASIZE] */ 
  reg mut ptr u8[ABUFLEN::_ASIZE] buf2;
  buf2 =
    buf[(2 * ABUFLEN::_ASIZE) : ABUFLEN::_ASIZE]; /* u8[ABUFLEN::_ASIZE] */ 
  reg mut ptr u8[ABUFLEN::_ASIZE] buf3;
  buf3 =
    buf[(3 * ABUFLEN::_ASIZE) : ABUFLEN::_ASIZE]; /* u8[ABUFLEN::_ASIZE] */ 
  #[Internal::wint::unsigned]
  reg ui64 offset;
  offset = ((64ui /* of int */) 0); /* u64 */ 
  st = _keccakf1600_avx2x4(st);
  #[inline]
  (buf0, buf1, buf2, buf3, offset) =
    ABUFLEN::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, R168, st);
  st = _keccakf1600_avx2x4(st);
  #[inline]
  (buf0, buf1, buf2, buf3, offset) =
    ABUFLEN::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, R168, st);
  st = _keccakf1600_avx2x4(st);
  #[inline]
  (buf0, buf1, buf2, buf3, offset) =
    ABUFLEN::__dumpstate_avx2x4(buf0, buf1, buf2, buf3, offset, 200, st);
  buf[(0 * ABUFLEN::_ASIZE) : ABUFLEN::_ASIZE] =
    buf0; /* u8[ABUFLEN::_ASIZE] */ 
  buf[(1 * ABUFLEN::_ASIZE) : ABUFLEN::_ASIZE] =
    buf1; /* u8[ABUFLEN::_ASIZE] */ 
  buf[(2 * ABUFLEN::_ASIZE) : ABUFLEN::_ASIZE] =
    buf2; /* u8[ABUFLEN::_ASIZE] */ 
  buf[(3 * ABUFLEN::_ASIZE) : ABUFLEN::_ASIZE] =
    buf3; /* u8[ABUFLEN::_ASIZE] */ 
  return (st, buf);
}

fn _sha3_256A_A1184 (#[spill_to_mmx] reg mut ptr u8[32] out, #[spill_to_mmx]
                    reg const ptr u8[1184] in) -> (reg mut ptr u8[32])
requires #[prover=Cas] {is_arr_init(in, 0, 1184)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, 32)} 
{
  reg u256[7] st;
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* int */) = A1184::__absorb_avx2(st, 0, in, SHA3, R136);
  #[inline]
  (_ /* u256[?] */, out) = A32::__squeeze_avx2(st, out, R136);
  return (out);
}

fn _sha3_256A_A1568 (#[spill_to_mmx] reg mut ptr u8[32] out, #[spill_to_mmx]
                    reg const ptr u8[1568] in) -> (reg mut ptr u8[32])
requires #[prover=Cas] {is_arr_init(in, 0, 1568)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, 32)} 
{
  reg u256[7] st;
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* int */) = A1568::__absorb_avx2(st, 0, in, SHA3, R136);
  #[inline]
  (_ /* u256[?] */, out) = A32::__squeeze_avx2(st, out, R136);
  return (out);
}

fn _shake256_A32__A1120 (reg mut ptr u8[32] out, reg const ptr u8[1120] in) -> 
(reg mut ptr u8[32])
requires #[prover=Cas] {is_arr_init(in, 0, 1120)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, 32)} 
{
  reg u256[7] st;
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* int */) = A1120::__absorb_avx2(st, 0, in, SHAKE, R136);
  #[inline]
  (_ /* u256[?] */, out) = A32::__squeeze_avx2(st, out, R136);
  return (out);
}

fn _shake256_A32__A1600 (reg mut ptr u8[32] out, reg const ptr u8[1600] in) -> 
(reg mut ptr u8[32])
requires #[prover=Cas] {is_arr_init(in, 0, 1600)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, 32)} 
{
  reg u256[7] st;
  #[inline]
  st = __state_init_avx2();
  #[inline]
  (st, _ /* int */) = A1600::__absorb_avx2(st, 0, in, SHAKE, R136);
  #[inline]
  (_ /* u256[?] */, out) = A32::__squeeze_avx2(st, out, R136);
  return (out);
}

fn _poly_add2 (reg mut ptr u16[MLKEM_N] rp, reg const ptr u16[MLKEM_N] bp) -> 
(reg mut ptr u16[MLKEM_N])

{
  inline int i;
  for i = 0 to 16 {
    reg u256 a;
    a = rp.[#unaligned :u256 (32 * i)]; /* u256 */ 
    reg u256 b;
    b = bp.[#unaligned :u256 (32 * i)]; /* u256 */ 
    reg u256 r;
    r = #VPADD_16u16(a, b); /*  */ 
    rp.[#unaligned :u256 (32 * i)] = r; /* u256 */ 
  }
  return (rp);
}

fn _poly_csubq (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N])

{
  reg u256 qx16;
  qx16 = /* global: */ jqx16[:u256 0]; /* u256 */ 
  inline int i;
  for i = 0 to 16 {
    reg u256 r;
    r = rp.[#unaligned :u256 (32 * i)]; /* u256 */ 
    #[inline]
    r = __csubq(r, qx16);
    rp.[#unaligned :u256 (32 * i)] = r; /* u256 */ 
  }
  return (rp);
}

inline
fn __w256_interleave_u16 (reg u256 al, reg u256 ah) -> (reg u256, reg u256)

{
  reg u256 a0;
  a0 = #VPUNPCKL_16u16(al, ah); /*  */ 
  reg u256 a1;
  a1 = #VPUNPCKH_16u16(al, ah); /*  */ 
  return (a0, a1);
}

inline
fn __w256_deinterleave_u16 (reg u256 _zero, reg u256 a0, reg u256 a1) -> 
(reg u256, reg u256)

{
  reg u256 al;
  al = #VPBLEND_16u16(a0, _zero, ((8u) 170)); /*  */ 
  reg u256 ah;
  ah = #VPBLEND_16u16(a1, _zero, ((8u) 170)); /*  */ 
  al = #VPACKUS_8u32(al, ah); /*  */ 
  a0 = #VPSRL_8u32(a0, ((128u) 16)); /*  */ 
  a1 = #VPSRL_8u32(a1, ((128u) 16)); /*  */ 
  ah = #VPACKUS_8u32(a0, a1); /*  */ 
  return (al, ah);
}

inline
fn __mont_red (reg u256 lo, reg u256 hi, reg u256 qx16, reg u256 qinvx16) -> 
(reg u256)

{
  reg u256 m;
  m = #VPMULL_16u16(lo, qinvx16); /*  */ 
  m = #VPMULH_16u16(m, qx16); /*  */ 
  lo = #VPSUB_16u16(hi, m); /*  */ 
  return (lo);
}

inline
fn __wmul_16u16 (reg u256 x, reg u256 y) -> (reg u256, reg u256)

{
  reg u256 xyL;
  xyL = #VPMULL_16u16(x, y); /*  */ 
  reg u256 xyH;
  xyH = #VPMULH_16u16(x, y); /*  */ 
  reg u256 xy0;
  reg u256 xy1;
  #[inline]
  (xy0, xy1) = __w256_interleave_u16(xyL, xyH);
  return (xy0, xy1);
}

inline
fn __schoolbook16x (reg u256 are, reg u256 aim, reg u256 bre, reg u256 bim,
                   reg u256 zeta, reg u256 zetaqinv, reg u256 qx16,
                   reg u256 qinvx16, inline int sign) -> (reg u256, reg u256)

{
  reg u256 zaim;
  #[inline]
  zaim = __fqmulprecomp16x(aim, zetaqinv, zeta, qx16);
  reg u256 ac0;
  reg u256 ac1;
  #[inline]
  (ac0, ac1) = __wmul_16u16(are, bre);
  reg u256 ad0;
  reg u256 ad1;
  #[inline]
  (ad0, ad1) = __wmul_16u16(are, bim);
  reg u256 bc0;
  reg u256 bc1;
  #[inline]
  (bc0, bc1) = __wmul_16u16(aim, bre);
  reg u256 zbd0;
  reg u256 zbd1;
  #[inline]
  (zbd0, zbd1) = __wmul_16u16(zaim, bim);
  reg u256 x0;
  reg u256 x1;
  if (sign == 0) {
    x0 = #VPADD_8u32(ac0, zbd0); /*  */ 
    x1 = #VPADD_8u32(ac1, zbd1); /*  */ 
  } else {
    x0 = #VPSUB_8u32(ac0, zbd0); /*  */ 
    x1 = #VPSUB_8u32(ac1, zbd1); /*  */ 
  }
  reg u256 y0;
  y0 = #VPADD_8u32(bc0, ad0); /*  */ 
  reg u256 y1;
  y1 = #VPADD_8u32(bc1, ad1); /*  */ 
  reg u256 _zero;
  _zero = #set0_256(); /*  */ 
  #[inline]
  (x0, x1) = __w256_deinterleave_u16(_zero, x0, x1);
  #[inline]
  (y0, y1) = __w256_deinterleave_u16(_zero, y0, y1);
  #[inline]
  x0 = __mont_red(x0, x1, qx16, qinvx16);
  #[inline]
  y0 = __mont_red(y0, y1, qx16, qinvx16);
  return (x0, y0);
}

fn _poly_basemul (reg mut ptr u16[MLKEM_N] rp, reg const ptr u16[MLKEM_N] ap,
                 reg const ptr u16[MLKEM_N] bp) -> (reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {(is_arr_init(ap, 0, (32 * 16)) &&
                        is_arr_init(bp, 0, (32 * 16)))}

ensures #[prover=Cas] {is_arr_init(result.0, 0, (32 * 16))} 
{
  reg u256 qx16;
  qx16 = /* global: */ jqx16.[#unaligned :u256 0]; /* u256 */ 
  reg u256 qinvx16;
  qinvx16 = /* global: */ jqinvx16.[#unaligned :u256 0]; /* u256 */ 
  reg u256 zetaqinv;
  zetaqinv = /* global: */ jzetas_exp.[#unaligned :u256 272]; /* u256 */ 
  reg u256 zeta;
  zeta = /* global: */ jzetas_exp.[#unaligned :u256 304]; /* u256 */ 
  reg u256 are;
  are = ap.[#unaligned :u256 (32 * 0)]; /* u256 */ 
  reg u256 aim;
  aim = ap.[#unaligned :u256 (32 * 1)]; /* u256 */ 
  reg u256 bre;
  bre = bp.[#unaligned :u256 (32 * 0)]; /* u256 */ 
  reg u256 bim;
  bim = bp.[#unaligned :u256 (32 * 1)]; /* u256 */ 
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[#unaligned :u256 (32 * 0)] = are; /* u256 */ 
  rp.[#unaligned :u256 (32 * 1)] = aim; /* u256 */ 
  are = ap.[#unaligned :u256 (32 * 2)]; /* u256 */ 
  aim = ap.[#unaligned :u256 (32 * 3)]; /* u256 */ 
  bre = bp.[#unaligned :u256 (32 * 2)]; /* u256 */ 
  bim = bp.[#unaligned :u256 (32 * 3)]; /* u256 */ 
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[#unaligned :u256 (32 * 2)] = are; /* u256 */ 
  rp.[#unaligned :u256 (32 * 3)] = aim; /* u256 */ 
  zetaqinv = /* global: */ jzetas_exp.[#unaligned :u256 336]; /* u256 */ 
  zeta = /* global: */ jzetas_exp.[#unaligned :u256 368]; /* u256 */ 
  are = ap.[#unaligned :u256 (32 * 4)]; /* u256 */ 
  aim = ap.[#unaligned :u256 (32 * 5)]; /* u256 */ 
  bre = bp.[#unaligned :u256 (32 * 4)]; /* u256 */ 
  bim = bp.[#unaligned :u256 (32 * 5)]; /* u256 */ 
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[#unaligned :u256 (32 * 4)] = are; /* u256 */ 
  rp.[#unaligned :u256 (32 * 5)] = aim; /* u256 */ 
  are = ap.[#unaligned :u256 (32 * 6)]; /* u256 */ 
  aim = ap.[#unaligned :u256 (32 * 7)]; /* u256 */ 
  bre = bp.[#unaligned :u256 (32 * 6)]; /* u256 */ 
  bim = bp.[#unaligned :u256 (32 * 7)]; /* u256 */ 
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[#unaligned :u256 (32 * 6)] = are; /* u256 */ 
  rp.[#unaligned :u256 (32 * 7)] = aim; /* u256 */ 
  zetaqinv = /* global: */ jzetas_exp.[#unaligned :u256 664]; /* u256 */ 
  zeta = /* global: */ jzetas_exp.[#unaligned :u256 696]; /* u256 */ 
  are = ap.[#unaligned :u256 (32 * 8)]; /* u256 */ 
  aim = ap.[#unaligned :u256 (32 * 9)]; /* u256 */ 
  bre = bp.[#unaligned :u256 (32 * 8)]; /* u256 */ 
  bim = bp.[#unaligned :u256 (32 * 9)]; /* u256 */ 
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[#unaligned :u256 (32 * 8)] = are; /* u256 */ 
  rp.[#unaligned :u256 (32 * 9)] = aim; /* u256 */ 
  are = ap.[#unaligned :u256 (32 * 10)]; /* u256 */ 
  aim = ap.[#unaligned :u256 (32 * 11)]; /* u256 */ 
  bre = bp.[#unaligned :u256 (32 * 10)]; /* u256 */ 
  bim = bp.[#unaligned :u256 (32 * 11)]; /* u256 */ 
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[#unaligned :u256 (32 * 10)] = are; /* u256 */ 
  rp.[#unaligned :u256 (32 * 11)] = aim; /* u256 */ 
  zetaqinv = /* global: */ jzetas_exp.[#unaligned :u256 728]; /* u256 */ 
  zeta = /* global: */ jzetas_exp.[#unaligned :u256 760]; /* u256 */ 
  are = ap.[#unaligned :u256 (32 * 12)]; /* u256 */ 
  aim = ap.[#unaligned :u256 (32 * 13)]; /* u256 */ 
  bre = bp.[#unaligned :u256 (32 * 12)]; /* u256 */ 
  bim = bp.[#unaligned :u256 (32 * 13)]; /* u256 */ 
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[#unaligned :u256 (32 * 12)] = are; /* u256 */ 
  rp.[#unaligned :u256 (32 * 13)] = aim; /* u256 */ 
  are = ap.[#unaligned :u256 (32 * 14)]; /* u256 */ 
  aim = ap.[#unaligned :u256 (32 * 15)]; /* u256 */ 
  bre = bp.[#unaligned :u256 (32 * 14)]; /* u256 */ 
  bim = bp.[#unaligned :u256 (32 * 15)]; /* u256 */ 
  #[inline]
  (are, aim) =
    __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[#unaligned :u256 (32 * 14)] = are; /* u256 */ 
  rp.[#unaligned :u256 (32 * 15)] = aim; /* u256 */ 
  return (rp);
}

fn _i_poly_frombytes (reg mut ptr u16[MLKEM_N] rp,
                     reg const ptr u8[MLKEM_POLYBYTES] ap) -> (reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {is_arr_init(ap, 0, MLKEM_POLYBYTES)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, (16 * 32))} 
{
  reg u256 mask;
  mask = /* global: */ maskx16[:u256 0]; /* u256 */ 
  inline int i;
  for i = 0 to 2 {
    reg u256 t0;
    t0 = ap.[#unaligned :u256 (192 * i)]; /* u256 */ 
    reg u256 t1;
    t1 = ap.[#unaligned :u256 ((192 * i) + 32)]; /* u256 */ 
    reg u256 t2;
    t2 = ap.[#unaligned :u256 ((192 * i) + 64)]; /* u256 */ 
    reg u256 t3;
    t3 = ap.[#unaligned :u256 ((192 * i) + 96)]; /* u256 */ 
    reg u256 t4;
    t4 = ap.[#unaligned :u256 ((192 * i) + 128)]; /* u256 */ 
    reg u256 t5;
    t5 = ap.[#unaligned :u256 ((192 * i) + 160)]; /* u256 */ 
    reg u256 tt;
    #[inline]
    (tt, t3) = __shuffle8(t0, t3);
    #[inline]
    (t0, t4) = __shuffle8(t1, t4);
    #[inline]
    (t1, t5) = __shuffle8(t2, t5);
    #[inline]
    (t2, t4) = __shuffle4(tt, t4);
    #[inline]
    (tt, t1) = __shuffle4(t3, t1);
    #[inline]
    (t3, t5) = __shuffle4(t0, t5);
    #[inline]
    (t0, t1) = __shuffle2(t2, t1);
    #[inline]
    (t2, t3) = __shuffle2(t4, t3);
    #[inline]
    (t4, t5) = __shuffle2(tt, t5);
    reg u256 t6;
    #[inline]
    (t6, t3) = __shuffle1(t0, t3);
    #[inline]
    (t0, t4) = __shuffle1(t1, t4);
    #[inline]
    (t1, t5) = __shuffle1(t2, t5);
    reg u256 t7;
    t7 = #VPSRL_16u16(t6, ((128u) 12)); /*  */ 
    reg u256 t8;
    t8 = #VPSLL_16u16(t3, ((128u) 4)); /*  */ 
    t7 = #VPOR_256(t7, t8); /*  */ 
    t6 = #VPAND_256(mask, t6); /*  */ 
    t7 = #VPAND_256(mask, t7); /*  */ 
    t8 = #VPSRL_16u16(t3, ((128u) 8)); /*  */ 
    reg u256 t9;
    t9 = #VPSLL_16u16(t0, ((128u) 8)); /*  */ 
    t8 = #VPOR_256(t8, t9); /*  */ 
    t8 = #VPAND_256(mask, t8); /*  */ 
    t9 = #VPSRL_16u16(t0, ((128u) 4)); /*  */ 
    t9 = #VPAND_256(mask, t9); /*  */ 
    reg u256 t10;
    t10 = #VPSRL_16u16(t4, ((128u) 12)); /*  */ 
    reg u256 t11;
    t11 = #VPSLL_16u16(t1, ((128u) 4)); /*  */ 
    t10 = #VPOR_256(t10, t11); /*  */ 
    t4 = #VPAND_256(mask, t4); /*  */ 
    t10 = #VPAND_256(mask, t10); /*  */ 
    t11 = #VPSRL_16u16(t1, ((128u) 8)); /*  */ 
    tt = #VPSLL_16u16(t5, ((128u) 8)); /*  */ 
    t11 = #VPOR_256(t11, tt); /*  */ 
    t11 = #VPAND_256(mask, t11); /*  */ 
    tt = #VPSRL_16u16(t5, ((128u) 4)); /*  */ 
    tt = #VPAND_256(mask, tt); /*  */ 
    rp[:u256 (8 * i)] = t6; /* u256 */ 
    rp[:u256 ((8 * i) + 1)] = t7; /* u256 */ 
    rp[:u256 ((8 * i) + 2)] = t8; /* u256 */ 
    rp[:u256 ((8 * i) + 3)] = t9; /* u256 */ 
    rp[:u256 ((8 * i) + 4)] = t4; /* u256 */ 
    rp[:u256 ((8 * i) + 5)] = t10; /* u256 */ 
    rp[:u256 ((8 * i) + 6)] = t11; /* u256 */ 
    rp[:u256 ((8 * i) + 7)] = tt; /* u256 */ 
  }
  return (rp);
}

param int DMONT = 1353;

fn _poly_frommont (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N])

{
  reg u256 qx16;
  qx16 = /* global: */ jqx16[:u256 0]; /* u256 */ 
  reg u256 qinvx16;
  qinvx16 = /* global: */ jqinvx16[:u256 0]; /* u256 */ 
  reg u256 dmontx16;
  dmontx16 = /* global: */ jdmontx16[:u256 0]; /* u256 */ 
  inline int i;
  for i = 0 to (MLKEM_N / 16) {
    reg u256 t;
    t = rp[:u256 i]; /* u256 */ 
    #[inline]
    t = __fqmulx16(t, dmontx16, qx16, qinvx16);
    rp[:u256 i] = t; /* u256 */ 
  }
  return (rp);
}

u32[4] pfm_shift_s = {((32u) 3), ((32u) 2), ((32u) 1), ((32u) 0)};

u8[16] pfm_idx_s = {((8u) 0), ((8u) 1), ((8u) 4), ((8u) 5), ((8u) 8),
                    ((8u) 9), ((8u) 12), ((8u) 13), ((8u) 2), ((8u) 3),
                    ((8u) 6), ((8u) 7), ((8u) 10), ((8u) 11), ((8u) 14),
                    ((8u) 15)};

fn _i_poly_frommsg (reg mut ptr u16[MLKEM_N] rp, reg const ptr u8[32] ap) -> 
(reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {is_arr_init(ap, 0, 32)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, (16 * 32))} 
{
  reg u256 hqs;
  hqs = /* global: */ hqx16_p1[:u256 0]; /* u256 */ 
  reg u256 shift;
  shift = #VPBROADCAST_2u128(/* global: */ pfm_shift_s[:u128 0]); /*  */ 
  reg u256 idx;
  idx = #VPBROADCAST_2u128(/* global: */ pfm_idx_s[:u128 0]); /*  */ 
  reg u256 f;
  f = ap[:u256 0]; /* u256 */ 
  inline int i;
  for i = 0 to 4 {
    reg u256 g3;
    g3 = #VPSHUFD_256(f, ((8u) (85 * i))); /*  */ 
    g3 = #VPSLLV_8u32(g3, shift); /*  */ 
    g3 = #VPSHUFB_256(g3, idx); /*  */ 
    reg u256 g0;
    g0 = #VPSLL_16u16(g3, ((128u) 12)); /*  */ 
    reg u256 g1;
    g1 = #VPSLL_16u16(g3, ((128u) 8)); /*  */ 
    reg u256 g2;
    g2 = #VPSLL_16u16(g3, ((128u) 4)); /*  */ 
    g0 = #VPSRA_16u16(g0, ((128u) 15)); /*  */ 
    g1 = #VPSRA_16u16(g1, ((128u) 15)); /*  */ 
    g2 = #VPSRA_16u16(g2, ((128u) 15)); /*  */ 
    g3 = #VPSRA_16u16(g3, ((128u) 15)); /*  */ 
    g0 = #VPAND_256(g0, hqs); /*  */ 
    g1 = #VPAND_256(g1, hqs); /*  */ 
    g2 = #VPAND_256(g2, hqs); /*  */ 
    g3 = #VPAND_256(g3, hqs); /*  */ 
    reg u256 h0;
    h0 = #VPUNPCKL_4u64(g0, g1); /*  */ 
    reg u256 h2;
    h2 = #VPUNPCKH_4u64(g0, g1); /*  */ 
    reg u256 h1;
    h1 = #VPUNPCKL_4u64(g2, g3); /*  */ 
    reg u256 h3;
    h3 = #VPUNPCKH_4u64(g2, g3); /*  */ 
    g0 = #VPERM2I128(h0, h1, ((8u) 32)); /*  */ 
    g2 = #VPERM2I128(h0, h1, ((8u) 49)); /*  */ 
    g1 = #VPERM2I128(h2, h3, ((8u) 32)); /*  */ 
    g3 = #VPERM2I128(h2, h3, ((8u) 49)); /*  */ 
    rp[:u256 (2 * i)] = g0; /* u256 */ 
    rp[:u256 ((2 * i) + 1)] = g1; /* u256 */ 
    rp[:u256 ((2 * i) + 8)] = g2; /* u256 */ 
    rp[:u256 (((2 * i) + 8) + 1)] = g3; /* u256 */ 
  }
  return (rp);
}

inline
fn __cbd2 (reg mut ptr u16[MLKEM_N] rp,
          reg const ptr u8[((MLKEM_ETA2 * MLKEM_N) / 4)] buf) -> (reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {is_arr_init(buf, 0, (MLKEM_N / 2))} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, (MLKEM_N * 2))} 
{
  stack u32 mask55_s;
  mask55_s = ((32u) 1431655765); /* u32 */ 
  stack u32 mask33_s;
  mask33_s = ((32u) 858993459); /* u32 */ 
  stack u32 mask03_s;
  mask03_s = ((32u) 50529027); /* u32 */ 
  stack u32 mask0F_s;
  mask0F_s = ((32u) 252645135); /* u32 */ 
  reg u256 mask55;
  mask55 = #VPBROADCAST_8u32(mask55_s); /*  */ 
  reg u256 mask33;
  mask33 = #VPBROADCAST_8u32(mask33_s); /*  */ 
  reg u256 mask03;
  mask03 = #VPBROADCAST_8u32(mask03_s); /*  */ 
  reg u256 mask0F;
  mask0F = #VPBROADCAST_8u32(mask0F_s); /*  */ 
  inline int i;
  for i = 0 to (MLKEM_N / 64) {
    reg u256 f0;
    f0 = buf[:u256 i]; /* u256 */ 
    reg u256 f1;
    f1 = #VPSRL_16u16(f0, ((128u) 1)); /*  */ 
    f0 = #VPAND_256(mask55, f0); /*  */ 
    f1 = #VPAND_256(mask55, f1); /*  */ 
    f0 = #VPADD_32u8(f0, f1); /*  */ 
    f1 = #VPSRL_16u16(f0, ((128u) 2)); /*  */ 
    f0 = #VPAND_256(mask33, f0); /*  */ 
    f1 = #VPAND_256(mask33, f1); /*  */ 
    f0 = #VPADD_32u8(f0, mask33); /*  */ 
    f0 = #VPSUB_32u8(f0, f1); /*  */ 
    f1 = #VPSRL_16u16(f0, ((128u) 4)); /*  */ 
    f0 = #VPAND_256(mask0F, f0); /*  */ 
    f1 = #VPAND_256(mask0F, f1); /*  */ 
    f0 = #VPSUB_32u8(f0, mask03); /*  */ 
    f1 = #VPSUB_32u8(f1, mask03); /*  */ 
    reg u256 f2;
    f2 = #VPUNPCKL_32u8(f0, f1); /*  */ 
    reg u256 f3;
    f3 = #VPUNPCKH_32u8(f0, f1); /*  */ 
    reg u128 t;
    t = f2; /* u128 */ 
    f0 = #VPMOVSX_16u8_16u16(t); /*  */ 
    t = #VEXTRACTI128(f2, ((8u) 1)); /*  */ 
    f1 = #VPMOVSX_16u8_16u16(t); /*  */ 
    t = f3; /* u128 */ 
    f2 = #VPMOVSX_16u8_16u16(t); /*  */ 
    t = #VEXTRACTI128(f3, ((8u) 1)); /*  */ 
    f3 = #VPMOVSX_16u8_16u16(t); /*  */ 
    rp[:u256 (4 * i)] = f0; /* u256 */ 
    rp[:u256 ((4 * i) + 1)] = f2; /* u256 */ 
    rp[:u256 ((4 * i) + 2)] = f1; /* u256 */ 
    rp[:u256 ((4 * i) + 3)] = f3; /* u256 */ 
  }
  return (rp);
}

inline
fn __poly_cbd_eta1 (reg mut ptr u16[MLKEM_N] rp,
                   reg const ptr u8[(((MLKEM_ETA1 * MLKEM_N) / 4) +
                                    ((MLKEM_ETA1 - 2) * 8))] buf) -> 
(reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {is_arr_init(buf, 0, (MLKEM_N / 2))} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, (MLKEM_N * 2))} 
{
  #[inline]
  rp = __cbd2(rp, buf[0 : ((MLKEM_ETA2 * MLKEM_N) / 4)]);
  return (rp);
}

fn _poly_getnoise_eta2 (#[spill_to_mmx] reg mut ptr u16[MLKEM_N] rp,
                       reg const ptr u8[MLKEM_SYMBYTES] seed, reg u8 nonce) -> 
(reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {is_arr_init(seed, 0, MLKEM_SYMBYTES)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, (MLKEM_N * 2))} 
{
  () = #spill(rp); /* :k */ 
  stack u8[1] nonce_s;
  nonce_s[0] = nonce; /* u8 */ 
  stack u8[((MLKEM_ETA2 * MLKEM_N) / 4)] buf;
  buf = _shake256_A128__A32_A1(buf, seed, nonce_s);
  () = #unspill(rp); /* :k */ 
  #[inline]
  rp = __poly_cbd_eta1(rp, buf);
  return (rp);
}

fn _poly_getnoise_eta1_4x (reg mut ptr u16[MLKEM_N] r0,
                          reg mut ptr u16[MLKEM_N] r1,
                          reg mut ptr u16[MLKEM_N] r2,
                          reg mut ptr u16[MLKEM_N] r3,
                          reg const ptr u8[MLKEM_SYMBYTES] seed,
                          reg u8 nonce) -> (reg mut ptr u16[MLKEM_N],
                                           reg mut ptr u16[MLKEM_N],
                                           reg mut ptr u16[MLKEM_N],
                                           reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {is_arr_init(seed, 0, MLKEM_SYMBYTES)} 
ensures #[prover=Cas] {(((is_arr_init(result.0, 0, (MLKEM_N * 2)) &&
                         is_arr_init(result.1, 0, (MLKEM_N * 2))) &&
                        is_arr_init(result.2, 0, (MLKEM_N * 2))) &&
                       is_arr_init(result.3, 0, (MLKEM_N * 2)))}

{
  stack u8[128] buf0_s;
  reg mut ptr u8[128] buf0;
  buf0 = buf0_s; /* u8[128] */ 
  stack u8[128] buf1_s;
  reg mut ptr u8[128] buf1;
  buf1 = buf1_s; /* u8[128] */ 
  stack u8[128] buf2_s;
  reg mut ptr u8[128] buf2;
  buf2 = buf2_s; /* u8[128] */ 
  stack u8[128] buf3_s;
  reg mut ptr u8[128] buf3;
  buf3 = buf3_s; /* u8[128] */ 
  () = #spill(r0, r1, r2, r3); /* :k */ 
  stack u8[4] nonces;
  nonces[0] = nonce; /* u8 */ 
  nonce = (nonce +8u ((8u) 1)); /* u8 */ 
  nonces[1] = nonce; /* u8 */ 
  nonce = (nonce +8u ((8u) 1)); /* u8 */ 
  nonces[2] = nonce; /* u8 */ 
  nonce = (nonce +8u ((8u) 1)); /* u8 */ 
  nonces[3] = nonce; /* u8 */ 
  (buf0, buf1, buf2, buf3) =
    _shake256x4_A128__A32_A1(buf0, buf1, buf2, buf3, seed, nonces);
  _ /* u64 */ = #init_msf(); /* :k */ 
  () = #unspill(r0, r1, r2, r3); /* :k */ 
  #[inline]
  r0 = __poly_cbd_eta1(r0, buf0);
  #[inline]
  r1 = __poly_cbd_eta1(r1, buf1);
  #[inline]
  r2 = __poly_cbd_eta1(r2, buf2);
  #[inline]
  r3 = __poly_cbd_eta1(r3, buf3);
  return (r0, r1, r2, r3);
}

inline
fn __invntt___butterfly64x (reg u256 rl0, reg u256 rl1, reg u256 rl2,
                           reg u256 rl3, reg u256 rh0, reg u256 rh1,
                           reg u256 rh2, reg u256 rh3, reg u256 zl0,
                           reg u256 zl1, reg u256 zh0, reg u256 zh1,
                           reg u256 qx16) -> (reg u256, reg u256, reg u256,
                                             reg u256, reg u256, reg u256,
                                             reg u256, reg u256)

{
  reg u256 t0;
  t0 = #VPSUB_16u16(rl0, rh0); /*  */ 
  reg u256 t1;
  t1 = #VPSUB_16u16(rl1, rh1); /*  */ 
  reg u256 t2;
  t2 = #VPSUB_16u16(rl2, rh2); /*  */ 
  rl0 = #VPADD_16u16(rh0, rl0); /*  */ 
  rl1 = #VPADD_16u16(rh1, rl1); /*  */ 
  rh0 = #VPMULL_16u16(zl0, t0); /*  */ 
  rl2 = #VPADD_16u16(rh2, rl2); /*  */ 
  rh1 = #VPMULL_16u16(zl0, t1); /*  */ 
  reg u256 t3;
  t3 = #VPSUB_16u16(rl3, rh3); /*  */ 
  rl3 = #VPADD_16u16(rh3, rl3); /*  */ 
  rh2 = #VPMULL_16u16(zl1, t2); /*  */ 
  rh3 = #VPMULL_16u16(zl1, t3); /*  */ 
  t0 = #VPMULH_16u16(zh0, t0); /*  */ 
  t1 = #VPMULH_16u16(zh0, t1); /*  */ 
  t2 = #VPMULH_16u16(zh1, t2); /*  */ 
  t3 = #VPMULH_16u16(zh1, t3); /*  */ 
  rh0 = #VPMULH_16u16(qx16, rh0); /*  */ 
  rh1 = #VPMULH_16u16(qx16, rh1); /*  */ 
  rh2 = #VPMULH_16u16(qx16, rh2); /*  */ 
  rh3 = #VPMULH_16u16(qx16, rh3); /*  */ 
  rh0 = #VPSUB_16u16(t0, rh0); /*  */ 
  rh1 = #VPSUB_16u16(t1, rh1); /*  */ 
  rh2 = #VPSUB_16u16(t2, rh2); /*  */ 
  rh3 = #VPSUB_16u16(t3, rh3); /*  */ 
  return (rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3);
}

fn _poly_invntt (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N])

{
  reg u256 qx16;
  qx16 = /* global: */ jqx16[:u256 0]; /* u256 */ 
  reg u256 zeta0;
  reg u256 zeta1;
  reg u256 r0;
  reg u256 r1;
  reg u256 r2;
  reg u256 r3;
  reg u256 r4;
  reg u256 r5;
  reg u256 r6;
  reg u256 r7;
  inline int i;
  for i = 0 to 2 {
    zeta0 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (0 + (392 * i))]; /* u256 */ 
    zeta1 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (64 + (392 * i))]; /* u256 */ 
    reg u256 zeta2;
    zeta2 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (32 + (392 * i))]; /* u256 */ 
    reg u256 zeta3;
    zeta3 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (96 + (392 * i))]; /* u256 */ 
    r0 = rp.[#unaligned :u256 ((32 * 0) + (256 * i))]; /* u256 */ 
    r1 = rp.[#unaligned :u256 ((32 * 1) + (256 * i))]; /* u256 */ 
    r2 = rp.[#unaligned :u256 ((32 * 2) + (256 * i))]; /* u256 */ 
    r3 = rp.[#unaligned :u256 ((32 * 3) + (256 * i))]; /* u256 */ 
    r4 = rp.[#unaligned :u256 ((32 * 4) + (256 * i))]; /* u256 */ 
    r5 = rp.[#unaligned :u256 ((32 * 5) + (256 * i))]; /* u256 */ 
    r6 = rp.[#unaligned :u256 ((32 * 6) + (256 * i))]; /* u256 */ 
    r7 = rp.[#unaligned :u256 ((32 * 7) + (256 * i))]; /* u256 */ 
    #[inline]
    (r0, r1, r4, r5, r2, r3, r6, r7) =
      __invntt___butterfly64x(r0, r1, r4, r5, r2, r3, r6, r7, zeta0, zeta1,
                              zeta2, zeta3, qx16);
    reg u256 vx16;
    vx16 = /* global: */ jvx16[:u256 0]; /* u256 */ 
    zeta0 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (128 + (392 * i))]; /* u256 */ 
    zeta1 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (160 + (392 * i))]; /* u256 */ 
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    #[inline]
    r1 = __red16x(r1, qx16, vx16);
    #[inline]
    r4 = __red16x(r4, qx16, vx16);
    #[inline]
    r5 = __red16x(r5, qx16, vx16);
    #[inline]
    (r0, r1, r2, r3, r4, r5, r6, r7) =
      __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    #[inline]
    (r0, r1) = __shuffle1(r0, r1);
    #[inline]
    (r2, r3) = __shuffle1(r2, r3);
    #[inline]
    (r4, r5) = __shuffle1(r4, r5);
    #[inline]
    (r6, r7) = __shuffle1(r6, r7);
    zeta0 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (192 + (392 * i))]; /* u256 */ 
    zeta1 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (224 + (392 * i))]; /* u256 */ 
    #[inline]
    (r0, r2, r4, r6, r1, r3, r5, r7) =
      __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    #[inline]
    (r0, r2) = __shuffle2(r0, r2);
    #[inline]
    (r4, r6) = __shuffle2(r4, r6);
    #[inline]
    (r1, r3) = __shuffle2(r1, r3);
    #[inline]
    (r5, r7) = __shuffle2(r5, r7);
    zeta0 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (256 + (392 * i))]; /* u256 */ 
    zeta1 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (288 + (392 * i))]; /* u256 */ 
    #[inline]
    (r0, r4, r1, r5, r2, r6, r3, r7) =
      __invntt___butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    #[inline]
    (r0, r4) = __shuffle4(r0, r4);
    #[inline]
    (r1, r5) = __shuffle4(r1, r5);
    #[inline]
    (r2, r6) = __shuffle4(r2, r6);
    #[inline]
    (r3, r7) = __shuffle4(r3, r7);
    zeta0 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (320 + (392 * i))]; /* u256 */ 
    zeta1 =
      /* global: */ jzetas_inv_exp.[#unaligned :u256 (352 + (392 * i))]; /* u256 */ 
    #[inline]
    (r0, r1, r2, r3, r4, r5, r6, r7) =
      __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    #[inline]
    (r0, r1) = __shuffle8(r0, r1);
    #[inline]
    (r2, r3) = __shuffle8(r2, r3);
    #[inline]
    (r4, r5) = __shuffle8(r4, r5);
    #[inline]
    (r6, r7) = __shuffle8(r6, r7);
    zeta0 =
      #VPBROADCAST_8u32(/* global: */ jzetas_inv_exp.[#unaligned :u32 
                        (384 + (392 * i))]); /*  */ 
    zeta1 =
      #VPBROADCAST_8u32(/* global: */ jzetas_inv_exp.[#unaligned :u32 
                        (388 + (392 * i))]); /*  */ 
    #[inline]
    (r0, r2, r4, r6, r1, r3, r5, r7) =
      __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    if (i == 0) {
      rp.[#unaligned :u256 ((32 * 0) + (256 * i))] = r0; /* u256 */ 
      rp.[#unaligned :u256 ((32 * 1) + (256 * i))] = r2; /* u256 */ 
      rp.[#unaligned :u256 ((32 * 2) + (256 * i))] = r4; /* u256 */ 
      rp.[#unaligned :u256 ((32 * 3) + (256 * i))] = r6; /* u256 */ 
    }
    rp.[#unaligned :u256 ((32 * 4) + (256 * i))] = r1; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 5) + (256 * i))] = r3; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 6) + (256 * i))] = r5; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 7) + (256 * i))] = r7; /* u256 */ 
  }
  zeta0 =
    #VPBROADCAST_8u32(/* global: */ jzetas_inv_exp.[#unaligned :u32 784]); /*  */ 
  zeta1 =
    #VPBROADCAST_8u32(/* global: */ jzetas_inv_exp.[#unaligned :u32 788]); /*  */ 
  for i = 0 to 2 {
    if (i == 0) {
      r7 = r6; /* u256 */ 
      r6 = r4; /* u256 */ 
      r5 = r2; /* u256 */ 
      r4 = r0; /* u256 */ 
    } else {
      r4 = rp.[#unaligned :u256 ((32 * 8) + (128 * i))]; /* u256 */ 
      r5 = rp.[#unaligned :u256 ((32 * 9) + (128 * i))]; /* u256 */ 
      r6 = rp.[#unaligned :u256 ((32 * 10) + (128 * i))]; /* u256 */ 
      r7 = rp.[#unaligned :u256 ((32 * 11) + (128 * i))]; /* u256 */ 
    }
    r0 = rp.[#unaligned :u256 ((32 * 0) + (128 * i))]; /* u256 */ 
    r1 = rp.[#unaligned :u256 ((32 * 1) + (128 * i))]; /* u256 */ 
    r2 = rp.[#unaligned :u256 ((32 * 2) + (128 * i))]; /* u256 */ 
    r3 = rp.[#unaligned :u256 ((32 * 3) + (128 * i))]; /* u256 */ 
    #[inline]
    (r0, r1, r2, r3, r4, r5, r6, r7) =
      __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0,
                              zeta1, zeta1, qx16);
    reg u256 flox16;
    flox16 = /* global: */ jflox16[:u256 0]; /* u256 */ 
    reg u256 fhix16;
    fhix16 = /* global: */ jfhix16[:u256 0]; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 8) + (128 * i))] = r4; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 9) + (128 * i))] = r5; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 10) + (128 * i))] = r6; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 11) + (128 * i))] = r7; /* u256 */ 
    #[inline]
    r0 = __fqmulprecomp16x(r0, flox16, fhix16, qx16);
    #[inline]
    r1 = __fqmulprecomp16x(r1, flox16, fhix16, qx16);
    #[inline]
    r2 = __fqmulprecomp16x(r2, flox16, fhix16, qx16);
    #[inline]
    r3 = __fqmulprecomp16x(r3, flox16, fhix16, qx16);
    rp.[#unaligned :u256 ((32 * 0) + (128 * i))] = r0; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 1) + (128 * i))] = r1; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 2) + (128 * i))] = r2; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 3) + (128 * i))] = r3; /* u256 */ 
  }
  return (rp);
}

inline
fn __butterfly64x (reg u256 rl0, reg u256 rl1, reg u256 rl2, reg u256 rl3,
                  reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3,
                  reg u256 zl0, reg u256 zl1, reg u256 zh0, reg u256 zh1,
                  reg u256 qx16) -> (reg u256, reg u256, reg u256, reg u256,
                                    reg u256, reg u256, reg u256, reg u256)

{
  reg u256 t0;
  t0 = #VPMULL_16u16(zl0, rh0); /*  */ 
  reg u256 t1;
  t1 = #VPMULH_16u16(zh0, rh0); /*  */ 
  reg u256 t2;
  t2 = #VPMULL_16u16(zl0, rh1); /*  */ 
  reg u256 t3;
  t3 = #VPMULH_16u16(zh0, rh1); /*  */ 
  reg u256 t4;
  t4 = #VPMULL_16u16(zl1, rh2); /*  */ 
  reg u256 t5;
  t5 = #VPMULH_16u16(zh1, rh2); /*  */ 
  reg u256 t6;
  t6 = #VPMULL_16u16(zl1, rh3); /*  */ 
  reg u256 t7;
  t7 = #VPMULH_16u16(zh1, rh3); /*  */ 
  t0 = #VPMULH_16u16(t0, qx16); /*  */ 
  t2 = #VPMULH_16u16(t2, qx16); /*  */ 
  t4 = #VPMULH_16u16(t4, qx16); /*  */ 
  t6 = #VPMULH_16u16(t6, qx16); /*  */ 
  rh1 = #VPSUB_16u16(rl1, t3); /*  */ 
  rl1 = #VPADD_16u16(t3, rl1); /*  */ 
  rh0 = #VPSUB_16u16(rl0, t1); /*  */ 
  rl0 = #VPADD_16u16(t1, rl0); /*  */ 
  rh3 = #VPSUB_16u16(rl3, t7); /*  */ 
  rl3 = #VPADD_16u16(t7, rl3); /*  */ 
  rh2 = #VPSUB_16u16(rl2, t5); /*  */ 
  rl2 = #VPADD_16u16(t5, rl2); /*  */ 
  rh0 = #VPADD_16u16(t0, rh0); /*  */ 
  rl0 = #VPSUB_16u16(rl0, t0); /*  */ 
  rh1 = #VPADD_16u16(t2, rh1); /*  */ 
  rl1 = #VPSUB_16u16(rl1, t2); /*  */ 
  rh2 = #VPADD_16u16(t4, rh2); /*  */ 
  rl2 = #VPSUB_16u16(rl2, t4); /*  */ 
  rh3 = #VPADD_16u16(t6, rh3); /*  */ 
  rl3 = #VPSUB_16u16(rl3, t6); /*  */ 
  return (rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3);
}

fn _poly_ntt (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N])

{
  reg u256 qx16;
  qx16 = /* global: */ jqx16[:u256 0]; /* u256 */ 
  reg u256 zeta0;
  zeta0 = #VPBROADCAST_8u32(/* global: */ jzetas_exp[:u32 0]); /*  */ 
  reg u256 zeta1;
  zeta1 = #VPBROADCAST_8u32(/* global: */ jzetas_exp[:u32 1]); /*  */ 
  reg u256 r0;
  r0 = rp.[#unaligned :u256 (32 * 0)]; /* u256 */ 
  reg u256 r1;
  r1 = rp.[#unaligned :u256 (32 * 1)]; /* u256 */ 
  reg u256 r2;
  r2 = rp.[#unaligned :u256 (32 * 2)]; /* u256 */ 
  reg u256 r3;
  r3 = rp.[#unaligned :u256 (32 * 3)]; /* u256 */ 
  reg u256 r4;
  r4 = rp.[#unaligned :u256 (32 * 8)]; /* u256 */ 
  reg u256 r5;
  r5 = rp.[#unaligned :u256 (32 * 9)]; /* u256 */ 
  reg u256 r6;
  r6 = rp.[#unaligned :u256 (32 * 10)]; /* u256 */ 
  reg u256 r7;
  r7 = rp.[#unaligned :u256 (32 * 11)]; /* u256 */ 
  #[inline]
  (r0, r1, r2, r3, r4, r5, r6, r7) =
    __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1,
                   zeta1, qx16);
  rp.[#unaligned :u256 (32 * 0)] = r0; /* u256 */ 
  rp.[#unaligned :u256 (32 * 1)] = r1; /* u256 */ 
  rp.[#unaligned :u256 (32 * 2)] = r2; /* u256 */ 
  rp.[#unaligned :u256 (32 * 3)] = r3; /* u256 */ 
  rp.[#unaligned :u256 (32 * 8)] = r4; /* u256 */ 
  rp.[#unaligned :u256 (32 * 9)] = r5; /* u256 */ 
  rp.[#unaligned :u256 (32 * 10)] = r6; /* u256 */ 
  rp.[#unaligned :u256 (32 * 11)] = r7; /* u256 */ 
  r0 = rp.[#unaligned :u256 (32 * 4)]; /* u256 */ 
  r1 = rp.[#unaligned :u256 (32 * 5)]; /* u256 */ 
  r2 = rp.[#unaligned :u256 (32 * 6)]; /* u256 */ 
  r3 = rp.[#unaligned :u256 (32 * 7)]; /* u256 */ 
  r4 = rp.[#unaligned :u256 (32 * 12)]; /* u256 */ 
  r5 = rp.[#unaligned :u256 (32 * 13)]; /* u256 */ 
  r6 = rp.[#unaligned :u256 (32 * 14)]; /* u256 */ 
  r7 = rp.[#unaligned :u256 (32 * 15)]; /* u256 */ 
  #[inline]
  (r0, r1, r2, r3, r4, r5, r6, r7) =
    __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1,
                   zeta1, qx16);
  rp.[#unaligned :u256 (32 * 12)] = r4; /* u256 */ 
  rp.[#unaligned :u256 (32 * 13)] = r5; /* u256 */ 
  rp.[#unaligned :u256 (32 * 14)] = r6; /* u256 */ 
  rp.[#unaligned :u256 (32 * 15)] = r7; /* u256 */ 
  inline int i;
  for i = 0 to 2 {
    zeta0 =
      #VPBROADCAST_8u32(/* global: */ jzetas_exp.[#unaligned :u32 (8 +
                                                                  (392 * i))]); /*  */ 
    zeta1 =
      #VPBROADCAST_8u32(/* global: */ jzetas_exp.[#unaligned :u32 (12 +
                                                                  (392 * i))]); /*  */ 
    if (i == 0) {
      r4 = r0; /* u256 */ 
      r5 = r1; /* u256 */ 
      r6 = r2; /* u256 */ 
      r7 = r3; /* u256 */ 
    } else {
      r4 = rp.[#unaligned :u256 ((32 * 4) + (256 * i))]; /* u256 */ 
      r5 = rp.[#unaligned :u256 ((32 * 5) + (256 * i))]; /* u256 */ 
      r6 = rp.[#unaligned :u256 ((32 * 6) + (256 * i))]; /* u256 */ 
      r7 = rp.[#unaligned :u256 ((32 * 7) + (256 * i))]; /* u256 */ 
    }
    r0 = rp.[#unaligned :u256 ((32 * 0) + (256 * i))]; /* u256 */ 
    r1 = rp.[#unaligned :u256 ((32 * 1) + (256 * i))]; /* u256 */ 
    r2 = rp.[#unaligned :u256 ((32 * 2) + (256 * i))]; /* u256 */ 
    r3 = rp.[#unaligned :u256 ((32 * 3) + (256 * i))]; /* u256 */ 
    #[inline]
    (r0, r1, r2, r3, r4, r5, r6, r7) =
      __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1,
                     zeta1, qx16);
    zeta0 =
      /* global: */ jzetas_exp.[#unaligned :u256 (16 + (392 * i))]; /* u256 */ 
    zeta1 =
      /* global: */ jzetas_exp.[#unaligned :u256 (48 + (392 * i))]; /* u256 */ 
    #[inline]
    (r0, r4) = __shuffle8(r0, r4);
    #[inline]
    (r1, r5) = __shuffle8(r1, r5);
    #[inline]
    (r2, r6) = __shuffle8(r2, r6);
    #[inline]
    (r3, r7) = __shuffle8(r3, r7);
    #[inline]
    (r0, r4, r1, r5, r2, r6, r3, r7) =
      __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1,
                     zeta1, qx16);
    zeta0 =
      /* global: */ jzetas_exp.[#unaligned :u256 (80 + (392 * i))]; /* u256 */ 
    zeta1 =
      /* global: */ jzetas_exp.[#unaligned :u256 (112 + (392 * i))]; /* u256 */ 
    #[inline]
    (r0, r2) = __shuffle4(r0, r2);
    #[inline]
    (r4, r6) = __shuffle4(r4, r6);
    #[inline]
    (r1, r3) = __shuffle4(r1, r3);
    #[inline]
    (r5, r7) = __shuffle4(r5, r7);
    #[inline]
    (r0, r2, r4, r6, r1, r3, r5, r7) =
      __butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1,
                     zeta1, qx16);
    zeta0 =
      /* global: */ jzetas_exp.[#unaligned :u256 (144 + (392 * i))]; /* u256 */ 
    zeta1 =
      /* global: */ jzetas_exp.[#unaligned :u256 (176 + (392 * i))]; /* u256 */ 
    #[inline]
    (r0, r1) = __shuffle2(r0, r1);
    #[inline]
    (r2, r3) = __shuffle2(r2, r3);
    #[inline]
    (r4, r5) = __shuffle2(r4, r5);
    #[inline]
    (r6, r7) = __shuffle2(r6, r7);
    #[inline]
    (r0, r1, r2, r3, r4, r5, r6, r7) =
      __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1,
                     zeta1, qx16);
    zeta0 =
      /* global: */ jzetas_exp.[#unaligned :u256 (208 + (392 * i))]; /* u256 */ 
    zeta1 =
      /* global: */ jzetas_exp.[#unaligned :u256 (240 + (392 * i))]; /* u256 */ 
    #[inline]
    (r0, r4) = __shuffle1(r0, r4);
    #[inline]
    (r1, r5) = __shuffle1(r1, r5);
    #[inline]
    (r2, r6) = __shuffle1(r2, r6);
    #[inline]
    (r3, r7) = __shuffle1(r3, r7);
    #[inline]
    (r0, r4, r1, r5, r2, r6, r3, r7) =
      __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1,
                     zeta1, qx16);
    zeta0 =
      /* global: */ jzetas_exp.[#unaligned :u256 (272 + (392 * i))]; /* u256 */ 
    reg u256 zeta2;
    zeta2 =
      /* global: */ jzetas_exp.[#unaligned :u256 (304 + (392 * i))]; /* u256 */ 
    zeta1 =
      /* global: */ jzetas_exp.[#unaligned :u256 (336 + (392 * i))]; /* u256 */ 
    reg u256 zeta3;
    zeta3 =
      /* global: */ jzetas_exp.[#unaligned :u256 (368 + (392 * i))]; /* u256 */ 
    #[inline]
    (r0, r4, r2, r6, r1, r5, r3, r7) =
      __butterfly64x(r0, r4, r2, r6, r1, r5, r3, r7, zeta0, zeta1, zeta2,
                     zeta3, qx16);
    reg u256 vx16;
    vx16 = /* global: */ jvx16[:u256 0]; /* u256 */ 
    #[inline]
    r0 = __red16x(r0, qx16, vx16);
    #[inline]
    r4 = __red16x(r4, qx16, vx16);
    #[inline]
    r2 = __red16x(r2, qx16, vx16);
    #[inline]
    r6 = __red16x(r6, qx16, vx16);
    #[inline]
    r1 = __red16x(r1, qx16, vx16);
    #[inline]
    r5 = __red16x(r5, qx16, vx16);
    #[inline]
    r3 = __red16x(r3, qx16, vx16);
    #[inline]
    r7 = __red16x(r7, qx16, vx16);
    rp.[#unaligned :u256 ((32 * 0) + (256 * i))] = r0; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 1) + (256 * i))] = r4; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 2) + (256 * i))] = r1; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 3) + (256 * i))] = r5; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 4) + (256 * i))] = r2; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 5) + (256 * i))] = r6; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 6) + (256 * i))] = r3; /* u256 */ 
    rp.[#unaligned :u256 ((32 * 7) + (256 * i))] = r7; /* u256 */ 
  }
  return (rp);
}

inline
fn __poly_reduce (reg mut ptr u16[MLKEM_N] rp) -> (reg mut ptr u16[MLKEM_N])

{
  reg u256 qx16;
  qx16 = /* global: */ jqx16[:u256 0]; /* u256 */ 
  reg u256 vx16;
  vx16 = /* global: */ jvx16[:u256 0]; /* u256 */ 
  inline int i;
  for i = 0 to 16 {
    reg u256 r;
    r = rp.[#unaligned :u256 (32 * i)]; /* u256 */ 
    #[inline]
    r = __red16x(r, qx16, vx16);
    rp.[#unaligned :u256 (32 * i)] = r; /* u256 */ 
  }
  return (rp);
}

fn _poly_sub (reg mut ptr u16[MLKEM_N] rp, reg const ptr u16[MLKEM_N] ap,
             reg const ptr u16[MLKEM_N] bp) -> (reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {(is_arr_init(ap, 0, (32 * 16)) &&
                        is_arr_init(bp, 0, (32 * 16)))}

ensures #[prover=Cas] {is_arr_init(result.0, 0, (32 * 16))} 
{
  inline int i;
  for i = 0 to 16 {
    reg u256 a;
    a = ap.[#unaligned :u256 (32 * i)]; /* u256 */ 
    reg u256 b;
    b = bp.[#unaligned :u256 (32 * i)]; /* u256 */ 
    reg u256 r;
    r = #VPSUB_16u16(a, b); /*  */ 
    rp.[#unaligned :u256 (32 * i)] = r; /* u256 */ 
  }
  return (rp);
}

fn _i_poly_tobytes (reg mut ptr u8[MLKEM_POLYBYTES] rp,
                   reg mut ptr u16[MLKEM_N] a) -> (reg mut ptr u8[MLKEM_POLYBYTES],
                                                  reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {is_arr_init(a, 0, (2 * MLKEM_N))} 
ensures #[prover=Cas] {(is_arr_init(result.0, 0, MLKEM_POLYBYTES) &&
                       is_arr_init(result.1, 0, (2 * MLKEM_N)))}

{
  a = _poly_csubq(a);
  inline int i;
  for i = 0 to 2 {
    reg u256 t0;
    t0 = a[:u256 (8 * i)]; /* u256 */ 
    reg u256 t1;
    t1 = a[:u256 ((8 * i) + 1)]; /* u256 */ 
    reg u256 t2;
    t2 = a[:u256 ((8 * i) + 2)]; /* u256 */ 
    reg u256 t3;
    t3 = a[:u256 ((8 * i) + 3)]; /* u256 */ 
    reg u256 t4;
    t4 = a[:u256 ((8 * i) + 4)]; /* u256 */ 
    reg u256 t5;
    t5 = a[:u256 ((8 * i) + 5)]; /* u256 */ 
    reg u256 t6;
    t6 = a[:u256 ((8 * i) + 6)]; /* u256 */ 
    reg u256 t7;
    t7 = a[:u256 ((8 * i) + 7)]; /* u256 */ 
    reg u256 tt;
    tt = #VPSLL_16u16(t1, ((128u) 12)); /*  */ 
    tt = (tt |256u t0); /* u256 */ 
    t0 = #VPSRL_16u16(t1, ((128u) 4)); /*  */ 
    t1 = #VPSLL_16u16(t2, ((128u) 8)); /*  */ 
    t0 = (t0 |256u t1); /* u256 */ 
    t1 = #VPSRL_16u16(t2, ((128u) 8)); /*  */ 
    t2 = #VPSLL_16u16(t3, ((128u) 4)); /*  */ 
    t1 = (t1 |256u t2); /* u256 */ 
    t2 = #VPSLL_16u16(t5, ((128u) 12)); /*  */ 
    t2 = (t2 |256u t4); /* u256 */ 
    t3 = #VPSRL_16u16(t5, ((128u) 4)); /*  */ 
    t4 = #VPSLL_16u16(t6, ((128u) 8)); /*  */ 
    t3 = (t3 |256u t4); /* u256 */ 
    t4 = #VPSRL_16u16(t6, ((128u) 8)); /*  */ 
    t5 = #VPSLL_16u16(t7, ((128u) 4)); /*  */ 
    t4 = (t4 |256u t5); /* u256 */ 
    reg u256 ttt;
    #[inline]
    (ttt, t0) = __shuffle1(tt, t0);
    #[inline]
    (tt, t2) = __shuffle1(t1, t2);
    #[inline]
    (t1, t4) = __shuffle1(t3, t4);
    #[inline]
    (t3, tt) = __shuffle2(ttt, tt);
    #[inline]
    (ttt, t0) = __shuffle2(t1, t0);
    #[inline]
    (t1, t4) = __shuffle2(t2, t4);
    #[inline]
    (t2, ttt) = __shuffle4(t3, ttt);
    #[inline]
    (t3, tt) = __shuffle4(t1, tt);
    #[inline]
    (t1, t4) = __shuffle4(t0, t4);
    #[inline]
    (t0, t3) = __shuffle8(t2, t3);
    #[inline]
    (t2, ttt) = __shuffle8(t1, ttt);
    #[inline]
    (t1, t4) = __shuffle8(tt, t4);
    rp.[#unaligned :u256 (192 * i)] = t0; /* u256 */ 
    rp.[#unaligned :u256 ((192 * i) + 32)] = t2; /* u256 */ 
    rp.[#unaligned :u256 ((192 * i) + 64)] = t1; /* u256 */ 
    rp.[#unaligned :u256 ((192 * i) + 96)] = t3; /* u256 */ 
    rp.[#unaligned :u256 ((192 * i) + 128)] = ttt; /* u256 */ 
    rp.[#unaligned :u256 ((192 * i) + 160)] = t4; /* u256 */ 
  }
  return (rp, a);
}

fn _i_poly_tomsg (reg mut ptr u8[MLKEM_INDCPA_MSGBYTES] rp,
                 reg mut ptr u16[MLKEM_N] a) -> (reg mut ptr u8[MLKEM_INDCPA_MSGBYTES],
                                                reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {is_arr_init(a, 0, (2 * MLKEM_N))} 
ensures #[prover=Cas] {(is_arr_init(result.0, 0, (MLKEM_N / 8)) &&
                       is_arr_init(result.1, 0, (2 * MLKEM_N)))}

{
  a = _poly_csubq(a);
  reg mut ptr u16[16] px16;
  px16 = /* global: */ hqx16_m1; /* u16[16] */ 
  reg u256 hq;
  hq = px16[:u256 0]; /* u256 */ 
  px16 = /* global: */ hhqx16; /* u16[16] */ 
  reg u256 hhq;
  hhq = px16[:u256 0]; /* u256 */ 
  inline int i;
  for i = 0 to (MLKEM_N / 32) {
    reg u256 f0;
    f0 = a[:u256 (2 * i)]; /* u256 */ 
    reg u256 f1;
    f1 = a[:u256 ((2 * i) + 1)]; /* u256 */ 
    f0 = #VPSUB_16u16(hq, f0); /*  */ 
    f1 = #VPSUB_16u16(hq, f1); /*  */ 
    reg u256 g0;
    g0 = #VPSRA_16u16(f0, ((128u) 15)); /*  */ 
    reg u256 g1;
    g1 = #VPSRA_16u16(f1, ((128u) 15)); /*  */ 
    f0 = #VPXOR_256(f0, g0); /*  */ 
    f1 = #VPXOR_256(f1, g1); /*  */ 
    f0 = #VPSUB_16u16(f0, hhq); /*  */ 
    f1 = #VPSUB_16u16(f1, hhq); /*  */ 
    f0 = #VPACKSS_16u16(f0, f1); /*  */ 
    f0 = #VPERMQ(f0, ((8u) 216)); /*  */ 
    reg u32 c;
    c = #MOVEMASK_32u8(f0); /*  */ 
    rp[:u32 i] = c; /* u32 */ 
  }
  return (rp, a);
}

u16 pc_shift1_s = ((16u) 512);

u16 pc_mask_s = ((16u) 15);

u16 pc_shift2_s = ((16u) 4097);

u32[8] pc_permidx_s = {((32u) 0), ((32u) 4), ((32u) 1), ((32u) 5), ((32u) 2),
                       ((32u) 6), ((32u) 3), ((32u) 7)};

fn _i_poly_compress (reg mut ptr u8[MLKEM_POLYCOMPRESSEDBYTES] rp,
                    reg mut ptr u16[MLKEM_N] a) -> (reg mut ptr u8[MLKEM_POLYCOMPRESSEDBYTES],
                                                   reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {is_arr_init(a, 0, (2 * MLKEM_N))} 
ensures #[prover=Cas] {(is_arr_init(result.0, 0, MLKEM_POLYCOMPRESSEDBYTES) &&
                       is_arr_init(result.1, 0, (2 * MLKEM_N)))}

{
  a = _poly_csubq(a);
  reg mut ptr u16[16] x16p;
  x16p = /* global: */ jvx16; /* u16[16] */ 
  reg u256 v;
  v = x16p[:u256 0]; /* u256 */ 
  reg u256 shift1;
  shift1 = #VPBROADCAST_16u16(/* global: */ pc_shift1_s); /*  */ 
  reg u256 mask;
  mask = #VPBROADCAST_16u16(/* global: */ pc_mask_s); /*  */ 
  reg u256 shift2;
  shift2 = #VPBROADCAST_16u16(/* global: */ pc_shift2_s); /*  */ 
  reg u256 permidx;
  permidx = /* global: */ pc_permidx_s[:u256 0]; /* u256 */ 
  inline int i;
  for i = 0 to (MLKEM_N / 64) {
    reg u256 f0;
    f0 = a[:u256 (4 * i)]; /* u256 */ 
    reg u256 f1;
    f1 = a[:u256 ((4 * i) + 1)]; /* u256 */ 
    reg u256 f2;
    f2 = a[:u256 ((4 * i) + 2)]; /* u256 */ 
    reg u256 f3;
    f3 = a[:u256 ((4 * i) + 3)]; /* u256 */ 
    f0 = #VPMULH_16u16(f0, v); /*  */ 
    f1 = #VPMULH_16u16(f1, v); /*  */ 
    f2 = #VPMULH_16u16(f2, v); /*  */ 
    f3 = #VPMULH_16u16(f3, v); /*  */ 
    f0 = #VPMULHRS_16u16(f0, shift1); /*  */ 
    f1 = #VPMULHRS_16u16(f1, shift1); /*  */ 
    f2 = #VPMULHRS_16u16(f2, shift1); /*  */ 
    f3 = #VPMULHRS_16u16(f3, shift1); /*  */ 
    f0 = #VPAND_256(f0, mask); /*  */ 
    f1 = #VPAND_256(f1, mask); /*  */ 
    f2 = #VPAND_256(f2, mask); /*  */ 
    f3 = #VPAND_256(f3, mask); /*  */ 
    f0 = #VPACKUS_16u16(f0, f1); /*  */ 
    f2 = #VPACKUS_16u16(f2, f3); /*  */ 
    f0 = #VPMADDUBSW_256(f0, shift2); /*  */ 
    f2 = #VPMADDUBSW_256(f2, shift2); /*  */ 
    f0 = #VPACKUS_16u16(f0, f2); /*  */ 
    f0 = #VPERMD(permidx, f0); /*  */ 
    rp.[#unaligned :u256 (32 * i)] = f0; /* u256 */ 
  }
  return (rp, a);
}

u8[32] pd_jshufbidx = {((8u) 0), ((8u) 0), ((8u) 0), ((8u) 0), ((8u) 1),
                       ((8u) 1), ((8u) 1), ((8u) 1), ((8u) 2), ((8u) 2),
                       ((8u) 2), ((8u) 2), ((8u) 3), ((8u) 3), ((8u) 3),
                       ((8u) 3), ((8u) 4), ((8u) 4), ((8u) 4), ((8u) 4),
                       ((8u) 5), ((8u) 5), ((8u) 5), ((8u) 5), ((8u) 6),
                       ((8u) 6), ((8u) 6), ((8u) 6), ((8u) 7), ((8u) 7),
                       ((8u) 7), ((8u) 7)};

u32 pd_mask_s = ((32u) 15728655);

u32 pd_shift_s = ((32u) 8390656);

fn _i_poly_decompress (reg mut ptr u16[MLKEM_N] rp,
                      reg const ptr u8[MLKEM_POLYCOMPRESSEDBYTES] a) -> 
(reg mut ptr u16[MLKEM_N])
requires #[prover=Cas] {is_arr_init(a, 0, MLKEM_POLYCOMPRESSEDBYTES)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, (2 * MLKEM_N))} 
{
  reg mut ptr u16[16] x16p;
  x16p = /* global: */ jqx16; /* u16[16] */ 
  reg u256 q;
  q = x16p[:u256 0]; /* u256 */ 
  reg mut ptr u8[32] x32p;
  x32p = /* global: */ pd_jshufbidx; /* u8[32] */ 
  reg u256 shufbidx;
  shufbidx = x32p[:u256 0]; /* u256 */ 
  reg u256 mask;
  mask = #VPBROADCAST_8u32(/* global: */ pd_mask_s); /*  */ 
  reg u256 shift;
  shift = #VPBROADCAST_8u32(/* global: */ pd_shift_s); /*  */ 
  inline int i;
  for i = 0 to (MLKEM_N / 16) {
    reg u128 h;
    h = ((128u) a.[#unaligned :u64 (8 * i)]); /* u128 */ 
    stack u128 sh;
    sh = h; /* u128 */ 
    reg u256 f;
    f = #VPBROADCAST_2u128(sh); /*  */ 
    f = #VPSHUFB_256(f, shufbidx); /*  */ 
    f = #VPAND_256(f, mask); /*  */ 
    f = #VPMULL_16u16(f, shift); /*  */ 
    f = #VPMULHRS_16u16(f, q); /*  */ 
    rp[:u256 i] = f; /* u256 */ 
  }
  return (rp);
}

inline
fn __polyvec_add2 (stack u16[MLKEM_VECN] r, stack u16[MLKEM_VECN] b) -> 
(stack u16[MLKEM_VECN])

{
  inline int i;
  for i = 0 to MLKEM_K {
    r[(MLKEM_N * i) : MLKEM_N] =
      _poly_add2(r[(MLKEM_N * i) : MLKEM_N], b[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __polyvec_csubq (stack u16[MLKEM_VECN] r) -> (stack u16[MLKEM_VECN])

{
  inline int i;
  for i = 0 to MLKEM_K {
    r[(MLKEM_N * i) : MLKEM_N] = _poly_csubq(r[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __polyvec_invntt (stack u16[MLKEM_VECN] r) -> (stack u16[MLKEM_VECN])

{
  inline int i;
  for i = 0 to MLKEM_K {
    r[(MLKEM_N * i) : MLKEM_N] = _poly_invntt(r[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __polyvec_ntt (stack u16[MLKEM_VECN] r) -> (stack u16[MLKEM_VECN])

{
  inline int i;
  for i = 0 to MLKEM_K {
    r[(MLKEM_N * i) : MLKEM_N] = _poly_ntt(r[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __polyvec_reduce (stack u16[MLKEM_VECN] r) -> (stack u16[MLKEM_VECN])

{
  inline int i;
  for i = 0 to MLKEM_K {
    #[inline]
    r[(MLKEM_N * i) : MLKEM_N] = __poly_reduce(r[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __i_polyvec_frombytes (reg const ptr u8[MLKEM_POLYVECBYTES] a) -> 
(stack u16[MLKEM_VECN])

{
  stack u16[MLKEM_VECN] r;
  inline int i;
  for i = 0 to MLKEM_K {
    r[(MLKEM_N * i) : MLKEM_N] =
      _i_poly_frombytes(r[(MLKEM_N * i) : MLKEM_N],
                        a[(MLKEM_POLYBYTES * i) : MLKEM_POLYBYTES]);
  }
  return (r);
}

inline
fn __i_polyvec_tobytes (reg mut ptr u8[MLKEM_POLYVECBYTES] r,
                       stack u16[MLKEM_VECN] a) -> (reg mut ptr u8[MLKEM_POLYVECBYTES])
requires #[prover=Cas] {is_arr_init(a, 0, (2 * MLKEM_VECN))} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, MLKEM_POLYVECBYTES)} 
{
  inline int i;
  for i = 0 to MLKEM_K {
    (r[(MLKEM_POLYBYTES * i) : MLKEM_POLYBYTES], a[(MLKEM_N * i) : MLKEM_N]) =
      _i_poly_tobytes(r[(MLKEM_POLYBYTES * i) : MLKEM_POLYBYTES],
                      a[(MLKEM_N * i) : MLKEM_N]);
  }
  return (r);
}

inline
fn __polyvec_pointwise_acc (stack u16[MLKEM_N] r, stack u16[MLKEM_VECN] a,
                           stack u16[MLKEM_VECN] b) -> (stack u16[MLKEM_N])
requires #[prover=Cas] {(is_arr_init(a, 0, (MLKEM_VECN * 2)) &&
                        is_arr_init(b, 0, (MLKEM_VECN * 2)))}

ensures #[prover=Cas] {is_arr_init(result.0, 0, (MLKEM_N * 2))} 
{
  r = _poly_basemul(r, a[0 : MLKEM_N], b[0 : MLKEM_N]);
  inline int i;
  for i = 1 to MLKEM_K {
    stack u16[MLKEM_N] t;
    t =
      _poly_basemul(t, a[(MLKEM_N * i) : MLKEM_N], b[(MLKEM_N * i) : MLKEM_N]);
    r = _poly_add2(r, t);
  }
  return (r);
}

u32 pvd_q_s = ((32u) 218182660);

u8[32] pvd_shufbdidx_s = {((8u) 0), ((8u) 1), ((8u) 1), ((8u) 2), ((8u) 2),
                          ((8u) 3), ((8u) 3), ((8u) 4), ((8u) 5), ((8u) 6),
                          ((8u) 6), ((8u) 7), ((8u) 7), ((8u) 8), ((8u) 8),
                          ((8u) 9), ((8u) 2), ((8u) 3), ((8u) 3), ((8u) 4),
                          ((8u) 4), ((8u) 5), ((8u) 5), ((8u) 6), ((8u) 7),
                          ((8u) 8), ((8u) 8), ((8u) 9), ((8u) 9), ((8u) 10),
                          ((8u) 10), ((8u) 11)};

u64 pvd_sllvdidx_s = ((64u) 4);

u32 pvd_mask_s = ((32u) 2145394680);

inline
fn __i_polyvec_decompress (reg const ptr u8[MLKEM_CIPHERTEXTBYTES] rp) -> 
(stack u16[MLKEM_VECN])
requires #[prover=Cas] {is_arr_init(rp, 0, MLKEM_CIPHERTEXTBYTES)} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, (MLKEM_VECN * 2))} 
{
  reg u256 q;
  q = #VPBROADCAST_8u32(/* global: */ pvd_q_s); /*  */ 
  reg u256 shufbidx;
  shufbidx = /* global: */ pvd_shufbdidx_s[:u256 0]; /* u256 */ 
  reg u256 sllvdidx;
  sllvdidx = #VPBROADCAST_4u64(/* global: */ pvd_sllvdidx_s); /*  */ 
  reg u256 mask;
  mask = #VPBROADCAST_8u32(/* global: */ pvd_mask_s); /*  */ 
  inline int k;
  stack u16[MLKEM_VECN] r;
  for k = 0 to MLKEM_K {
    inline int i;
    for i = 0 to (MLKEM_N / 16) {
      reg u256 f;
      f = rp.[#unaligned :u256 ((320 * k) + (20 * i))]; /* u256 */ 
      f = #VPERMQ(f, ((8u) 148)); /*  */ 
      f = #VPSHUFB_256(f, shufbidx); /*  */ 
      f = #VPSLLV_8u32(f, sllvdidx); /*  */ 
      f = #VPSRL_16u16(f, ((128u) 1)); /*  */ 
      f = #VPAND_256(f, mask); /*  */ 
      f = #VPMULHRS_16u16(f, q); /*  */ 
      r[:u256 ((16 * k) + i)] = f; /* u256 */ 
    }
  }
  return (r);
}

u16 pvc_off_s = ((16u) 15);

u16 pvc_shift1_s = ((16u) 4096);

u16 pvc_mask_s = ((16u) 1023);

u64 pvc_shift2_s = ((64u) 288230380513787905);

u64 pvc_sllvdidx_s = ((64u) 12);

u8[32] pvc_shufbidx_s = {((8u) 0), ((8u) 1), ((8u) 2), ((8u) 3), ((8u) 4),
                         ((8u) 8), ((8u) 9), ((8u) 10), ((8u) 11), ((8u) 12),
                         ((8u) (- 1)), ((8u) (- 1)), ((8u) (- 1)),
                         ((8u) (- 1)), ((8u) (- 1)), ((8u) (- 1)), ((8u) 9),
                         ((8u) 10), ((8u) 11), ((8u) 12), ((8u) (- 1)),
                         ((8u) (- 1)), ((8u) (- 1)), ((8u) (- 1)),
                         ((8u) (- 1)), ((8u) (- 1)), ((8u) 0), ((8u) 1),
                         ((8u) 2), ((8u) 3), ((8u) 4), ((8u) 8)};

inline
fn __i_polyvec_compress (reg mut ptr u8[MLKEM_POLYVECCOMPRESSEDBYTES] rp,
                        stack u16[MLKEM_VECN] a) -> (reg mut ptr u8[MLKEM_POLYVECCOMPRESSEDBYTES])
requires #[prover=Cas] {is_arr_init(a, 0, (MLKEM_VECN * 2))} 
ensures #[prover=Cas] {is_arr_init(result.0, 0, MLKEM_POLYVECCOMPRESSEDBYTES)}

{
  #[inline]
  a = __polyvec_csubq(a);
  reg mut ptr u16[16] x16p;
  x16p = /* global: */ jvx16; /* u16[16] */ 
  reg u256 v;
  v = x16p[:u256 0]; /* u256 */ 
  reg u256 v8;
  v8 = #VPSLL_16u16(v, ((128u) 3)); /*  */ 
  reg u256 off;
  off = #VPBROADCAST_16u16(/* global: */ pvc_off_s); /*  */ 
  reg u256 shift1;
  shift1 = #VPBROADCAST_16u16(/* global: */ pvc_shift1_s); /*  */ 
  reg u256 mask;
  mask = #VPBROADCAST_16u16(/* global: */ pvc_mask_s); /*  */ 
  reg u256 shift2;
  shift2 = #VPBROADCAST_4u64(/* global: */ pvc_shift2_s); /*  */ 
  reg u256 sllvdidx;
  sllvdidx = #VPBROADCAST_4u64(/* global: */ pvc_sllvdidx_s); /*  */ 
  reg u256 shufbidx;
  shufbidx = /* global: */ pvc_shufbidx_s[:u256 0]; /* u256 */ 
  inline int i;
  for i = 0 to (MLKEM_VECN / 16) {
    reg u256 f0;
    f0 = a[:u256 i]; /* u256 */ 
    reg u256 f1;
    f1 = #VPMULL_16u16(f0, v8); /*  */ 
    reg u256 f2;
    f2 = #VPADD_16u16(f0, off); /*  */ 
    f0 = #VPSLL_16u16(f0, ((128u) 3)); /*  */ 
    f0 = #VPMULH_16u16(f0, v); /*  */ 
    f2 = #VPSUB_16u16(f1, f2); /*  */ 
    f1 = #VPANDN_256(f1, f2); /*  */ 
    f1 = #VPSRL_16u16(f1, ((128u) 15)); /*  */ 
    f0 = #VPSUB_16u16(f0, f1); /*  */ 
    f0 = #VPMULHRS_16u16(f0, shift1); /*  */ 
    f0 = #VPAND_256(f0, mask); /*  */ 
    f0 = #VPMADDWD_256(f0, shift2); /*  */ 
    f0 = #VPSLLV_8u32(f0, sllvdidx); /*  */ 
    f0 = #VPSRL_4u64(f0, ((128u) 12)); /*  */ 
    f0 = #VPSHUFB_256(f0, shufbidx); /*  */ 
    reg u128 t0;
    t0 = f0; /* u128 */ 
    reg u128 t1;
    t1 = #VEXTRACTI128(f0, ((8u) 1)); /*  */ 
    t0 = #VPBLEND_8u16(t0, t1, ((8u) 224)); /*  */ 
    rp.[#unaligned :u128 (20 * i)] = t0; /* u128 */ 
    rp.[#unaligned :u32 ((20 * i) + 16)] = #VPEXTR_32(t1, ((8u) 0)); /*  */ 
  }
  return (rp);
}

u8[32] sample_load_shuffle = {((8u) 0), ((8u) 1), ((8u) 1), ((8u) 2),
                              ((8u) 3), ((8u) 4), ((8u) 4), ((8u) 5),
                              ((8u) 6), ((8u) 7), ((8u) 7), ((8u) 8),
                              ((8u) 9), ((8u) 10), ((8u) 10), ((8u) 11),
                              ((8u) 4), ((8u) 5), ((8u) 5), ((8u) 6),
                              ((8u) 7), ((8u) 8), ((8u) 8), ((8u) 9),
                              ((8u) 10), ((8u) 11), ((8u) 11), ((8u) 12),
                              ((8u) 13), ((8u) 14), ((8u) 14), ((8u) 15)};

u256 sample_ones = (32u8)[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1];

u256 sample_mask = (16u16)[4095, 4095, 4095, 4095, 4095, 4095, 4095, 4095,
                   4095, 4095, 4095, 4095, 4095, 4095, 4095, 4095];

u256 sample_q = (16u16)[MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q,
                MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q,
                MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q];

u8[(256 * 8)] sample_shuffle_table = {((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 6),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 6), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 6), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 8), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 8), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 6),
                                      ((8u) 8), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 6),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 6), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 8),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 6),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 6), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 6), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 8),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 8),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) (- 1)), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 6),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 6), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 6), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 8),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 8),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 8),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 8),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 8),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 4),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 2),
                                      ((8u) 4), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 4), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 6), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 6), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 8),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 8),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 8),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 8), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 4),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 4), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 6),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 6), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 2),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 4), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) (- 1)), ((8u) 0), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 2), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) (- 1)), ((8u) 0),
                                      ((8u) 2), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) 14), ((8u) (- 1)), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 4), ((8u) 6), ((8u) 8),
                                      ((8u) 10), ((8u) 12), ((8u) 14),
                                      ((8u) (- 1)), ((8u) 2), ((8u) 4),
                                      ((8u) 6), ((8u) 8), ((8u) 10),
                                      ((8u) 12), ((8u) 14), ((8u) (- 1)),
                                      ((8u) 0), ((8u) 2), ((8u) 4), ((8u) 6),
                                      ((8u) 8), ((8u) 10), ((8u) 12),
                                      ((8u) 14)};

u8[32] gen_matrix_indexes = {((8u) 0), ((8u) 0), ((8u) 1), ((8u) 0),
                             ((8u) 2), ((8u) 0), ((8u) 0), ((8u) 1),
                             ((8u) 1), ((8u) 1), ((8u) 2), ((8u) 1),
                             ((8u) 0), ((8u) 2), ((8u) 1), ((8u) 2),
                             ((8u) 0), ((8u) 0), ((8u) 0), ((8u) 1),
                             ((8u) 0), ((8u) 2), ((8u) 1), ((8u) 0),
                             ((8u) 1), ((8u) 1), ((8u) 1), ((8u) 2),
                             ((8u) 2), ((8u) 0), ((8u) 2), ((8u) 1)};

param int BUF_size = 536;

inline
fn __gen_matrix_buf_rejection_filter48 (reg mut ptr u16[MLKEM_N] pol,
                                       reg u64 counter,
                                       reg const ptr u8[BUF_size] buf,
                                       reg u64 buf_offset,
                                       reg u256 load_shuffle, reg u256 mask,
                                       reg u256 bounds,
                                       reg const ptr u8[2048] sst,
                                       reg u256 ones, #[msf] reg u64 ms) -> 
(reg mut ptr u16[MLKEM_N], reg u64)
requires #[prover=Cas] {(((((is_arr_init(buf, 0, BUF_size) &&
                            is_arr_init(sst, 0, 2048)) &&
                           (0 <= ((uint /* of u64 */) counter))) &&
                          ((((uint /* of u64 */) counter) + 32) <= MLKEM_N)) &&
                         (0 <= ((uint /* of u64 */) buf_offset))) &&
                        (((((uint /* of u64 */) buf_offset) + 24) + 32) <=
                        BUF_size))}

ensures #[prover=Cas] {(\big[&&/true] (k \in 0:(2 * MLKEM_N))
                       (((is_arr_init(pol, k, 1) ||
                         (((((uint /* of u64 */) counter) * 2) <= k) &&
                         (k < (2 * ((uint /* of u64 */) result.1))))) ?
                        is_arr_init(result.0, k, 1) : true)))}
ensures #[prover=Cas] {((((uint /* of u64 */) counter) <=
                        ((uint /* of u64 */) result.1)) &&
                       (((uint /* of u64 */) result.1) <= MLKEM_N))}

{
  reg u256 f0;
  f0 =
    #VPERMQ(buf.[#unaligned :u256 (((uint /* of u64 */) buf_offset) + 0)],
            (4u2)[2, 1, 1, 0]); /*  */ 
  reg u256 f1;
  f1 =
    #VPERMQ(buf.[#unaligned :u256 (((uint /* of u64 */) buf_offset) + 24)],
            (4u2)[2, 1, 1, 0]); /*  */ 
  f0 = #VPSHUFB_256(f0, load_shuffle); /*  */ 
  f1 = #VPSHUFB_256(f1, load_shuffle); /*  */ 
  reg u256 g0;
  g0 = #VPSRL_16u16(f0, ((128u) 4)); /*  */ 
  reg u256 g1;
  g1 = #VPSRL_16u16(f1, ((128u) 4)); /*  */ 
  f0 = #VPBLEND_16u16(f0, g0, ((8u) 170)); /*  */ 
  f1 = #VPBLEND_16u16(f1, g1, ((8u) 170)); /*  */ 
  f0 = #VPAND_256(f0, mask); /*  */ 
  f1 = #VPAND_256(f1, mask); /*  */ 
  g0 = #VPCMPGT_16u16(bounds, f0); /*  */ 
  g1 = #VPCMPGT_16u16(bounds, f1); /*  */ 
  g0 = #VPACKSS_16u16(g0, g1); /*  */ 
  reg u64 good;
  #[declassify]
  good = (64u)#MOVEMASK_32u8(g0); /*  */ 
  good = #protect_64(good, ms); /* :k */ 
  reg u64 t0_0;
  t0_0 = good; /* u64 */ 
  t0_0 = (t0_0 &64u ((64u) 255)); /* u64 */ 
  reg u256 shuffle_0;
  shuffle_0 = (256u)#VMOV_64(sst[:u64 t0_0]); /*  */ 
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, t0_0) =
    #POPCNT_64(t0_0); /*  */ 
  t0_0 = (t0_0 +64u counter); /* u64 */ 
  reg u64 t0_1;
  t0_1 = good; /* u64 */ 
  t0_1 = (t0_1 >>64u ((8u) 16)); /* u64 */ 
  t0_1 = (t0_1 &64u ((64u) 255)); /* u64 */ 
  reg u128 shuffle_0_1;
  shuffle_0_1 = #VMOV_64(sst[:u64 t0_1]); /*  */ 
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, t0_1) =
    #POPCNT_64(t0_1); /*  */ 
  t0_1 = (t0_1 +64u t0_0); /* u64 */ 
  reg u64 t1_0;
  t1_0 = good; /* u64 */ 
  t1_0 = (t1_0 >>64u ((8u) 8)); /* u64 */ 
  t1_0 = (t1_0 &64u ((64u) 255)); /* u64 */ 
  reg u256 shuffle_1;
  shuffle_1 = (256u)#VMOV_64(sst[:u64 t1_0]); /*  */ 
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, t1_0) =
    #POPCNT_64(t1_0); /*  */ 
  t1_0 = (t1_0 +64u t0_1); /* u64 */ 
  reg u64 t1_1;
  t1_1 = good; /* u64 */ 
  t1_1 = (t1_1 >>64u ((8u) 24)); /* u64 */ 
  t1_1 = (t1_1 &64u ((64u) 255)); /* u64 */ 
  reg u128 shuffle_1_1;
  shuffle_1_1 = #VMOV_64(sst[:u64 t1_1]); /*  */ 
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, t1_1) =
    #POPCNT_64(t1_1); /*  */ 
  t1_1 = (t1_1 +64u t1_0); /* u64 */ 
  shuffle_0 = #VINSERTI128(shuffle_0, shuffle_0_1, ((8u) 1)); /*  */ 
  shuffle_1 = #VINSERTI128(shuffle_1, shuffle_1_1, ((8u) 1)); /*  */ 
  reg u256 shuffle_t;
  shuffle_t = #VPADD_32u8(shuffle_0, ones); /*  */ 
  shuffle_0 = #VPUNPCKL_32u8(shuffle_0, shuffle_t); /*  */ 
  shuffle_t = #VPADD_32u8(shuffle_1, ones); /*  */ 
  shuffle_1 = #VPUNPCKL_32u8(shuffle_1, shuffle_t); /*  */ 
  f0 = #VPSHUFB_256(f0, shuffle_0); /*  */ 
  f1 = #VPSHUFB_256(f1, shuffle_1); /*  */ 
  pol.[#unaligned :u128 (((64u) 2) *64u counter)] = f0; /* u128 */ 
  pol.[#unaligned :u128 (((64u) 2) *64u t0_0)] =
    #VEXTRACTI128(f0, ((8u) 1)); /*  */ 
  pol.[#unaligned :u128 (((64u) 2) *64u t0_1)] = f1; /* u128 */ 
  pol.[#unaligned :u128 (((64u) 2) *64u t1_0)] =
    #VEXTRACTI128(f1, ((8u) 1)); /*  */ 
  counter = t1_1; /* u64 */ 
  return (pol, counter);
}

inline
fn __write_u128_boundchk (reg mut ptr u16[MLKEM_N] pol, reg u64 ctr,
                         reg u128 data, #[msf] reg u64 ms) -> (reg mut ptr u16[MLKEM_N],
                                                              #[msf] reg u64)

ensures #[prover=Cas] {(\big[&&/true] (k \in 0:(2 * MLKEM_N))
                       ((is_arr_init(result.0, k, 1) ==
                        (is_arr_init(pol, k, 1) ||
                        (((2 * ((uint /* of u64 */) ctr)) <= k) &&
                        (k <
                        ((2 * ((uint /* of u64 */) ctr)) +
                        ((((16 < (2 * (MLKEM_N - ((uint /* of u64 */) ctr)))) ?
                          16 : (2 * (MLKEM_N - ((uint /* of u64 */) ctr)))) <
                         0) ?
                        0 :
                        ((16 < (2 * (MLKEM_N - ((uint /* of u64 */) ctr)))) ?
                        16 : (2 * (MLKEM_N - ((uint /* of u64 */) ctr))))))))))))}

{
  reg bool condition_8;
  condition_8 = (ctr <=u ((64u) (MLKEM_N - 8))); /* bool */ 
  if condition_8 {
    ms = #update_msf(condition_8, ms); /* :k */ 
    pol.[#unaligned :u128 (2 * ((uint /* of u64 */) ctr))] =
      data; /* u128 */ 
  } else {
    ms = #update_msf((! condition_8), ms); /* :k */ 
    reg u64 data_u64;
    data_u64 = #MOVV_64(data); /*  */ 
    reg bool condition_4;
    condition_4 = (ctr <=u ((64u) (MLKEM_N - 4))); /* bool */ 
    if condition_4 {
      ms = #update_msf(condition_4, ms); /* :k */ 
      pol.[#unaligned :u64 (2 * ((uint /* of u64 */) ctr))] =
        data_u64; /* u64 */ 
      data_u64 = #VPEXTR_64(data, ((8u) 1)); /*  */ 
      ctr = (ctr +64u ((64u) 4)); /* u64 */ 
    } else {
      ms = #update_msf((! condition_4), ms); /* :k */ 
    }
    reg bool condition_2;
    condition_2 = (ctr <=u ((64u) (MLKEM_N - 2))); /* bool */ 
    if condition_2 {
      ms = #update_msf(condition_2, ms); /* :k */ 
      pol.[#unaligned :u32 (2 * ((uint /* of u64 */) ctr))] =
        data_u64; /* u32 */ 
      data_u64 = (data_u64 >>64u ((8u) 32)); /* u64 */ 
      ctr = (ctr +64u ((64u) 2)); /* u64 */ 
    } else {
      ms = #update_msf((! condition_2), ms); /* :k */ 
    }
    reg bool condition_1;
    condition_1 = (ctr <=u ((64u) (MLKEM_N - 1))); /* bool */ 
    if condition_1 {
      ms = #update_msf(condition_1, ms); /* :k */ 
      pol.[#unaligned (2 * ((uint /* of u64 */) ctr))] = data_u64; /* u16 */ 
    } else {
      ms = #update_msf((! condition_1), ms); /* :k */ 
    }
  }
  return (pol, ms);
}

inline
fn __gen_matrix_buf_rejection_filter24 (reg mut ptr u16[MLKEM_N] pol,
                                       reg u64 counter,
                                       reg const ptr u8[BUF_size] buf,
                                       reg u64 buf_offset,
                                       reg u256 load_shuffle, reg u256 mask,
                                       reg u256 bounds,
                                       reg const ptr u8[2048] sst,
                                       reg u256 ones, #[msf] reg u64 ms) -> 
(reg mut ptr u16[MLKEM_N], reg u64, #[msf] reg u64)
requires #[prover=Cas] {(((((is_arr_init(buf, 0, BUF_size) &&
                            is_arr_init(sst, 0, 2048)) &&
                           (0 <= ((uint /* of u64 */) counter))) &&
                          (((uint /* of u64 */) counter) < MLKEM_N)) &&
                         (0 <= ((uint /* of u64 */) buf_offset))) &&
                        ((((uint /* of u64 */) buf_offset) + 32) < BUF_size))}

ensures #[prover=Cas] {(\big[&&/true] (k \in 0:(2 * MLKEM_N))
                       (((is_arr_init(pol, k, 1) ||
                         (((2 * ((uint /* of u64 */) counter)) <= k) &&
                         (k < (2 * ((uint /* of u64 */) result.1))))) ?
                        is_arr_init(result.0, k, 1) : true)))}
ensures #[prover=Cas] {(((64u) ((uint /* of u64 */) counter)) <=u result.1)} 
{
  reg u256 f0;
  f0 =
    #VPERMQ(buf.[#unaligned :u256 (((uint /* of u64 */) buf_offset) + 0)],
            (4u2)[2, 1, 1, 0]); /*  */ 
  f0 = #VPSHUFB_256(f0, load_shuffle); /*  */ 
  reg u256 g0;
  g0 = #VPSRL_16u16(f0, ((128u) 4)); /*  */ 
  f0 = #VPBLEND_16u16(f0, g0, ((8u) 170)); /*  */ 
  f0 = #VPAND_256(f0, mask); /*  */ 
  g0 = #VPCMPGT_16u16(bounds, f0); /*  */ 
  reg u256 g1;
  g1 = #set0_256(); /*  */ 
  g0 = #VPACKSS_16u16(g0, g1); /*  */ 
  reg u64 good;
  #[declassify]
  good = (64u)#MOVEMASK_32u8(g0); /*  */ 
  good = #protect_64(good, ms); /* :k */ 
  reg u64 t0_0;
  t0_0 = good; /* u64 */ 
  t0_0 = (t0_0 &64u ((64u) 255)); /* u64 */ 
  reg u256 shuffle_0;
  shuffle_0 = (256u)#VMOV_64(sst[:u64 t0_0]); /*  */ 
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, t0_0) =
    #POPCNT_64(t0_0); /*  */ 
  t0_0 = (t0_0 +64u counter); /* u64 */ 
  reg u64 t0_1;
  t0_1 = good; /* u64 */ 
  t0_1 = (t0_1 >>64u ((8u) 16)); /* u64 */ 
  t0_1 = (t0_1 &64u ((64u) 255)); /* u64 */ 
  reg u128 shuffle_0_1;
  shuffle_0_1 = #VMOV_64(sst[:u64 t0_1]); /*  */ 
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, t0_1) =
    #POPCNT_64(t0_1); /*  */ 
  t0_1 = (t0_1 +64u t0_0); /* u64 */ 
  shuffle_0 = #VINSERTI128(shuffle_0, shuffle_0_1, ((8u) 1)); /*  */ 
  reg u256 shuffle_t;
  shuffle_t = #VPADD_32u8(shuffle_0, ones); /*  */ 
  shuffle_0 = #VPUNPCKL_32u8(shuffle_0, shuffle_t); /*  */ 
  f0 = #VPSHUFB_256(f0, shuffle_0); /*  */ 
  reg u128 t128;
  t128 = f0; /* u128 */ 
  #[inline]
  (pol, ms) = __write_u128_boundchk(pol, counter, t128, ms);
  t128 = #VEXTRACTI128(f0, ((8u) 1)); /*  */ 
  #[inline]
  (pol, ms) = __write_u128_boundchk(pol, t0_0, t128, ms);
  counter = t0_1; /* u64 */ 
  return (pol, counter, ms);
}

fn _gen_matrix_buf_rejection (reg mut ptr u16[MLKEM_N] pol, reg u64 counter,
                             reg const ptr u8[BUF_size] buf,
                             reg u64 buf_offset) -> (reg mut ptr u16[MLKEM_N],
                                                    reg u64)
requires #[prover=Cas] {((((is_arr_init(buf, 0, BUF_size) &&
                           (0 <= ((uint /* of u64 */) counter))) &&
                          (((uint /* of u64 */) counter) <= MLKEM_N)) &&
                         (0 <= ((uint /* of u64 */) buf_offset))) &&
                        (((uint /* of u64 */) buf_offset) < BUF_size))}

ensures #[prover=Cas] {(\big[&&/true] (k \in 0:(2 * MLKEM_N))
                       (((is_arr_init(pol, k, 1) ||
                         (((2 * ((uint /* of u64 */) counter)) <= k) &&
                         (k < (2 * ((uint /* of u64 */) result.1))))) ?
                        is_arr_init(result.0, k, 1) : true)))}
ensures #[prover=Cas] {(((64u) ((uint /* of u64 */) counter)) <=u result.1)} 
{
  #[msf]
  reg u64 ms;
  ms = #init_msf(); /* :k */ 
  reg u256 load_shuffle;
  load_shuffle = /* global: */ sample_load_shuffle[:u256 0]; /* u256 */ 
  reg u256 mask;
  mask = /* global: */ sample_mask; /* u256 */ 
  reg u256 bounds;
  bounds = /* global: */ sample_q; /* u256 */ 
  reg u256 ones;
  ones = /* global: */ sample_ones; /* u256 */ 
  reg mut ptr u8[2048] sst;
  sst = /* global: */ sample_shuffle_table; /* u8[2048] */ 
  stack u64 saved_buf_offset;
  saved_buf_offset = buf_offset; /* u64 */ 
  buf_offset = buf_offset; /* u64 */ 
  reg bool condition_loop;
  while {
    condition_loop =
      (buf_offset <u ((64u) (((3 * 168) - 48) + 1))); /* bool */ 
  } (condition_loop) {
    ms = #update_msf(condition_loop, ms); /* :k */ 
    condition_loop = (counter <u ((64u) ((MLKEM_N - 32) + 1))); /* bool */ 
    if condition_loop {
      ms = #update_msf(condition_loop, ms); /* :k */ 
      #[inline]
      (pol, counter) =
        __gen_matrix_buf_rejection_filter48(pol, counter, buf, buf_offset,
                                            load_shuffle, mask, bounds, sst,
                                            ones, ms);
      saved_buf_offset = (saved_buf_offset +64u ((64u) 48)); /* u64 */ 
      buf_offset = saved_buf_offset; /* u64 */ 
      buf_offset = #protect_64(buf_offset, ms); /* :k */ 
    } else {
      ms = #update_msf((! condition_loop), ms); /* :k */ 
      buf_offset = ((64u) (3 * 168)); /* u64 */ 
    }
  }
  ms = #update_msf((! condition_loop), ms); /* :k */ 
  buf_offset = saved_buf_offset; /* u64 */ 
  buf_offset = #protect_64(buf_offset, ms); /* :k */ 
  while {
    condition_loop =
      (buf_offset <u ((64u) (((3 * 168) - 24) + 1))); /* bool */ 
  } (condition_loop) {
    ms = #update_msf(condition_loop, ms); /* :k */ 
    condition_loop = (counter <u ((64u) MLKEM_N)); /* bool */ 
    if condition_loop {
      ms = #update_msf(condition_loop, ms); /* :k */ 
      () = #spill(buf_offset); /* :k */ 
      #[inline]
      (pol, counter, ms) =
        __gen_matrix_buf_rejection_filter24(pol, counter, buf, buf_offset,
                                            load_shuffle, mask, bounds, sst,
                                            ones, ms);
      () = #unspill(buf_offset); /* :k */ 
      buf_offset = #protect_64(buf_offset, ms); /* :k */ 
      buf_offset = (buf_offset +64u ((64u) 24)); /* u64 */ 
    } else {
      ms = #update_msf((! condition_loop), ms); /* :k */ 
      buf_offset = ((64u) (3 * 168)); /* u64 */ 
    }
  }
  return (pol, counter);
}

param int IDX_TABLE_SIZE = (32 * (MLKEM_K - 2));

inline
fn gen_matrix_get_indexes (reg u64 b, reg u64 _t) -> (reg u64)
requires #[prover=Cas] {((((64u) 0) <=u
                         (b +64u (_t <<64u ((8u) (MLKEM_K + 1))))) &&
                        ((b +64u (_t <<64u ((8u) (MLKEM_K + 1)))) <=u
                        ((64u) ((32 * (MLKEM_K - 2)) - 8))))}


{
  reg mut ptr u8[IDX_TABLE_SIZE] idxs;
  idxs = /* global: */ gen_matrix_indexes; /* u8[IDX_TABLE_SIZE] */ 
  reg u64 t;
  t = _t; /* u64 */ 
  t = (t <<64u ((8u) (MLKEM_K + 1))); /* u64 */ 
  b = (b +64u t); /* u64 */ 
  t = idxs.[#unaligned :u64 b]; /* u64 */ 
  return (t);
}

fn __gen_matrix_fill_polynomial (reg mut ptr u16[MLKEM_N] pol,
                                reg mut ptr u8[BUF_size] buf) -> (reg mut ptr u16[MLKEM_N],
                                                                 reg mut ptr u8[BUF_size])
requires #[prover=Cas] {is_arr_init(buf, 0, BUF_size)} 
ensures #[prover=Cas] {(is_arr_init(result.0, 0, (MLKEM_N * 2)) &&
                       is_arr_init(result.1, 0, BUF_size))}

{
  reg u64 buf_offset;
  buf_offset = ((64u) 0); /* u64 */ 
  reg u64 counter;
  counter = ((64u) 0); /* u64 */ 
  (pol, counter) = _gen_matrix_buf_rejection(pol, counter, buf, buf_offset);
  buf_offset = ((64u) (2 * 168)); /* u64 */ 
  while ((counter <u ((64u) MLKEM_N))) {
    buf = _shake128_next_state(buf);
    (pol, counter) =
      _gen_matrix_buf_rejection(pol, counter, buf, buf_offset);
  }
  return (pol, buf);
}

fn _gen_matrix_sample_four_polynomials (reg mut ptr u16[(4 * MLKEM_N)] polx4,
                                       reg mut ptr u8[(BUF_size * 4)] buf,
                                       reg const ptr u8[32] rho,
                                       reg u64 pos_entry, reg u64 transposed) -> 
(reg mut ptr u16[(4 * MLKEM_N)], reg mut ptr u8[(BUF_size * 4)])
requires #[prover=Cas] {((is_arr_init(rho, 0, 32) &&
                         (((64u) 0) <=u
                         (pos_entry +64u
                         (transposed <<64u ((8u) (MLKEM_K + 1)))))) &&
                        ((pos_entry +64u
                         (transposed <<64u ((8u) (MLKEM_K + 1)))) <=u
                        ((64u) ((32 * (MLKEM_K - 2)) - 8))))}

ensures #[prover=Cas] {is_arr_init(result.0, 0, 2048)} 
{
  stack u8[8] indexes;
  #[inline]
  indexes.[#unaligned :u64 0] =
    gen_matrix_get_indexes(pos_entry, transposed);
  stack u256[25] state;
  reg mut ptr u256[25] stx4;
  stx4 = state; /* u256[25] */ 
  stx4 = _shake128x4_absorb_A32_A2(stx4, rho, indexes);
  (_ /* u256[?] */, buf) = _shake128x4_squeeze3blocks(stx4, buf);
  reg mut ptr u16[MLKEM_N] pol;
  pol = polx4[(0 * MLKEM_N) : MLKEM_N]; /* u16[MLKEM_N] */ 
  (pol, buf[(BUF_size * 0) : BUF_size]) =
    __gen_matrix_fill_polynomial(pol, buf[(BUF_size * 0) : BUF_size]);
  polx4[(0 * MLKEM_N) : MLKEM_N] = pol; /* u16[MLKEM_N] */ 
  pol = polx4[(1 * MLKEM_N) : MLKEM_N]; /* u16[MLKEM_N] */ 
  (pol, buf[(BUF_size * 1) : BUF_size]) =
    __gen_matrix_fill_polynomial(pol, buf[(BUF_size * 1) : BUF_size]);
  polx4[(1 * MLKEM_N) : MLKEM_N] = pol; /* u16[MLKEM_N] */ 
  pol = polx4[(2 * MLKEM_N) : MLKEM_N]; /* u16[MLKEM_N] */ 
  (pol, buf[(BUF_size * 2) : BUF_size]) =
    __gen_matrix_fill_polynomial(pol, buf[(BUF_size * 2) : BUF_size]);
  polx4[(2 * MLKEM_N) : MLKEM_N] = pol; /* u16[MLKEM_N] */ 
  pol = polx4[(3 * MLKEM_N) : MLKEM_N]; /* u16[MLKEM_N] */ 
  (pol, buf[(BUF_size * 3) : BUF_size]) =
    __gen_matrix_fill_polynomial(pol, buf[(BUF_size * 3) : BUF_size]);
  polx4[(3 * MLKEM_N) : MLKEM_N] = pol; /* u16[MLKEM_N] */ 
  return (polx4, buf);
}

inline
fn __gen_matrix_sample_one_polynomial (reg mut ptr u16[MLKEM_N] pol,
                                      reg mut ptr u8[BUF_size] buf,
                                      reg const ptr u8[32] rho, reg u16 rc) -> 
(reg mut ptr u16[MLKEM_N], reg mut ptr u8[BUF_size])
requires #[prover=Cas] {is_arr_init(rho, 0, 32)} 
ensures #[prover=Cas] {(is_arr_init(result.0, 0, (MLKEM_N * 2)) &&
                       is_arr_init(result.1, 0, BUF_size))}

{
  stack u8[2] pos;
  pos[:u16 0] = rc; /* u16 */ 
  reg u256[7] stavx2;
  stavx2 = _shake128_absorb_A32_A2(rho, pos);
  buf = _shake128_squeeze3blocks(buf, stavx2);
  (pol, buf) = __gen_matrix_fill_polynomial(pol, buf);
  return (pol, buf);
}

fn _gen_matrix_avx2 (reg mut ptr u16[((MLKEM_K * MLKEM_K) * MLKEM_N)] matrix,
                    reg const ptr u8[32] rho, #[spill_to_mmx]
                    reg u64 transposed) -> (reg mut ptr u16[((MLKEM_K *
                                                             MLKEM_K) *
                                                            MLKEM_N)])
requires #[prover=Cas] {((is_arr_init(rho, 0, 32) &&
                         (((64u) 0) <=u transposed)) &&
                        (transposed <=u ((64u) 1)))}

ensures #[prover=Cas] {is_arr_init(result.0, 0,
                         (((MLKEM_K * MLKEM_K) * MLKEM_N) * 2))}

{
  () = #spill(transposed); /* :k */ 
  stack u8[(BUF_size * 4)] buf_s;
  reg mut ptr u8[(BUF_size * 4)] buf;
  buf = buf_s; /* u8[(BUF_size * 4)] */ 
  inline int i;
  for i = 0 to 2 {
    reg u64 pos_entry;
    pos_entry = ((64u) (8 * i)); /* u64 */ 
    reg mut ptr u16[(4 * MLKEM_N)] polx4;
    polx4 =
      matrix[((4 * i) * MLKEM_N) : (4 * MLKEM_N)]; /* u16[(4 * MLKEM_N)] */ 
    () = #unspill(transposed); /* :k */ 
    (polx4, buf) =
      _gen_matrix_sample_four_polynomials(polx4, buf, rho, pos_entry,
                                          transposed);
    matrix[((i * 4) * MLKEM_N) : (4 * MLKEM_N)] =
      polx4; /* u16[(4 * MLKEM_N)] */ 
  }
  reg mut ptr u16[MLKEM_N] pol;
  pol = matrix[(8 * MLKEM_N) : MLKEM_N]; /* u16[MLKEM_N] */ 
  reg u16 rc;
  rc = ((16u) 514); /* u16 */ 
  #[inline]
  (pol, buf[(BUF_size * 0) : BUF_size]) =
    __gen_matrix_sample_one_polynomial(pol, buf[(BUF_size * 0) : BUF_size],
                                       rho, rc);
  matrix[(8 * MLKEM_N) : MLKEM_N] = pol; /* u16[MLKEM_N] */ 
  for i = 0 to MLKEM_K {
    inline int j;
    for j = 0 to MLKEM_K {
      matrix[((i * MLKEM_VECN) + (j * MLKEM_N)) : MLKEM_N] =
        _nttunpack(matrix[((i * MLKEM_VECN) + (j * MLKEM_N)) : MLKEM_N]);
    }
  }
  return (matrix);
}

inline
fn __indcpa_keypair (#[spill_to_mmx] reg mut ptr u8[MLKEM_PUBLICKEYBYTES] pk,
                    #[spill_to_mmx] reg mut ptr u8[MLKEM_POLYVECBYTES] sk,
                    reg const ptr u8[MLKEM_SYMBYTES] randomnessp) -> 
(reg mut ptr u8[MLKEM_PUBLICKEYBYTES], reg mut ptr u8[MLKEM_POLYVECBYTES])
requires #[prover=Cas] {is_arr_init(randomnessp, 0, MLKEM_SYMBYTES)} 
ensures #[prover=Cas] {(is_arr_init(result.0, 0, MLKEM_PUBLICKEYBYTES) &&
                       is_arr_init(result.1, 0, MLKEM_POLYVECBYTES))}

{
  () = #spill(pk, sk); /* :k */ 
  stack u8[33] inbuf;
  reg u64 t64;
  inline int i;
  for i = 0 to (MLKEM_SYMBYTES / 8) {
    t64 = randomnessp[:u64 i]; /* u64 */ 
    inbuf[:u64 i] = t64; /* u64 */ 
  }
  inbuf[32] = ((8u) MLKEM_K); /* u8 */ 
  stack u8[64] buf;
  buf = _sha3_512A_A33(buf, inbuf);
  stack u8[MLKEM_SYMBYTES] publicseed;
  stack u8[MLKEM_SYMBYTES] noiseseed;
  for i = 0 to (MLKEM_SYMBYTES / 8) {
    #[declassify]
    t64 = buf[:u64 i]; /* u64 */ 
    publicseed[:u64 i] = t64; /* u64 */ 
    t64 = buf[:u64 (i + (MLKEM_SYMBYTES / 8))]; /* u64 */ 
    noiseseed[:u64 i] = t64; /* u64 */ 
  }
  reg u64 transposed;
  transposed = ((64u) 0); /* u64 */ 
  stack u16[(MLKEM_K * MLKEM_VECN)] aa;
  aa = _gen_matrix_avx2(aa, publicseed, transposed);
  reg u8 nonce;
  nonce = ((8u) 0); /* u8 */ 
  stack u16[MLKEM_VECN] e;
  stack u16[MLKEM_VECN] skpv;
  (skpv[0 : MLKEM_N], skpv[MLKEM_N : MLKEM_N], skpv[(2 * MLKEM_N) : MLKEM_N],
   e[0 : MLKEM_N]) =
    _poly_getnoise_eta1_4x(skpv[0 : MLKEM_N], skpv[MLKEM_N : MLKEM_N],
                           skpv[(2 * MLKEM_N) : MLKEM_N], e[0 : MLKEM_N],
                           noiseseed, nonce);
  nonce = ((8u) 4); /* u8 */ 
  stack u16[MLKEM_VECN] pkpv;
  (e[MLKEM_N : MLKEM_N], e[(2 * MLKEM_N) : MLKEM_N], pkpv[0 : MLKEM_N],
   pkpv[MLKEM_N : MLKEM_N]) =
    _poly_getnoise_eta1_4x(e[MLKEM_N : MLKEM_N], e[(2 * MLKEM_N) : MLKEM_N],
                           pkpv[0 : MLKEM_N], pkpv[MLKEM_N : MLKEM_N],
                           noiseseed, nonce);
  #[inline]
  skpv = __polyvec_ntt(skpv);
  #[inline]
  e = __polyvec_ntt(e);
  for i = 0 to MLKEM_K {
    #[inline]
    pkpv[(i * MLKEM_N) : MLKEM_N] =
      __polyvec_pointwise_acc(pkpv[(i * MLKEM_N) : MLKEM_N],
                              aa[(i * MLKEM_VECN) : MLKEM_VECN], skpv);
    pkpv[(i * MLKEM_N) : MLKEM_N] =
      _poly_frommont(pkpv[(i * MLKEM_N) : MLKEM_N]);
  }
  #[inline]
  pkpv = __polyvec_add2(pkpv, e);
  #[inline]
  pkpv = __polyvec_reduce(pkpv);
  () = #unspill(pk, sk); /* :k */ 
  #[inline]
  sk = __i_polyvec_tobytes(sk, skpv);
  #[inline]
  pk[0 : MLKEM_POLYVECBYTES] =
    __i_polyvec_tobytes(pk[0 : MLKEM_POLYVECBYTES], pkpv);
  for i = 0 to (MLKEM_SYMBYTES / 8) {
    t64 = publicseed[:u64 i]; /* u64 */ 
    pk.[#unaligned :u64 ((i + (MLKEM_POLYVECBYTES / 8)) * 8)] =
      t64; /* u64 */ 
  }
  return (pk, sk);
}

inline
fn __indcpa_enc (#[spill_to_mmx] reg mut ptr u8[MLKEM_CIPHERTEXTBYTES] ct,
                reg const ptr u8[MLKEM_INDCPA_MSGBYTES] msgp,
                reg const ptr u8[MLKEM_PUBLICKEYBYTES] pk,
                reg const ptr u8[MLKEM_SYMBYTES] noiseseed) -> (reg mut ptr u8[MLKEM_CIPHERTEXTBYTES])
requires #[prover=Cas] {((is_arr_init(msgp, 0, MLKEM_INDCPA_MSGBYTES) &&
                         is_arr_init(pk, 0, MLKEM_PUBLICKEYBYTES)) &&
                        is_arr_init(noiseseed, 0, MLKEM_SYMBYTES))}

ensures #[prover=Cas] {is_arr_init(result.0, 0, MLKEM_CIPHERTEXTBYTES)} 
{
  () = #spill(ct); /* :k */ 
  stack u16[MLKEM_VECN] pkpv;
  #[inline]
  pkpv = __i_polyvec_frombytes(pk[0 : MLKEM_POLYVECBYTES]);
  stack u8[MLKEM_SYMBYTES] publicseed;
  inline int w;
  for w = 0 to (MLKEM_SYMBYTES / 8) {
    reg u64 t64;
    #[declassify]
    t64 =
      pk.[#unaligned :u64 (((MLKEM_POLYVECBYTES / 8) + w) * 8)]; /* u64 */ 
    publicseed[:u64 w] = t64; /* u64 */ 
  }
  stack u16[MLKEM_N] k;
  k = _i_poly_frommsg(k, msgp);
  reg u64 transposed;
  transposed = ((64u) 1); /* u64 */ 
  stack u16[(MLKEM_K * MLKEM_VECN)] aat;
  aat = _gen_matrix_avx2(aat, publicseed, transposed);
  reg u8 nonce;
  nonce = ((8u) 0); /* u8 */ 
  stack u16[MLKEM_VECN] sp;
  stack u16[MLKEM_VECN] ep;
  (sp[0 : MLKEM_N], sp[MLKEM_N : MLKEM_N], sp[(2 * MLKEM_N) : MLKEM_N],
   ep[0 : MLKEM_N]) =
    _poly_getnoise_eta1_4x(sp[0 : MLKEM_N], sp[MLKEM_N : MLKEM_N],
                           sp[(2 * MLKEM_N) : MLKEM_N], ep[0 : MLKEM_N],
                           noiseseed, nonce);
  nonce = ((8u) 4); /* u8 */ 
  stack u16[MLKEM_VECN] bp;
  stack u16[MLKEM_N] epp;
  (ep[MLKEM_N : MLKEM_N], ep[(2 * MLKEM_N) : MLKEM_N], epp, bp[0 : MLKEM_N]) =
    _poly_getnoise_eta1_4x(ep[MLKEM_N : MLKEM_N],
                           ep[(2 * MLKEM_N) : MLKEM_N], epp, bp[0 : MLKEM_N],
                           noiseseed, nonce);
  #[inline]
  sp = __polyvec_ntt(sp);
  for w = 0 to MLKEM_K {
    #[inline]
    bp[(w * MLKEM_N) : MLKEM_N] =
      __polyvec_pointwise_acc(bp[(w * MLKEM_N) : MLKEM_N],
                              aat[(w * MLKEM_VECN) : MLKEM_VECN], sp);
  }
  stack u16[MLKEM_N] v;
  #[inline]
  v = __polyvec_pointwise_acc(v, pkpv, sp);
  #[inline]
  bp = __polyvec_invntt(bp);
  v = _poly_invntt(v);
  #[inline]
  bp = __polyvec_add2(bp, ep);
  v = _poly_add2(v, epp);
  v = _poly_add2(v, k);
  #[inline]
  bp = __polyvec_reduce(bp);
  #[inline]
  v = __poly_reduce(v);
  () = #unspill(ct); /* :k */ 
  #[inline]
  ct[0 : MLKEM_POLYVECCOMPRESSEDBYTES] =
    __i_polyvec_compress(ct[0 : MLKEM_POLYVECCOMPRESSEDBYTES], bp);
  (ct[MLKEM_POLYVECCOMPRESSEDBYTES : MLKEM_POLYCOMPRESSEDBYTES], v) =
    _i_poly_compress(ct[MLKEM_POLYVECCOMPRESSEDBYTES : MLKEM_POLYCOMPRESSEDBYTES],
                     v);
  return (ct);
}

inline
fn __indcpa_dec (reg mut ptr u8[MLKEM_INDCPA_MSGBYTES] msgp,
                reg const ptr u8[MLKEM_CIPHERTEXTBYTES] ct,
                reg const ptr u8[MLKEM_POLYVECBYTES] sk) -> (reg mut ptr u8[MLKEM_INDCPA_MSGBYTES])
requires #[prover=Cas] {(is_arr_init(ct, 0, MLKEM_CIPHERTEXTBYTES) &&
                        is_arr_init(sk, 0, MLKEM_POLYVECBYTES))}

ensures #[prover=Cas] {is_arr_init(result.0, 0, MLKEM_INDCPA_MSGBYTES)} 
{
  stack u16[MLKEM_VECN] bp;
  #[inline]
  bp = __i_polyvec_decompress(ct);
  stack u16[MLKEM_N] v;
  v =
    _i_poly_decompress(v,
                       ct[MLKEM_POLYVECCOMPRESSEDBYTES : MLKEM_POLYCOMPRESSEDBYTES]);
  stack u16[MLKEM_VECN] skpv;
  #[inline]
  skpv = __i_polyvec_frombytes(sk);
  #[inline]
  bp = __polyvec_ntt(bp);
  stack u16[MLKEM_N] t;
  #[inline]
  t = __polyvec_pointwise_acc(t, skpv, bp);
  t = _poly_invntt(t);
  stack u16[MLKEM_N] mp;
  mp = _poly_sub(mp, v, t);
  #[inline]
  mp = __poly_reduce(mp);
  (msgp, mp) = _i_poly_tomsg(msgp, mp);
  return (msgp);
}

inline
fn __verify (reg const ptr u8[MLKEM_INDCPA_CIPHERTEXTBYTES] ct,
            reg const ptr u8[MLKEM_INDCPA_CIPHERTEXTBYTES] ctpc) -> (reg u64)

{
  reg u64 cnd;
  cnd = ((64u) 0); /* u64 */ 
  reg u64 t64;
  t64 = ((64u) 1); /* u64 */ 
  reg u256 h;
  h = #set0_256(); /*  */ 
  inline int i;
  for i = 0 to (MLKEM_INDCPA_CIPHERTEXTBYTES / 32) {
    reg u256 f;
    f = ctpc.[#unaligned :u256 (32 * i)]; /* u256 */ 
    reg u256 g;
    g = ct.[#unaligned :u256 (32 * i)]; /* u256 */ 
    f = #VPXOR_256(f, g); /*  */ 
    h = #VPOR_256(h, f); /*  */ 
  }
  reg bool zf;
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, zf) =
    #VPTEST_256(h, h); /*  */ 
  cnd = ((! zf) ? t64 : cnd); /* u64 */ 
  return (cnd);
}

inline
fn __cmov (reg mut ptr u8[MLKEM_SYMBYTES] dst,
          reg const ptr u8[MLKEM_SYMBYTES] src, reg u64 cnd) -> (reg mut ptr u8[MLKEM_SYMBYTES])

{
  cnd = (-64u cnd); /* u64 */ 
  stack u64 scnd;
  scnd = cnd; /* u64 */ 
  reg u256 m;
  m = #VPBROADCAST_4u64(scnd); /*  */ 
  reg u256 f;
  f = src.[#unaligned :u256 0]; /* u256 */ 
  reg u256 g;
  g = dst.[#unaligned :u256 0]; /* u256 */ 
  f = #BLENDV_32u8(f, g, m); /*  */ 
  dst.[#unaligned :u256 0] = f; /* u256 */ 
  return (dst);
}

inline
fn __crypto_kem_keypair_jazz (reg mut ptr u8[MLKEM_PUBLICKEYBYTES] pk,
                             reg mut ptr u8[MLKEM_SECRETKEYBYTES] sk,
                             reg const ptr u8[(MLKEM_SYMBYTES * 2)] randomnessp) -> 
(reg mut ptr u8[MLKEM_PUBLICKEYBYTES], reg mut ptr u8[MLKEM_SECRETKEYBYTES])
requires #[prover=Cas] {is_arr_init(randomnessp, 0, (MLKEM_SYMBYTES * 2))} 
ensures #[prover=Cas] {(is_arr_init(result.0, 0, MLKEM_PUBLICKEYBYTES) &&
                       is_arr_init(result.1, 0, MLKEM_SECRETKEYBYTES))}

{
  #[mmx]
  reg mut ptr u8[(MLKEM_SYMBYTES * 2)] s_randomnessp;
  s_randomnessp = randomnessp; /* u8[(MLKEM_SYMBYTES * 2)] */ 
  reg mut ptr u8[MLKEM_SYMBYTES] randomnessp1;
  randomnessp1 = randomnessp[0 : MLKEM_SYMBYTES]; /* u8[MLKEM_SYMBYTES] */ 
  reg mut ptr u8[MLKEM_POLYVECBYTES] skcpa;
  skcpa = sk[0 : MLKEM_POLYVECBYTES]; /* u8[MLKEM_POLYVECBYTES] */ 
  #[mmx]
  reg mut ptr u8[MLKEM_SECRETKEYBYTES] sk_s;
  sk_s = sk; /* u8[MLKEM_SECRETKEYBYTES] */ 
  #[inline]
  (pk, skcpa) = __indcpa_keypair(pk, skcpa, randomnessp1);
  sk = sk_s; /* u8[MLKEM_SECRETKEYBYTES] */ 
  sk[0 : MLKEM_POLYVECBYTES] = skcpa; /* u8[MLKEM_POLYVECBYTES] */ 
  reg u64 t64;
  inline int i;
  for i = 0 to (MLKEM_INDCPA_PUBLICKEYBYTES / 8) {
    t64 = pk[:u64 i]; /* u64 */ 
    sk.[#unaligned :u64 (((MLKEM_POLYVECBYTES / 8) + i) * 8)] =
      t64; /* u64 */ 
  }
  sk[(MLKEM_POLYVECBYTES + MLKEM_PUBLICKEYBYTES) : 32] =
    _sha3_256A_A1184(sk[(MLKEM_POLYVECBYTES + MLKEM_PUBLICKEYBYTES) : 32], pk);
  randomnessp = s_randomnessp; /* u8[(MLKEM_SYMBYTES * 2)] */ 
  for i = 0 to (MLKEM_SYMBYTES / 8) {
    t64 = randomnessp[:u64 ((MLKEM_SYMBYTES / 8) + i)]; /* u64 */ 
    sk.[#unaligned :u64 (((((MLKEM_POLYVECBYTES + MLKEM_PUBLICKEYBYTES) +
                           MLKEM_SYMBYTES) /
                          8) +
                         i) *
                        8)] = t64; /* u64 */ 
  }
  return (pk, sk);
}

inline
fn __crypto_kem_enc_jazz (reg mut ptr u8[MLKEM_CIPHERTEXTBYTES] ct,
                         #[spill_to_mmx] reg mut ptr u8[MLKEM_SYMBYTES] shk,
                         #[spill_to_mmx]
                         reg const ptr u8[MLKEM_PUBLICKEYBYTES] pk,
                         reg const ptr u8[MLKEM_SYMBYTES] randomnessp) -> 
(reg mut ptr u8[MLKEM_CIPHERTEXTBYTES], reg mut ptr u8[MLKEM_SYMBYTES])
requires #[prover=Cas] {(is_arr_init(randomnessp, 0, MLKEM_SYMBYTES) &&
                        is_arr_init(pk, 0, MLKEM_PUBLICKEYBYTES))}

ensures #[prover=Cas] {(is_arr_init(result.0, 0, MLKEM_CIPHERTEXTBYTES) &&
                       is_arr_init(result.1, 0, MLKEM_SYMBYTES))}

{
  () = #spill(pk, shk); /* :k */ 
  stack u8[(MLKEM_SYMBYTES * 2)] buf;
  buf[0 : MLKEM_SYMBYTES] = #copy_64(randomnessp); /*  */ 
  buf[MLKEM_SYMBYTES : MLKEM_SYMBYTES] =
    _sha3_256A_A1184(buf[MLKEM_SYMBYTES : MLKEM_SYMBYTES], pk);
  stack u8[(MLKEM_SYMBYTES * 2)] kr;
  kr = _sha3_512A_A64(kr, buf);
  () = #unspill(pk); /* :k */ 
  #[inline]
  ct =
    __indcpa_enc(ct, buf[0 : MLKEM_INDCPA_MSGBYTES], pk,
                 kr[MLKEM_SYMBYTES : MLKEM_SYMBYTES]);
  () = #unspill(shk); /* :k */ 
  shk = #copy_64(kr[0 : MLKEM_SYMBYTES]); /*  */ 
  return (ct, shk);
}

inline
fn __crypto_kem_dec_jazz (#[spill_to_mmx] reg mut ptr u8[MLKEM_SYMBYTES] shk,
                         #[spill_to_mmx]
                         reg const ptr u8[MLKEM_CIPHERTEXTBYTES] ct,
                         #[spill_to_mmx]
                         reg const ptr u8[MLKEM_SECRETKEYBYTES] sk) -> 
(reg mut ptr u8[MLKEM_SYMBYTES])
requires #[prover=Cas] {(is_arr_init(ct, 0, MLKEM_CIPHERTEXTBYTES) &&
                        is_arr_init(sk, 0, MLKEM_SECRETKEYBYTES))}

ensures #[prover=Cas] {is_arr_init(result.0, 0, MLKEM_SYMBYTES)} 
{
  () = #spill(shk, ct); /* :k */ 
  stack u8[(MLKEM_SYMBYTES + MLKEM_CIPHERTEXTBYTES)] zp_ct;
  zp_ct[0 : MLKEM_SYMBYTES] =
    #copy_64(sk[(MLKEM_SECRETKEYBYTES - MLKEM_SYMBYTES) : MLKEM_SYMBYTES]); /*  */ 
  stack u8[(2 * MLKEM_SYMBYTES)] buf;
  #[inline]
  buf[0 : MLKEM_INDCPA_MSGBYTES] =
    __indcpa_dec(buf[0 : MLKEM_INDCPA_MSGBYTES], ct,
                 sk[0 : MLKEM_POLYVECBYTES]);
  buf[MLKEM_SYMBYTES : MLKEM_SYMBYTES] =
    #copy_64(sk[(MLKEM_INDCPA_SECRETKEYBYTES + MLKEM_INDCPA_PUBLICKEYBYTES) : MLKEM_SYMBYTES]); /*  */ 
  stack u8[(2 * MLKEM_SYMBYTES)] kr;
  kr = _sha3_512A_A64(kr, buf);
  stack u8[MLKEM_INDCPA_CIPHERTEXTBYTES] ctc;
  #[inline]
  ctc =
    __indcpa_enc(ctc, buf[0 : MLKEM_SYMBYTES],
                 sk[MLKEM_POLYVECBYTES : MLKEM_PUBLICKEYBYTES],
                 kr[MLKEM_SYMBYTES : MLKEM_SYMBYTES]);
  () = #unspill(ct); /* :k */ 
  reg u64 cnd;
  #[inline]
  cnd = __verify(ct, ctc);
  zp_ct[MLKEM_SYMBYTES : MLKEM_CIPHERTEXTBYTES] = #copy_64(ct); /*  */ 
  () = #unspill(shk); /* :k */ 
  shk = _shake256_A32__A1120(shk, zp_ct);
  #[inline]
  shk = __cmov(shk, kr[0 : MLKEM_SYMBYTES], cnd);
  return (shk);
}

#[sct="
{ ptr: transient, val: secret } 
{ ptr: transient, val: secret } 
{ ptr: transient, val: secret } 
{ ptr: public, val: secret } 
{ ptr: public, val: secret } 
public
"]
export
fn jade_kem_mlkem_mlkem768_amd64_avx2_keypair_derand (#[secret]
                                                     reg mut ptr u8[MLKEM_PUBLICKEYBYTES] public_key,
                                                     #[secret]
                                                     reg mut ptr u8[MLKEM_SECRETKEYBYTES] secret_key,
                                                     #[secret]
                                                     reg const ptr u8[
                                                     (2 * MLKEM_SYMBYTES)] coins) -> 
(#[secret] reg mut ptr u8[MLKEM_PUBLICKEYBYTES], #[secret]
reg mut ptr u8[MLKEM_SECRETKEYBYTES], #[public] reg u64)
requires #[prover=Cas] {is_arr_init(coins, 0, (2 * MLKEM_SYMBYTES))} 
ensures #[prover=Cas] {(is_arr_init(result.0, 0, MLKEM_PUBLICKEYBYTES) &&
                       is_arr_init(result.1, 0, MLKEM_SECRETKEYBYTES))}

{
  _ /* u64 */ = #init_msf(); /* :k */ 
  #[inline]
  (public_key, secret_key) =
    __crypto_kem_keypair_jazz(public_key, secret_key, coins);
  reg u64 r;
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, r) =
    #set0_64(); /*  */ 
  return (public_key, secret_key, r);
}

#[sct="
{ ptr: transient, val: secret } 
{ ptr: transient, val: secret } 
{ ptr: transient, val: secret } 
{ ptr: transient, val: secret } 
{ ptr: public, val: secret } 
{ ptr: public, val: secret } 
public
"]
export
fn jade_kem_mlkem_mlkem768_amd64_avx2_enc_derand (#[secret]
                                                 reg mut ptr u8[MLKEM_CIPHERTEXTBYTES] ciphertext,
                                                 #[secret]
                                                 reg mut ptr u8[MLKEM_SYMBYTES] shared_secret,
                                                 #[secret]
                                                 reg const ptr u8[MLKEM_PUBLICKEYBYTES] public_key,
                                                 #[secret]
                                                 reg const ptr u8[MLKEM_SYMBYTES] coins) -> 
(#[secret] reg mut ptr u8[MLKEM_CIPHERTEXTBYTES], #[secret]
reg mut ptr u8[MLKEM_SYMBYTES], #[public] reg u64)
requires #[prover=Cas] {(is_arr_init(coins, 0, MLKEM_SYMBYTES) &&
                        is_arr_init(public_key, 0, MLKEM_PUBLICKEYBYTES))}

ensures #[prover=Cas] {(is_arr_init(result.0, 0, MLKEM_CIPHERTEXTBYTES) &&
                       is_arr_init(result.1, 0, MLKEM_SYMBYTES))}

{
  _ /* u64 */ = #init_msf(); /* :k */ 
  public_key = public_key; /* u8[MLKEM_PUBLICKEYBYTES] */ 
  #[inline]
  (ciphertext, shared_secret) =
    __crypto_kem_enc_jazz(ciphertext, shared_secret, public_key, coins);
  reg u64 r;
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, r) =
    #set0_64(); /*  */ 
  return (ciphertext, shared_secret, r);
}

#[sct="
{ ptr: transient, val: secret } 
{ ptr: transient, val: secret } 
{ ptr: public, val: secret } 
{ ptr: public, val: secret } 
public
"]
export
fn jade_kem_mlkem_mlkem768_amd64_avx2_keypair (#[secret]
                                              reg mut ptr u8[MLKEM_PUBLICKEYBYTES] public_key,
                                              #[secret]
                                              reg mut ptr u8[MLKEM_SECRETKEYBYTES] secret_key) -> 
(#[secret] reg mut ptr u8[MLKEM_PUBLICKEYBYTES], #[secret]
reg mut ptr u8[MLKEM_SECRETKEYBYTES], #[public] reg u64)

ensures #[prover=Cas] {(is_arr_init(result.0, 0, MLKEM_PUBLICKEYBYTES) &&
                       is_arr_init(result.1, 0, MLKEM_SECRETKEYBYTES))}

{
  _ /* u64 */ = #init_msf(); /* :k */ 
  public_key = public_key; /* u8[MLKEM_PUBLICKEYBYTES] */ 
  secret_key = secret_key; /* u8[MLKEM_SECRETKEYBYTES] */ 
  stack u8[(MLKEM_SYMBYTES * 2)] randomness;
  reg mut ptr u8[(MLKEM_SYMBYTES * 2)] randomnessp;
  randomnessp = randomness; /* u8[(MLKEM_SYMBYTES * 2)] */ 
  randomnessp = #randombytes(randomnessp);
  #[inline]
  (public_key, secret_key) =
    __crypto_kem_keypair_jazz(public_key, secret_key, randomnessp);
  reg u64 r;
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, r) =
    #set0_64(); /*  */ 
  return (public_key, secret_key, r);
}

#[sct="
{ ptr: transient, val: secret } 
{ ptr: transient, val: secret } 
{ ptr: transient, val: secret } 
{ ptr: public, val: secret } 
{ ptr: public, val: secret } 
public
"]
export
fn jade_kem_mlkem_mlkem768_amd64_avx2_enc (#[secret]
                                          reg mut ptr u8[MLKEM_CIPHERTEXTBYTES] ciphertext,
                                          #[secret]
                                          reg mut ptr u8[MLKEM_SYMBYTES] shared_secret,
                                          #[secret]
                                          reg const ptr u8[MLKEM_PUBLICKEYBYTES] public_key) -> 
(#[secret] reg mut ptr u8[MLKEM_CIPHERTEXTBYTES], #[secret]
reg mut ptr u8[MLKEM_SYMBYTES], #[public] reg u64)
requires #[prover=Cas] {is_arr_init(public_key, 0, MLKEM_PUBLICKEYBYTES)} 
ensures #[prover=Cas] {(is_arr_init(result.0, 0, MLKEM_CIPHERTEXTBYTES) &&
                       is_arr_init(result.1, 0, MLKEM_SYMBYTES))}

{
  _ /* u64 */ = #init_msf(); /* :k */ 
  ciphertext = ciphertext; /* u8[MLKEM_CIPHERTEXTBYTES] */ 
  shared_secret = shared_secret; /* u8[MLKEM_SYMBYTES] */ 
  public_key = public_key; /* u8[MLKEM_PUBLICKEYBYTES] */ 
  stack u8[MLKEM_SYMBYTES] randomness;
  reg mut ptr u8[MLKEM_SYMBYTES] randomnessp;
  randomnessp = randomness; /* u8[MLKEM_SYMBYTES] */ 
  randomnessp = #randombytes(randomnessp);
  #[inline]
  (ciphertext, shared_secret) =
    __crypto_kem_enc_jazz(ciphertext, shared_secret, public_key, randomnessp);
  reg u64 r;
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, r) =
    #set0_64(); /*  */ 
  return (ciphertext, shared_secret, r);
}

#[sct="
{ ptr: transient, val: secret } 
{ ptr: transient, val: secret } 
{ ptr: transient, val: secret } 
{ ptr: public, val: secret } 
public
"]
export
fn jade_kem_mlkem_mlkem768_amd64_avx2_dec (#[secret]
                                          reg mut ptr u8[MLKEM_SYMBYTES] shared_secret,
                                          #[secret]
                                          reg const ptr u8[MLKEM_CIPHERTEXTBYTES] ciphertext,
                                          #[secret]
                                          reg const ptr u8[MLKEM_SECRETKEYBYTES] secret_key) -> 
(#[secret] reg mut ptr u8[MLKEM_SYMBYTES], #[public] reg u64)
requires #[prover=Cas] {(is_arr_init(ciphertext, 0, MLKEM_CIPHERTEXTBYTES) &&
                        is_arr_init(secret_key, 0, MLKEM_SECRETKEYBYTES))}

ensures #[prover=Cas] {is_arr_init(result.0, 0, MLKEM_SYMBYTES)} 
{
  _ /* u64 */ = #init_msf(); /* :k */ 
  #[inline]
  shared_secret =
    __crypto_kem_dec_jazz(shared_secret, ciphertext, secret_key);
  reg u64 r;
  (_ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, _ /* bool */, r) =
    #set0_64(); /*  */ 
  return (shared_secret, r);
}


