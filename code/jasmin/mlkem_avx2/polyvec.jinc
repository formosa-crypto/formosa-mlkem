require "params.jinc"
require "poly.jinc"
require "shuffle.jinc"

inline
fn __polyvec_add2(stack u16[MLKEM_VECN] r, stack u16[MLKEM_VECN] b) -> stack u16[MLKEM_VECN]
{
  r[0:MLKEM_N]         = _poly_add2(r[0:MLKEM_N], b[0:MLKEM_N]);
  r[MLKEM_N:MLKEM_N]   = _poly_add2(r[MLKEM_N:MLKEM_N], b[MLKEM_N:MLKEM_N]);
  r[2*MLKEM_N:MLKEM_N] = _poly_add2(r[2*MLKEM_N:MLKEM_N], b[2*MLKEM_N:MLKEM_N]);

  return r;
}

inline
fn __polyvec_csubq(stack u16[MLKEM_VECN] r) -> stack u16[MLKEM_VECN]
{
  r[0:MLKEM_N] = _poly_csubq(r[0:MLKEM_N]);
  r[MLKEM_N:MLKEM_N] = _poly_csubq(r[MLKEM_N:MLKEM_N]);
  r[2*MLKEM_N:MLKEM_N] = _poly_csubq(r[2*MLKEM_N:MLKEM_N]);

  return r;
}

u32 pvd_q_s = 0x0d013404;
u8[32] pvd_shufbdidx_s = {0, 1, 1, 2, 2, 3, 3, 4,
                     5, 6, 6, 7, 7, 8, 8, 9,
                     2, 3, 3, 4, 4, 5, 5, 6,
                     7, 8, 8, 9, 9, 10, 10, 11};
u64 pvd_sllvdidx_s = 0x04;
u32 pvd_mask_s = 0x7fe01ff8;

inline
fn __polyvec_decompress(reg u64 rp) -> stack u16[MLKEM_VECN]
{
  inline int i k;
  reg u256 f q shufbidx sllvdidx mask;
  stack u16[MLKEM_VECN] r;

  q = #VPBROADCAST_8u32(pvd_q_s);
  shufbidx = pvd_shufbdidx_s[u256 0];
  sllvdidx = #VPBROADCAST_4u64(pvd_sllvdidx_s);
  mask = #VPBROADCAST_8u32(pvd_mask_s);

  for k=0 to MLKEM_K
  {
    for i=0 to MLKEM_N/16
    {
      f = (u256)[rp + 320 * k + 20 * i];
      f = #VPERMQ(f, 0x94);
      f = #VPSHUFB_256(f, shufbidx);
      f = #VPSLLV_8u32(f, sllvdidx);
      f = #VPSRL_16u16(f, 1);
      f = #VPAND_256(f, mask);
      f = #VPMULHRS_16u16(f, q);
      r[u256 16*k + i] = f;
    }
  }

  return r;
}

u16 pvc_off_s = 0x0f;
u16 pvc_shift1_s = 0x1000;
u16 pvc_mask_s = 0x03ff;
u64 pvc_shift2_s = 0x0400000104000001;
u64 pvc_sllvdidx_s = 0x0C;
u8[32] pvc_shufbidx_s = {0, 1, 2, 3, 4, 8, 9, 10, 11, 12, -1, -1, -1, -1, -1, -1,
                         9, 10, 11, 12, -1, -1, -1, -1, -1, -1, 0, 1, 2, 3, 4, 8};

/*
inline
fn __polyvec_compress(reg u64 rp, stack u16[MLKEM_VECN] a)
{
  inline int i;
  reg u256 f0 f1 f2 v v8 off shift1 mask shift2 sllvdidx shufbidx;
  reg u128 t0 t1;
  reg ptr u16[16] x16p;

  a = __polyvec_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  v8 = #VPSLL_16u16(v, 3);
  off = #VPBROADCAST_16u16(pvc_off_s);
  shift1 = #VPBROADCAST_16u16(pvc_shift1_s);
  mask = #VPBROADCAST_16u16(pvc_mask_s);
  shift2 = #VPBROADCAST_4u64(pvc_shift2_s);
  sllvdidx = #VPBROADCAST_4u64(pvc_sllvdidx_s);
  shufbidx = pvc_shufbidx_s[u256 0];

  for i=0 to MLKEM_VECN/16
  {
    f0 = a[u256 i];
    f1 = #VPMULL_16u16(f0, v8);
    f2 = #VPADD_16u16(f0, off);
    f0 = #VPSLL_16u16(f0, 3);
    f0 = #VPMULH_16u16(f0, v);
    f2 = #VPSUB_16u16(f1, f2);
    f1 = #VPANDN_256(f1, f2);
    f1 = #VPSRL_16u16(f1, 15);
    f0 = #VPSUB_16u16(f0, f1);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f0 = #VPAND_256(f0, mask);
    f0 = #VPMADDWD_256(f0, shift2);
    f0 = #VPSLLV_8u32(f0, sllvdidx);
    f0 = #VPSRL_4u64(f0, 12);
    f0 = #VPSHUFB_256(f0, shufbidx);
    t0 = (128u)f0;
    t1 = #VEXTRACTI128(f0, 1);
    t0 = #VPBLEND_8u16(t0, t1, 0xE0);
    (u128)[rp + 20*i] = t0;
    (u32)[rp + 20*i + 16] = #VPEXTR_32(t1, 0);
  }
} */

u16 compress10_b0 = -20553;
u16 compress10_b1 = 20;
u16 compress10_b2 = 1 << 9;
u16 compress10_mask10 = 0x3ff;
u32 compress10_shift = 0x04000001;
u64 compress10_sllv_indx = 12;
u8[32] compress10_shuffle = { 0,  1,  2,  3,  4,  8,  9, 10,
                             11, 12, -1, -1, -1, -1, -1, -1,
                              9, 10, 11, 12, -1, -1, -1, -1,
                             -1, -1,  0,  1,  2,  3,  4,  8};

// Inputs:
// forall i = 0, ..., 15, a_i with -q/2 <= a_i <= q/2
// Outputs:
// forall i = 0, ..., 15, round( 1024 a_i / q) & 1023
inline
fn compress10_16x16_inline(reg u256 a b0 b1 b2 mask) -> reg u256 {

    reg u256 p0 p1;

    p0 = #VPMULH_16u16(a, b0);
    p1 = #VPMULL_16u16(a, b1);
    p0 = #VPADD_16u16(p0, p1);
    p0 = #VPMULHRS_16u16(p0, b2);
    p0 = #VPAND_256(p0, mask);

    return p0;

}

// Inputs:
// forall i = 0, ..., 15, a_i with 0 <= a_i < 1024
// Outputs:
// lo32(hi) || lo = lo10(a15) || ... || lo10(a0)
inline
fn pack10_16x16(reg u256 a shift sllv_indx shuffle) -> reg u128, reg u128 {

    reg u128 lo hi;

    a = #VPMADDWD_256(a, shift);
    a = #VPSLLV_8u32(a, sllv_indx);
    a = #VPSRL_4u64(a, 12);
    a = #VPSHUFB_256(a, shuffle);

    lo = (128u)a;
    hi = #VEXTRACTI128(a, 1);
    lo = #VPBLEND_8u16(lo, hi, 0xe0);

    return lo, hi;

}

inline
fn __polyvec_compress(reg u64 r, stack u16[MLKEM_VECN] a){

    reg u256 a0;
    reg u128 lo hi;
    reg u256 b0 b1 b2;
    reg u256 mask10 shift sllv_indx shuffle;
    inline int i;

    b0 = #VPBROADCAST_16u16(compress10_b0);
    b1 = #VPBROADCAST_16u16(compress10_b1);
    b2 = #VPBROADCAST_16u16(compress10_b2);
    mask10 = #VPBROADCAST_16u16(compress10_mask10);
    shift = #VPBROADCAST_8u32(compress10_shift);
    sllv_indx = #VPBROADCAST_4u64(compress10_sllv_indx);
    shuffle = compress10_shuffle[u256 0];

    for i = 0 to MLKEM_VECN / 16 {

        a0 = a[u256 i];
        a0 = compress10_16x16_inline(a0, b0, b1, b2, mask10);
        lo, hi = pack10_16x16(a0, shift, sllv_indx, shuffle);
        (u128)[r + i * 20 + 0] = lo;
        (u32)[r + i * 20 + 16] = #VPEXTR_32(hi, 0);

    }

}

/*
inline
fn __polyvec_compress_1(reg ptr u8[MLKEM_POLYVECCOMPRESSEDBYTES] rp, stack u16[MLKEM_VECN] a) -> reg ptr u8[MLKEM_POLYVECCOMPRESSEDBYTES]
{
  inline int i;
  reg u256 f0 f1 f2 v v8 off shift1 mask shift2 sllvdidx shufbidx;
  reg u128 t0 t1;
  reg ptr u16[16] x16p;

  a = __polyvec_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  v8 = #VPSLL_16u16(v, 3);
  off = #VPBROADCAST_16u16(pvc_off_s);
  shift1 = #VPBROADCAST_16u16(pvc_shift1_s);
  mask = #VPBROADCAST_16u16(pvc_mask_s);
  shift2 = #VPBROADCAST_4u64(pvc_shift2_s);
  sllvdidx = #VPBROADCAST_4u64(pvc_sllvdidx_s);
  shufbidx = pvc_shufbidx_s[u256 0];

  for i=0 to MLKEM_VECN/16
  {
    f0 = a[u256 i];
    f1 = #VPMULL_16u16(f0, v8);
    f2 = #VPADD_16u16(f0, off);
    f0 = #VPSLL_16u16(f0, 3);
    f0 = #VPMULH_16u16(f0, v);
    f2 = #VPSUB_16u16(f1, f2);
    f1 = #VPANDN_256(f1, f2);
    f1 = #VPSRL_16u16(f1, 15);
    f0 = #VPSUB_16u16(f0, f1);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f0 = #VPAND_256(f0, mask);
    f0 = #VPMADDWD_256(f0, shift2);
    f0 = #VPSLLV_8u32(f0, sllvdidx);
    f0 = #VPSRL_4u64(f0, 12);
    f0 = #VPSHUFB_256(f0, shufbidx);
    t0 = (128u)f0;
    t1 = #VEXTRACTI128(f0, 1);
    t0 = #VPBLEND_8u16(t0, t1, 0xE0);
    rp.[u128 20*i] = t0;
    rp.[u32 20*i + 16] = #VPEXTR_32(t1, 0);
  }

  return rp;
}
*/

inline
fn __polyvec_compress_1(reg ptr u8[MLKEM_POLYVECCOMPRESSEDBYTES] rp, stack u16[MLKEM_VECN] a) -> reg ptr u8[MLKEM_POLYVECCOMPRESSEDBYTES] {

    reg u256 a0;
    reg u128 lo hi;
    reg u256 b0 b1 b2;
    reg u256 mask10 shift sllv_indx shuffle;
    inline int i;

    b0 = #VPBROADCAST_16u16(compress10_b0);
    b1 = #VPBROADCAST_16u16(compress10_b1);
    b2 = #VPBROADCAST_16u16(compress10_b2);
    mask10 = #VPBROADCAST_16u16(compress10_mask10);
    shift = #VPBROADCAST_8u32(compress10_shift);
    sllv_indx = #VPBROADCAST_4u64(compress10_sllv_indx);
    shuffle = compress10_shuffle[u256 0];

    for i = 0 to MLKEM_VECN / 16 {
        a0 = a[u256 i];
        a0 = compress10_16x16_inline(a0, b0, b1, b2, mask10);
        lo, hi = pack10_16x16(a0, shift, sllv_indx, shuffle);
        rp.[u128 20*i] = lo;
        rp.[u32 20*i + 16] = #VPEXTR_32(hi, 0);
    }
    return rp;
}


inline
fn __polyvec_frombytes(reg u64 ap) -> stack u16[MLKEM_VECN]
{
  stack u16[MLKEM_VECN] r;
  reg u64 pp;

  pp = ap;
  r[0:MLKEM_N] = _poly_frombytes(r[0:MLKEM_N], pp);
  pp += MLKEM_POLYBYTES;
  r[MLKEM_N:MLKEM_N] = _poly_frombytes(r[MLKEM_N:MLKEM_N], pp);
  pp += MLKEM_POLYBYTES;
  r[2*MLKEM_N:MLKEM_N] = _poly_frombytes(r[2*MLKEM_N:MLKEM_N], pp);

  return r;
}


inline
fn __polyvec_invntt(stack u16[MLKEM_VECN] r) -> stack u16[MLKEM_VECN]
{
  r[0:MLKEM_N] = _poly_invntt(r[0:MLKEM_N]);
  r[MLKEM_N:MLKEM_N] = _poly_invntt(r[MLKEM_N:MLKEM_N]);
  r[2*MLKEM_N:MLKEM_N] = _poly_invntt(r[2*MLKEM_N:MLKEM_N]);

  return r;
}


inline
fn __polyvec_ntt(stack u16[MLKEM_VECN] r) -> stack u16[MLKEM_VECN]
{
  r[0:MLKEM_N] = _poly_ntt(r[0:MLKEM_N]);
  r[MLKEM_N:MLKEM_N] = _poly_ntt(r[MLKEM_N:MLKEM_N]);
  r[2*MLKEM_N:MLKEM_N] = _poly_ntt(r[2*MLKEM_N:MLKEM_N]);

  return r;
}


inline
fn __polyvec_reduce(stack u16[MLKEM_VECN] r) -> stack u16[MLKEM_VECN]
{
  r[0:MLKEM_N] = __poly_reduce(r[0:MLKEM_N]);
  r[MLKEM_N:MLKEM_N] = __poly_reduce(r[MLKEM_N:MLKEM_N]);
  r[2*MLKEM_N:MLKEM_N] = __poly_reduce(r[2*MLKEM_N:MLKEM_N]);

  return r;
}

inline
fn __polyvec_reduce_sig(stack u16[MLKEM_VECN] r) -> stack u16[MLKEM_VECN]
{
  r[0:MLKEM_N] = __poly_reduce_sig(r[0:MLKEM_N]);
  r[MLKEM_N:MLKEM_N] = __poly_reduce_sig(r[MLKEM_N:MLKEM_N]);
  r[2*MLKEM_N:MLKEM_N] = __poly_reduce_sig(r[2*MLKEM_N:MLKEM_N]);

  return r;
}

inline
fn __polyvec_pointwise_acc(stack u16[MLKEM_N] r, stack u16[MLKEM_VECN] a b) -> stack u16[MLKEM_N]
{
  stack u16[MLKEM_N] t;

  r = _poly_basemul(r, a[0:MLKEM_N], b[0:MLKEM_N]);
  t = _poly_basemul(t, a[MLKEM_N:MLKEM_N], b[MLKEM_N:MLKEM_N]);
  r = _poly_add2(r, t);
  t = _poly_basemul(t, a[2*MLKEM_N:MLKEM_N], b[2*MLKEM_N:MLKEM_N]);
  r = _poly_add2(r, t);

  // r = __poly_reduce(r);
  
  return r;
}


inline
fn __polyvec_tobytes(reg u64 rp, stack u16[MLKEM_VECN] a)
{
  reg u64 pp;

  pp = rp;
  a[0:MLKEM_N] = _poly_tobytes(pp, a[0:MLKEM_N]);
  pp += MLKEM_POLYBYTES;
  a[MLKEM_N:MLKEM_N] = _poly_tobytes(pp, a[MLKEM_N:MLKEM_N]);
  pp += MLKEM_POLYBYTES;
  a[2*MLKEM_N:MLKEM_N] = _poly_tobytes(pp, a[2*MLKEM_N:MLKEM_N]);
}
