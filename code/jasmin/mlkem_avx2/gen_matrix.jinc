require "../mlkem_ref/keccakf1600/keccakf1600_bmi1_compact.jinc" // note: _bmi_compact is faster
require "keccakf1600/keccakf1600_4x_compact.jinc"
require "../mlkem_ref/gen_matrix.jinc"
require "gen_matrix_globals.jinc"
require "shuffle.jinc"

// xof related code

inline fn xof_init_x4(
  reg const ptr u8[32] rho,
  reg u16[4] indexes)
  ->
  stack u256[25]
{
  stack u256[25] state;
  inline int i;
  reg u256 t;
  reg u64 r;

  // copy rho sto state
  for i=0 to 4
  { r = rho[u64 i];
    state[u64 4*i + 0] = r;
    state[u64 4*i + 1] = r;
    state[u64 4*i + 2] = r;
    state[u64 4*i + 3] = r;
   }

  // init to zero
  t = #set0_256();
  for i=4 to 25
  { state[i] = t; }

  // complete the state initialization
  for i=0 to 4
  { state[u16 64 + 4*i]  = indexes[i];
    state[u8 130 + 8*i] ^= 0x1F;
    state[u8 (160*4) + 7 + 8*i] ^= 0x80;
  }

  return state;
}


inline fn xof_next_bytes_x3_x4(
  stack u256[25] state)
  ->
  stack u64[((21 * 3) + 4 + 1) * 4],
  stack u256[25]
{
  reg ptr u256[25] state_ptr;
  stack u64[((21 * 3) + 4 + 1) * 4] b168_x4;
  reg ptr u64[(21 * 3) + 4 + 1] b168_0 b168_1 b168_2 b168_3;

  reg u64 n i;
  reg u256 t;
  reg u128 l h;

  b168_0 = b168_x4[((21 * 3) + 4 + 1)*0:((21 * 3) + 4 + 1)];
  b168_1 = b168_x4[((21 * 3) + 4 + 1)*1:((21 * 3) + 4 + 1)];
  b168_2 = b168_x4[((21 * 3) + 4 + 1)*2:((21 * 3) + 4 + 1)];
  b168_3 = b168_x4[((21 * 3) + 4 + 1)*3:((21 * 3) + 4 + 1)];

  n = 0;
  while(n < 3 * 21) // executes 3 times
  {
    state_ptr = state;
    state = _keccakf1600_4x(state_ptr);

    i = 0;
    while(i < 21 * 32) // executes 21 times
    {
      t = state.[(int)i];

      l = (128u) t;
      h = #VEXTRACTI128(t, 1);

		  b168_0[(int)n] = #VMOVLPD(l);
		  b168_1[(int)n] = #VMOVHPD(l);
		  b168_2[(int)n] = #VMOVLPD(h);
		  b168_3[(int)n] = #VMOVHPD(h);

      n += 1;
      i += 32;
	  }
  }

  // note: the last 4 elements of all b168_ contain the remaining keccak state;
  //       this saves some copies if the scalar version of next_bytes is needed

  // note: uncomment the following statements if it simplifies the proof /
  //       helps the safety checker

  // n = 3 * 21;
  i = 21 * 32;

  while(i < 25 * 32) // execs 4 times: 21*32, 22*32, 23*32, 24*32
  {
    t = state.[(int)i];

    l = (128u) t;
    h = #VEXTRACTI128(t, 1);

	  b168_0[(int)n] = #VMOVLPD(l);
	  b168_1[(int)n] = #VMOVHPD(l);
	  b168_2[(int)n] = #VMOVLPD(h);
	  b168_3[(int)n] = #VMOVHPD(h);

    n += 1;
    i += 32;
  }

  b168_0[(int)n] = 0;
  b168_1[(int)n] = 0;
  b168_2[(int)n] = 0;
  b168_3[(int)n] = 0;


  b168_x4[((21 * 3) + 4 + 1)*0:((21 * 3) + 4 + 1)] = b168_0;
  b168_x4[((21 * 3) + 4 + 1)*1:((21 * 3) + 4 + 1)] = b168_1;
  b168_x4[((21 * 3) + 4 + 1)*2:((21 * 3) + 4 + 1)] = b168_2;
  b168_x4[((21 * 3) + 4 + 1)*3:((21 * 3) + 4 + 1)] = b168_3;

  return b168_x4, state;
}

inline fn xof_next_bytes_x3(
  reg ptr u64[25] state)
  ->
  stack u64[21*3],
  reg ptr u64[25]
{
  stack u64[21*3] b168x3;
  inline int n;
  reg u64 i t;

  for n=0 to 3 // there are no scalar/#mmx registers available
  {
    state = _keccakf1600(state);

    i = 0;
    while(i < 21)
    { t = state[(int)i];
      b168x3[n*21 + (int) i] = t;
      i += 1;
    }
  }

  return b168x3, state;
}

inline fn gen_matrix_sample_iterate_x3_fast_filter48(
  reg u64 counter,
  reg ptr u64[21*3] b168x3,
  reg u64 b168x3_offset,
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N] matrix,
  reg u64 matrix_offset,

  reg u256 load_shuffle,
  reg u256 mask,
  reg u256 bounds,
  reg ptr u8[2048] sst,
  reg u256 ones,
  #msf reg u64 ms)
  ->
  reg u64,
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N],
  reg u64
{
  reg u256 f0 f1 g0 g1;
  reg u256 shuffle_0 shuffle_1 shuffle_t;
  reg u128 shuffle_0_1 shuffle_1_1;
  reg u64 good t0_0 t0_1 t1_0 t1_1;

  // loads 24 bytes (while touching 32 bytes of memory) into f0 and another
  // 24 bytes into f1 while doing some rearrangements:
  // - consider that the memory contains the following 32 bytes (in u64s)
  // - 0x01aaaaaaaaaaaa08, 0x01bbbbbbbbbbbb08, 0x01cccccccccccc08, 0x01dddddddddddd08
  // - the command given to vpermq is 0x94, or (8u1)[1,0,0,1, 0,1,0,0], or (4u2)[2,1,1,0]
  // - so the last 8 bytes will be discarded:
  //   - 0x01aaaaaaaaaaaa08, 0x01bbbbbbbbbbbb08, 0x01bbbbbbbbbbbb08, 0x01cccccccccccc08

  f0 = #VPERMQ(b168x3.[u256 (int) b168x3_offset + 0 ], (4u2)[2,1,1,0]);
  f1 = #VPERMQ(b168x3.[u256 (int )b168x3_offset + 24], (4u2)[2,1,1,0]);

  // next, the data is shuffled at byte level. For a given state (in u64s):
  // - 0xa8a7a6a5a4a3a2a1, 0xb8b7b6b5b4b3b2b1, 0xc8c7c6c5c4c3c2c1, 0xd8d7d6d5d4d3d2d1
  // f's get rearranged into:
  // - 0xa6a5a5a4a3a2a2a1, 0xb4b3b3b2b1a8a8a7, 0xd2d1d1c8c7c6c6c5, 0xd8d7d7d6d5d4d4d3

  f0 = #VPSHUFB_256(f0, load_shuffle);
  f1 = #VPSHUFB_256(f1, load_shuffle);

  // next, a shift right by 4 (u16) is performed, for a given state:
  // (consider that c's hold the same values as b's ++ some underscores to help the reading)
  //
  // - 0xa6a5_a5a4_a3a2_a2a1, 0xb4b3_b3b2_b1a8_a8a7, 0xd2d1_d1c8_c7c6_c6c5, 0xd8d7_d7d6_d5d4_d4d3
  // to:
  // - 0x0a6a_0a5a_0a3a_0a2a, 0x0b4b_0b3b_0b1a_0a8a, 0x0d2d_0d1c_0c7c_0c6c, 0x0d8d_0d7d_0d5d_0d4d

  g0 = #VPSRL_16u16(f0, 4);
  g1 = #VPSRL_16u16(f1, 4);

  // next, blend.
  // from:
  // - 0xAA (1010 1010 in binary)
  //
  //   bottom  top    b    t       b    t    b    t
  //        1    0    1    0       1    0    1    0   (same for next 128-bit lane)
  // - 0xa6a5_a5a4_a3a2_a2a1, 0xb4b3_b3b2_b1a8_a8a7,  0xd2d1_d1c8_c7c6_c6c5, 0xd8d7_d7d6_d5d4_d4d3
  // - 0x0a6a_0a5a_0a3a_0a2a, 0x0b4b_0b3b_0b1a_0a8a,  0x0d2d_0d1c_0c7c_0c6c, 0x0d8d_0d7d_0d5d_0d4d
  // to:
  // - 0x0a6a_a5a4_0a3a_a2a1, 0x0b4b_b3b2_0b1a_a8a7, 0x0d2d_d1c8_0c7c_c6c5, 0x0d8d_d7d6_0d5d_d4d3

  f0 = #VPBLEND_16u16(f0, g0, 0xAA);
  f1 = #VPBLEND_16u16(f1, g1, 0xAA);

  // next, mask at 12 bits (0xFFF)
  // from:
  // - 0x0a6a_a5a4_0a3a_a2a1, 0x0b4b_b3b2_0b1a_a8a7, 0x0d2d_d1c8_0c7c_c6c5, 0x0d8d_d7d6_0d5d_d4d3
  // to:
  // - 0x0a6a_05a4_0a3a_02a1, 0x0b4b_03b2_0b1a_08a7, 0x0d2d_01c8_0c7c_06c5, 0x0d8d_07d6_0d5d_04d3

  f0 = #VPAND_256(f0, mask);
  f1 = #VPAND_256(f1, mask);

  // MLKEM_Q is 3329 or 0xd01
  //
  // bounds:
  // - 0x0d01_0d01_0d01_0d01, ...
  //
  // some input:
  // - 0x0a6a_05a4_0a3a_02a1, 0x0b4b_03b2_0b1a_08a7, 0x0d2d_01c8_0c7c_06c5, 0x0d8d_07d6_0d5d_04d3
  //
  // output (the 'good' results are highlighted with Fs; what about when equal to 3329?)
  // - 0xffff_ffff_ffff_ffff, 0xffff_ffff_ffff_ffff, 0x0000_ffff_ffff_ffff, 0x0000_ffff_0000_ffff
  //
  // intuitively, for i=0 to 15: if bounds[i] > input[i] then 0xffff else 0x0
  g0 = #VPCMPGT_16u16(bounds, f0);
  g1 = #VPCMPGT_16u16(bounds, f1);

  // from Intel intrinsics: "Convert packed signed 16-bit integers from a and b to packed 8-bit integers using signed saturation"
  // intuitively, each u16 ffff -> ff and 0000 -> 00
  // g0 = g0[0..7] || g1[0..7] || g0[8..15] || g1[8..15], where each u16 "goes to" u8
  g0 = #VPACKSS_16u16(g0, g1);

  // from Intel intrinsics: "Create mask from the most significant bit of each 8-bit element in a, and store the result in dst."
  good = #VPMOVMSKB_u256u64(g0);

  good = #protect(good, ms); // TODO: double check why this variable needs protect

  // at this point, the bit count of good contains the number of 'good' elements

  // g0
  t0_0 = good;
  t0_0 &= 0xFF; // g0[0..7]

  shuffle_0 = (256u) #VMOV(sst[u64 (int)t0_0]);
  ?{}, t0_0 = #POPCNT_64(t0_0);
  counter += t0_0;
  t0_0 += matrix_offset;

  t0_1 = good;
  t0_1 >>= 16;
  t0_1 &= 0xFF; // g0[8..15]
  shuffle_0_1 = #VMOV(sst[u64 (int)t0_1]);
  ?{}, t0_1 = #POPCNT_64(t0_1);
  counter += t0_1;
  t0_1 += t0_0;

  // g1
  t1_0 = good;
  t1_0 >>= 8;
  t1_0 &= 0xFF; // g1[0..7]
  shuffle_1 = (256u) #VMOV(sst[u64 (int)t1_0]);
  ?{}, t1_0 = #POPCNT_64(t1_0);
  counter += t1_0;
  t1_0 += t0_1;

  t1_1 = good;
  t1_1 >>= 24;
  t1_1 &= 0xFF; // g1[8..15]
  shuffle_1_1 = #VMOV(sst[u64 (int)t1_1]);
  ?{}, t1_1 = #POPCNT_64(t1_1);
  counter += t1_1;
  t1_1 += t1_0;

  //

  shuffle_0 = #VINSERTI128(shuffle_0, shuffle_0_1, 1);
  shuffle_1 = #VINSERTI128(shuffle_1, shuffle_1_1, 1);

  //

  shuffle_t = #VPADD_32u8(shuffle_0, ones);
  shuffle_0 = #VPUNPCKL_32u8(shuffle_0, shuffle_t);

  shuffle_t = #VPADD_32u8(shuffle_1, ones);
  shuffle_1 = #VPUNPCKL_32u8(shuffle_1, shuffle_t);

  f0 = #VPSHUFB_256(f0, shuffle_0);
  f1 = #VPSHUFB_256(f1, shuffle_1);

  //

  matrix.[u128 2*(int) matrix_offset] = (128u)f0;
  matrix.[u128 2*(int) t0_0] = #VEXTRACTI128(f0, 1);
  matrix.[u128 2*(int) t0_1] = (128u)f1;
  matrix.[u128 2*(int) t1_0] = #VEXTRACTI128(f1, 1);
  matrix_offset = t1_1;

  return counter, matrix, matrix_offset;
}

inline fn gen_matrix_sample_iterate_x3_fast_filter24(
  reg u64 counter,
  reg ptr u64[21*3] b168x3,
  reg u64 b168x3_offset,
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N] matrix,
  reg u64 matrix_offset,

  reg u256 load_shuffle mask bounds,
  reg ptr u8[2048] sst,
  reg u256 ones,
  #msf reg u64 ms)
  ->
  reg u64,
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N],
  reg u64
{
  reg u256 f0 g0 g1;
  reg u256 shuffle_0 shuffle_t;
  reg u128 shuffle_0_1;
  reg u64 good t0_0 t0_1 t1_0 t1_1;

  f0 = #VPERMQ(b168x3.[u256 (int) b168x3_offset + 0 ], (4u2)[2,1,1,0]);
  f0 = #VPSHUFB_256(f0, load_shuffle);
  g0 = #VPSRL_16u16(f0, 4);
  f0 = #VPBLEND_16u16(f0, g0, 0xAA);
  f0 = #VPAND_256(f0, mask);
  g0 = #VPCMPGT_16u16(bounds, f0);
  g1 = #set0_256();
  g0 = #VPACKSS_16u16(g0, g1);
  good = #VPMOVMSKB_u256u64(g0);

  good = #protect(good, ms);

  // g0
  t0_0 = good;
  t0_0 &= 0xFF; // g0[0..7]
  shuffle_0 = (256u) #VMOV(sst[u64 (int)t0_0]);
  ?{}, t0_0 = #POPCNT_64(t0_0);
  counter += t0_0;
  t0_0 += matrix_offset;

  t0_1 = good;
  t0_1 >>= 16;
  t0_1 &= 0xFF; // g0[8..15]
  shuffle_0_1 = #VMOV(sst[u64 (int)t0_1]);
  ?{}, t0_1 = #POPCNT_64(t0_1);
  counter += t0_1;
  t0_1 += t0_0;

  // g1
  t1_0 = 0;
  t1_0 += t0_1;

  t1_1 = 0;
  t1_1 += t1_0;

  //
  shuffle_0 = #VINSERTI128(shuffle_0, shuffle_0_1, 1);
  shuffle_t = #VPADD_32u8(shuffle_0, ones);
  shuffle_0 = #VPUNPCKL_32u8(shuffle_0, shuffle_t);
  f0 = #VPSHUFB_256(f0, shuffle_0);
  //

  matrix.[u128 2*(int) matrix_offset] = (128u)f0;
  matrix.[u128 2*(int) t0_0] = #VEXTRACTI128(f0, 1);
  matrix_offset = t1_1;

  return counter, matrix, matrix_offset;
}

// 'sample' implementation from crypto-specs/ml-kem/Sampling.ec
fn gen_matrix_sample_iterate_x3_fast(
  reg u64 counter,
  reg ptr u64[21*3] b168x3,
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N] matrix,
  reg u64 matrix_offset,

  reg u256 load_shuffle mask bounds ones)
  ->
  reg u64,
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N],
  reg u64
{
  reg u64 b168x3_offset;
  reg ptr u8[2048] sst;
  reg bool condition_loop;
  #msf reg u64 ms;

  ms = #init_msf();

  //
  sst = sample_shuffle_table;

  //
  b168x3_offset = 0;
  while { condition_loop = b168x3_offset <= (168*3 - 24 - 48); } // last 432, exits, 480, 10 iterations
        (condition_loop)
  {
    ms = #update_msf(condition_loop, ms);

    counter, matrix, matrix_offset =
      gen_matrix_sample_iterate_x3_fast_filter48(counter, b168x3, b168x3_offset,
        matrix, matrix_offset, load_shuffle, mask, bounds, sst, ones, ms);

    b168x3_offset += 48;
  }
  ms = #update_msf(!condition_loop, ms);

  counter, matrix, matrix_offset =
      gen_matrix_sample_iterate_x3_fast_filter24(counter, b168x3, b168x3_offset,
        matrix, matrix_offset, load_shuffle, mask, bounds, sst, ones, ms);

  // in b168x3 there are 504 bytes, or 4032 bits, which, at 12 bits per element,
  // counter can be at most 4032/12 = 336. This is +80 above limit.

  return counter, matrix, matrix_offset;
}

fn gen_matrix_sample_iterate_x3_fast_no_overwrites(
  reg u64 counter,
  reg ptr u64[21*3] b168x3,
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N] matrix,
  reg u64 matrix_offset,

  reg u256 load_shuffle mask bounds ones)
  ->
  reg u64,
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N],
  reg u64
{
  reg u64 b168x3_offset;
  reg ptr u8[2048] sst;
  reg bool condition_loop;
  reg ptr u64[21] b168;
  reg u64 b168_offset;
  #msf reg u64 ms;

  ms = #init_msf();

  //
  sst = sample_shuffle_table;

  //
  b168x3_offset = 0;
  while { condition_loop = comp_u64_l_int_and_u64_l_int(counter, (224+1), b168x3_offset, (432+1)); } // last 432, exits, 480
        (condition_loop)

  {
    ms = #update_msf(condition_loop, ms);

    counter, matrix, matrix_offset =
      gen_matrix_sample_iterate_x3_fast_filter48(counter, b168x3, b168x3_offset,
        matrix, matrix_offset, load_shuffle, mask, bounds, sst, ones, ms);

    b168x3_offset += 48;
  }
  ms = #update_msf(!condition_loop, ms);

  b168 = b168x3[21*2:21];
  b168_offset = #LEA(b168x3_offset - 168*2);
  counter, matrix, matrix_offset = gen_matrix_sample_iterate_generic(counter, b168, b168_offset, matrix, matrix_offset);

  return counter, matrix, matrix_offset;
}

inline fn gen_matrix_sample_fast(
  #spill_to_mmx reg u64 counter,
  #spill_to_mmx reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N] matrix,
  #spill_to_mmx reg u64 matrix_offset,
                reg mut ptr u64[25] xof_state)
  ->
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N],
  reg u64,
  reg ptr u64[25]
{
  stack u64[21*3] _b168x3;
  reg ptr u64[21*3] b168x3;
  reg u256 load_shuffle mask bounds ones;

    () = #spill(counter, matrix, matrix_offset);

  _b168x3, xof_state = xof_next_bytes_x3(xof_state);

    () = #unspill(counter, matrix, matrix_offset);

  b168x3 = _b168x3;
  load_shuffle = sample_load_shuffle[u256 0];
  mask = sample_mask;
  bounds = sample_q;
  ones = sample_ones;

  counter, matrix, matrix_offset = gen_matrix_sample_iterate_x3_fast_no_overwrites(
                                     counter, b168x3, matrix, matrix_offset,
                                     load_shuffle, mask, bounds, ones);

  matrix, matrix_offset, _ = gen_matrix_sample(counter, matrix, matrix_offset, xof_state);

  return matrix, matrix_offset, xof_state;
}

inline fn gen_matrix_sample_x4(
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N] matrix,
  reg u64 matrix_offset,
  stack u256[25] xof_state_x4)
  ->
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N],
  reg u64
{
  // local variables
  stack u64[((21 * 3) + 4 + 1) * 4] b168x3x4; // the +1 is for u256 alignment
  reg ptr u64[21 * 3] b168x3;
  inline int p;
  #mmx reg u64 matrix_offset_at_entry;
  reg u64 counter;
  reg ptr u64[25] xof_state;

  reg u256 load_shuffle mask bounds ones;

  b168x3x4, xof_state_x4 = xof_next_bytes_x3_x4(xof_state_x4);

  //

  load_shuffle = sample_load_shuffle[u256 0];
  mask = sample_mask;
  bounds = sample_q;
  ones = sample_ones;

  //

  for p=0 to 4 // #newfeature (to swap for by while)
  {
    matrix_offset_at_entry = matrix_offset;
    counter = 0;
    b168x3 = b168x3x4[((21 * 3) + 4 + 1) * p : 21 * 3];

    counter, matrix, matrix_offset = gen_matrix_sample_iterate_x3_fast(
                                       counter, b168x3, matrix, matrix_offset,
                                       load_shuffle, mask, bounds, ones);

    xof_state = b168x3x4[((21 * 3) + 4 + 1) * (p+1) - (25 + 1) : 25];

    matrix, matrix_offset, _ = gen_matrix_sample(counter, matrix, matrix_offset, xof_state);

    matrix_offset = matrix_offset_at_entry;
    matrix_offset = #LEA(matrix_offset + 256);
  }

  return matrix, matrix_offset;
}

u16[2*4*2] gen_matrix_indexes =
{
  0x0000, 0x0001, 0x0002, 0x0100, // (0,0) (0,1) (0,2) (1,0)
  0x0101, 0x0102, 0x0200, 0x0201, // (1,1) (1,2) (2,0) (2,1)

  0x0000, 0x0100, 0x0200, 0x0001, // previous indexes: swapped for transposed
  0x0101, 0x0201, 0x0002, 0x0102
};

inline fn gen_matrix_get_indexes(
  reg u64 _b,
  reg u64 _t)
  ->
  reg u16[4]
{
  reg u64 b t;
  reg u16[4] idx;
  reg ptr u16[2*4*2] gmi;

  gmi = gen_matrix_indexes;

  b = _b; b <<= 2; // b * 4
  t = _t; t <<= 3; // t * 8
  b += t;

  idx[0] = gmi[(int) b + 0];
  idx[1] = gmi[(int) b + 1];
  idx[2] = gmi[(int) b + 2];
  idx[3] = gmi[(int) b + 3];

  return idx;
}

inline fn gen_matrix_sample_one_polynomial_fast(
  reg u64 i,
  reg u64 j,
  reg u64 transposed,
  reg ptr u8[32] rho,
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N] matrix,
  reg u64 matrix_offset)
  ->
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N],
  reg u64
{
  // local variables
  stack u64[25] _xof_state;
  reg ptr u64[25] xof_state;
  reg u16 index;
  reg u64 counter;

  xof_state = _xof_state;

  index = gen_matrix_get_index(i, j, transposed);

  // XOF.init(rho, j, i);
  xof_state = xof_init(xof_state, rho, index);

  // c <@ Parse(XOF).sample(); a.[(i,j)] <- c;
  counter = 0;
  matrix, matrix_offset, _ = gen_matrix_sample_fast(counter, matrix, matrix_offset, xof_state);

  return matrix, matrix_offset;
}

inline fn gen_matrix_sample_four_polynomials(
  reg u64 b,
  reg u64 transposed,
  reg ptr u8[32] rho,
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N] matrix,
  reg u64 matrix_offset)
  ->
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N],
  reg u64
{
  reg u16[4] indexes;
  stack u256[25] xof_state_x4;

  indexes = gen_matrix_get_indexes(b, transposed);

  xof_state_x4 = xof_init_x4(rho, indexes);

  matrix, matrix_offset = gen_matrix_sample_x4(matrix, matrix_offset, xof_state_x4);

  return matrix, matrix_offset;
}

inline fn __gen_matrix_nttunpack(
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N] matrix)
  ->
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N]
{
  inline int i j;

  for i = 0 to MLKEM_K
  { for j = 0 to MLKEM_K
    { matrix[i*MLKEM_VECN+j*MLKEM_N:MLKEM_N] = _nttunpack(matrix[i*MLKEM_VECN+j*MLKEM_N:MLKEM_N]); }
  }

  return matrix;
}

fn _gen_matrix_avx2(
                reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N] matrix,
  #spill_to_mmx reg ptr u8[32] rho,
  #spill_to_mmx reg u64 transposed)
  ->
  reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N]
{
  reg u64 matrix_offset;
  #spill_to_mmx reg u64 b;
  reg u64 i j;

  matrix_offset = 0;
  b = 0;

  while(b < 2)
  {
      () = #spill(b, transposed, rho);

    matrix, matrix_offset = gen_matrix_sample_four_polynomials(b, transposed, rho, matrix, matrix_offset);

      () = #unspill(b, transposed, rho);

    b += 1;
  }


  // sample the last one, (2,2), using scalar code
  i = 2;
  j = 2;
  matrix, matrix_offset = gen_matrix_sample_one_polynomial_fast(i, j, transposed, rho, matrix, matrix_offset);


  matrix = __gen_matrix_nttunpack(matrix);

  return matrix;
}


