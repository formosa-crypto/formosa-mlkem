require "mlkem_keccak_avx2.jinc"


require "params.jinc"
require "gen_matrix_globals.jinc"

// a < b && c < d
inline fn comp_u64_l_int_and_u64_l_int(
  reg u64 a,
  inline int b,
  reg u64 c,
  inline int d) 
  ->
  reg bool
{
  reg bool c1 c2 c3;
  reg u8 bc1 bc2;

  ?{ "<u" = c1 } = #CMP_64(a, b);
  // if(c1) <=> if(a <u b)
  bc1 = #SETcc(c1);

  ?{ "<u" = c2 } = #CMP_64(c, d);
  // if(c2) <=> if(a <u c)
  bc2 = #SETcc(c2);

  // zf == 1 => bc1 & bc2 == 0 => cond = false
  // zf == 0 => bc1 & bc2 == 1 => cond = true
  ?{ "!=" = c3 } = #TEST_8(bc1, bc2); 

  return c3;
}

// BUF_size per entry: 21(rate) + 21(rate) + 25(keccak_state) + 1(pad) 
param int BUF_size = 536; // 168*2+200      (was in u64s: 3*21 + 4 + 1; //544 bytes; 


inline fn __gen_matrix_buf_rejection_filter48
( reg mut ptr u16[MLKEM_N] pol
, reg u64 counter
, reg const ptr u8[BUF_size] buf
, reg u64 buf_offset // bytes

, reg u256 load_shuffle
, reg u256 mask
, reg u256 bounds
, reg ptr u8[2048] sst
, reg u256 ones
, #msf reg u64 ms
) -> reg ptr u16[MLKEM_N], reg u64
{
  reg u256 f0 f1 g0 g1;
  reg u256 shuffle_0 shuffle_1 shuffle_t;
  reg u128 shuffle_0_1 shuffle_1_1;
  reg u64 good t0_0 t0_1 t1_0 t1_1;

  // loads 24 bytes (while touching 32 bytes of memory) into f0 and another
  // 24 bytes into f1 while doing some rearrangements:
  // - consider that the memory contains the following 32 bytes (in u64s)
  // - 0x01aaaaaaaaaaaa08, 0x01bbbbbbbbbbbb08, 0x01cccccccccccc08, 0x01dddddddddddd08
  // - the command given to vpermq is 0x94, or (8u1)[1,0,0,1, 0,1,0,0], or (4u2)[2,1,1,0]
  // - so the last 8 bytes will be discarded:
  //   - 0x01aaaaaaaaaaaa08, 0x01bbbbbbbbbbbb08, 0x01bbbbbbbbbbbb08, 0x01cccccccccccc08

  f0 = #VPERMQ(buf.[u256 (int) buf_offset + 0 ], (4u2)[2,1,1,0]);
  f1 = #VPERMQ(buf.[u256 (int) buf_offset + 24], (4u2)[2,1,1,0]);

  // next, the data is shuffled at byte level. For a given state (in u64s): 
  // - 0xa8a7a6a5a4a3a2a1, 0xb8b7b6b5b4b3b2b1, 0xc8c7c6c5c4c3c2c1, 0xd8d7d6d5d4d3d2d1
  // f's get rearranged into: 
  // - 0xa6a5a5a4a3a2a2a1, 0xb4b3b3b2b1a8a8a7, 0xd2d1d1c8c7c6c6c5, 0xd8d7d7d6d5d4d4d3

  f0 = #VPSHUFB_256(f0, load_shuffle);
  f1 = #VPSHUFB_256(f1, load_shuffle);

  // next, a shift right by 4 (u16) is performed, for a given state:
  // (consider that c's hold the same values as b's ++ some underscores to help the reading)
  //
  // - 0xa6a5_a5a4_a3a2_a2a1, 0xb4b3_b3b2_b1a8_a8a7, 0xd2d1_d1c8_c7c6_c6c5, 0xd8d7_d7d6_d5d4_d4d3
  // to:
  // - 0x0a6a_0a5a_0a3a_0a2a, 0x0b4b_0b3b_0b1a_0a8a, 0x0d2d_0d1c_0c7c_0c6c, 0x0d8d_0d7d_0d5d_0d4d

  g0 = #VPSRL_16u16(f0, 4);
  g1 = #VPSRL_16u16(f1, 4);

  // next, blend.
  // from: 
  // - 0xAA (1010 1010 in binary)
  //
  //   bottom  top    b    t       b    t    b    t
  //        1    0    1    0       1    0    1    0   (same for next 128-bit lane)
  // - 0xa6a5_a5a4_a3a2_a2a1, 0xb4b3_b3b2_b1a8_a8a7,  0xd2d1_d1c8_c7c6_c6c5, 0xd8d7_d7d6_d5d4_d4d3
  // - 0x0a6a_0a5a_0a3a_0a2a, 0x0b4b_0b3b_0b1a_0a8a,  0x0d2d_0d1c_0c7c_0c6c, 0x0d8d_0d7d_0d5d_0d4d
  // to:
  // - 0x0a6a_a5a4_0a3a_a2a1, 0x0b4b_b3b2_0b1a_a8a7, 0x0d2d_d1c8_0c7c_c6c5, 0x0d8d_d7d6_0d5d_d4d3

  f0 = #VPBLEND_16u16(f0, g0, 0xAA);
  f1 = #VPBLEND_16u16(f1, g1, 0xAA);

  // next, mask at 12 bits (0xFFF)
  // from:
  // - 0x0a6a_a5a4_0a3a_a2a1, 0x0b4b_b3b2_0b1a_a8a7, 0x0d2d_d1c8_0c7c_c6c5, 0x0d8d_d7d6_0d5d_d4d3
  // to:
  // - 0x0a6a_05a4_0a3a_02a1, 0x0b4b_03b2_0b1a_08a7, 0x0d2d_01c8_0c7c_06c5, 0x0d8d_07d6_0d5d_04d3

  f0 = #VPAND_256(f0, mask);
  f1 = #VPAND_256(f1, mask);

  // KYBER_Q is 3329 or 0xd01
  //
  // bounds:
  // - 0x0d01_0d01_0d01_0d01, ...
  //
  // some input:
  // - 0x0a6a_05a4_0a3a_02a1, 0x0b4b_03b2_0b1a_08a7, 0x0d2d_01c8_0c7c_06c5, 0x0d8d_07d6_0d5d_04d3
  //
  // output (the 'good' results are highlighted with Fs; what about when equal to 3329?) 
  // - 0xffff_ffff_ffff_ffff, 0xffff_ffff_ffff_ffff, 0x0000_ffff_ffff_ffff, 0x0000_ffff_0000_ffff
  //
  // intuitively, for i=0 to 15: if bounds[i] > input[i] then 0xffff else 0x0
  g0 = #VPCMPGT_16u16(bounds, f0);
  g1 = #VPCMPGT_16u16(bounds, f1);

  // from Intel intrinsics: "Convert packed signed 16-bit integers from a and b to packed 8-bit integers using signed saturation"
  // intuitively, each u16 ffff -> ff and 0000 -> 00
  // g0 = g0[0..7] || g1[0..7] || g0[8..15] || g1[8..15], where each u16 "goes to" u8
  g0 = #VPACKSS_16u16(g0, g1);

  // from Intel intrinsics: "Create mask from the most significant bit of each 8-bit element in a, and store the result in dst."
  good = #VPMOVMSKB_u256u64(g0);

  good = #protect(good, ms);

  // at this point, the bit count of good contains the number of 'good' elements

  // g0
  t0_0 = good;
  t0_0 &= 0xFF; // g0[0..7]

  shuffle_0 = (256u) #VMOV(sst[u64 (int)t0_0]);
  ?{}, t0_0 = #POPCNT_64(t0_0);
  t0_0 += counter;  

  t0_1 = good;
  t0_1 >>= 16;
  t0_1 &= 0xFF; // g0[8..15]
  shuffle_0_1 = #VMOV(sst[u64 (int)t0_1]);
  ?{}, t0_1 = #POPCNT_64(t0_1);
  t0_1 += t0_0;

  // g1
  t1_0 = good;
  t1_0 >>= 8;
  t1_0 &= 0xFF; // g1[0..7]
  shuffle_1 = (256u) #VMOV(sst[u64 (int)t1_0]);
  ?{}, t1_0 = #POPCNT_64(t1_0);
  t1_0 += t0_1;

  t1_1 = good;
  t1_1 >>= 24;
  t1_1 &= 0xFF; // g1[8..15]
  shuffle_1_1 = #VMOV(sst[u64 (int)t1_1]);
  ?{}, t1_1 = #POPCNT_64(t1_1);
  t1_1 += t1_0;

  //

  shuffle_0 = #VINSERTI128(shuffle_0, shuffle_0_1, 1);
  shuffle_1 = #VINSERTI128(shuffle_1, shuffle_1_1, 1);

  //

  shuffle_t = #VPADD_32u8(shuffle_0, ones);
  shuffle_0 = #VPUNPCKL_32u8(shuffle_0, shuffle_t);

  shuffle_t = #VPADD_32u8(shuffle_1, ones);
  shuffle_1 = #VPUNPCKL_32u8(shuffle_1, shuffle_t);

  f0 = #VPSHUFB_256(f0, shuffle_0);
  f1 = #VPSHUFB_256(f1, shuffle_1);

  //

  pol.[u128 2*counter] = (128u)f0;
  pol.[u128 2*t0_0] = #VEXTRACTI128(f0, 1);
  pol.[u128 2*t0_1] = (128u)f1;
  pol.[u128 2*t1_0] = #VEXTRACTI128(f1, 1);

  counter = t1_1;

  return pol, counter;
}

// safe-write (ensured to write inside the array...)
inline fn __write_u128_boundchk
( reg mut ptr u16[MLKEM_N] pol
, reg u64 ctr
, reg u128 data
, #msf reg u64 ms
) -> reg ptr u16[MLKEM_N], #msf reg u64
{
  reg u64 data_u64;
  reg bool condition_8 condition_4 condition_2 condition_1;

  condition_8 = (ctr <= MLKEM_N-8);
  if ( condition_8 ) {
    ms = #update_msf(condition_8, ms);

    pol.[u128 2*(int)ctr] = data;
    ctr += 8;
  } else
  {
    ms = #update_msf(!condition_8, ms);

    data_u64 = #MOVV(data);

    condition_4 = (ctr <= MLKEM_N-4);
    if ( condition_4 ) {
      ms = #update_msf(condition_4, ms);

      pol.[u64 2*(int)ctr] = data_u64;
      data_u64 = #VPEXTR_64(data, 1);
      ctr += 4;
    } else
    { ms = #update_msf(!condition_4, ms); }

    condition_2 = (ctr <= MLKEM_N-2);
    if ( condition_2 ) {
      ms = #update_msf(condition_2, ms);

      pol.[u32 2*(int)ctr] = (32u) data_u64;
      data_u64 >>= 32;
      ctr += 2;
    } else
    { ms = #update_msf(!condition_2, ms); }

    condition_1 = (ctr <= MLKEM_N-1);
    if ( condition_1 ) {
      ms = #update_msf(condition_1, ms);

      pol.[u16 2*(int)ctr] = (16u) data_u64;
      //ctr += 1;
    } else
    { ms = #update_msf(!condition_1, ms); }
  }

  return pol, ms;
}

inline fn __gen_matrix_buf_rejection_filter24
( reg mut ptr u16[MLKEM_N] pol
, reg u64 counter
, reg const ptr u8[BUF_size] buf
, reg u64 buf_offset // in bytes

, reg u256 load_shuffle mask bounds
, reg ptr u8[2048] sst
, reg u256 ones
, #msf reg u64 ms
) -> reg ptr u16[MLKEM_N], reg u64, #msf reg u64
{
  reg u256 f0 g0 g1;
  reg u256 shuffle_0 shuffle_t;
  reg u128 shuffle_0_1 t128;
  reg u64 good t0_0 t0_1;

  f0 = #VPERMQ(buf.[u256 (int) buf_offset + 0 ], (4u2)[2,1,1,0]);
  f0 = #VPSHUFB_256(f0, load_shuffle);
  g0 = #VPSRL_16u16(f0, 4);
  f0 = #VPBLEND_16u16(f0, g0, 0xAA);
  f0 = #VPAND_256(f0, mask);
  g0 = #VPCMPGT_16u16(bounds, f0);
  g1 = #set0_256();
  g0 = #VPACKSS_16u16(g0, g1);
  good = #VPMOVMSKB_u256u64(g0);

  good = #protect(good, ms);

  // g0
  t0_0 = good;
  t0_0 &= 0xFF; // g0[0..7]
  shuffle_0 = (256u) #VMOV(sst[u64 (int)t0_0]);
  ?{}, t0_0 = #POPCNT_64(t0_0);
  t0_0 += counter;

  t0_1 = good;
  t0_1 >>= 16;
  t0_1 &= 0xFF; // g0[8..15]
  shuffle_0_1 = #VMOV(sst[u64 (int)t0_1]);
  ?{}, t0_1 = #POPCNT_64(t0_1);
  t0_1 += t0_0;

  //
  shuffle_0 = #VINSERTI128(shuffle_0, shuffle_0_1, 1);
  shuffle_t = #VPADD_32u8(shuffle_0, ones);
  shuffle_0 = #VPUNPCKL_32u8(shuffle_0, shuffle_t);
  f0 = #VPSHUFB_256(f0, shuffle_0);
  //

  t128 = (128u) f0;
  pol, ms = __write_u128_boundchk(pol, counter, t128, ms);
  
  t128 = #VEXTRACTI128(f0, 1);
  pol, ms = __write_u128_boundchk(pol, t0_0, t128, ms);

  counter = t0_1;
  
  return pol, counter, ms;
}


fn _gen_matrix_buf_rejection
( reg mut ptr u16[MLKEM_N] pol		// polynomial
, reg u64 counter			// number of coefs. already sampled
, reg const ptr u8[BUF_size] buf	// whole buffer (size=21+21+25 (+1 pad))
, reg u64 buf_offset			// start looking at... (bytes)

) -> reg ptr u16[MLKEM_N], reg u64	// pol. and counter
{
  reg bool condition_loop;
  reg ptr u8[2048] sst;
  reg u256 load_shuffle mask bounds ones;
  #msf reg u64 ms;

  ms = #init_msf();

  load_shuffle = sample_load_shuffle[u256 0];
  mask = sample_mask;
  bounds = sample_q;
  ones = sample_ones;
  sst = sample_shuffle_table;

  buf_offset = buf_offset;

  while 
    { condition_loop = comp_u64_l_int_and_u64_l_int(buf_offset, 3*168-48+1, counter, MLKEM_N-32+1);
    }
  ( condition_loop )
    {
    ms = #update_msf(condition_loop, ms);
() = #spill(buf_offset);
    pol, counter = __gen_matrix_buf_rejection_filter48(pol, counter, buf, buf_offset, load_shuffle, mask, bounds, sst, ones, ms);
() = #unspill(buf_offset);
    buf_offset += 48;
    }
  ms = #update_msf(!condition_loop, ms);

  while { condition_loop = comp_u64_l_int_and_u64_l_int(buf_offset, 3*168-24+1, counter, MLKEM_N); }
        ( condition_loop )
  {
    ms = #update_msf(condition_loop, ms);
() = #spill(buf_offset);
    pol, counter, ms = __gen_matrix_buf_rejection_filter24(pol, counter, buf, buf_offset, load_shuffle, mask, bounds, sst, ones, ms);
() = #unspill(buf_offset);
    buf_offset += 24;
  }

  return pol, counter;
}

inline fn gen_matrix_get_indexes(
  reg u64 b,
  reg u64 _t)
  ->
  reg u64
{
  reg u64 t;
  reg ptr u8[32] idxs;
  idxs = gen_matrix_indexes;

  t = _t; t <<= 4; // t * 16
  b += t;

  t = idxs.[u64 b];

  return t;
}

fn __gen_matrix_fill_polynomial
( reg mut ptr u16[MLKEM_N] pol
, reg mut ptr u8[BUF_size] buf
) -> reg ptr u16[MLKEM_N], reg ptr u8[BUF_size]
{
  reg u64 counter, buf_offset;

  buf_offset = 0;
  counter = 0;
  pol, counter = _gen_matrix_buf_rejection(pol, counter, buf, buf_offset);
  buf_offset = 2*168;
  while (counter < MLKEM_N) {
    buf = _shake128_next_state(buf);
    pol, counter = _gen_matrix_buf_rejection(pol, counter, buf, buf_offset);
  }
  
  return pol, buf;      
}

fn _gen_matrix_sample_four_polynomials
( reg mut ptr u16[4*MLKEM_N] polx4
, reg mut ptr u8[BUF_size * 4] buf
, reg ptr u8[32] rho
, reg u64 pos_entry
, reg u64 transposed
) -> reg ptr u16[4*MLKEM_N], reg ptr u8[BUF_size * 4]
{
  reg ptr u16[MLKEM_N] pol;
  stack u256[25] state;
  reg ptr u256[25] stx4;
  stack u8[8] indexes;

  indexes.[u64 0] = gen_matrix_get_indexes(pos_entry, transposed);

  stx4 = state;
  stx4 = _shake128x4_absorb_A32_A2(stx4, rho, indexes);
  _, buf = _shake128x4_squeeze3blocks(stx4, buf);

  pol = polx4[0*MLKEM_N:MLKEM_N];
  pol, buf[BUF_size * 0 : BUF_size] = __gen_matrix_fill_polynomial(pol, buf[BUF_size * 0 : BUF_size]);
  polx4[0*MLKEM_N:MLKEM_N] = pol;

  pol = polx4[1*MLKEM_N:MLKEM_N];
  pol, buf[BUF_size * 1 : BUF_size] = __gen_matrix_fill_polynomial(pol, buf[BUF_size * 1 : BUF_size]);
  polx4[1*MLKEM_N:MLKEM_N] = pol;

  pol = polx4[2*MLKEM_N:MLKEM_N];
  pol, buf[BUF_size * 2 : BUF_size] = __gen_matrix_fill_polynomial(pol, buf[BUF_size * 2 : BUF_size]);
  polx4[2*MLKEM_N:MLKEM_N] = pol;

  pol = polx4[3*MLKEM_N:MLKEM_N];
  pol, buf[BUF_size * 3 : BUF_size] = __gen_matrix_fill_polynomial(pol, buf[BUF_size * 3 : BUF_size]);
  polx4[3*MLKEM_N:MLKEM_N] = pol;

  return polx4, buf;
}

inline fn __gen_matrix_sample_one_polynomial
( reg mut ptr u16[MLKEM_N] pol
, reg mut ptr u8[BUF_size] buf
, reg ptr u8[32] rho
, reg u16 rc
) -> reg ptr u16[MLKEM_N], reg ptr u8[BUF_size]
{
  reg u256[7] stavx2;
  stack u8[2] pos;

  pos[u16 0] = rc;
  stavx2 = _shake128_absorb_A32_A2(rho, pos);
  buf = _shake128_squeeze3blocks(buf, stavx2);

  pol, buf = __gen_matrix_fill_polynomial(pol, buf);

  return pol, buf;      
}

export fn _gen_matrix_avx2
( reg mut ptr u16[MLKEM_K * MLKEM_K * MLKEM_N] matrix
, reg const ptr u8[32] rho
, #spill_to_mmx reg u64 transposed
) -> reg ptr u16[MLKEM_K * MLKEM_K * MLKEM_N]
{
  // local variables
  inline int i j;
  stack u8[BUF_size * 4] buf_s;
  reg ptr u8[BUF_size * 4] buf;
  reg ptr u16[4*MLKEM_N] polx4;
  reg ptr u16[MLKEM_N] pol;
  reg u64 pos_entry;
  reg u16 rc;

  // Enforce small range
  transposed &= 1;

  () = #spill(transposed);

  buf = buf_s;

  for i = 0 to 2
  {
    pos_entry = 8*i;
    polx4 = matrix[4*i*MLKEM_N:4*MLKEM_N];
    () = #unspill(transposed);
    polx4, buf = _gen_matrix_sample_four_polynomials(polx4, buf, rho, pos_entry, transposed);
    matrix[i*4*MLKEM_N:4*MLKEM_N] = polx4;
  }

  
  // sample the last one, (2,2), using single-lane code
  pol = matrix[8*MLKEM_N:MLKEM_N];
  rc = 0x0202;
  pol, buf[BUF_size * 0 : BUF_size] = __gen_matrix_sample_one_polynomial(pol, buf[BUF_size * 0 : BUF_size], rho, rc);

  matrix[8*MLKEM_N:MLKEM_N] = pol;

  for i = 0 to MLKEM_K
  { for j = 0 to MLKEM_K
    { matrix[i*MLKEM_VECN+j*MLKEM_N:MLKEM_N] = _nttunpack(matrix[i*MLKEM_VECN+j*MLKEM_N:MLKEM_N]);
    }
  }

  return matrix;
}

