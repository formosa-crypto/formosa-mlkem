/* DEPENDENCIES:
require "keccak1600x4_avx2.jinc"
param int ASIZE = 1001;
*/

require "subreadwrite_array_ASIZE.jinc"


/*
   INCREMENTAL ARRAY BROADCAST ABSORB
   ==================================
*/

inline fn __addstate_bcast_array_avx2x4
( reg mut ptr u256[25] st
, inline int AT /* bytes (0 <= AT < 200) */
, reg const ptr u8[ASIZE] buf
, reg u64 offset
, inline int LEN
, inline int TRAILB
) -> reg ptr u256[25] /* st */
   , inline int /* AT */
   , reg u64 /* offset */
{
  inline int DELTA, LO, ALL;
  reg u64 at;
  reg u256 t256;

  ALL = AT+LEN; // total bytes to process (excluding trail byte, if !=0)
  LO = AT % 8; // leftover bytes
  at = 32 * (AT / 8); // current pstate position
  DELTA = 0;

  if ( 0 < LO ) { // process first word...
    if ( LO+LEN < 8) { // ...not enough to fill a word (just update it)
      if ( TRAILB != 0 ) { ALL += 1; }
      DELTA, _, TRAILB, t256 = __aread_bcast_4subu64(buf, offset, DELTA, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, 8*LO);
      t256 ^= st.[u256 (int) at];
      st.[u256 (int) at] = t256;
      LO = 0;
      AT = 0;
      LEN = 0;
    } else { // process first word
      if ( 8 <= LEN ) {
        t256 = #VPBROADCAST_4u64(buf.[u64 offset + DELTA]);
        DELTA += (8-LO);
      } else {
        DELTA, _, _, t256 = __aread_bcast_4subu64(buf, offset, DELTA, 8-LO, 0);
      }
      LEN -= 8-LO;
      AT += 8-LO;
      t256 = #VPSLL_4u64(t256, 8*LO);
      t256 ^= st.[u256 (int) at];
      st.[u256 (int) at] = t256;
      at += 32;
    }
  }

  offset += DELTA;
  DELTA = 0;
  // continue processing remaining bytes
  if (8 <= LEN) {
    while ( at < 32*(AT/8)+32*(LEN/8)) {
      t256 = #VPBROADCAST_4u64(buf.[u64 offset]);
      offset += 8;
      t256 ^= st.[u256 at];
      st.[u256 at] = t256;
      at += 32;
    }
    LEN = (AT+LEN) % 8;
  }

  // process last word (possibly closing the state)
  LO = (AT+LEN) % 8;
  if ( 0 < LO || TRAILB != 0 ) {
    if ( TRAILB != 0 ) { ALL += 1; }
    DELTA, _, TRAILB, t256 = __aread_bcast_4subu64(buf, offset, DELTA, LO, TRAILB);
    offset += DELTA;
    t256 ^= st.[u256 at];
    st.[u256 at] = t256;
  }
  return st, ALL, offset;
} 

inline fn __absorb_bcast_array_avx2x4
( reg mut ptr u256[25] st
, inline int AT
, reg const ptr u8[ASIZE] buf
, reg u64 offset
, inline int LEN
, inline int RATE8
, inline int TRAILB /* closes state if !=0 (i.e. adds trailbyte and padding) */
) -> reg ptr u256[25] /* st */
   , inline int /* AT */
   , reg u64 /* offset */
{
  reg u64 i;
  inline int ALL, ITERS;

  ALL = AT + LEN;
  if ( (AT+LEN) < RATE8 ) { // not enough to fill a block!
    st, AT, offset = __addstate_bcast_array_avx2x4(st, AT, buf, offset, LEN, TRAILB);
    if (TRAILB != 0) { // add pstate and closes the state
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else { // at least a block is filled
    if ( AT != 0 ) { // start by filling the first block
      st, _, offset = __addstate_bcast_array_avx2x4(st, AT, buf, offset, RATE8-AT, 0);
      LEN = LEN - (RATE8-AT);
      st = _keccakf1600_avx2x4(st);
      AT = 0;
    }

    // continue by processing full blocks
    ITERS = LEN / RATE8; // number of full blocks
    i = 0;
    while ( i < ITERS ) {
      st, _, offset = __addstate_bcast_array_avx2x4(st, 0, buf, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i += 1;
    }

    // last incomplete block
    LEN = ALL % RATE8;
    st, AT, offset = __addstate_bcast_array_avx2x4(st, 0, buf, offset, LEN, TRAILB);
    if (TRAILB!=0) { st = __addratebit_avx2x4(st, RATE8); }
  }
  return st, AT, offset;
} 

/*
   INCREMENTAL (FIXED-SIZE) MEMORY 4-way ABSORB
   ============================================
*/

inline fn __addstate_array_avx2x4
( reg mut ptr u256[25] st
, inline int AT /* bytes (0 <= AT < 200) */
, reg const ptr u8[ASIZE] buf0 buf1 buf2 buf3
, reg u64 offset
, inline int LEN
, inline int TRAILB
) -> reg ptr u256[25] /* st */
   , inline int /* AT */
   , reg u64 /* offset */
{
  inline int DELTA, LO, ALL;
  reg u64 at, t0, t1, t2, t3;
  reg u256 t256_0, t256_1, t256_2, t256_3;

  ALL = AT+LEN; // total bytes to process (excluding trail byte, if !=0)
  LO = AT % 8; // leftover bytes
  at = 4 * (AT / 8); // current pstate position (referencing u64 words)
//at = 0, 4, 8, ...
  DELTA = 0;

  if ( 0 < LO ) { // process first word...
    if ( LO+LEN < 8) { // ...not enough to fill a word (just update it)
      if ( TRAILB != 0 ) { ALL += 1; }
      _, _, _, t0 = __aread_subu64(buf0, offset, DELTA, LEN, TRAILB);
      _, _, _, t1 = __aread_subu64(buf1, offset, DELTA, LEN, TRAILB);
      _, _, _, t2 = __aread_subu64(buf2, offset, DELTA, LEN, TRAILB);
      DELTA, _, _, t3 = __aread_subu64(buf3, offset, DELTA, LEN, TRAILB);
      t0 <<= 8*LO;
      t0 ^= st[u64 at + 0];
      st[u64 at + 0] = t0;
      t1 <<= 8*LO;
      t1 ^= st[u64 at + 1];
      st[u64 at + 1] = t1;
      t2 <<= 8*LO;
      t2 ^= st[u64 at + 2];
      st[u64 at + 2] = t2;
      t3 <<= 8*LO;
      t3 ^= st[u64 at + 3];
      st[u64 at + 3] = t3;
      LO = 0;
      AT = 0;
      LEN = 0;
      TRAILB = 0;
    } else { // process first word
      if ( 8 <= LEN ) {
	t0 = buf0.[u64 offset + DELTA];
	t1 = buf1.[u64 offset + DELTA];
	t2 = buf2.[u64 offset + DELTA];
	t3 = buf3.[u64 offset + DELTA];
	offset += 8-LO;
      } else {
        _, _, _, t0 = __aread_subu64(buf0, offset, DELTA, 8-LO, TRAILB);
        _, _, _, t1 = __aread_subu64(buf1, offset, DELTA, 8-LO, TRAILB);
        _, _, _, t2 = __aread_subu64(buf2, offset, DELTA, 8-LO, TRAILB);
        DELTA, _, _, t3 = __aread_subu64(buf3, offset, DELTA, 8-LO, TRAILB);
      }
      LEN -= 8-LO;
      AT += 8-LO;
      t0 <<= 8*LO;
      t0 ^= st[u64 at + 0];
      st[u64 at + 0] = t0;
      t1 <<= 8*LO;
      t1 ^= st[u64 at + 1];
      st[u64 at + 1] = t1;
      t2 <<= 8*LO;
      t2 ^= st[u64 at + 2];
      st[u64 at + 2] = t2;
      t3 <<= 8*LO;
      t3 ^= st[u64 at + 3];
      st[u64 at + 3] = t3;
      at += 4;
    }
  }
  offset += DELTA;
  DELTA = 0;
  // continue processing remaining bytes
  if (8 <= LEN) {
    while ( at < 4*(AT/8)+32*(LEN/32) ) {
      t256_0 = buf0.[u256 offset];
      t256_1 = buf1.[u256 offset];
      t256_2 = buf0.[u256 offset];
      t256_3 = buf0.[u256 offset];
      offset += 32;
      t256_0, t256_1, t256_2, t256_3 = __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      st.[u256 8*at] = t256_0;
      st.[u256 8*at+32] = t256_1;
      st.[u256 8*at+64] = t256_2;
      st.[u256 8*at+96] = t256_3;
      at += 32;
    }
    while ( at < 4*(AT/8)+4*(LEN/8)) {
      t0 = buf0.[u64 offset];
      t0 ^= st[u64 at + 0];
      st[u64 at + 0] = t0;
      t1 = buf1.[u64 offset];
      t1 ^= st[u64 at + 1];
      st[u64 at + 1] = t1;
      t2 = buf2.[u64 offset];
      t2 ^= st[u64 at + 2];
      st[u64 at + 2] = t2;
      t3 = buf3.[u64 offset];
      offset += 8;
      t3 ^= st[u64 at + 3];
      st[u64 at + 3] = t3;
      at += 4;
    }
    LEN = (AT+LEN) % 8;
  }

  // process last word (possibly closing the state)
  LO = (AT+LEN) % 8;
  if ( 0 < LO || TRAILB != 0 ) {
    _, _, _, t0 = __aread_subu64(buf0, offset, DELTA, LO, TRAILB);
    _, _, _, t1 = __aread_subu64(buf1, offset, DELTA, LO, TRAILB);
    _, _, _, t2 = __aread_subu64(buf2, offset, DELTA, LO, TRAILB);
    DELTA, _, _, t3 = __aread_subu64(buf3, offset, DELTA, LO, TRAILB);
    offset += DELTA;
    if ( TRAILB != 0 ) { ALL += 1; TRAILB = 0; }
    t0 ^= st[u64 at + 0];
    st[u64 at + 0] = t0;
    t1 ^= st[u64 at + 1];
    st[u64 at + 1] = t1;
    t2 ^= st[u64 at + 2];
    st[u64 at + 2] = t2;
    t3 ^= st[u64 at + 3];
    st[u64 at + 3] = t3;
  }
    
  return st, ALL, offset;
} 


inline fn __absorb_array_avx2x4
( reg mut ptr u256[25] st
, inline int AT
, reg const ptr u8[ASIZE] buf0 buf1 buf2 buf3
, reg u64 offset
, inline int LEN
, inline int RATE8
, inline int TRAILB /* closes state if !=0 (i.e. adds trailbyte and padding) */
) -> reg ptr u256[25] /* st */
   , inline int /* AT */
   , reg u64 /* offset */
{
  reg u64 i;
  inline int ALL, ITERS;

  ALL = AT + LEN;
  if ( (AT+LEN) < RATE8 ) { // not enough to fill a block!
    st, AT, offset
      = __addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, LEN, TRAILB);
    if (TRAILB != 0) { // add pstate and closes the state
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else { // at least a block is filled
    if ( AT != 0 ) { // start by filling the first block
      st, _, offset
        = __addstate_array_avx2x4(st, AT, buf0, buf1, buf2, buf3, offset, RATE8-AT, 0);
      LEN = LEN - (RATE8-AT);
      st = _keccakf1600_avx2x4(st);
      AT = 0;
    }

    // continue by processing full blocks
    ITERS = LEN / RATE8; // number of full blocks
    i = 0;
    while ( i < ITERS ) {
      st, _, offset
        = __addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i += 1;
    }

    // last incomplete block
    LEN = ALL % RATE8;
    st, AT, offset
      = __addstate_array_avx2x4(st, 0, buf0, buf1, buf2, buf3, offset, LEN, TRAILB);
    if (TRAILB!=0) { st = __addratebit_avx2x4(st, RATE8); }
  }
  return st, AT, offset;
} 


/*
   ONE-SHOT (FIXED-SIZE) MEMORY SQUEEZE
   ====================================
*/
inline fn __dumpstate_array_avx2x4
( reg mut ptr u8[ASIZE] buf0 buf1 buf2 buf3
, reg u64 offset
, inline int LEN
, reg const ptr u256[25] st
) -> reg ptr u8[ASIZE] /* buf0 */
   , reg ptr u8[ASIZE] /* buf1 */
   , reg ptr u8[ASIZE] /* buf2 */
   , reg ptr u8[ASIZE] /* buf3 */
   , reg u64 /* offset */
{ 
  reg u256 x0, x1, x2, x3;
  reg u64 i, t0, t1, t2, t3;
  i = 0;
  while (i <s 32*(LEN/32)) {
    x0 = st.[u256 4*i+0*32];
    x1 = st.[u256 4*i+1*32];
    x2 = st.[u256 4*i+2*32];
    x3 = st.[u256 4*i+3*32];
    i += 32;
    x0, x1, x2, x3 = __4u64x4_u256x4(x0, x1, x2, x3);
    buf0.[u256 offset] = x0;
    buf1.[u256 offset] = x1;
    buf2.[u256 offset] = x2;
    buf3.[u256 offset] = x3;
    offset +=32;
  } // 0 32 (64) 
  while (i <s 8*(LEN/8)) {
    t0 = st.[u64 4*i+0*8];
    buf0.[u64 offset] = t0;
    t1 = st.[u64 4*i+1*8];
    buf1.[u64 offset] = t1;
    t2 = st.[u64 4*i+2*8];
    buf2.[u64 offset] = t2;
    t3 = st.[u64 4*i+3*8];
    buf3.[u64 offset] = t3;
    i += 8;
    offset += 8;
  }
  if (0 < LEN%8) {
    t0 = st.[u64 4*i+0*8];
    buf0, _, _ = __awrite_subu64( buf0, offset, 0, LEN%8, t0);
    t1 = st.[u64 4*i+1*8];
    buf1, _, _ = __awrite_subu64( buf1, offset, 0, LEN%8, t1);
    t2 = st.[u64 4*i+2*8];
    buf2, _, _ = __awrite_subu64( buf2, offset, 0, LEN%8, t2);
    t3 = st.[u64 4*i+3*8];
    buf3, _, _ = __awrite_subu64( buf3, offset, 0, LEN%8, t3);
    offset += LEN%8;
  }

  return buf0, buf1, buf2, buf3, offset;
}

inline fn __squeeze_array_avx2x4
( reg mut ptr u8[ASIZE] buf0 buf1 buf2 buf3
, reg u64 offset
, inline int LEN
, reg mut ptr u256[25] st
, inline int RATE8
) -> reg ptr u8[ASIZE] /* buf0 */
   , reg ptr u8[ASIZE] /* buf1 */
   , reg ptr u8[ASIZE] /* buf2 */
   , reg ptr u8[ASIZE] /* buf3 */
   , reg u64           /* offset */
   , reg ptr u256[25] /* st */
{
  reg u64 i;
  inline int ITERS, LO;
  ITERS = LEN/RATE8;
  LO = LEN%RATE8;

  if (0 <s LEN) {
    if (0 < ITERS) {
      i = 0;
      while (i < ITERS) {
        st = _keccakf1600_avx2x4(st);
        buf0, buf1, buf2, buf3, offset
          = __dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, RATE8, st);
        i += 1;
      }
    }
    if (0 < LO) {
        st = _keccakf1600_avx2x4(st);
        buf0, buf1, buf2, buf3, offset
          = __dumpstate_array_avx2x4(buf0, buf1, buf2, buf3, offset, LO, st);
    }
  }

  return buf0, buf1, buf2, buf3, offset, st;
}

