require "params.jinc"
require "shuffle.jinc"
require "consts.jinc"
require "reduce.jinc"
require "mlkem_keccak_avx2.jinc"

/*
_poly_add2: add arrays pointwise

requires:
  - elements in rp input to be in the range [-q*A...q*A) for 0<=A<=6
  - elements in bp input to be in the range [-q*B...q*B) for 0<=B<=3
ensures:
  - each entry in rp represents the Zq element that results from summing
    the Zq elements represented by the corresponding rp and bp entries on
     input
  - each entry in rp is in the range [-q*(A+B)...q*(A+B))
*/

fn _poly_add2(reg mut ptr u16[MLKEM_N] rp, reg const ptr u16[MLKEM_N] bp) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 a;
  reg u256 b;
  reg u256 r;

  for i = 0 to 16 {
    a = rp.[:u256 32*i];
    b = bp.[:u256 32*i];
    r = #VPADD_16u16(a, b);
    rp.[:u256 32*i] = r;
  }

  return rp;
}

/*
_poly_csubq: reduce from range [0..2q) to range [0..q) by conditional subtraction
         do this for each element in the input array

requires:
  - each 16-bit element in the input array is in the range [0..2q)
ensures:
  - every 16-bit element in the output array is in the range [0..q)
  - every 16-bit element in the output array encodes the same element in Zq
    as the corresponding element in the input array
*/

fn _poly_csubq(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u256 r qx16;
  inline int i;

  qx16 = jqx16[:u256 0];

  for i = 0 to 16 {
    r = rp.[:u256 32*i];
    r = __csubq(r, qx16);
    rp.[:u256 32*i] = r;
  }

  return rp;
}

/* __w256_interleave_u16
  joins 32-bit values whose low and high halves
  are stored in al and ah respectively
   a0 -> [0,1,2,3,8,9,10,11]
   a1 ->[4,5,6,7,12,13,14,15]
  */
inline
fn __w256_interleave_u16(reg u256 al ah) -> reg u256, reg u256 {
  reg u256 a0 a1;

  a0  = #VPUNPCKL_16u16(al, ah);
  a1  = #VPUNPCKH_16u16(al, ah);

  return a0, a1;
}

/* __w256_deinterleave_u16
  inverts the __w256_interleave_u16 operation
  */

inline
fn __w256_deinterleave_u16(reg u256 _zero a0 a1) -> reg u256, reg u256 {
  reg u256 al ah;

  al = #VPBLEND_16u16(a0,_zero,0xAA);
  ah = #VPBLEND_16u16(a1,_zero,0xAA);
  al = #VPACKUS_8u32(al, ah);
  a0 = #VPSRL_8u32(a0,16);
  a1 = #VPSRL_8u32(a1,16);
  ah = #VPACKUS_8u32(a0, a1);

  return al, ah;
}

/* __mont_red: montgomery reduction step with R = 2^16
  gets 32-bit values in de-interleaved form, each value
  in the range [-88657928,88657928).
  Requires qx16 to contain 16 copies of q
  Requires qinvx16 to contain 16 copies of 62209
  returns each input value multiplied by R^-1 % q
  and reduced to the range [-q,q)
*/

inline
fn __mont_red(reg u256 lo hi qx16 qinvx16) -> reg u256 {
  reg u256 m;

  m  = #VPMULL_16u16(lo, qinvx16);
  m  = #VPMULH_16u16(m, qx16);
  lo = #VPSUB_16u16(hi, m);

  return lo;
}

/* Wide multiplication of two 16x16 bit registers, resulting
   in 16 32-bit values. Packed as

   xy0 -> [0,1,2,3,8,9,10,11]
   xy1 ->[4,5,6,7,12,13,14,15] */
inline
fn __wmul_16u16(reg u256 x y) -> reg u256, reg u256 {
  reg u256 xyL xyH xy0 xy1;
  xyL = #VPMULL_16u16(x, y);
  xyH = #VPMULH_16u16(x, y);
  xy0, xy1 = __w256_interleave_u16(xyL, xyH);

  return xy0, xy1;
}

/*
__schoolbook16x: complex multiplication are*bre+(-1)^sign*zeta*aim*bim of 16x2 zp x zp values
requires:
     are,aim: real/imag parts of a in the range [-2q,2q)
     bre,bim: real/imag parts of b in the range [-2q,2q)
     zeta contains 16 zetas in the range [0,q)
     zetaqinv constains zetas x qinv
     qx16 containts 16x q
     qinvx16 contains 16x qinv
ensures:
     output coefficients are in the range [-q,q)
     they represent the coefficients of complex multiplication with an extra montgomery
     factor R^-1 % q.
*/

inline
fn __schoolbook16x(reg u256 are aim bre bim zeta zetaqinv qx16 qinvx16, inline int sign) -> reg u256, reg u256
{ reg u256 zaim ac0 ac1 zbd0 zbd1 ad0 ad1 bc0 bc1 x0 x1 y0 y1 _zero;

  zaim = __fqmulprecomp16x(aim, zetaqinv, zeta, qx16);

  ac0, ac1 = __wmul_16u16(are, bre);
  ad0, ad1 = __wmul_16u16(are, bim);
  bc0, bc1 = __wmul_16u16(aim, bre);
  zbd0, zbd1 = __wmul_16u16(zaim, bim);

  if (sign == 0) {
    x0 = #VPADD_8u32(ac0, zbd0);
    x1 = #VPADD_8u32(ac1, zbd1);
  } else {
    x0 = #VPSUB_8u32(ac0, zbd0);
    x1 = #VPSUB_8u32(ac1, zbd1);
  }
  y0 = #VPADD_8u32(bc0, ad0);
  y1 = #VPADD_8u32(bc1, ad1);

  _zero = #set0_256();
  x0, x1 = __w256_deinterleave_u16(_zero, x0, x1);
  y0, y1 = __w256_deinterleave_u16(_zero, y0, y1);
  x0 = __mont_red(x0, x1, qx16, qinvx16);
  y0 = __mont_red(y0, y1, qx16, qinvx16);
  return x0, y0;
}

/*
_poly_basemul: complex multiplication are*bre+zeta*aim*bim of 128x2 zp x zp values
requires:
     ap: 8*(16x16-bit real,16x16-bit imag), all in the range [-2q,2q)
     bp: 8*(16x16-bit real,16x16-bit imag), all in the range [-2q,2q)
ensures:
     output coefficients are in the range [-q,q)
     they represent the coefficients of complex multiplication with an extra montgomery
     factor R^-1 % q.
*/

fn _poly_basemul(reg ptr u16[MLKEM_N] rp ap bp) -> reg ptr u16[MLKEM_N]
{
  reg u256 zeta zetaqinv qx16 qinvx16 are aim bre bim;

  qx16    = jqx16.[:u256 0];
  qinvx16 = jqinvx16.[:u256 0];

  zetaqinv = jzetas_exp.[:u256 272];
  zeta = jzetas_exp.[:u256 304];

  are = ap.[:u256 32*0];
  aim = ap.[:u256 32*1];
  bre = bp.[:u256 32*0];
  bim = bp.[:u256 32*1];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[:u256 32*0] = are;
  rp.[:u256 32*1] = aim;

  are = ap.[:u256 32*2];
  aim = ap.[:u256 32*3];
  bre = bp.[:u256 32*2];
  bim = bp.[:u256 32*3];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[:u256 32*2] = are;
  rp.[:u256 32*3] = aim;

  zetaqinv = jzetas_exp.[:u256 336];
  zeta = jzetas_exp.[:u256 368];

  are = ap.[:u256 32*4];
  aim = ap.[:u256 32*5];
  bre = bp.[:u256 32*4];
  bim = bp.[:u256 32*5];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[:u256 32*4] = are;
  rp.[:u256 32*5] = aim;

  are = ap.[:u256 32*6];
  aim = ap.[:u256 32*7];
  bre = bp.[:u256 32*6];
  bim = bp.[:u256 32*7];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[:u256 32*6] = are;
  rp.[:u256 32*7] = aim;

  zetaqinv = jzetas_exp.[:u256 664];
  zeta = jzetas_exp.[:u256 696];

  are = ap.[:u256 32*8];
  aim = ap.[:u256 32*9];
  bre = bp.[:u256 32*8];
  bim = bp.[:u256 32*9];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[:u256 32*8] = are;
  rp.[:u256 32*9] = aim;

  are = ap.[:u256 32*10];
  aim = ap.[:u256 32*11];
  bre = bp.[:u256 32*10];
  bim = bp.[:u256 32*11];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[:u256 32*10] = are;
  rp.[:u256 32*11] = aim;

  zetaqinv = jzetas_exp.[:u256 728];
  zeta = jzetas_exp.[:u256 760];

  are = ap.[:u256 32*12];
  aim = ap.[:u256 32*13];
  bre = bp.[:u256 32*12];
  bim = bp.[:u256 32*13];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[:u256 32*12] = are;
  rp.[:u256 32*13] = aim;

  are = ap.[:u256 32*14];
  aim = ap.[:u256 32*15];
  bre = bp.[:u256 32*14];
  bim = bp.[:u256 32*15];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[:u256 32*14] = are;
  rp.[:u256 32*15] = aim;

  return rp;
}

/*
_poly_frombytes: unpacks a polynomial encoded as byte array (12bits per coefficient)
                 and permutes it to be in avx order where the permutation is given by

  [0; 16; 32; 48; 64; 80; 96; 112; 1; 17; 33; 49; 65; 81; 97; 113;
   2; 18; 34; 50; 66; 82; 98; 114; 3; 19; 35; 51; 67; 83; 99; 115;
   4; 20; 36; 52; 68; 84; 100; 116; 5; 21; 37; 53; 69; 85; 101; 117;
   6; 22; 38; 54; 70; 86; 102; 118; 7; 23; 39; 55; 71; 87; 103; 119;
   8; 24; 40; 56; 72; 88; 104; 120; 9; 25; 41; 57; 73; 89; 105; 121;
   10; 26; 42; 58; 74; 90; 106; 122; 11; 27; 43; 59; 75; 91; 107; 123;
   12; 28; 44; 60; 76; 92; 108; 124; 13; 29; 45; 61; 77; 93; 109; 125;
   14; 30; 46; 62; 78; 94; 110; 126; 15; 31; 47; 63; 79; 95; 111; 127;
   128; 144; 160; 176; 192; 208; 224; 240; 129; 145; 161; 177; 193; 209; 225; 241;
   130; 146; 162; 178; 194; 210; 226; 242; 131; 147; 163; 179; 195; 211; 227; 243;
   132; 148; 164; 180; 196; 212; 228; 244; 133; 149; 165; 181; 197; 213; 229; 245;
   134; 150; 166; 182; 198; 214; 230; 246; 135; 151; 167; 183; 199; 215; 231; 247;
   136; 152; 168; 184; 200; 216; 232; 248; 137; 153; 169; 185; 201; 217; 233; 249;
   138; 154; 170; 186; 202; 218; 234; 250; 139; 155; 171; 187; 203; 219; 235; 251;
   140; 156; 172; 188; 204; 220; 236; 252; 141; 157; 173; 189; 205; 221; 237; 253;
   142; 158; 174; 190; 206; 222; 238; 254; 143; 159; 175; 191; 207; 223; 239; 255].


requires: ap to point to 384 byte valid region

ensures:
    memory unchanged (writes to stack)
    every output coefficient is in the range [0..2*q)

*/

fn _poly_frombytes(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 tt mask;
  reg ptr u16[16] maskp;

  maskp = maskx16;
  mask = maskp[:u256 0];

  for i = 0 to 2
  {
    t0 = [:u256 ap + 192*i];
    t1 = [:u256 ap + 192*i + 32];
    t2 = [:u256 ap + 192*i + 64];
    t3 = [:u256 ap + 192*i + 96];
    t4 = [:u256 ap + 192*i + 128];
    t5 = [:u256 ap + 192*i + 160];

    tt, t3 = __shuffle8(t0, t3);
    t0, t4 = __shuffle8(t1, t4);
    t1, t5 = __shuffle8(t2, t5);

    t2, t4 = __shuffle4(tt, t4);
    tt, t1 = __shuffle4(t3, t1);
    t3, t5 = __shuffle4(t0, t5);

    t0, t1 = __shuffle2(t2, t1);
    t2, t3 = __shuffle2(t4, t3);
    t4, t5 = __shuffle2(tt, t5);

    t6, t3 = __shuffle1(t0, t3);
    t0, t4 = __shuffle1(t1, t4);
    t1, t5 = __shuffle1(t2, t5);

    t7 = #VPSRL_16u16(t6, 12);
    t8 = #VPSLL_16u16(t3, 4);
    t7 = #VPOR_256(t7, t8);
    t6 = #VPAND_256(mask, t6);
    t7 = #VPAND_256(mask, t7);

    t8 = #VPSRL_16u16(t3, 8);
    t9 = #VPSLL_16u16(t0, 8);
    t8 = #VPOR_256(t8,t9);
    t8 = #VPAND_256(mask,t8);

    t9 = #VPSRL_16u16(t0, 4);
    t9 = #VPAND_256(mask, t9);

    t10 = #VPSRL_16u16(t4, 12);
    t11 = #VPSLL_16u16(t1, 4);
    t10 = #VPOR_256(t10, t11);
    t4 = #VPAND_256(mask,t4);
    t10 = #VPAND_256(mask, t10);

    t11 = #VPSRL_16u16(t1, 8);
    tt = #VPSLL_16u16(t5, 8);
    t11 = #VPOR_256(t11, tt);
    t11 = #VPAND_256(mask, t11);

    tt = #VPSRL_16u16(t5, 4);
    tt = #VPAND_256(mask, tt);

    rp[:u256 8*i] = t6;
    rp[:u256 8*i + 1] = t7;
    rp[:u256 8*i + 2] = t8;
    rp[:u256 8*i + 3] = t9;
    rp[:u256 8*i + 4] = t4;
    rp[:u256 8*i + 5] = t10;
    rp[:u256 8*i + 6] = t11;
    rp[:u256 8*i + 7] = tt;
  }

  return rp;
}


/*
_poly_frommont: takes a polynomial where all
   coefficients are affected by an R^-1 Montgomery factor mod q
   and removes the R^-1 factor by performing (signed) Montgomery
   multiplication with R^2 modulo q: one of the Rs cancels the
   existing R^-1 term, and the second one cancels the one introduced
   by montgomery Multiplication.

requires: no requirements

ensures:
    memory unchanged (writes to stack)
    every output coefficient is in the range [-q..q)

*/

param int DMONT   = 1353;      /* (1ULL << 32) % MLKEM_Q */

fn _poly_frommont(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u256 t qx16 qinvx16 dmontx16;
  inline int i;
  reg ptr u16[16] x16p;

  x16p = jqx16;
  qx16 = x16p[:u256 0];
  x16p = jqinvx16;
  qinvx16 = x16p[:u256 0];
  x16p = jdmontx16;
  dmontx16 = x16p[:u256 0];

  for i = 0 to MLKEM_N/16
  {
    t = rp[:u256 i];
    t = __fqmulx16(t, dmontx16, qx16, qinvx16);
    rp[:u256 i] = t;
  }

  return rp;
}

/*
_poly_frommsg_1 : unpacks a polynomial encoded as byte array (1bit per coefficient)

requires: no requires

ensures:
    memory unchanged (writes to stack)
    every output coefficient is in the range [0..q)

*/


u32[4] pfm_shift_s = {3, 2, 1, 0};
u8[16] pfm_idx_s = {0, 1, 4, 5, 8, 9, 12, 13,
                    2, 3, 6, 7, 10, 11, 14, 15};

fn _poly_frommsg_1(reg mut ptr u16[MLKEM_N] rp, reg const ptr u8[32] ap) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 f g0 g1 g2 g3 h0 h1 h2 h3;
  reg u256 shift idx hqs;
  reg ptr u16[16] x16p;

  x16p = hqx16_p1;
  hqs = x16p[:u256 0];
  shift = #VPBROADCAST_2u128(pfm_shift_s[:u128 0]);
  idx = #VPBROADCAST_2u128(pfm_idx_s[:u128 0]);

  f = ap[:u256 0];

  for i = 0 to 4
  {
    g3 = #VPSHUFD_256(f, 0x55*i);
    g3 = #VPSLLV_8u32(g3, shift);
    g3 = #VPSHUFB_256(g3, idx);
    g0 = #VPSLL_16u16(g3,12);
    g1 = #VPSLL_16u16(g3,8);
    g2 = #VPSLL_16u16(g3,4);
    g0 = #VPSRA_16u16(g0,15);
    g1 = #VPSRA_16u16(g1,15);
    g2 = #VPSRA_16u16(g2,15);
    g3 = #VPSRA_16u16(g3,15);
    g0 = #VPAND_256(g0,hqs);
    g1 = #VPAND_256(g1,hqs);
    g2 = #VPAND_256(g2,hqs);
    g3 = #VPAND_256(g3,hqs);
    h0 = #VPUNPCKL_4u64(g0,g1);
    h2 = #VPUNPCKH_4u64(g0,g1);
    h1 = #VPUNPCKL_4u64(g2,g3);
    h3 = #VPUNPCKH_4u64(g2,g3);
    g0 = #VPERM2I128(h0,h1,0x20);
    g2 = #VPERM2I128(h0,h1,0x31);
    g1 = #VPERM2I128(h2,h3,0x20);
    g3 = #VPERM2I128(h2,h3,0x31);
    rp[:u256 2*i] = g0;
    rp[:u256 2*i + 1] = g1;
    rp[:u256 2*i + 8] = g2;
    rp[:u256 2*i + 8 + 1] = g3;
  }

  return rp;
}

/*
__cbd2 generates a polynomial with binomial noise with eta=2.


requires: no requires

ensures: output coefficient j is computed as
       buf[j/2][j%2*4]+buf[j/2][j%2*4+1] -
           (buf[j/2][j%2*4+2]+buf[j/2][j%2*4+3]).
*/
inline
fn __cbd2(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA2*MLKEM_N/4] buf) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3;
  reg u256 mask55 mask33 mask03 mask0F;
  reg u128 t;
  stack u32 mask55_s mask33_s mask03_s mask0F_s;

  mask55_s = 0x55555555;
  mask33_s = 0x33333333;
  mask03_s = 0x03030303;
  mask0F_s = 0x0F0F0F0F;

  mask55 = #VPBROADCAST_8u32(mask55_s);
  mask33 = #VPBROADCAST_8u32(mask33_s);
  mask03 = #VPBROADCAST_8u32(mask03_s);
  mask0F = #VPBROADCAST_8u32(mask0F_s);

  for i = 0 to MLKEM_N/64
  {
    f0 = buf[:u256 i];

    f1 = #VPSRL_16u16(f0, 1);
    f0 = #VPAND_256(mask55, f0);
    f1 = #VPAND_256(mask55, f1);
    f0 = #VPADD_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 2);
    f0 = #VPAND_256(mask33, f0);
    f1 = #VPAND_256(mask33, f1);
    f0 = #VPADD_32u8(f0, mask33);
    f0 = #VPSUB_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 4);
    f0 = #VPAND_256(mask0F, f0);
    f1 = #VPAND_256(mask0F, f1);
    f0 = #VPSUB_32u8(f0, mask03);
    f1 = #VPSUB_32u8(f1, mask03);

    f2 = #VPUNPCKL_32u8(f0, f1);
    f3 = #VPUNPCKH_32u8(f0, f1);

    t = (128u)f2;
    f0 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f2, 1);
    f1 = #VPMOVSX_16u8_16u16(t);
    t = (128u)f3;
    f2 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f3, 1);
    f3 = #VPMOVSX_16u8_16u16(t);
    rp[:u256 4*i] = f0;
    rp[:u256 4*i + 1] = f2;
    rp[:u256 4*i + 2] = f1;
    rp[:u256 4*i + 3] = f3;
  }

  return rp;
}

/* In MLKEM 768 and 1024 eta1 and eta2 is cbd2 */
inline
fn __poly_cbd_eta1(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8] buf) -> reg ptr u16[MLKEM_N]
{
  rp = __cbd2(rp, buf[0:MLKEM_ETA2*MLKEM_N/4]);
  return rp;
}

fn _poly_getnoise_eta2(#spill_to_mmx reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_SYMBYTES] seed, reg u8 nonce) -> reg ptr u16[MLKEM_N]
{
  stack u8[MLKEM_ETA2*MLKEM_N/4] buf;

  () = #spill(rp);

  stack u8[1] nonce_s;
  nonce_s[0] = nonce;
  buf = _shake256_A128__A32_A1(buf, seed, nonce_s);

  () = #unspill(rp);
  rp =__poly_cbd_eta1(rp, buf);

  return rp;
}

/* _poly_getnoise_eta1_4x: computes 4x the binomial noise,
   where only the shake computation is parallelized. */

#[returnaddress="stack"]
fn _poly_getnoise_eta1_4x
( reg ptr u16[MLKEM_N] r0 r1 r2 r3
, reg ptr u8[MLKEM_SYMBYTES] seed
, reg u8 nonce
) -> reg ptr u16[MLKEM_N]
   , reg ptr u16[MLKEM_N]
   , reg ptr u16[MLKEM_N]
   , reg ptr u16[MLKEM_N]
{
  stack u8[128] buf0_s buf1_s buf2_s buf3_s;
  stack u8[4] nonces;
  reg ptr u8[128] buf0, buf1, buf2, buf3;

  buf0 = buf0_s; buf1 = buf1_s; buf2 = buf2_s; buf3 = buf3_s;

() = #spill(r0,r1,r2,r3);

  nonces[0] = nonce;
  nonce += 1;
  nonces[1] = nonce;
  nonce += 1;
  nonces[2] = nonce;
  nonce += 1;
  nonces[3] = nonce;


  buf0, buf1, buf2, buf3 = _shake256x4_A128__A32_A1(buf0, buf1, buf2, buf3, seed, nonces);

() = #unspill(r0,r1,r2,r3);
  r0 = __poly_cbd_eta1(r0, buf0);
  r1 = __poly_cbd_eta1(r1, buf1);
  r2 = __poly_cbd_eta1(r2, buf2);
  r3 = __poly_cbd_eta1(r3, buf3);

  return r0, r1, r2, r3;
}

/* __invntt___butterfly64x: computes 64 inverse butterflies in parallel.
    Inputs which come in as 2(h vs l) x 64 = 2 x 4(0..3) x 16(simd)
    16-bit words. The result on each lane is:

       rl[j][i] = rl[j][i] + rh[j][i]
       rh[j][i] = z*(rl[j][i] - rh[j][i])

    zs come represented as zl and zh, where zh contains zl*qinv
    precomputed for montgomery multiplication.
    See contract for __fqmulprecomp16x in reduce.jinc for a
    a more detailed explanation of input ranges and output ranges. */

inline
fn __invntt___butterfly64x(reg u256 rl0 rl1 rl2 rl3 rh0 rh1 rh2 rh3 zl0 zl1 zh0 zh1 qx16)
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  reg u256 t0 t1 t2 t3;

  t0  = #VPSUB_16u16(rl0, rh0);
  t1  = #VPSUB_16u16(rl1, rh1);
  t2  = #VPSUB_16u16(rl2, rh2);

  rl0 = #VPADD_16u16(rh0, rl0);
  rl1 = #VPADD_16u16(rh1, rl1);
  rh0 = #VPMULL_16u16(zl0, t0);

  rl2 = #VPADD_16u16(rh2, rl2);
  rh1 = #VPMULL_16u16(zl0, t1);
  t3  = #VPSUB_16u16(rl3, rh3);

  rl3 = #VPADD_16u16(rh3, rl3);
  rh2 = #VPMULL_16u16(zl1, t2);
  rh3 = #VPMULL_16u16(zl1, t3);

  t0  = #VPMULH_16u16(zh0, t0);
  t1  = #VPMULH_16u16(zh0, t1);

  t2  = #VPMULH_16u16(zh1, t2);
  t3  = #VPMULH_16u16(zh1, t3);

  // Reduce
  rh0  = #VPMULH_16u16(qx16, rh0);
  rh1  = #VPMULH_16u16(qx16, rh1);
  rh2  = #VPMULH_16u16(qx16, rh2);
  rh3  = #VPMULH_16u16(qx16, rh3);

  rh0  = #VPSUB_16u16(t0, rh0);
  rh1  = #VPSUB_16u16(t1, rh1);
  rh2  = #VPSUB_16u16(t2, rh2);
  rh3  = #VPSUB_16u16(t3, rh3);

  return rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3;
}

/*
_poly_invntt computes the inverse NTT over an input polynomial
that is given with coefficients in AVX order, and produces
the output with coefficients in the logical order.

Input range accepted is [-4q,4q)
Output range is (-q,q)

Introduces a multiplicative scalar of R mod q in all coefficients,
which cancels a lingering R^-1 mod q factor that remains from
montgomery multiplications carried out in basemul.

*/

fn _poly_invntt(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u256 zeta0 zeta1 zeta2 zeta3 r0 r1 r2 r3 r4 r5 r6 r7 qx16 vx16 flox16 fhix16;
  reg ptr u16[400] zetasp;
  inline int i;

  zetasp = jzetas_inv_exp;
  qx16 = jqx16[:u256 0];

  for i = 0 to 2
  {
    // level 0:
    zeta0 = zetasp.[:u256 0+392*i];
    zeta1 = zetasp.[:u256 64+392*i];
    zeta2 = zetasp.[:u256 32+392*i];
    zeta3 = zetasp.[:u256 96+392*i];

    r0 = rp.[:u256 32*0+256*i];
    r1 = rp.[:u256 32*1+256*i];
    r2 = rp.[:u256 32*2+256*i];
    r3 = rp.[:u256 32*3+256*i];
    r4 = rp.[:u256 32*4+256*i];
    r5 = rp.[:u256 32*5+256*i];
    r6 = rp.[:u256 32*6+256*i];
    r7 = rp.[:u256 32*7+256*i];

    r0, r1, r4, r5, r2, r3, r6, r7 = __invntt___butterfly64x(r0, r1, r4, r5, r2, r3, r6, r7, zeta0, zeta1, zeta2, zeta3, qx16);

    // level 1:
    vx16 = jvx16[:u256 0];
    zeta0 = zetasp.[:u256 128+392*i];
    zeta1 = zetasp.[:u256 160+392*i];
    r0 = __red16x(r0, qx16, vx16);
    r1 = __red16x(r1, qx16, vx16);
    r4 = __red16x(r4, qx16, vx16);
    r5 = __red16x(r5, qx16, vx16);

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0, r1 = __shuffle1(r0, r1);
    r2, r3 = __shuffle1(r2, r3);
    r4, r5 = __shuffle1(r4, r5);
    r6, r7 = __shuffle1(r6, r7);

    // level 2:
    zeta0 = zetasp.[:u256 192+392*i];
    zeta1 = zetasp.[:u256 224+392*i];


    r0, r2, r4, r6, r1, r3, r5, r7 = __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r2 = __shuffle2(r0, r2);
    r4, r6 = __shuffle2(r4, r6);
    r1, r3 = __shuffle2(r1, r3);
    r5, r7 = __shuffle2(r5, r7);

    // level 3:
    zeta0 = zetasp.[:u256 256+392*i];
    zeta1 = zetasp.[:u256 288+392*i];

    r0, r4, r1, r5, r2, r6, r3, r7 = __invntt___butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r4 = __shuffle4(r0, r4);
    r1, r5 = __shuffle4(r1, r5);
    r2, r6 = __shuffle4(r2, r6);
    r3, r7 = __shuffle4(r3, r7);

    // level 4:
    zeta0 = zetasp.[:u256 320+392*i];
    zeta1 = zetasp.[:u256 352+392*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r1 = __shuffle8(r0, r1);
    r2, r3 = __shuffle8(r2, r3);
    r4, r5 = __shuffle8(r4, r5);
    r6, r7 = __shuffle8(r6, r7);

    // level 5:
    zeta0 = #VPBROADCAST_8u32(zetasp.[:u32 384+392*i]);
    zeta1 = #VPBROADCAST_8u32(zetasp.[:u32 388+392*i]);

    r0, r2, r4, r6, r1, r3, r5, r7 = __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    if (i==0) {
      rp.[:u256 32*0+256*i] = r0;
      rp.[:u256 32*1+256*i] = r2;
      rp.[:u256 32*2+256*i] = r4;
      rp.[:u256 32*3+256*i] = r6;
    }
    rp.[:u256 32*4+256*i] = r1;
    rp.[:u256 32*5+256*i] = r3;
    rp.[:u256 32*6+256*i] = r5;
    rp.[:u256 32*7+256*i] = r7;
  }

  zeta0 = #VPBROADCAST_8u32(zetasp.[:u32 784]);
  zeta1 = #VPBROADCAST_8u32(zetasp.[:u32 788]);

  for i = 0 to 2
  {
    if (i == 0) {
      r7 = r6;
      r6 = r4;
      r5 = r2;
      r4 = r0;
    } else {
      r4 = rp.[:u256 32*8+128*i];
      r5 = rp.[:u256 32*9+128*i];
      r6 = rp.[:u256 32*10+128*i];
      r7 = rp.[:u256 32*11+128*i];
    }
    r0 = rp.[:u256 32*0+128*i];
    r1 = rp.[:u256 32*1+128*i];
    r2 = rp.[:u256 32*2+128*i];
    r3 = rp.[:u256 32*3+128*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    flox16 = jflox16[:u256 0];
    fhix16 = jfhix16[:u256 0];

    rp.[:u256 32*8+128*i]  = r4;
    rp.[:u256 32*9+128*i]  = r5;
    rp.[:u256 32*10+128*i] = r6;
    rp.[:u256 32*11+128*i] = r7;

    r0 = __fqmulprecomp16x(r0, flox16, fhix16, qx16);
    r1 = __fqmulprecomp16x(r1, flox16, fhix16, qx16);
    r2 = __fqmulprecomp16x(r2, flox16, fhix16, qx16);
    r3 = __fqmulprecomp16x(r3, flox16, fhix16, qx16);

    rp.[:u256 32*0+128*i] = r0;
    rp.[:u256 32*1+128*i] = r1;
    rp.[:u256 32*2+128*i] = r2;
    rp.[:u256 32*3+128*i] = r3;
  }

  return rp;
}

/* __butterfly64x: computes 64 butterflies in parallel.
    Inputs which come in as 2(h vs l) x 64 = 2 x 4(0..3) x 16(simd)
    16-bit words. The result is

       xl[j][i] = xl.[i]+z.[i]*xr.[i]
       xh[j][i] = xl.[i]-z.[i]*xr.[i]

    zs come represented as zl and zh, where zh contains zl*qinv
    precomputed for montgomery multiplication.
    See contract for __fqmulprecomp16x in reduce.jinc for a
    a more detailed explanation of input ranges and output ranges. */

inline
fn __butterfly64x(reg u256 rl0 rl1 rl2 rl3 rh0 rh1 rh2 rh3 zl0 zl1 zh0 zh1 qx16)
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7;

  t0 = #VPMULL_16u16(zl0, rh0);
  t1 = #VPMULH_16u16(zh0, rh0);
  t2 = #VPMULL_16u16(zl0, rh1);
  t3 = #VPMULH_16u16(zh0, rh1);
  t4 = #VPMULL_16u16(zl1, rh2);
  t5 = #VPMULH_16u16(zh1, rh2);
  t6 = #VPMULL_16u16(zl1, rh3);
  t7 = #VPMULH_16u16(zh1, rh3);

  t0 = #VPMULH_16u16(t0, qx16);
  t2 = #VPMULH_16u16(t2, qx16);
  t4 = #VPMULH_16u16(t4, qx16);
  t6 = #VPMULH_16u16(t6, qx16);

  rh1 = #VPSUB_16u16(rl1, t3);
  rl1 = #VPADD_16u16(t3, rl1);
  rh0 = #VPSUB_16u16(rl0, t1);
  rl0 = #VPADD_16u16(t1, rl0);
  rh3 = #VPSUB_16u16(rl3, t7);
  rl3 = #VPADD_16u16(t7, rl3);
  rh2 = #VPSUB_16u16(rl2, t5);
  rl2 = #VPADD_16u16(t5, rl2);

  rh0 = #VPADD_16u16(t0, rh0);
  rl0 = #VPSUB_16u16(rl0, t0);
  rh1 = #VPADD_16u16(t2, rh1);
  rl1 = #VPSUB_16u16(rl1, t2);
  rh2 = #VPADD_16u16(t4, rh2);
  rl2 = #VPSUB_16u16(rl2, t4);
  rh3 = #VPADD_16u16(t6, rh3);
  rl3 = #VPSUB_16u16(rl3, t6);

  return rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3;
}

/*
_poly_ntt computes the NTT over an input polynomial
that is given with coefficients in logical order, and produces
the output with coefficients in AVX order.

Input range accepted is [-2q,2q)
Output range is (-2q,2q)

*/

fn _poly_ntt(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u256 zeta0 zeta1 zeta2 zeta3 r0 r1 r2 r3 r4 r5 r6 r7 qx16 vx16;
  reg ptr u16[400] zetasp;
  inline int i;

  zetasp = jzetas_exp;
  qx16 = jqx16[:u256 0];

  zeta0 = #VPBROADCAST_8u32(zetasp[:u32 0]);
  zeta1 = #VPBROADCAST_8u32(zetasp[:u32 1]);

  r0 = rp.[:u256 32*0];
  r1 = rp.[:u256 32*1];
  r2 = rp.[:u256 32*2];
  r3 = rp.[:u256 32*3];
  r4 = rp.[:u256 32*8];
  r5 = rp.[:u256 32*9];
  r6 = rp.[:u256 32*10];
  r7 = rp.[:u256 32*11];

  r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

  rp.[:u256 32*0] = r0;
  rp.[:u256 32*1] = r1;
  rp.[:u256 32*2] = r2;
  rp.[:u256 32*3] = r3;
  rp.[:u256 32*8] = r4;
  rp.[:u256 32*9] = r5;
  rp.[:u256 32*10] = r6;
  rp.[:u256 32*11] = r7;

  r0 = rp.[:u256 32*4];
  r1 = rp.[:u256 32*5];
  r2 = rp.[:u256 32*6];
  r3 = rp.[:u256 32*7];
  r4 = rp.[:u256 32*12];
  r5 = rp.[:u256 32*13];
  r6 = rp.[:u256 32*14];
  r7 = rp.[:u256 32*15];

  r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

  rp.[:u256 32*12] = r4;
  rp.[:u256 32*13] = r5;
  rp.[:u256 32*14] = r6;
  rp.[:u256 32*15] = r7;

  for i = 0 to 2 {

    // level 1
    zeta0 = #VPBROADCAST_8u32(zetasp.[:u32 8 + 392*i]);
    zeta1 = #VPBROADCAST_8u32(zetasp.[:u32 12 + 392*i]);

    if ( i == 0) {
      r4 = r0;
      r5 = r1;
      r6 = r2;
      r7 = r3;
    } else {
      r4 = rp.[:u256 32*4+256*i];
      r5 = rp.[:u256 32*5+256*i];
      r6 = rp.[:u256 32*6+256*i];
      r7 = rp.[:u256 32*7+256*i];
    }
    r0 = rp.[:u256 32*0+256*i];
    r1 = rp.[:u256 32*1+256*i];
    r2 = rp.[:u256 32*2+256*i];
    r3 = rp.[:u256 32*3+256*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 2
    zeta0 = zetasp.[:u256 16 + 392*i];
    zeta1 = zetasp.[:u256 48 + 392*i];

    r0, r4 = __shuffle8(r0, r4);
    r1, r5 = __shuffle8(r1, r5);
    r2, r6 = __shuffle8(r2, r6);
    r3, r7 = __shuffle8(r3, r7);

    r0, r4, r1, r5, r2, r6, r3, r7 = __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 3
    zeta0 = zetasp.[:u256 80 + 392*i];
    zeta1 = zetasp.[:u256 112 + 392*i];

    r0, r2 = __shuffle4(r0, r2);
    r4, r6 = __shuffle4(r4, r6);
    r1, r3 = __shuffle4(r1, r3);
    r5, r7 = __shuffle4(r5, r7);

    r0, r2, r4, r6, r1, r3, r5, r7 = __butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 4
    zeta0 = zetasp.[:u256 144 + 392*i];
    zeta1 = zetasp.[:u256 176 + 392*i];

    r0, r1 = __shuffle2(r0, r1);
    r2, r3 = __shuffle2(r2, r3);
    r4, r5 = __shuffle2(r4, r5);
    r6, r7 = __shuffle2(r6, r7);

    r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 5
    zeta0 = zetasp.[:u256 208 + 392*i];
    zeta1 = zetasp.[:u256 240 + 392*i];

    r0, r4 = __shuffle1(r0, r4);
    r1, r5 = __shuffle1(r1, r5);
    r2, r6 = __shuffle1(r2, r6);
    r3, r7 = __shuffle1(r3, r7);

    r0, r4, r1, r5, r2, r6, r3, r7 = __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 6
    zeta0 = zetasp.[:u256 272 + 392*i];
    zeta2 = zetasp.[:u256 304 + 392*i];
    zeta1 = zetasp.[:u256 336 + 392*i];
    zeta3 = zetasp.[:u256 368 + 392*i];

    r0, r4, r2, r6, r1, r5, r3, r7 = __butterfly64x(r0, r4, r2, r6, r1, r5, r3, r7, zeta0, zeta1, zeta2, zeta3, qx16);

    vx16 = jvx16[:u256 0];

    r0 = __red16x(r0, qx16, vx16);
    r4 = __red16x(r4, qx16, vx16);
    r2 = __red16x(r2, qx16, vx16);
    r6 = __red16x(r6, qx16, vx16);
    r1 = __red16x(r1, qx16, vx16);
    r5 = __red16x(r5, qx16, vx16);
    r3 = __red16x(r3, qx16, vx16);
    r7 = __red16x(r7, qx16, vx16);

    rp.[:u256 32*0+256*i] = r0;
    rp.[:u256 32*1+256*i] = r4;
    rp.[:u256 32*2+256*i] = r1;
    rp.[:u256 32*3+256*i] = r5;
    rp.[:u256 32*4+256*i] = r2;
    rp.[:u256 32*5+256*i] = r6;
    rp.[:u256 32*6+256*i] = r3;
    rp.[:u256 32*7+256*i] = r7;
  }

  return rp;
}

/*
__poly_reduce: reduce from arbitrary signed range [-2^15..2^15) to range [0..2q)
          usint Barrett reduction

requires: nothing

ensures:
  - memory unchanged
  - every coefficient of the output is in the range [0..q)
  - every coefficient of the output encodes the same element in Zq
    as the corresponding input coefficient
*/
inline
fn __poly_reduce(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 r qx16 vx16;

  qx16 = jqx16[:u256 0];
  vx16 = jvx16[:u256 0];

  for i = 0 to 16
  {
    r = rp.[:u256 32*i];
    r = __red16x(r, qx16, vx16);
    rp.[:u256 32*i] = r;
  }
  return rp;
}

/*
_poly_sub: subtract arrays pointwise

requires:
  - elements in ap input to be in the range [-q*A...q*A) for 0<=A<=6
  - elements in bp input to be in the range [-q*B...q*B) for 0<=B<=3
ensures:
  - modifies only the stack region pointed to by rp
  - each entry in rp represents the Zq element that results from subtracting
    the Zq elements represented by the corresponding rp and bp entries on
     input
  - each entry in rp is in the range [-q*(A+B)...q*(A+B))
*/


fn _poly_sub(reg ptr u16[MLKEM_N] rp ap bp) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 a;
  reg u256 b;
  reg u256 r;

  for i = 0 to 16 {
    a = ap.[:u256 32*i];
    b = bp.[:u256 32*i];
    r = #VPSUB_16u16(a, b);
    rp.[:u256 32*i] = r;
  }

  return rp;
}

/*
_poly_tobytes: serializes a polynomial (given in AVX2 order) using 12 bits
    per coefficient and packs in byte array stored in rp

requires: input coefficients to be in the range [0..2q)
      rp to point to 384 byte valid region

ensures:
    only 384 bytes pointed by rp are modified in memory
    these bytes encode the serialized polynomial

    it returns the input polynomial reduced to the range [0..q)

*/

fn _poly_tobytes(reg u64 rp, reg ptr u16[MLKEM_N] a) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7 tt ttt;

  a = _poly_csubq(a);

  for i = 0 to 2
  {
    t0 = a[:u256 8*i];
    t1 = a[:u256 8*i + 1];
    t2 = a[:u256 8*i + 2];
    t3 = a[:u256 8*i + 3];
    t4 = a[:u256 8*i + 4];
    t5 = a[:u256 8*i + 5];
    t6 = a[:u256 8*i + 6];
    t7 = a[:u256 8*i + 7];

    tt = #VPSLL_16u16(t1, 12);
    tt |= t0;

    t0 = #VPSRL_16u16(t1, 4);
    t1 = #VPSLL_16u16(t2, 8);
    t0 |= t1;

    t1 = #VPSRL_16u16(t2, 8);
    t2 = #VPSLL_16u16(t3, 4);
    t1 |= t2;

    t2 = #VPSLL_16u16(t5, 12);
    t2 |= t4;

    t3 = #VPSRL_16u16(t5, 4);
    t4 = #VPSLL_16u16(t6, 8);
    t3 |= t4;

    t4 = #VPSRL_16u16(t6, 8);
    t5 = #VPSLL_16u16(t7, 4);
    t4 |= t5;

    ttt, t0 = __shuffle1(tt, t0);
    tt, t2 = __shuffle1(t1, t2);
    t1, t4 = __shuffle1(t3, t4);

    t3, tt= __shuffle2(ttt, tt);
    ttt, t0 = __shuffle2(t1, t0);
    t1, t4 = __shuffle2(t2, t4);

    t2, ttt = __shuffle4(t3, ttt);
    t3, tt = __shuffle4(t1, tt);
    t1, t4 = __shuffle4(t0, t4);

    t0, t3 = __shuffle8(t2, t3);
    t2, ttt = __shuffle8(t1, ttt);
    t1, t4 = __shuffle8(tt, t4);

    [:u256 rp + 192*i] = t0;
    [:u256 rp + 192*i + 32] = t2;
    [:u256 rp + 192*i + 64] = t1;
    [:u256 rp + 192*i + 96] = t3;
    [:u256 rp + 192*i + 128] = ttt;
    [:u256 rp + 192*i + 160] = t4;
  }

  return a;
}


/*
_poly_tomsg_1: compresses a polynomial using 1 bit per coefficient and packs
                in byte array rp

requires: input coefficients to be in the range [0..2q)
      rp to point to 32 byte valid region


ensures:
    32 bytes in rp are the compressed polynomial

    it also returns the input polynomial reduced to the range [0..q)

*/

fn _poly_tomsg_1(reg ptr u8[MLKEM_INDCPA_MSGBYTES] rp, reg ptr u16[MLKEM_N] a) -> reg ptr u8[MLKEM_INDCPA_MSGBYTES], reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 f0 f1 g0 g1 hq hhq;
  reg ptr u16[16] px16;
  reg u32 c;

  a = _poly_csubq(a);

  px16 = hqx16_m1;
  hq = px16[:u256 0];

  px16 = hhqx16;
  hhq = px16[:u256 0];

  for i = 0 to MLKEM_N/32
  {
    f0 = a[:u256 2*i];
    f1 = a[:u256 2*i + 1];
    f0 = #VPSUB_16u16(hq, f0);
    f1 = #VPSUB_16u16(hq, f1);
    g0 = #VPSRA_16u16(f0, 15);
    g1 = #VPSRA_16u16(f1, 15);
    f0 = #VPXOR_256(f0, g0);
    f1 = #VPXOR_256(f1, g1);
    f0 = #VPSUB_16u16(f0, hhq);
    f1 = #VPSUB_16u16(f1, hhq);
    f0 = #VPACKSS_16u16(f0, f1);
    f0 = #VPERMQ(f0, 0xD8);
    c = #VPMOVMSKB_u256u32(f0);
    rp[:u32 i] = c;
  }
  return rp, a;
}
